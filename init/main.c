/*
 *  linux/init/main.c
 *
 *  Copyright (C) 1991, 1992  Linus Torvalds
 *
 *  GK 2/5/95  -  Changed to support mounting root fs via NFS
 *  Added initrd & change_root: Werner Almesberger & Hans Lermen, Feb '96
 *  Moan early if gcc is old, avoiding bogus kernels - Paul Gortmaker, May '96
 *  Simplified starting of init:  Michael A. Griffith <grif@acm.org>
 */

#define DEBUG		/* Enable initcall_debug */




#include "types2.h"


#define IS_ENABLED(option) \
	(config_enabled(option) || config_enabled(option##_MODULE))

extern int strcmp_tlx(const char *, const char *);
extern int strncmp_tlx(const char *, const char *, __kernel_size_t);
extern __kernel_size_t strlen_tlx(const char *);
extern void *memcpy_tlx(void *, const void *, __kernel_size_t);
extern void *memmove_tlx (void *, const void *, __kernel_size_t);
extern void *memset_tlx(void *, int, __kernel_size_t);
extern int memcmp_tlx(const void *, const void *, size_t);


size_t strlen_tlx_tlx(const char *s)
{
	const char *sc;

	for (sc = s; *sc != '\0'; ++sc)
		/* nothing */;
	return sc - s;
}

size_t strlcpy_tlx(char *dest, const char *src, size_t size)
{
	size_t ret = strlen_tlx_tlx(src);

	if (size) {
		size_t len = (ret >= size) ? size - 1 : ret;
		memcpy_tlx(dest, src, len);
		dest[len] = '\0';
	}
	return ret;
}

char *strcpy_tlx(char *dest, const char *src)
{
	char *tmp = dest;

	while ((*dest++ = *src++) != '\0')
		/* nothing */;
	return tmp;
}

#define _C      0x08    /* cntrl */
#define _L      0x02    /* lower */
#define _D      0x04    /* digit */
#define _U      0x01    /* upper */
#define _P      0x10    /* punct */
#define _S      0x20    /* white space (space/lf/tab) */
#define _X      0x40    /* hex digit */
#define _SP     0x80    /* hard space (0x20) */


const unsigned char _ctype_tlx[] = {
 _C,_C,_C,_C,_C,_C,_C,_C,                                /* 0-7 */
 _C,_C|_S,_C|_S,_C|_S,_C|_S,_C|_S,_C,_C,                 /* 8-15 */
 _C,_C,_C,_C,_C,_C,_C,_C,                                /* 16-23 */
 _C,_C,_C,_C,_C,_C,_C,_C,                                /* 24-31 */
 _S|_SP,_P,_P,_P,_P,_P,_P,_P,                            /* 32-39 */
 _P,_P,_P,_P,_P,_P,_P,_P,                                /* 40-47 */
 _D,_D,_D,_D,_D,_D,_D,_D,                                /* 48-55 */
 _D,_D,_P,_P,_P,_P,_P,_P,                                /* 56-63 */
 _P,_U|_X,_U|_X,_U|_X,_U|_X,_U|_X,_U|_X,_U,              /* 64-71 */
 _U,_U,_U,_U,_U,_U,_U,_U,                                /* 72-79 */
 _U,_U,_U,_U,_U,_U,_U,_U,                                /* 80-87 */
 _U,_U,_U,_P,_P,_P,_P,_P,                                /* 88-95 */
 _P,_L|_X,_L|_X,_L|_X,_L|_X,_L|_X,_L|_X,_L,              /* 96-103 */
 _L,_L,_L,_L,_L,_L,_L,_L,                                /* 104-111 */
 _L,_L,_L,_L,_L,_L,_L,_L,                                /* 112-119 */
 _L,_L,_L,_P,_P,_P,_P,_C,                                /* 120-127 */
 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,                        /* 128-143 */
 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,                        /* 144-159 */
 _S|_SP,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,    /* 160-175 */
 _P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,_P,        /* 176-191 */
 _U,_U,_U,_U,_U,_U,_U,_U,_U,_U,_U,_U,_U,_U,_U,_U,        /* 192-207 */
 _U,_U,_U,_U,_U,_U,_U,_P,_U,_U,_U,_U,_U,_U,_U,_L,        /* 208-223 */
 _L,_L,_L,_L,_L,_L,_L,_L,_L,_L,_L,_L,_L,_L,_L,_L,        /* 224-239 */
 _L,_L,_L,_L,_L,_L,_L,_P,_L,_L,_L,_L,_L,_L,_L,_L};       /* 240-255 */


#define __ismask(x) (_ctype_tlx[(int)(unsigned char)(x)])
#define isupper(c)	((__ismask(c)&(_U)) != 0)

static inline unsigned char __tolower(unsigned char c)
{
	if (isupper(c))
		c -= 'A'-'a';
	return c;
}

int strcasecmp_tlx(const char *s1, const char *s2)
{
	int c1, c2;

	do {
		c1 = __tolower(*s1++);
		c2 = __tolower(*s2++);
	} while (c1 == c2 && c1 != 0);
	return c1 - c2;
}

void *kmalloc_tlx(size_t size, gfp_t flags);
char *kstrdup_tlx(const char *s, gfp_t gfp)
{
	size_t len;
	char *buf;

	if (!s)
		return NULL;

	len = strlen_tlx_tlx(s) + 1;
	buf = kmalloc_tlx(len, gfp);
	if (buf)
		memcpy_tlx(buf, s, len);
	return buf;
}

# define __chk_user_ptr(x) (void)0
# define __chk_io_ptr(x) (void)0

#define NR_LOCKDEP_CACHING_CLASSES      2



typedef struct {
				uid_t val;
} kuid_t_tlx;


typedef atomic64_t atomic_long_t;

struct user_struct {
	atomic_t __count;	/* reference count */
	atomic_t processes;	/* How many processes does this user have? */
	atomic_t sigpending;	/* How many pending signals does this user have? */
#ifdef CONFIG_INOTIFY_USER
	atomic_t inotify_watches; /* How many inotify watches does this user have? */
	atomic_t inotify_devs;	/* How many inotify devs does this user have opened? */
#endif
#ifdef CONFIG_FANOTIFY
	atomic_t fanotify_listeners;
#endif
#ifdef CONFIG_EPOLL
	atomic_long_t epoll_watches; /* The number of file descriptors currently watched */
#endif
#ifdef CONFIG_POSIX_MQUEUE
	/* protected by mq_lock	*/
	unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
#endif
	unsigned long locked_shm; /* How many pages of mlocked shm ? */

#ifdef CONFIG_KEYS
	struct key *uid_keyring;	/* UID specific keyring */
	struct key *session_keyring;	/* UID's default session keyring */
#endif

	/* Hash table maintenance information */
	struct hlist_node uidhash_node;
	kuid_t_tlx uid;

#ifdef CONFIG_PERF_EVENTS
	atomic_long_t locked_vm;
#endif
};

enum perf_event_task_context {
	perf_invalid_context = -1,
	perf_hw_context = 0,
	perf_sw_context,
	perf_nr_task_contexts,
};



union ktime_tlx {
				s64     tv64;
#if BITS_PER_LONG != 64 && !defined(CONFIG_KTIME_SCALAR)
				struct {
# ifdef __BIG_ENDIAN
				s32     sec, nsec;
# else
				s32     nsec, sec;
# endif
				} tv;
#endif
};

typedef union ktime_tlx ktime_t_tlx;            /* Kill this */

struct rb_node_tlx {
			unsigned long  __rb_parent_color;
			struct rb_node_tlx *rb_right;
			struct rb_node_tlx *rb_left;
} __attribute__((aligned(sizeof(long))));

struct timerqueue_node_tlx {
				struct rb_node_tlx node;
				ktime_t_tlx expires;
};







struct lockdep_map_tlx {
				struct lock_class_key           *key;
				struct lock_class               *class_cache[NR_LOCKDEP_CACHING_CLASSES];
				const char                      *name;
};



struct load_weight {
				unsigned long weight;
				u32 inv_weight;
};



struct sched_avg {
				u32 runnable_avg_sum, runnable_avg_period;
				u64 last_runnable_update;
				s64 decay_count;
				unsigned long load_avg_contrib;
};


struct sched_entity {
	struct load_weight	load;		/* for load-balancing */
	struct rb_node_tlx		run_node;
	struct list_head	group_node;
	unsigned int		on_rq;

	u64			exec_start;
	u64			sum_exec_runtime;
	u64			vruntime;
	u64			prev_sum_exec_runtime;

	u64			nr_migrations;

#ifdef CONFIG_SCHEDSTATS
	struct sched_statistics statistics;
#endif

#ifdef CONFIG_FAIR_GROUP_SCHED
	int			depth;
	struct sched_entity	*parent;
	/* rq on which this entity is (to be) queued: */
	struct cfs_rq		*cfs_rq;
	/* rq "owned" by this entity/group: */
	struct cfs_rq		*my_q;
#endif

#ifdef CONFIG_SMP
	/* Per-entity load-tracking */
	struct sched_avg	avg;
#endif
};

struct sched_rt_entity {
	struct list_head run_list;
	unsigned long timeout;
	unsigned long watchdog_stamp;
	unsigned int time_slice;

	struct sched_rt_entity *back;
#ifdef CONFIG_RT_GROUP_SCHED
	struct sched_rt_entity	*parent;
	/* rq on which this entity is (to be) queued: */
	struct rt_rq		*rt_rq;
	/* rq "owned" by this entity/group: */
	struct rt_rq		*my_q;
#endif
};





enum perf_event_context_type {
				task_context,
				cpu_context,
};

typedef struct {
#ifdef __AARCH64EB__
					u16 next;
					u16 owner;
#else
					u16 owner;
					u16 next;
#endif
} __aligned(4) arch_spinlock_t_tlx;


typedef struct raw_spinlock_tlx {
				arch_spinlock_t_tlx raw_lock;
#ifdef CONFIG_GENERIC_LOCKBREAK
				unsigned int break_lock;
#endif
				struct lockdep_map_tlx dep_map;
} raw_spinlock_t_tlx;


typedef struct spinlock_tlx {
										struct raw_spinlock_tlx rlock;
} spinlock_t_tlx;



typedef unsigned long __nocast cputime_t;

struct cpu_itimer {
	cputime_t expires;
	cputime_t incr;
	u32 error;
	u32 incr_error;
};

struct task_cputime {
				cputime_t utime;
				cputime_t stime;
				unsigned long long sum_exec_runtime;
};

struct thread_group_cputimer {
				struct task_cputime cputime;
				int running;
				raw_spinlock_t_tlx lock;
};

struct cputime {
				cputime_t utime;
				cputime_t stime;
};

struct __wait_queue_head_tlx {
				spinlock_t_tlx              lock;
				struct list_head        task_list;
};


typedef struct __wait_queue_head_tlx wait_queue_head_t_tlx;
#define _NSIG           64
#define _NSIG_BPW       __BITS_PER_LONG
#define _NSIG_WORDS     (_NSIG / _NSIG_BPW)

typedef void __signalfn_t(int);
typedef __signalfn_t __user *__sighandler_t;






#define VMACACHE_BITS 2
#define VMACACHE_SIZE (1U << VMACACHE_BITS)
#define TASK_COMM_LEN 16
#define BUILD_BUG() (0)
#define TASK_WAKEKILL           128
#define TASK_INTERRUPTIBLE      1
#define TIF_SIGPENDING          0

struct sched_param {
	int sched_priority;
};



#define _LINUX_CAPABILITY_U32S_3     2
#define _KERNEL_CAPABILITY_U32S    _LINUX_CAPABILITY_U32S_3


typedef struct kernel_cap_struct {
				__u32 cap[_KERNEL_CAPABILITY_U32S];
} kernel_cap_t;



#define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000)
#define PID_MAX_LIMIT (CONFIG_BASE_SMALL ? PAGE_SIZE * 8 : \
					(sizeof(long) > 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT))


#define BIT(nr)			(1UL << (nr))
#define BIT_ULL(nr)		(1ULL << (nr))
#define BIT_MASK(nr)		(1UL << ((nr) % BITS_PER_LONG))
#define BIT_WORD(nr)		((nr) / BITS_PER_LONG)
#define dsb(opt)	asm volatile("dsb " #opt : : : "memory")

#ifndef __LITTLE_ENDIAN
#define __LITTLE_ENDIAN 1234
#endif
#ifndef __LITTLE_ENDIAN_BITFIELD
#define __LITTLE_ENDIAN_BITFIELD
#endif


#define ___constant_swab32(x) ((__u32)(				\
	(((__u32)(x) & (__u32)0x000000ffUL) << 24) |		\
	(((__u32)(x) & (__u32)0x0000ff00UL) <<  8) |		\
	(((__u32)(x) & (__u32)0x00ff0000UL) >>  8) |		\
	(((__u32)(x) & (__u32)0xff000000UL) >> 24)))

static inline __attribute_const__ __u32 __fswab32(__u32 val)
{
#ifdef __HAVE_BUILTIN_BSWAP32__
	return __builtin_bswap32(val);
#elif defined(__arch_swab32)
	return __arch_swab32(val);
#else
	return ___constant_swab32(val);
#endif
}


#define __swab32(x)				\
	(__builtin_constant_p((__u32)(x)) ?	\
	___constant_swab32(x) :			\
	__fswab32(x))


static inline __u32 __swab32p(const __u32 *p)
{
#ifdef __arch_swab32p
	return __arch_swab32p(p);
#else
	return __swab32(*p);
#endif
}



typedef int (*initcall_t)(void);
#define ICACHE_POLICY_AIVIVT    1
struct obs_kernel_param {
				const char *str;
				int (*setup_func)(char *);
				int early;
};

#define L1_CACHE_SHIFT          6
#define L1_CACHE_BYTES          (1 << L1_CACHE_SHIFT)
#define SMP_CACHE_BYTES L1_CACHE_BYTES
#define INTERNODE_CACHE_SHIFT L1_CACHE_SHIFT
#define ____cacheline_aligned __attribute__((__aligned__(SMP_CACHE_BYTES)))
#define ____cacheline_aligned_in_smp ____cacheline_aligned
#define __cacheline_aligned                                     \
	__attribute__((__aligned__(SMP_CACHE_BYTES),                  \
									__section__(".data..cacheline_aligned")))
#define __cacheline_aligned_in_smp __cacheline_aligned
#define ____cacheline_internodealigned_in_smp \
				__attribute__((__aligned__(1 << (INTERNODE_CACHE_SHIFT))))
#define CTR_L1IP_SHIFT          14
#define CTR_L1IP_MASK           3
#define read_cpuid(reg) ({                                              \
				u64 __val;                                                      \
				asm("mrs        %0, " #reg : "=r" (__val));                     \
				__val;                                                          \
})

#define NR_CPUS         CONFIG_NR_CPUS
#define BITS_PER_BYTE           8
#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))
#define BITS_TO_LONGS(nr)       DIV_ROUND_UP(nr, BITS_PER_BYTE * sizeof(long))

enum system_states {
			SYSTEM_BOOTING,
			SYSTEM_RUNNING,
			SYSTEM_HALT,
			SYSTEM_POWER_OFF,
			SYSTEM_RESTART,
} system_state;




struct restart_block {
	long (*fn)(struct restart_block *);
	union {
		/* For futex_wait and futex_wait_requeue_pi */
		struct {
			u32 __user *uaddr;
			u32 val;
			u32 flags;
			u32 bitset;
			u64 time;
			u32 __user *uaddr2;
		} futex;
		/* For nanosleep */
		struct {
			clockid_t clockid;
			struct timespec __user *rmtp;
#ifdef CONFIG_COMPAT
			struct compat_timespec __user *compat_rmtp;
#endif
			u64 expires;
		} nanosleep;
		/* For poll */
		struct {
			struct pollfd __user *ufds;
			int nfds;
			int has_timeout;
			unsigned long tv_sec;
			unsigned long tv_nsec;
		} poll;
	};
};



struct user_fpsimd_state {
					__uint128_t     vregs[32];
					__u32           fpsr;
					__u32           fpcr;
	};

struct user_pt_regs {
				__u64           regs[31];
				__u64           sp;
				__u64           pc;
				__u64           pstate;
};

struct pt_regs {
				union {
								struct user_pt_regs user_regs;
								struct {
												u64 regs[31];
												u64 sp;
												u64 pc;
												u64 pstate;
								};
				};
				u64 orig_x0;
				u64 syscallno;
};

/*
* Must define these before including other files, inline functions need them
*/
#define LOCK_SECTION_NAME ".text..lock."KBUILD_BASENAME

#define LOCK_SECTION_START(extra)               \
				".subsection 1\n\t"                     \
				extra                                   \
				".ifndef " LOCK_SECTION_NAME "\n\t"     \
				LOCK_SECTION_NAME ":\n\t"               \
				".endif\n"

#define LOCK_SECTION_END                        \
				".previous\n\t"

#define __lockfunc __attribute__((section(".spinlock.text")))

/*
* Pull the arch_spinlock_t and arch_rwlock_t definitions:
*/
#define TICKET_SHIFT	16

typedef struct {
#ifdef __AARCH64EB__
	u16 next;
	u16 owner;
#else
	u16 owner;
	u16 next;
#endif
} __aligned(4) arch_spinlock_t;

#define __ARCH_SPIN_LOCK_UNLOCKED	{ 0 , 0 }

typedef struct {
	volatile unsigned int lock;
} arch_rwlock_t;



struct lock_class_key {
// 55         struct lockdep_subclass_key     subkeys[MAX_LOCKDEP_SUBCLASSES];
};

struct lockdep_map {
				struct lock_class_key           *key;
				struct lock_class               *class_cache[NR_LOCKDEP_CACHING_CLASSES];
				const char                      *name;
};

typedef struct raw_spinlock {
	arch_spinlock_t raw_lock;
#ifdef CONFIG_GENERIC_LOCKBREAK
	unsigned int break_lock;
#endif
#ifdef CONFIG_DEBUG_SPINLOCK
	unsigned int magic, owner_cpu;
	void *owner;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map dep_map;
#endif
} raw_spinlock_t;

#define SPINLOCK_MAGIC		0xdead4ead

#define SPINLOCK_OWNER_INIT	((void *)-1L)

#ifdef CONFIG_DEBUG_LOCK_ALLOC
# define SPIN_DEP_MAP_INIT(lockname)	.dep_map = { .name = #lockname }
#else
# define SPIN_DEP_MAP_INIT(lockname)
#endif

#ifdef CONFIG_DEBUG_SPINLOCK
# define SPIN_DEBUG_INIT(lockname)		\
	.magic = SPINLOCK_MAGIC,		\
	.owner_cpu = -1,			\
	.owner = SPINLOCK_OWNER_INIT,
#else
# define SPIN_DEBUG_INIT(lockname)
#endif

#define __RAW_SPIN_LOCK_INITIALIZER(lockname)	\
	{					\
	.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,	\
	SPIN_DEBUG_INIT(lockname)		\
	SPIN_DEP_MAP_INIT(lockname) }

#define __RAW_SPIN_LOCK_UNLOCKED(lockname)	\
	(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)

#define DEFINE_RAW_SPINLOCK(x)	raw_spinlock_t x = __RAW_SPIN_LOCK_UNLOCKED(x)

typedef struct spinlock {
	union {
		struct raw_spinlock rlock;

#ifdef CONFIG_DEBUG_LOCK_ALLOC
# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
		struct {
			u8 __padding[LOCK_PADSIZE];
			struct lockdep_map dep_map;
		};
#endif
	};
} spinlock_t;

#define __SPIN_LOCK_INITIALIZER(lockname) \
	{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }

#define __SPIN_LOCK_UNLOCKED(lockname) \
	(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)

#define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)

typedef struct {
	arch_rwlock_t raw_lock;
#ifdef CONFIG_GENERIC_LOCKBREAK
	unsigned int break_lock;
#endif
#ifdef CONFIG_DEBUG_SPINLOCK
	unsigned int magic, owner_cpu;
	void *owner;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map dep_map;
#endif
} rwlock_t;




struct cpu_context {
				unsigned long x19;
				unsigned long x20;
				unsigned long x21;
				unsigned long x22;
				unsigned long x23;
				unsigned long x24;
				unsigned long x25;
				unsigned long x26;
				unsigned long x27;
				unsigned long x28;
				unsigned long fp;
				unsigned long sp;
				unsigned long pc;
};



struct fpsimd_state {
				union {
								struct user_fpsimd_state user_fpsimd;
								struct {
												__uint128_t vregs[32];
												u32 fpsr;
												u32 fpcr;
								};
				};
				/* the id of the last cpu to have restored this state */
				unsigned int cpu;
};
#define ARM_MAX_BRP             16
#define ARM_MAX_WRP             16

struct debug_info {
			/* Have we suspended stepping by a debugger? */
				int                     suspended_step;
				/* Allow breakpoints and watchpoints to be disabled for this thread. */
				int                     bps_disabled;
				int                     wps_disabled;
				/* Hardware breakpoints pinned to this task. */
			struct perf_event       *hbp_break[ARM_MAX_BRP];
			struct perf_event       *hbp_watch[ARM_MAX_WRP];
	};


struct thread_struct {
				struct cpu_context      cpu_context;    /* cpu context */
				unsigned long           tp_value;
				struct fpsimd_state     fpsimd_state;
				unsigned long           fault_address;  /* fault info */
				unsigned long           fault_code;     /* ESR_EL1 value */
				struct debug_info       debug;          /* debugging */
};



typedef struct seqcount {
				unsigned sequence;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
				struct lockdep_map dep_map;
#endif
	} seqcount_t;

typedef struct {
				struct seqcount seqcount;
				spinlock_t lock;
} seqlock_t;

struct timespec {
				__kernel_time_t tv_sec;                 /* seconds */
				long            tv_nsec;                /* nanoseconds */
};




typedef unsigned long mm_segment_t;

#define AT_VECTOR_SIZE_BASE 20 /* NEW_AUX_ENT entries in auxiliary table */
#define AT_VECTOR_SIZE_ARCH 0
#define AT_VECTOR_SIZE (2*(AT_VECTOR_SIZE_ARCH + AT_VECTOR_SIZE_BASE + 1))


union ktime {
				s64     tv64;
#if BITS_PER_LONG != 64 && !defined(CONFIG_KTIME_SCALAR)
				struct {
# ifdef __BIG_ENDIAN
				s32     sec, nsec;
# else
				s32     nsec, sec;
# endif
				} tv;
#endif
};


typedef union ktime ktime_t;            /* Kill this */

struct timer_list {
				struct list_head entry;
				unsigned long expires;
				struct tvec_base *base;

				void (*function)(unsigned long);
				unsigned long data;

				int slack;

#ifdef CONFIG_TIMER_STATS
				int start_pid;
				void *start_site;
				char start_comm[16];
#endif
#ifdef CONFIG_LOCKDEP
				struct lockdep_map lockdep_map;
#endif
};


typedef u64 pmdval_t;
typedef u64 pteval_t;
typedef u64 pgdval_t;


typedef pteval_t pte_t;
typedef pmdval_t pmd_t;
typedef pgdval_t pgd_t;
typedef struct { pgd_t pgd; } pud_t;
typedef struct page *pgtable_t;
typedef pteval_t pgprot_t;
#define pte_val(x)      (x)
#define pmd_val(x)      (x)
#define pgd_val(x)      (x)

#define __pmd(x)        ((pmd_t) { (x) } )
#define __pte(x)        ((pte_t) { (x) } )
#define __pgd(x)        ((pgd_t) { (x) } )


#define PTE_USER                (_AT(pteval_t, 1) << 6)         /* AP[1] */
#define PTE_RDONLY              (_AT(pteval_t, 1) << 7)         /* AP[2] */
#define PTE_UXN                 (_AT(pteval_t, 1) << 54)        /* User XN */
#define PTE_AF                  (_AT(pteval_t, 1) << 10)        /* Access Flag */
#define PTE_VALID		(_AT(pteval_t, 1) << 0)
#define PTE_FILE		(_AT(pteval_t, 1) << 2)	/* only when !pte_present() */
#define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
#define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
#define PTE_WRITE		(_AT(pteval_t, 1) << 57)
#define PTE_PROT_NONE		(_AT(pteval_t, 1) << 58) /* only when !PTE_VALID */

#define pte_valid_user(pte) \
	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
#define pte_special(pte)	(!!(pte_val(pte) & PTE_SPECIAL))
#define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
#define pte_dirty(pte)		(!!(pte_val(pte) & PTE_DIRTY))
#define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
#define pte_young(pte)		(!!(pte_val(pte) & PTE_AF))
#define pmd_young(pmd)		pte_young(pmd_pte(pmd))

static inline pte_t pmd_pte(pmd_t pmd)
{
	return __pte(pmd_val(pmd));
}

static inline pmd_t pte_pmd(pte_t pte)
{
	return __pmd(pte_val(pte));
}


static inline void set_pte(pte_t *ptep, pte_t pte)
{
	*ptep = pte;
}

static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
{
	*pmdp = pmd;
	dsb(ishst);
}




static inline pte_t pte_mkold(pte_t pte)
{
	pte_val(pte) &= ~PTE_AF;
	return pte;
}

#define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
#define pmd_mkold(pmd)		pte_pmd(pte_mkold(pmd_pte(pmd)))
#define pte_clear(mm,addr,ptep)	set_pte(ptep, __pte(0))
static inline void pmd_clear(pmd_t *pmdp)
{
	set_pmd(pmdp, __pmd(0));
}

static inline pte_t pte_wrprotect(pte_t pte)
{
	pte_val(pte) &= ~PTE_WRITE;
	return pte;
}

#define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
#define pud_none(pud)		(!pud_val(pud))
#define pud_bad(pud)		(!(pud_val(pud) & 2))
#define pmd_none(pmd)		(!pmd_val(pmd))
#define pmd_bad(pmd)		(!(pmd_val(pmd) & 2))
#define pmd_trans_huge(pmd)	(pmd_val(pmd) && !(pmd_val(pmd) & PMD_TABLE_BIT))


struct optimistic_spin_queue {
				atomic_t tail;
};

struct rw_semaphore {
				long count;
				struct list_head wait_list;
				raw_spinlock_t wait_lock;
#ifdef CONFIG_RWSEM_SPIN_ON_OWNER
				struct optimistic_spin_queue osq; /* spinner MCS lock */
			/*
					* Write owner. Used as a speculative check to see
					* if the owner is running on the cpu.
					*/
				struct task_struct *owner;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
				struct lockdep_map      dep_map;
#endif
};


struct rb_node {
				unsigned long  __rb_parent_color;
				struct rb_node *rb_right;
				struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));

struct rb_root {
				struct rb_node *rb_node;
};


/*
struct optimistic_spin_queue {
				atomic_t tail;
};

struct rw_semaphore {
				long count;
				struct list_head wait_list;
				raw_spinlock_t wait_lock;
};
*/

struct __wait_queue_head {
				spinlock_t              lock;
				struct list_head        task_list;
};
typedef struct __wait_queue wait_queue_t;
typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
struct __wait_queue {
				unsigned int            flags;
#define WQ_FLAG_EXCLUSIVE       0x01
				void                    *private;
				wait_queue_func_t       func;
				struct list_head        task_list;
};

typedef struct __wait_queue_head wait_queue_head_t;
typedef struct cpumask { DECLARE_BITMAP(bits, NR_CPUS); } cpumask_t;

struct completion {
					unsigned int done;
				wait_queue_head_t wait;
};


typedef struct cpumask cpumask_var_t[1];

//typedef struct cpumask *cpumask_var_t;
//typedef struct cpumask { DECLARE_BITMAP(bits, NR_CPUS); } cpumask_t;


struct uprobes_state {
				struct xol_area         *xol_area;
};

typedef struct {
				unsigned int id;
				raw_spinlock_t id_lock;
				void *vdso;
} mm_context_t;


#ifndef AT_VECTOR_SIZE_ARCH
#define AT_VECTOR_SIZE_ARCH 0
#endif
#define AT_VECTOR_SIZE (2*(AT_VECTOR_SIZE_ARCH + AT_VECTOR_SIZE_BASE + 1))

struct address_space;

#define USE_SPLIT_PTE_PTLOCKS	(NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS)
#define USE_SPLIT_PMD_PTLOCKS	(USE_SPLIT_PTE_PTLOCKS && \
		IS_ENABLED(CONFIG_ARCH_ENABLE_SPLIT_PMD_PTLOCK))
#define ALLOC_SPLIT_PTLOCKS	(SPINLOCK_SIZE > BITS_PER_LONG/8)

struct page {
	/* First double word block */
	unsigned long flags;		/* Atomic flags, some possibly
					* updated asynchronously */
	union {
		struct address_space *mapping;	/* If low bit clear, points to
						* inode address_space, or NULL.
						* If page mapped as anonymous
						* memory, low bit is set, and
						* it points to anon_vma object:
						* see PAGE_MAPPING_ANON below.
						*/
		void *s_mem;			/* slab first object */
	};

	/* Second double word */
	struct {
		union {
			pgoff_t index;		/* Our offset within mapping. */
			void *freelist;		/* sl[aou]b first free object */
			bool pfmemalloc;	/* If set by the page allocator,
						* ALLOC_NO_WATERMARKS was set
						* and the low watermark was not
						* met implying that the system
						* is under some pressure. The
						* caller should try ensure
						* this page is only used to
						* free other pages.
						*/
		};

		union {
#if defined(CONFIG_HAVE_CMPXCHG_DOUBLE) && \
	defined(CONFIG_HAVE_ALIGNED_STRUCT_PAGE)
			/* Used for cmpxchg_double in slub */
			unsigned long counters;
#else
			/*
			* Keep _count separate from slub cmpxchg_double data.
			* As the rest of the double word is protected by
			* slab_lock but _count is not.
			*/
			unsigned counters;
#endif

			struct {

				union {
					/*
					* Count of ptes mapped in
					* mms, to show when page is
					* mapped & limit reverse map
					* searches.
					*
					* Used also for tail pages
					* refcounting instead of
					* _count. Tail pages cannot
					* be mapped and keeping the
					* tail page _count zero at
					* all times guarantees
					* get_page_unless_zero() will
					* never succeed on tail
					* pages.
					*/
					atomic_t _mapcount;

					struct { /* SLUB */
						unsigned inuse:16;
						unsigned objects:15;
						unsigned frozen:1;
					};
					int units;	/* SLOB */
				};
				atomic_t _count;		/* Usage count, see below. */
			};
			unsigned int active;	/* SLAB */
		};
	};

	/* Third double word block */
	union {
		struct list_head lru;	/* Pageout list, eg. active_list
					* protected by zone->lru_lock !
					* Can be used as a generic list
					* by the page owner.
					*/
		struct {		/* slub per cpu partial pages */
			struct page *next;	/* Next partial slab */
#ifdef CONFIG_64BIT
			int pages;	/* Nr of partial slabs left */
			int pobjects;	/* Approximate # of objects */
#else
			short int pages;
			short int pobjects;
#endif
		};

		struct slab *slab_page; /* slab fields */
		struct rcu_head rcu_head;	/* Used by SLAB
						* when destroying via RCU
						*/
#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && USE_SPLIT_PMD_PTLOCKS
		pgtable_t pmd_huge_pte; /* protected by page->ptl */
#endif
	};

	/* Remainder is not double word aligned */
	union {
		unsigned long private;		/* Mapping-private opaque data:
							* usually used for buffer_heads
						* if PagePrivate set; used for
						* swp_entry_t if PageSwapCache;
						* indicates order in the buddy
						* system if PG_buddy is set.
						*/
#if USE_SPLIT_PTE_PTLOCKS
#if ALLOC_SPLIT_PTLOCKS
		spinlock_t *ptl;
#else
		spinlock_t ptl;
#endif
#endif
		struct kmem_cache *slab_cache;	/* SL[AU]B: Pointer to slab */
		struct page *first_page;	/* Compound tail pages */
	};

	/*
	* On machines where all RAM is mapped into kernel address space,
	* we can simply calculate the virtual address. On machines with
	* highmem some memory is mapped into kernel virtual memory
	* dynamically, so we need a place to store that address.
	* Note that this field could be 16 bits on x86 ... ;)
	*
	* Architectures with slow multiplication can define
	* WANT_PAGE_VIRTUAL in asm/page.h
	*/
#if defined(WANT_PAGE_VIRTUAL)
	void *virtual;			/* Kernel virtual address (NULL if
						not kmapped, ie. highmem) */
#endif /* WANT_PAGE_VIRTUAL */
#ifdef CONFIG_WANT_PAGE_DEBUG_FLAGS
	unsigned long debug_flags;	/* Use atomic bitops on this */
#endif

#ifdef CONFIG_KMEMCHECK
	/*
	* kmemcheck wants to track the status of each byte in a page; this
	* is a pointer to such a status block. NULL if not tracked.
	*/
	void *shadow;
#endif

#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
	int _last_cpupid;
#endif
}
/*
* The struct page can be forced to be double word aligned so that atomic ops
* on double words work. The SLUB allocator can make use of such a feature.
*/
#ifdef CONFIG_HAVE_ALIGNED_STRUCT_PAGE
	__aligned(2 * sizeof(unsigned long))
#endif
;

struct page_frag {
	struct page *page;
#if (BITS_PER_LONG > 32) || (PAGE_SIZE >= 65536)
	__u32 offset;
	__u32 size;
#else
	__u16 offset;
	__u16 size;
#endif
};

typedef unsigned long __nocast vm_flags_t;

struct vm_area_struct {
	/* The first cache line has the info for VMA tree walking. */

	unsigned long vm_start;		/* Our start address within vm_mm. */
	unsigned long vm_end;		/* The first byte after our end address
						within vm_mm. */

	/* linked list of VM areas per task, sorted by address */
	struct vm_area_struct *vm_next, *vm_prev;

	struct rb_node vm_rb;

	/*
	* Largest free memory gap in bytes to the left of this VMA.
	* Either between this VMA and vma->vm_prev, or between one of the
	* VMAs below us in the VMA rbtree and its ->vm_prev. This helps
	* get_unmapped_area find a free area of the right size.
	*/
	unsigned long rb_subtree_gap;

	/* Second cache line starts here. */

	struct mm_struct *vm_mm;	/* The address space we belong to. */
	pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
	unsigned long vm_flags;		/* Flags, see mm.h. */

	/*
	* For areas with an address space and backing store,
	* linkage into the address_space->i_mmap interval tree, or
	* linkage of vma in the address_space->i_mmap_nonlinear list.
	*/
	union {
		struct {
			struct rb_node rb;
			unsigned long rb_subtree_last;
		} linear;
		struct list_head nonlinear;
	} shared;

	/*
	* A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
	* list, after a COW of one of the file pages.	A MAP_SHARED vma
	* can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack
	* or brk vma (with NULL file) can only be in an anon_vma list.
	*/
	struct list_head anon_vma_chain; /* Serialized by mmap_sem &
						* page_table_lock */
	struct anon_vma *anon_vma;	/* Serialized by page_table_lock */

	/* Function pointers to deal with this struct. */
	const struct vm_operations_struct *vm_ops;

	/* Information about our backing store: */
	unsigned long vm_pgoff;		/* Offset (within vm_file) in PAGE_SIZE
						units, *not* PAGE_CACHE_SIZE */
	struct file * vm_file;		/* File we map to (can be NULL). */
	void * vm_private_data;		/* was vm_pte (shared mem) */

#ifndef CONFIG_MMU
	struct vm_region *vm_region;	/* NOMMU mapping region */
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
#endif
};



enum {
	MM_FILEPAGES,
	MM_ANONPAGES,
	MM_SWAPENTS,
	NR_MM_COUNTERS
};

#if USE_SPLIT_PTE_PTLOCKS && defined(CONFIG_MMU)
#define SPLIT_RSS_COUNTING
/* per-thread cached information, */
struct task_rss_stat {
	int events;	/* for synchronization threshold */
	int count[NR_MM_COUNTERS];
};
#endif /* USE_SPLIT_PTE_PTLOCKS */

struct mm_rss_stat {
	atomic_long_t count[NR_MM_COUNTERS];
};

struct kioctx_table;
struct mm_struct {
	struct vm_area_struct *mmap;		/* list of VMAs */
	struct rb_root mm_rb;
	u32 vmacache_seqnum;                   /* per-thread vmacache */
#ifdef CONFIG_MMU
	unsigned long (*get_unmapped_area) (struct file *filp,
				unsigned long addr, unsigned long len,
				unsigned long pgoff, unsigned long flags);
#endif
	unsigned long mmap_base;		/* base of mmap area */
	unsigned long mmap_legacy_base;         /* base of mmap area in bottom-up allocations */
	unsigned long task_size;		/* size of task vm space */
	unsigned long highest_vm_end;		/* highest vma end address */
	pgd_t * pgd;
	atomic_t mm_users;			/* How many users with user space? */
	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
	atomic_long_t nr_ptes;			/* Page table pages */
	int map_count;				/* number of VMAs */

	spinlock_t page_table_lock;		/* Protects page tables and some counters */
	struct rw_semaphore mmap_sem;

	struct list_head mmlist;		/* List of maybe swapped mm's.	These are globally strung
						* together off init_mm_tlx.mmlist, and are protected
						* by mmlist_lock
						*/


	unsigned long hiwater_rss;	/* High-watermark of RSS usage */
	unsigned long hiwater_vm;	/* High-water virtual memory usage */

	unsigned long total_vm;		/* Total pages mapped */
	unsigned long locked_vm;	/* Pages that have PG_mlocked set */
	unsigned long pinned_vm;	/* Refcount permanently increased */
	unsigned long shared_vm;	/* Shared pages (files) */
	unsigned long exec_vm;		/* VM_EXEC & ~VM_WRITE */
	unsigned long stack_vm;		/* VM_GROWSUP/DOWN */
	unsigned long def_flags;
	unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;

	unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */

	/*
	* Special counters, in some configurations protected by the
	* page_table_lock, in other configurations by being atomic.
	*/
	struct mm_rss_stat rss_stat;

	struct linux_binfmt *binfmt;

	cpumask_var_t cpu_vm_mask_var;

	/* Architecture-specific MM context */
	mm_context_t context;

	unsigned long flags; /* Must use atomic bitops to access the bits */

	struct core_state *core_state; /* coredumping support */
#ifdef CONFIG_AIO
	spinlock_t			ioctx_lock;
	struct kioctx_table __rcu	*ioctx_table;
#endif
#ifdef CONFIG_MEMCG
	/*
	* "owner" points to a task that is regarded as the canonical
	* user/owner of this mm. All of the following must be true in
	* order for it to be changed:
	*
	* current == mm->owner
	* current->mm != mm
	* new_owner->mm == mm
	* new_owner->alloc_lock is held
	*/
	struct task_struct __rcu *owner;
#endif

	/* store ref to file /proc/<pid>/exe symlink points to */
	struct file *exe_file;
#ifdef CONFIG_MMU_NOTIFIER
	struct mmu_notifier_mm *mmu_notifier_mm;
#endif
#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
	pgtable_t pmd_huge_pte; /* protected by page_table_lock */
#endif
#ifdef CONFIG_CPUMASK_OFFSTACK
	struct cpumask cpumask_allocation;
#endif
#ifdef CONFIG_NUMA_BALANCING
	/*
	* numa_next_scan is the next time that the PTEs will be marked
	* pte_numa. NUMA hinting faults will gather statistics and migrate
	* pages to new nodes if necessary.
	*/
	unsigned long numa_next_scan;

	/* Restart point for scanning and setting pte_numa */
	unsigned long numa_scan_offset;

	/* numa_scan_seq prevents two threads setting pte_numa */
	int numa_scan_seq;
#endif
#if defined(CONFIG_NUMA_BALANCING) || defined(CONFIG_COMPACTION)
	/*
	* An operation with batched TLB flushing is going on. Anything that
	* can move process memory needs to flush the TLB when moving a
	* PROT_NONE or PROT_NUMA mapped page.
	*/
	bool tlb_flush_pending;
#endif
	struct uprobes_state uprobes_state;
};


typedef u64 cycle_t;



#define NODES_SHIFT     0
#define MAX_NUMNODES    (1 << NODES_SHIFT)
#define MAX_NR_ZONES 3 /* __MAX_NR_ZONES	// */

#ifdef CONFIG_ARM64_64K_PAGES
#define PAGE_SHIFT              16
#else
#define PAGE_SHIFT              12
#endif
#define PAGE_SIZE               (_AC(1,UL) << PAGE_SHIFT)
#define SECTIONS_SHIFT  (MAX_PHYSMEM_BITS - SECTION_SIZE_BITS)
#define MAX_PHYSMEM_BITS	40
#define SECTION_SIZE_BITS	30
#ifdef CONFIG_CPUMASK_OFFSTACK
#define nr_cpumask_bits nr_cpu_ids_tlx_tlx
#else
#define nr_cpumask_bits NR_CPUS
#endif

#define ZONES_SHIFT 2
#define NODES_WIDTH             0
#define TIF_NEED_RESCHED        1
#define TIMER_DEFERRABLE                0x1LU
#define NUMA_NO_NODE    (-1)



struct timerqueue_head {
				struct rb_root head;
				struct timerqueue_node *next;
};





struct plist_node {
				int                     prio;
				struct list_head        prio_list;
				struct list_head        node_list;
};


typedef struct { DECLARE_BITMAP(bits, MAX_NUMNODES); } nodemask_t;

#define nr_node_ids             1

struct llist_node {
				struct llist_node *next;
	};



struct sysv_sem {
				/* empty */
};


typedef struct {
				uid_t val;
} kuid_t;

typedef struct {
					gid_t val;
} kgid_t;


typedef void __restorefn_t(void);
typedef __restorefn_t __user *__sigrestore_t;

#define SA_RESTORER	0x04000000

#ifndef __ASSEMBLY__
typedef struct {
	unsigned long sig[_NSIG_WORDS];
} sigset_t;

/* not actually used, but required for linux/syscalls.h */
typedef unsigned long old_sigset_t;

#ifdef SA_RESTORER
#define __ARCH_HAS_SA_RESTORER
#endif



typedef struct sigaltstack {
	void __user *ss_sp;
	int ss_flags;
	size_t ss_size;
} stack_t;

#endif /* __ASSEMBLY__ */

#define __ARCH_SI_UID_T __kernel_uid32_t
#define __ARCH_SI_ATTRIBUTES
#define __ARCH_SI_BAND_T long
#define __ARCH_SI_PREAMBLE_SIZE (4 * sizeof(int))
typedef __kernel_long_t __kernel_clock_t;
#define __ARCH_SI_CLOCK_T __kernel_clock_t
#define SI_MAX_SIZE     128
#define SI_PAD_SIZE     ((SI_MAX_SIZE - __ARCH_SI_PREAMBLE_SIZE) / sizeof(int))

typedef union sigval {
			int sival_int;
			void __user *sival_ptr;
} sigval_t;


typedef struct siginfo {
	int si_signo;
	int si_errno;
	int si_code;

	union {
		int _pad[SI_PAD_SIZE];

		/* kill() */
		struct {
			__kernel_pid_t _pid;	/* sender's pid */
			__ARCH_SI_UID_T _uid;	/* sender's uid */
		} _kill;

		/* POSIX.1b timers */
		struct {
			__kernel_timer_t _tid;	/* timer id */
			int _overrun;		/* overrun count */
			char _pad[sizeof( __ARCH_SI_UID_T) - sizeof(int)];
			sigval_t _sigval;	/* same as below */
			int _sys_private;       /* not to be passed to user */
		} _timer;

		/* POSIX.1b signals */
		struct {
			__kernel_pid_t _pid;	/* sender's pid */
			__ARCH_SI_UID_T _uid;	/* sender's uid */
			sigval_t _sigval;
		} _rt;

		/* SIGCHLD */
		struct {
			__kernel_pid_t _pid;	/* which child */
			__ARCH_SI_UID_T _uid;	/* sender's uid */
			int _status;		/* exit code */
			__ARCH_SI_CLOCK_T _utime;
			__ARCH_SI_CLOCK_T _stime;
		} _sigchld;

		/* SIGILL, SIGFPE, SIGSEGV, SIGBUS */
		struct {
			void __user *_addr; /* faulting insn/memory ref. */
#ifdef __ARCH_SI_TRAPNO
			int _trapno;	/* TRAP # which caused the signal */
#endif
			short _addr_lsb; /* LSB of the reported address */
		} _sigfault;

		/* SIGPOLL */
		struct {
			__ARCH_SI_BAND_T _band;	/* POLL_IN, POLL_OUT, POLL_MSG */
			int _fd;
		} _sigpoll;

		/* SIGSYS */
		struct {
			void __user *_call_addr; /* calling user insn */
			int _syscall;	/* triggering system call number */
			unsigned int _arch;	/* AUDIT_ARCH_* of syscall */
		} _sigsys;
	} _sifields;
} __ARCH_SI_ATTRIBUTES siginfo_t;



struct sigqueue {
	struct list_head list;
	int flags;
	siginfo_t info;
	struct user_struct *user;
};

struct sigpending {
	struct list_head list;
	sigset_t signal;
};


struct sigaction {
#ifndef __ARCH_HAS_IRIX_SIGACTION
	__sighandler_t	sa_handler;
	unsigned long	sa_flags;
#else
	unsigned int	sa_flags;
	__sighandler_t	sa_handler;
#endif
#ifdef __ARCH_HAS_SA_RESTORER
	__sigrestore_t sa_restorer;
#endif
	sigset_t	sa_mask;	/* mask last for extensibility */
};

struct k_sigaction {
	struct sigaction sa;
#ifdef __ARCH_HAS_KA_RESTORER
	__sigrestore_t ka_restorer;
#endif
};

struct ksignal {
	struct k_sigaction ka;
	siginfo_t info;
	int sig;
};




struct pid_link
{
				struct hlist_node node;
				struct pid *pid;
};

struct upid {
				/* Try to keep pid_chain in the same cacheline as nr for find_vpid */
				int nr;
				struct pid_namespace *ns;
				struct hlist_node pid_chain;
};


enum pid_type
{
	PIDTYPE_PID,
	PIDTYPE_PGID,
	PIDTYPE_SID,
	PIDTYPE_MAX
};


struct pid
{
				atomic_t count;
				unsigned int level;
				/* lists of tasks that use this pid */
				struct hlist_head tasks[PIDTYPE_MAX];
				struct rcu_head rcu;
				struct upid numbers[1];
};




#define PER_CPU_BASE_SECTION ".data..percpu"
#define PER_CPU_ATTRIBUTES
#define PER_CPU_DEF_ATTRIBUTES

#define __PCPU_ATTRS(sec)                                               \
				__percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))     \
				PER_CPU_ATTRIBUTES

#define DEFINE_PER_CPU_SECTION(type, name, sec)                         \
				__PCPU_ATTRS(sec) PER_CPU_DEF_ATTRIBUTES                        \
				__typeof__(type) name

#define DEFINE_PER_CPU(type, name)                                      \
				DEFINE_PER_CPU_SECTION(type, name, "")


#define DECLARE_PER_CPU_SECTION(type, name, sec)                        \
					 __PCPU_ATTRS(sec) __typeof__(type) name

#define DECLARE_PER_CPU(type, name)                                     \


enum pageblock_bits {
	PB_migrate,
	PB_migrate_end = PB_migrate + 3 - 1,
			/* 3 bits required for migrate types */
	PB_migrate_skip,/* If set the block is skipped by compaction */

	/*
	* Assume the bits will always align on a word. If this assumption
	* changes then get/set pageblock needs updating.
	*/
	NR_PAGEBLOCK_BITS
};


/* Free memory management - zoned buddy allocator.  */
#ifndef CONFIG_FORCE_MAX_ZONEORDER
#define MAX_ORDER 11
#else
#define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER
#endif
#define MAX_ORDER_NR_PAGES (1 << (MAX_ORDER - 1))

/*
* PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed
* costly to service.  That is between allocation orders which should
* coalesce naturally under reasonable reclaim pressure and those which
* will not.
*/
#define PAGE_ALLOC_COSTLY_ORDER 3

enum {
	MIGRATE_UNMOVABLE,
	MIGRATE_RECLAIMABLE,
	MIGRATE_MOVABLE,
	MIGRATE_PCPTYPES,	/* the number of types on the pcp lists */
	MIGRATE_RESERVE = MIGRATE_PCPTYPES,
#ifdef CONFIG_CMA
	/*
	* MIGRATE_CMA migration type is designed to mimic the way
	* ZONE_MOVABLE works.  Only movable pages can be allocated
	* from MIGRATE_CMA pageblocks and page allocator never
	* implicitly change migration type of MIGRATE_CMA pageblock.
	*
	* The way to use it is to change migratetype of a range of
	* pageblocks to MIGRATE_CMA which can be done by
	* __free_pageblock_cma() function.  What is important though
	* is that a range of pageblocks must be aligned to
	* MAX_ORDER_NR_PAGES should biggest page be bigger then
	* a single pageblock.
	*/
	MIGRATE_CMA,
#endif
#ifdef CONFIG_MEMORY_ISOLATION
	MIGRATE_ISOLATE,	/* can't allocate from here */
#endif
	MIGRATE_TYPES
};


struct free_area {
	struct list_head	free_list[MIGRATE_TYPES];
	unsigned long		nr_free;
};

struct pglist_data;

/*
* zone->lock and zone->lru_lock are two of the hottest locks in the kernel.
* So add a wild amount of padding here to ensure that they fall into separate
* cachelines.  There are very few zone structures in the machine, so space
* consumption is not a concern here.
*/
#if defined(CONFIG_SMP)
struct zone_padding {
	char x[0];
} ____cacheline_internodealigned_in_smp;
#define ZONE_PADDING(name)	struct zone_padding name;
#else
#define ZONE_PADDING(name)
#endif

enum zone_stat_item {
	/* First 128 byte cacheline (assuming 64 bit words) */
	NR_FREE_PAGES,
	NR_ALLOC_BATCH,
	NR_LRU_BASE,
	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
	NR_ANON_PAGES,	/* Mapped anonymous pages */
	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
				only modified from process context */
	NR_FILE_PAGES,
	NR_FILE_DIRTY,
	NR_WRITEBACK,
	NR_SLAB_RECLAIMABLE,
	NR_SLAB_UNRECLAIMABLE,
	NR_PAGETABLE,		/* used for pagetables */
	NR_KERNEL_STACK,
	/* Second 128 byte cacheline */
	NR_UNSTABLE_NFS,	/* NFS unstable pages */
	NR_BOUNCE,
	NR_VMSCAN_WRITE,
	NR_VMSCAN_IMMEDIATE,	/* Prioritise for reclaim when writeback ends */
	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
	NR_DIRTIED,		/* page dirtyings since bootup */
	NR_WRITTEN,		/* page writings since bootup */
#ifdef CONFIG_NUMA
	NUMA_HIT,		/* allocated in intended node */
	NUMA_MISS,		/* allocated in non intended node */
	NUMA_FOREIGN,		/* was intended here, hit elsewhere */
	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */
	NUMA_LOCAL,		/* allocation from local node */
	NUMA_OTHER,		/* allocation from other node */
#endif
	WORKINGSET_REFAULT,
	WORKINGSET_ACTIVATE,
	WORKINGSET_NODERECLAIM,
	NR_ANON_TRANSPARENT_HUGEPAGES,
	NR_FREE_CMA_PAGES,
	NR_VM_ZONE_STAT_ITEMS };

/*
* We do arithmetic on the LRU lists in various places in the code,
* so it is important to keep the active lists LRU_ACTIVE higher in
* the array than the corresponding inactive lists, and to keep
* the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.
*
* This has to be kept in sync with the statistics in zone_stat_item
* above and the descriptions in vmstat_text in mm/vmstat.c
*/
#define LRU_BASE 0
#define LRU_ACTIVE 1
#define LRU_FILE 2

enum lru_list {
	LRU_INACTIVE_ANON = LRU_BASE,
	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
	LRU_UNEVICTABLE,
	NR_LRU_LISTS
};


struct zone_reclaim_stat {
	/*
	* The pageout code in vmscan.c keeps track of how many of the
	* mem/swap backed and file backed pages are referenced.
	* The higher the rotated/scanned ratio, the more valuable
	* that cache is.
	*
	* The anon LRU stats live in [0], file LRU stats in [1]
	*/
	unsigned long		recent_rotated[2];
	unsigned long		recent_scanned[2];
};

struct lruvec {
	struct list_head lists[NR_LRU_LISTS];
	struct zone_reclaim_stat reclaim_stat;
#ifdef CONFIG_MEMCG
	struct zone *zone;
#endif
};


typedef unsigned __bitwise__ isolate_mode_t;

enum zone_watermarks {
	WMARK_MIN,
	WMARK_LOW,
	WMARK_HIGH,
	NR_WMARK
};



struct per_cpu_pages {
	int count;		/* number of pages in the list */
	int high;		/* high watermark, emptying needed */
	int batch;		/* chunk size for buddy add/remove */

	/* Lists of pages, one per migrate type stored on the pcp-lists */
	struct list_head lists[MIGRATE_PCPTYPES];
};

struct per_cpu_pageset {
	struct per_cpu_pages pcp;
#ifdef CONFIG_NUMA
	s8 expire;
#endif
#ifdef CONFIG_SMP
	s8 stat_threshold;
	s8 vm_stat_tlx_diff[NR_VM_ZONE_STAT_ITEMS];
#endif
};



enum zone_type {
#ifdef CONFIG_ZONE_DMA
	/*
	* ZONE_DMA is used when there are devices that are not able
	* to do DMA to all of addressable memory (ZONE_NORMAL). Then we
	* carve out the portion of memory that is needed for these devices.
	* The range is arch specific.
	*
	* Some examples
	*
	* Architecture		Limit
	* ---------------------------
	* parisc, ia64, sparc	<4G
	* s390			<2G
	* arm			Various
	* alpha		Unlimited or 0-16MB.
	*
	* i386, x86_64 and multiple other arches
	* 			<16M.
	*/
	ZONE_DMA,
#endif
#ifdef CONFIG_ZONE_DMA32
	/*
	* x86_64 needs two ZONE_DMAs because it supports devices that are
	* only able to do DMA to the lower 16M but also 32 bit devices that
	* can only do DMA areas below 4G.
	*/
	ZONE_DMA32,
#endif
	/*
	* Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	* performed on pages in ZONE_NORMAL if the DMA devices support
	* transfers to all addressable memory.
	*/
	ZONE_NORMAL,
#ifdef CONFIG_HIGHMEM
	/*
	* A memory area that is only addressable by the kernel through
	* mapping portions into its own address space. This is for example
	* used by i386 to allow the kernel to address the memory beyond
	* 900MB. The kernel will set up special mappings (page
	* table entries on i386) for each page that the kernel needs to
	* access.
	*/
	ZONE_HIGHMEM,
#endif
	ZONE_MOVABLE,
	__MAX_NR_ZONES
};



struct zone {
	/* Fields commonly accessed by the page allocator */

	/* zone watermarks, access with *_wmark_pages(zone) macros */
	unsigned long watermark[NR_WMARK];

	/*
	* When free pages are below this point, additional steps are taken
	* when reading the number of free pages to avoid per-cpu counter
	* drift allowing watermarks to be breached
	*/
	unsigned long percpu_drift_mark;

	/*
	* We don't know if the memory that we're going to allocate will be freeable
	* or/and it will be released eventually, so to avoid totally wasting several
	* GB of ram we must reserve some of the lower zone memory (otherwise we risk
	* to run OOM on the lower zones despite there's tons of freeable ram
	* on the higher zones). This array is recalculated at runtime if the
	* sysctl_lowmem_reserve_ratio sysctl changes.
	*/
	unsigned long		lowmem_reserve[MAX_NR_ZONES];

	/*
	* This is a per-zone reserve of pages that should not be
	* considered dirtyable memory.
	*/
	unsigned long		dirty_balance_reserve;

#ifdef CONFIG_NUMA
	int node;
	/*
	* zone reclaim becomes active if more unmapped pages exist.
	*/
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
#endif
	struct per_cpu_pageset __percpu *pageset;
	/*
	* free areas of different sizes
	*/
	spinlock_t		lock;
#if defined CONFIG_COMPACTION || defined CONFIG_CMA
	/* Set to true when the PG_migrate_skip bits should be cleared */
	bool			compact_blockskip_flush;

	/* pfn where compaction free scanner should start */
	unsigned long		compact_cached_free_pfn;
	/* pfn where async and sync compaction migration scanner should start */
	unsigned long		compact_cached_migrate_pfn[2];
#endif
#ifdef CONFIG_MEMORY_HOTPLUG
	/* see spanned/present_pages for more description */
	seqlock_t		span_seqlock;
#endif
	struct free_area	free_area[MAX_ORDER];

#ifndef CONFIG_SPARSEMEM
	/*
	* Flags for a pageblock_nr_pages block. See pageblock-flags.h.
	* In SPARSEMEM, this map is stored in struct mem_section_tlx
	*/
	unsigned long		*pageblock_flags;
#endif /* CONFIG_SPARSEMEM */

#ifdef CONFIG_COMPACTION
	/*
	* On compaction failure, 1<<compact_defer_shift compactions_tlx
	* are skipped before trying again. The number attempted since
	* last failure is tracked with compact_considered.
	*/
	unsigned int		compact_considered;
	unsigned int		compact_defer_shift;
	int			compact_order_failed;
#endif

	ZONE_PADDING(_pad1_)

	/* Fields commonly accessed by the page reclaim scanner */
	spinlock_t		lru_lock;
	struct lruvec		lruvec;

	/* Evictions & activations on the inactive file list */
	atomic_long_t		inactive_age;

	unsigned long		pages_scanned;	   /* since last reclaim */
	unsigned long		flags;		   /* zone flags, see below */

	/* Zone statistics */
	atomic_long_t		vm_stat_tlx[NR_VM_ZONE_STAT_ITEMS];

	/*
	* The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	* this zone's LRU.  Maintained by the pageout code.
	*/
	unsigned int inactive_ratio;


	ZONE_PADDING(_pad2_)
	/* Rarely used or read-mostly fields */

	/*
	* wait_table		-- the array holding the hash table
	* wait_table_hash_nr_entries	-- the size of the hash table array
	* wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
	*
	* The purpose of all these is to keep track of the people
	* waiting for a page to become available and make them
	* runnable again when possible. The trouble is that this
	* consumes a lot of space, especially when so few things
	* wait on pages at a given time. So instead of using
	* per-page waitqueues, we use a waitqueue hash table.
	*
	* The bucket discipline is to sleep on the same queue when
	* colliding and wake all in that wait queue when removing.
	* When something wakes, it must check to be sure its page is
	* truly available, a la thundering herd. The cost of a
	* collision is great, but given the expected load of the
	* table, they should be so rare as to be outweighed by the
	* benefits from the saved space.
	*
	* __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	* primary users of these fields, and in mm/page_alloc.c
	* free_area_init_core() performs the initialization of them.
	*/
	wait_queue_head_t	* wait_table;
	unsigned long		wait_table_hash_nr_entries;
	unsigned long		wait_table_bits;

	/*
	* Discontig memory support fields.
	*/
	struct pglist_data	*zone_pgdat;
	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
	unsigned long		zone_start_pfn;

	/*
	* spanned_pages is the total pages spanned by the zone, including
	* holes, which is calculated as:
	* 	spanned_pages = zone_end_pfn - zone_start_pfn;
	*
	* present_pages is physical pages existing within the zone, which
	* is calculated as:
	*	present_pages = spanned_pages - absent_pages(pages in holes);
	*
	* managed_pages is present pages managed by the buddy system, which
	* is calculated as (reserved_pages includes pages allocated by the
	* bootmem allocator):
	*	managed_pages = present_pages - reserved_pages;
	*
	* So present_pages may be used by memory hotplug or memory power
	* management logic to figure out unmanaged pages by checking
	* (present_pages - managed_pages). And managed_pages should be used
	* by page allocator and vm scanner to calculate all kinds of watermarks
	* and thresholds.
	*
	* Locking rules:
	*
	* zone_start_pfn and spanned_pages are protected by span_seqlock.
	* It is a seqlock because it has to be read outside of zone->lock,
	* and it is done in the main allocator path.  But, it is written
	* quite infrequently.
	*
	* The span_seq lock is declared along with zone->lock because it is
	* frequently read in proximity to zone->lock.  It's good to
	* give them a chance of being in the same cacheline.
	*
	* Write access to present_pages at runtime should be protected by
	* mem_hotplug_begin/end(). Any reader who can't tolerant drift of
	* present_pages should get_online_mems() to get a stable value.
	*
	* Read access to managed_pages should be safe because it's unsigned
	* long. Write access to zone->managed_pages and totalram_pages_tlx are
	* protected by managed_page_count_lock at runtime. Idealy only
	* adjust_managed_page_count() should be used instead of directly
	* touching zone->managed_pages and totalram_pages_tlx.
	*/
	unsigned long		spanned_pages;
	unsigned long		present_pages;
	unsigned long		managed_pages;

	/*
	* Number of MIGRATE_RESEVE page block. To maintain for just
	* optimization. Protected by zone->lock.
	*/
	int			nr_migrate_reserve_block;

	/*
	* rarely used fields:
	*/
	const char		*name;
} ____cacheline_internodealigned_in_smp;

typedef enum {
	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
	ZONE_CONGESTED,			/* zone has many dirty pages backed by
					* a congested BDI
					*/
	ZONE_TAIL_LRU_DIRTY,		/* reclaim scanning has recently found
					* many dirty file pages at the tail
					* of the LRU.
					*/
	ZONE_WRITEBACK,			/* reclaim scanning has recently found
					* many pages under writeback
					*/
} zone_flags_t;


/*
* This struct contains information about a zone in a zonelist. It is stored
* here to avoid dereferences into large structures and lookups of tables
*/
struct zoneref {
	struct zone *zone;	/* Pointer to actual zone */
	int zone_idx;		/* zone_idx(zoneref->zone) */
};

#define MAX_ZONELISTS 1
#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)

struct zonelist {
	struct zonelist_cache *zlcache_ptr;		     // NULL or &zlcache
	struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];
#ifdef CONFIG_NUMA
	struct zonelist_cache zlcache;			     // optional ...
#endif
};

struct bootmem_data;
typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];
	struct zonelist node_zonelists[MAX_ZONELISTS];
	int nr_zones;
#ifdef CONFIG_FLAT_NODE_MEM_MAP	/* means !SPARSEMEM */
	struct page *node_mem_map;
#ifdef CONFIG_MEMCG
	struct page_cgroup *node_page_cgroup;
#endif
#endif
#ifndef CONFIG_NO_BOOTMEM
	struct bootmem_data *bdata;
#endif
#ifdef CONFIG_MEMORY_HOTPLUG
	/*
	* Must be held any time you expect node_start_pfn, node_present_pages
	* or node_spanned_pages stay constant.  Holding this will also
	* guarantee that any pfn_valid() stays that way.
	*
	* pgdat_resize_lock() and pgdat_resize_unlock() are provided to
	* manipulate node_size_lock without checking for CONFIG_MEMORY_HOTPLUG.
	*
	* Nests above zone->lock and zone->span_seqlock
	*/
	spinlock_t node_size_lock;
#endif
	unsigned long node_start_pfn;
	unsigned long node_present_pages; /* total number of physical pages */
	unsigned long node_spanned_pages; /* total size of physical page
							range, including holes */
	int node_id;
	wait_queue_head_t kswapd_wait;
	wait_queue_head_t pfmemalloc_wait;
	struct task_struct *kswapd;	/* Protected by
						mem_hotplug_begin/end() */
	int kswapd_max_order;
	enum zone_type classzone_idx;
#ifdef CONFIG_NUMA_BALANCING
	/* Lock serializing the migrate rate limiting window */
	spinlock_t numabalancing_migrate_lock;

	/* Rate limiting time interval */
	unsigned long numabalancing_migrate_next_window;

	/* Number of pages migrated during the rate limiting time interval */
	unsigned long numabalancing_migrate_nr_pages;
#endif
} pg_data_t;



struct mutex {
	/* 1: unlocked, 0: locked, negative: locked, possible waiters */
	atomic_t		count;
	spinlock_t		wait_lock;
	struct list_head	wait_list;
#if defined(CONFIG_DEBUG_MUTEXES) || defined(CONFIG_SMP)
	struct task_struct	*owner;
#endif
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
	struct optimistic_spin_queue osq; /* Spinner MCS lock */
#endif
#ifdef CONFIG_DEBUG_MUTEXES
	const char 		*name;
	void			*magic;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
};


struct work_struct;
typedef void (*work_func_t)(struct work_struct *work);
enum {
	WORK_STRUCT_PENDING_BIT	= 0,	/* work item is pending execution */
	WORK_STRUCT_DELAYED_BIT	= 1,	/* work item is delayed */
	WORK_STRUCT_PWQ_BIT	= 2,	/* data points to pwq */
	WORK_STRUCT_LINKED_BIT	= 3,	/* next work is linked to this one */
#ifdef CONFIG_DEBUG_OBJECTS_WORK
	WORK_STRUCT_STATIC_BIT	= 4,	/* static initializer (debugobjects) */
	WORK_STRUCT_COLOR_SHIFT	= 5,	/* color for workqueue flushing */
#else
	WORK_STRUCT_COLOR_SHIFT	= 4,	/* color for workqueue flushing */
#endif

	WORK_STRUCT_COLOR_BITS	= 4,

	WORK_STRUCT_PENDING	= 1 << WORK_STRUCT_PENDING_BIT,
	WORK_STRUCT_DELAYED	= 1 << WORK_STRUCT_DELAYED_BIT,
	WORK_STRUCT_PWQ		= 1 << WORK_STRUCT_PWQ_BIT,
	WORK_STRUCT_LINKED	= 1 << WORK_STRUCT_LINKED_BIT,
#ifdef CONFIG_DEBUG_OBJECTS_WORK
	WORK_STRUCT_STATIC	= 1 << WORK_STRUCT_STATIC_BIT,
#else
	WORK_STRUCT_STATIC	= 0,
#endif

	/*
	* The last color is no color used for works which don't
	* participate in workqueue flushing.
	*/
	WORK_NR_COLORS		= (1 << WORK_STRUCT_COLOR_BITS) - 1,
	WORK_NO_COLOR		= WORK_NR_COLORS,

	/* not bound to any CPU, prefer the local CPU */
	WORK_CPU_UNBOUND	= NR_CPUS,

	/*
	* Reserve 7 bits off of pwq pointer w/ debugobjects turned off.
	* This makes pwqs aligned to 256 bytes and allows 15 workqueue
	* flush colors.
	*/
	WORK_STRUCT_FLAG_BITS	= WORK_STRUCT_COLOR_SHIFT +
					WORK_STRUCT_COLOR_BITS,

	/* data contains off-queue information when !WORK_STRUCT_PWQ */
	WORK_OFFQ_FLAG_BASE	= WORK_STRUCT_COLOR_SHIFT,

	WORK_OFFQ_CANCELING	= (1 << WORK_OFFQ_FLAG_BASE),

	/*
	* When a work item is off queue, its high bits point to the last
	* pool it was on.  Cap at 31 bits and use the highest number to
	* indicate that no pool is associated.
	*/
	WORK_OFFQ_FLAG_BITS	= 1,
	WORK_OFFQ_POOL_SHIFT	= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
	WORK_OFFQ_LEFT		= BITS_PER_LONG - WORK_OFFQ_POOL_SHIFT,
	WORK_OFFQ_POOL_BITS	= WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,
	WORK_OFFQ_POOL_NONE	= (1LU << WORK_OFFQ_POOL_BITS) - 1,

	/* convenience constants */
	WORK_STRUCT_FLAG_MASK	= (1UL << WORK_STRUCT_FLAG_BITS) - 1,
	WORK_STRUCT_WQ_DATA_MASK = ~WORK_STRUCT_FLAG_MASK,
	WORK_STRUCT_NO_POOL	= (unsigned long)WORK_OFFQ_POOL_NONE << WORK_OFFQ_POOL_SHIFT,

	/* bit mask for work_busy() return values */
	WORK_BUSY_PENDING	= 1 << 0,
	WORK_BUSY_RUNNING	= 1 << 1,

	/* maximum string length for set_worker_desc() */
	WORKER_DESC_LEN		= 24,
};

struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
#ifdef CONFIG_LOCKDEP
	struct lockdep_map lockdep_map;
#endif
};



struct delayed_work {
	struct work_struct work;
	struct timer_list timer;

	/* target workqueue and CPU ->timer uses to queue ->work */
	struct workqueue_struct *wq;
	int cpu;
};

/*
* A struct for workqueue attributes.  This can be used to change
* attributes of an unbound workqueue.
*
* Unlike other fields, ->no_numa isn't a property of a worker_pool.  It
* only modifies how apply_workqueue_attrs() select pools and thus doesn't
* participate in pool hash calculations or equality comparisons.
*/



struct workqueue_attrs {
	int			nice;		/* nice level */
	cpumask_var_t		cpumask;	/* allowed CPUs */
	bool			no_numa;	/* disable NUMA affinity */
};

enum {
	WQ_UNBOUND		= 1 << 1, /* not bound to any cpu */
	WQ_FREEZABLE		= 1 << 2, /* freeze during suspend */
	WQ_MEM_RECLAIM		= 1 << 3, /* may be used for memory reclaim */
	WQ_HIGHPRI		= 1 << 4, /* high priority */
	WQ_CPU_INTENSIVE	= 1 << 5, /* cpu intensive workqueue */
	WQ_SYSFS		= 1 << 6, /* visible in sysfs, see wq_sysfs_register() */

	/*
	* Per-cpu workqueues are generally preferred because they tend to
	* show better performance thanks to cache locality.  Per-cpu
	* workqueues exclude the scheduler from choosing the CPU to
	* execute the worker threads, which has an unfortunate side effect
	* of increasing power consumption.
	*
	* The scheduler considers a CPU idle if it doesn't have any task
	* to execute and tries to keep idle cores idle to conserve power;
	* however, for example, a per-cpu work item scheduled from an
	* interrupt handler on an idle CPU will force the scheduler to
	* excute the work item on that CPU breaking the idleness, which in
	* turn may lead to more scheduling choices which are sub-optimal
	* in terms of power consumption.
	*
	* Workqueues marked with WQ_POWER_EFFICIENT are per-cpu by default
	* but become unbound if workqueue.power_efficient kernel param is
	* specified.  Per-cpu workqueues which are identified to
	* contribute significantly to power-consumption are identified and
	* marked with this flag and enabling the power_efficient mode
	* leads to noticeable power saving at the cost of small
	* performance disadvantage.
	*
	* http://thread.gmane.org/gmane.linux.kernel/1480396
	*/
	WQ_POWER_EFFICIENT	= 1 << 7,

	__WQ_DRAINING		= 1 << 16, /* internal: workqueue is draining */
	__WQ_ORDERED		= 1 << 17, /* internal: workqueue is ordered */

	WQ_MAX_ACTIVE		= 512,	  /* I like 512, better ideas? */
	WQ_MAX_UNBOUND_PER_CPU	= 4,	  /* 4 * #cpus for unbound wq */
	WQ_DFL_ACTIVE		= WQ_MAX_ACTIVE / 2,
};


struct rcu_batch {
	struct rcu_head *head, **tail;
};


struct srcu_struct {
	unsigned completed;
	struct srcu_struct_array __percpu *per_cpu_ref;
	spinlock_t queue_lock; /* protect ->batch_queue, ->running */
	bool running;
	/* callbacks just queued */
	struct rcu_batch batch_queue;
	/* callbacks try to do the first check_zero */
	struct rcu_batch batch_check0;
	/* callbacks done with the first check_zero and the flip */
	struct rcu_batch batch_check1;
	struct rcu_batch batch_done;
	struct delayed_work work;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map dep_map;
#endif /* #ifdef CONFIG_DEBUG_LOCK_ALLOC */
};



struct page;
struct page_cgroup;
struct mem_section_tlx {
	/*
	* This is, logically, a pointer to an array of struct
	* pages.  However, it is stored with some other magic.
	* (see sparse.c::sparse_init_one_section())
	*
	* Additionally during early boot we encode node id of
	* the location of the section here to guide allocation.
	* (see sparse.c::memory_present_tlx())
	*
	* Making it a UL at least makes someone do a cast
	* before using it wrong.
	*/
	unsigned long section_mem_map;

	/* See declaration of similar field in struct zone */
	unsigned long *pageblock_flags;
#ifdef CONFIG_MEMCG
	/*
	* If !SPARSEMEM, pgdat doesn't have page_cgroup pointer. We use
	* section. (see memcontrol.h/page_cgroup.h about this.)
	*/
	struct page_cgroup *page_cgroup;
	unsigned long pad;
#endif
	/*
	* WARNING: mem_section_tlx must be a power-of-2 in size for the
	* calculation and use of SECTION_ROOT_MASK to make sense.
	*/
};

#define NR_MIGRATETYPE_BITS (PB_migrate_end - PB_migrate + 1)
#define MIGRATETYPE_MASK ((1UL << NR_MIGRATETYPE_BITS) - 1)
#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section_tlx))
#define SECTION_ROOT_MASK       (SECTIONS_PER_ROOT - 1)
#define MAX_ZONELISTS 1
#define LRU_BASE 0
#define LRU_ACTIVE 1
#define LRU_FILE 2
#define MAX_ORDER 11
#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)

struct percpu_counter {
				raw_spinlock_t lock;
				s64 count;
#ifdef CONFIG_HOTPLUG_CPU
			struct list_head list;  /* All percpu_counters are on a list */
#endif
			s32 __percpu *counters;
};

struct seccomp { };

struct rlimit {
				__kernel_ulong_t        rlim_cur;
				__kernel_ulong_t        rlim_max;
};



#define RLIM_NLIMITS            16



struct hrtimer_clock_base {
				struct hrtimer_cpu_base *cpu_base;
				int                     index;
				clockid_t               clockid;
				struct timerqueue_head  active;
				ktime_t_tlx                 resolution;
				ktime_t_tlx                 (*get_time)(void);
				ktime_t_tlx                 softirq_time;
				ktime_t_tlx                 offset;
};



enum hrtimer_mode {
	HRTIMER_MODE_ABS = 0x0,		/* Time value is absolute */
	HRTIMER_MODE_REL = 0x1,		/* Time value is relative to now */
	HRTIMER_MODE_PINNED = 0x02,	/* Timer is bound to CPU */
	HRTIMER_MODE_ABS_PINNED = 0x02,
	HRTIMER_MODE_REL_PINNED = 0x03,
};



enum  hrtimer_base_type {
	HRTIMER_BASE_MONOTONIC,
	HRTIMER_BASE_REALTIME,
	HRTIMER_BASE_BOOTTIME,
	HRTIMER_BASE_TAI,
	HRTIMER_MAX_CLOCK_BASES,
};

struct hrtimer {
				struct timerqueue_node_tlx         node;
				ktime_t_tlx                         _softexpires;
				enum hrtimer_restart            (*function)(struct hrtimer *);
				struct hrtimer_clock_base       *base;
				unsigned long                   state;
};


struct sched_dl_entity {
	struct rb_node_tlx	rb_node;

	/*
	* Original scheduling parameters. Copied here from sched_attr
	* during sched_setattr(), they will remain the same until
	* the next sched_setattr().
	*/
	u64 dl_runtime;		/* maximum runtime for each instance	*/
	u64 dl_deadline;	/* relative deadline of each instance	*/
	u64 dl_period;		/* separation of two instances (period) */
	u64 dl_bw;		/* dl_runtime / dl_deadline		*/

	/*
	* Actual scheduling parameters. Initialized with the values above,
	* they are continously updated during task execution. Note that
	* the remaining runtime could be < 0 in case we are in overrun.
	*/
	s64 runtime;		/* remaining runtime for this instance	*/
	u64 deadline;		/* absolute deadline for this instance	*/
	unsigned int flags;	/* specifying the scheduler behaviour	*/

	/*
	* Some bool flags:
	*
	* @dl_throttled tells if we exhausted the runtime. If so, the
	* task has to wait for a replenishment to be performed at the
	* next firing of dl_timer.
	*
	* @dl_new tells if a new instance arrived. If so we must
	* start executing it with full runtime and reset its absolute
	* deadline;
	*
	* @dl_boosted tells if we are boosted due to DI. If so we are
	* outside bandwidth enforcement mechanism (but only until we
	* exit the critical section);
	*
	* @dl_yielded tells if task gave up the cpu before consuming
	* all its available runtime during the last job.
	*/
	int dl_throttled, dl_new, dl_boosted, dl_yielded;

	/*
	* Bandwidth enforcement timer. Each -deadline task has its
	* own bandwidth to be enforced, thus we need one timer per task.
	*/
	struct hrtimer dl_timer;
};

struct task_io_accounting {
#ifdef CONFIG_TASK_XACCT
	/* bytes read */
	u64 rchar;
	/*  bytes written */
	u64 wchar;
	/* # of read syscalls */
	u64 syscr;
	/* # of write syscalls */
	u64 syscw;
#endif /* CONFIG_TASK_XACCT */

#ifdef CONFIG_TASK_IO_ACCOUNTING
	/*
	* The number of bytes which this task has caused to be read from
	* storage.
	*/
	u64 read_bytes;

	/*
	* The number of bytes which this task has caused, or shall cause to be
	* written to disk.
	*/
	u64 write_bytes;

	/*
	* A task can cause "negative" IO too.  If this task truncates some
	* dirty pagecache, some IO which another task has been accounted for
	* (in its write_bytes) will not be happening.  We _could_ just
	* subtract that from the truncating task's write_bytes, but there is
	* information loss in doing that.
	*/
	u64 cancelled_write_bytes;
#endif /* CONFIG_TASK_IO_ACCOUNTING */
};





#define NGROUPS_SMALL           32

struct group_info {
				atomic_t        usage;
				int             ngroups;
				int             nblocks;
				kgid_t          small_block[NGROUPS_SMALL];
				kgid_t          *blocks[0];
};



struct cred {
				atomic_t        usage;
#ifdef CONFIG_DEBUG_CREDENTIALS
				atomic_t        subscribers;    /* number of processes subscribed */
				void            *put_addr;
				unsigned        magic;
#define CRED_MAGIC      0x43736564
#define CRED_MAGIC_DEAD 0x44656144
#endif
				kuid_t          uid;            /* real UID of the task */
				kgid_t          gid;            /* real GID of the task */
				kuid_t          suid;           /* saved UID of the task */
				kgid_t          sgid;           /* saved GID of the task */
				kuid_t          euid;           /* effective UID of the task */
				kgid_t          egid;           /* effective GID of the task */
				kuid_t          fsuid;          /* UID for VFS ops */
				kgid_t          fsgid;          /* GID for VFS ops */
				unsigned        securebits;     /* SUID-less security management */
				kernel_cap_t    cap_inheritable; /* caps our children can inherit */
				kernel_cap_t    cap_permitted;  /* caps we're permitted */
				kernel_cap_t    cap_effective;  /* caps we can actually use */
				kernel_cap_t    cap_bset;       /* capability bounding set */
// #ifdef CONFIG_KEYS
				unsigned char   jit_keyring;    /* default keyring to attach requested
																				* keys to */
				struct key __rcu *session_keyring; /* keyring inherited over fork */
				struct key      *process_keyring; /* keyring private to this process */
				struct key      *thread_keyring; /* keyring private to this thread */
				struct key      *request_key_auth; /* assumed request_key authority */
//#endif
#ifdef CONFIG_SECURITY
				void            *security;      /* subjective LSM security */
#endif
				struct user_struct *user;       /* real user ID subscription */
				struct user_namespace *user_ns; /* user_ns the caps and keyrings are relative to. */
				struct group_info *group_info;  /* supplementary groups for euid/fsgid */
				struct rcu_head rcu;            /* RCU deletion hook */
};



struct sched_attr {
	u32 size;

	u32 sched_policy;
	u64 sched_flags;

	/* SCHED_NORMAL, SCHED_BATCH */
	s32 sched_nice;

	/* SCHED_FIFO, SCHED_RR */
	u32 sched_priority;

	/* SCHED_DEADLINE */
	u64 sched_runtime;
	u64 sched_deadline;
	u64 sched_period;
};


struct sighand_struct {
	atomic_t		count;
	struct k_sigaction	action[_NSIG];
	spinlock_t		siglock;
	wait_queue_head_t_tlx	signalfd_wqh;
};

struct task_struct {
	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
	void *stack;
	atomic_t usage;
	unsigned int flags;	/* per process flags, defined below */
	unsigned int ptrace;

#ifdef CONFIG_SMP
	struct llist_node wake_entry;
	int on_cpu;
	struct task_struct *last_wakee;
	unsigned long wakee_flips;
	unsigned long wakee_flip_decay_ts;

	int wake_cpu;
#endif
	int on_rq;

	int prio, static_prio, normal_prio;
	unsigned int rt_priority;
	const struct sched_class *sched_class;
	struct sched_entity se;
	struct sched_rt_entity rt;
#ifdef CONFIG_CGROUP_SCHED
	struct task_group *sched_task_group;
#endif
	struct sched_dl_entity dl;

#ifdef CONFIG_PREEMPT_NOTIFIERS
	/* list of struct preempt_notifier: */
	struct hlist_head preempt_notifiers;
#endif

#ifdef CONFIG_BLK_DEV_IO_TRACE
	unsigned int btrace_seq;
#endif

	unsigned int policy;
	int nr_cpus_allowed;
	cpumask_t cpus_allowed;

#ifdef CONFIG_PREEMPT_RCU
	int rcu_read_lock_nesting;
	char rcu_read_unlock_special;
	struct list_head rcu_node_entry;
#endif /* #ifdef CONFIG_PREEMPT_RCU */
#ifdef CONFIG_TREE_PREEMPT_RCU
	struct rcu_node *rcu_blocked_node;
#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
#ifdef CONFIG_RCU_BOOST
	struct rt_mutex *rcu_boost_mutex;
#endif /* #ifdef CONFIG_RCU_BOOST */

#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
	struct sched_info sched_info;
#endif

	struct list_head tasks;
#ifdef CONFIG_SMP
	struct plist_node pushable_tasks;
	struct rb_node pushable_dl_tasks;
#endif

	struct mm_struct *mm, *active_mm;
#ifdef CONFIG_COMPAT_BRK
	unsigned brk_randomized:1;
#endif
	/* per-thread vma caching */
	u32 vmacache_seqnum;
	struct vm_area_struct *vmacache[VMACACHE_SIZE];
#if defined(SPLIT_RSS_COUNTING)
	struct task_rss_stat	rss_stat;
#endif
/* task state */
	int exit_state;
	int exit_code, exit_signal;
	int pdeath_signal;  /*  The signal sent when the parent dies  */
	unsigned int jobctl;	/* JOBCTL_*, siglock protected */

	/* Used for emulating ABI behavior of previous Linux versions */
	unsigned int personality;

	unsigned in_execve:1;	/* Tell the LSMs that the process is doing an
				* execve */
	unsigned in_iowait:1;

	/* task may not gain privileges */
	unsigned no_new_privs:1;

	/* Revert to default priority/policy when forking */
	unsigned sched_reset_on_fork:1;
	unsigned sched_contributes_to_load:1;

	pid_t pid;
	pid_t tgid;

#ifdef CONFIG_CC_STACKPROTECTOR
	/* Canary value for the -fstack-protector gcc feature */
	unsigned long stack_canary;
#endif
	/*
	* pointers to (original) parent process, youngest child, younger sibling,
	* older sibling, respectively.  (p->father can be replaced with
	* p->real_parent->pid)
	*/
	struct task_struct __rcu *real_parent; /* real parent process */
	struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */
	/*
	* children/sibling forms the list of my natural children
	*/
	struct list_head children;	/* list of my children */
	struct list_head sibling;	/* linkage in my parent's children list */
	struct task_struct *group_leader;	/* threadgroup leader */

	/*
	* ptraced is the list of tasks this task is using ptrace on.
	* This includes both natural children and PTRACE_ATTACH targets.
	* p->ptrace_entry is p's link on the p->parent->ptraced list.
	*/
	struct list_head ptraced;
	struct list_head ptrace_entry;

	/* PID/PID hash table linkage. */
	struct pid_link pids[PIDTYPE_MAX];
	struct list_head thread_group;
	struct list_head thread_node;

	struct completion *vfork_done;		/* for vfork() */
	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */

	cputime_t utime, stime, utimescaled, stimescaled;
	cputime_t gtime;
#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
	struct cputime prev_cputime;
#endif
#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
	seqlock_t vtime_seqlock;
	unsigned long long vtime_snap;
	enum {
		VTIME_SLEEPING = 0,
		VTIME_USER,
		VTIME_SYS,
	} vtime_snap_whence;
#endif
	unsigned long nvcsw, nivcsw; /* context switch counts */
	struct timespec start_time; 		/* monotonic time */
	struct timespec real_start_time;	/* boot based time */
/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
	unsigned long min_flt, maj_flt;

	struct task_cputime cputime_expires;
	struct list_head cpu_timers[3];

/* process credentials */
	const struct cred __rcu *real_cred; /* objective and real subjective task
					* credentials (COW) */
	const struct cred __rcu *cred;	/* effective (overridable) subjective task
					* credentials (COW) */
	char comm[TASK_COMM_LEN]; /* executable name excluding path
						- access with [gs]et_task_comm (which lock
							it with task_lock())
						- initialized normally by setup_new_exec */
/* file system info */
	int link_count, total_link_count;
#ifdef CONFIG_SYSVIPC
/* ipc stuff */
	struct sysv_sem sysvsem;
#endif
#ifdef CONFIG_DETECT_HUNG_TASK
/* hung task detection */
	unsigned long last_switch_count;
#endif
/* CPU-specific state of this task */
	struct thread_struct thread;
/* filesystem information */
	struct fs_struct *fs;
/* open file information */
	struct files_struct *files;
/* namespaces */
	struct nsproxy *nsproxy;
/* signal handlers */
	struct signal_struct *signal;
	struct sighand_struct *sighand;

	sigset_t blocked, real_blocked;
	sigset_t saved_sigmask;	/* restored if set_restore_sigmask() was used */
	struct sigpending pending;

	unsigned long sas_ss_sp;
	size_t sas_ss_size;
	int (*notifier)(void *priv);
	void *notifier_data;
	sigset_t *notifier_mask;
	struct callback_head *task_works;

	struct audit_context *audit_context;
#ifdef CONFIG_AUDITSYSCALL
	kuid_t loginuid;
	unsigned int sessionid;
#endif
	struct seccomp seccomp;

/* Thread group tracking */
		u32 parent_exec_id;
		u32 self_exec_id;
/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,
* mempolicy */
	spinlock_t alloc_lock;

	/* Protection of the PI data structures: */
	raw_spinlock_t pi_lock;

#ifdef CONFIG_RT_MUTEXES
	/* PI waiters blocked on a rt_mutex held by this task */
	struct rb_root pi_waiters;
	struct rb_node *pi_waiters_leftmost;
	/* Deadlock detection and priority inheritance handling */
	struct rt_mutex_waiter *pi_blocked_on;
	/* Top pi_waiters task */
	struct task_struct *pi_top_task;
#endif

#ifdef CONFIG_DEBUG_MUTEXES
	/* mutex deadlock detection */
	struct mutex_waiter *blocked_on;
#endif
#ifdef CONFIG_TRACE_IRQFLAGS
	unsigned int irq_events;
	unsigned long hardirq_enable_ip;
	unsigned long hardirq_disable_ip;
	unsigned int hardirq_enable_event;
	unsigned int hardirq_disable_event;
	int hardirqs_enabled;
	int hardirq_context;
	unsigned long softirq_disable_ip;
	unsigned long softirq_enable_ip;
	unsigned int softirq_disable_event;
	unsigned int softirq_enable_event;
	int softirqs_enabled;
	int softirq_context;
#endif
#ifdef CONFIG_LOCKDEP
# define MAX_LOCK_DEPTH 48UL
	u64 curr_chain_key;
	int lockdep_depth;
	unsigned int lockdep_recursion;
	struct held_lock held_locks[MAX_LOCK_DEPTH];
	gfp_t lockdep_reclaim_gfp;
#endif

/* journalling filesystem info */
	void *journal_info;

/* stacked block device info */
	struct bio_list *bio_list;

#ifdef CONFIG_BLOCK
/* stack plugging */
	struct blk_plug *plug;
#endif

/* VM state */
	struct reclaim_state *reclaim_state;

	struct backing_dev_info *backing_dev_info;

	struct io_context *io_context;

	unsigned long ptrace_message;
	siginfo_t *last_siginfo; /* For ptrace use.  */
	struct task_io_accounting ioac;
#if defined(CONFIG_TASK_XACCT)
	u64 acct_rss_mem1;	/* accumulated rss usage */
	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
	cputime_t acct_timexpd;	/* stime + utime since last update */
#endif
#ifdef CONFIG_CPUSETS
	nodemask_t mems_allowed;	/* Protected by alloc_lock */
	seqcount_t mems_allowed_seq;	/* Seqence no to catch updates */
	int cpuset_mem_spread_rotor;
	int cpuset_slab_spread_rotor;
#endif
#ifdef CONFIG_CGROUPS
	/* Control Group info protected by css_set_lock */
	struct css_set __rcu *cgroups;
	/* cg_list protected by css_set_lock and tsk->alloc_lock */
	struct list_head cg_list;
#endif
#ifdef CONFIG_FUTEX
	struct robust_list_head __user *robust_list;
#ifdef CONFIG_COMPAT
	struct compat_robust_list_head __user *compat_robust_list;
#endif
	struct list_head pi_state_list;
	struct futex_pi_state *pi_state_cache;
#endif
#ifdef CONFIG_PERF_EVENTS
	struct perf_event_context *perf_event_ctxp[perf_nr_task_contexts];
	struct mutex perf_event_mutex;
	struct list_head perf_event_list;
#endif
#ifdef CONFIG_DEBUG_PREEMPT
	unsigned long preempt_disable_ip;
#endif
#ifdef CONFIG_NUMA
	struct mempolicy *mempolicy;	/* Protected by alloc_lock */
	short il_next;
	short pref_node_fork;
#endif
#ifdef CONFIG_NUMA_BALANCING
	int numa_scan_seq;
	unsigned int numa_scan_period;
	unsigned int numa_scan_period_max;
	int numa_preferred_nid;
	unsigned long numa_migrate_retry;
	u64 node_stamp;			/* migration stamp  */
	u64 last_task_numa_placement;
	u64 last_sum_exec_runtime;
	struct callback_head numa_work;

	struct list_head numa_entry;
	struct numa_group *numa_group;

	/*
	* Exponential decaying average of faults on a per-node basis.
	* Scheduling placement decisions are made based on the these counts.
	* The values remain static for the duration of a PTE scan
	*/
	unsigned long *numa_faults_memory;
	unsigned long total_numa_faults;

	/*
	* numa_faults_buffer records faults per node during the current
	* scan window. When the scan completes, the counts in
	* numa_faults_memory decay and these values are copied.
	*/
	unsigned long *numa_faults_buffer_memory;

	/*
	* Track the nodes the process was running on when a NUMA hinting
	* fault was incurred.
	*/
	unsigned long *numa_faults_cpu;
	unsigned long *numa_faults_buffer_cpu;

	/*
	* numa_faults_locality tracks if faults recorded during the last
	* scan window were remote/local. The task scan period is adapted
	* based on the locality of the faults with different weights
	* depending on whether they were shared or private faults
	*/
	unsigned long numa_faults_locality[2];

	unsigned long numa_pages_migrated;
#endif /* CONFIG_NUMA_BALANCING */

	struct rcu_head rcu;

	/*
	* cache last used pipe for splice
	*/
	struct pipe_inode_info *splice_pipe;

	struct page_frag task_frag;

#ifdef	CONFIG_TASK_DELAY_ACCT
	struct task_delay_info *delays;
#endif
#ifdef CONFIG_FAULT_INJECTION
	int make_it_fail;
#endif
	/*
	* when (nr_dirtied >= nr_dirtied_pause), it's time to call
	* balance_dirty_pages() for some dirty throttling pause
	*/
	int nr_dirtied;
	int nr_dirtied_pause;
	unsigned long dirty_paused_when; /* start of a write-and-pause period */

#ifdef CONFIG_LATENCYTOP
	int latency_record_count;
	struct latency_record latency_record[LT_SAVECOUNT];
#endif
	/*
	* time slack values; these are used to round up poll() and
	* select() etc timeout values. These are in nanoseconds.
	*/
	unsigned long timer_slack_ns;
	unsigned long default_timer_slack_ns;

#ifdef CONFIG_FUNCTION_GRAPH_TRACER
	/* Index of current stored address in ret_stack */
	int curr_ret_stack;
	/* Stack of return addresses for return function tracing */
	struct ftrace_ret_stack	*ret_stack;
	/* time stamp for last schedule */
	unsigned long long ftrace_timestamp;
	/*
	* Number of functions that haven't been traced
	* because of depth overrun.
	*/
	atomic_t trace_overrun;
	/* Pause for the tracing */
	atomic_t tracing_graph_pause;
#endif
#ifdef CONFIG_TRACING
	/* state flags for use by tracers */
	unsigned long trace;
	/* bitmask and counter of trace recursion */
	unsigned long trace_recursion;
#endif /* CONFIG_TRACING */
#ifdef CONFIG_MEMCG /* memcg uses this to do batch job */
	struct memcg_batch_info {
		int do_batch;	/* incremented when batch uncharge started */
		struct mem_cgroup *memcg; /* target memcg of uncharge */
		unsigned long nr_pages;	/* uncharged usage */
		unsigned long memsw_nr_pages; /* uncharged mem+swap usage */
	} memcg_batch;
	unsigned int memcg_kmem_skip_account;
	struct memcg_oom_info {
		struct mem_cgroup *memcg;
		gfp_t gfp_mask;
		int order;
		unsigned int may_oom:1;
	} memcg_oom;
#endif
#ifdef CONFIG_UPROBES
	struct uprobe_task *utask;
#endif
#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)
	unsigned int	sequential_io;
	unsigned int	sequential_io_avg;
#endif
};



typedef struct qrwlock {
				atomic_t                cnts;
				arch_spinlock_t         lock;
};





struct perf_event_context {
	struct pmu			*pmu;
	enum perf_event_context_type	type;
	/*
	* Protect the states of the events in the list,
	* nr_active, and the list:
	*/
	raw_spinlock_t_tlx			lock;
	/*
	* Protect the list of events.  Locking either mutex or lock
	* is sufficient to ensure the list doesn't change; to change
	* the list you need to lock both the mutex and the spinlock.
	*/
	struct mutex			mutex;

	struct list_head		pinned_groups;
	struct list_head		flexible_groups;
	struct list_head		event_list;
	int				nr_events;
	int				nr_active;
	int				is_active;
	int				nr_stat;
	int				nr_freq;
	int				rotate_disable;
	atomic_t			refcount;
	struct task_struct		*task;

	/*
	* Context clock, runs when context enabled.
	*/
	u64				time;
	u64				timestamp;

	/*
	* These fields let us detect when two contexts have both
	* been cloned (inherited) from a common ancestor.
	*/
	struct perf_event_context	*parent_ctx;
	u64				parent_gen;
	u64				generation;
	int				pin_count;
	int				nr_cgroups;	 /* cgroup evts */
	int				nr_branch_stack; /* branch_stack evt */
	struct rcu_head			rcu_head;
};


struct signal_struct {
	atomic_t		sigcnt;
	atomic_t		live;
	int			nr_threads_tlx;
	struct list_head	thread_head;

	wait_queue_head_t	wait_chldexit;	/* for wait4() */

	/* current thread group signal load-balancing target: */
	struct task_struct	*curr_target;

	/* shared signal handling: */
	struct sigpending	shared_pending;

	/* thread group exit support */
	int			group_exit_code;
	/* overloaded:
	* - notify group_exit_task when ->count is equal to notify_count
	* - everyone except group_exit_task is stopped during signal delivery
	*   of fatal signals, group_exit_task processes the signal.
	*/
	int			notify_count;
	struct task_struct	*group_exit_task;

	/* thread group stop support, overloads group_exit_code too */
	int			group_stop_count;
	unsigned int		flags; /* see SIGNAL_* flags below */

	/*
	* PR_SET_CHILD_SUBREAPER marks a process, like a service
	* manager, to re-parent orphan (double-forking) child processes
	* to this process instead of 'init'. The service manager is
	* able to receive SIGCHLD signals and is able to investigate
	* the process until it calls wait(). All children of this
	* process will inherit a flag if they should look for a
	* child_subreaper process at exit.
	*/
	unsigned int		is_child_subreaper:1;
	unsigned int		has_child_subreaper:1;

	/* POSIX.1b Interval Timers */
	int			posix_timer_id;
	struct list_head	posix_timers;

	/* ITIMER_REAL timer for the process */
	struct hrtimer real_timer;
	struct pid *leader_pid;
	ktime_t it_real_incr;

	/*
	* ITIMER_PROF and ITIMER_VIRTUAL timers for the process, we use
	* CPUCLOCK_PROF and CPUCLOCK_VIRT for indexing array as these
	* values are defined to 0 and 1 respectively
	*/
	struct cpu_itimer it[2];

	/*
	* Thread group totals for process CPU timers.
	* See thread_group_cputimer(), et al, for details.
	*/
	struct thread_group_cputimer cputimer;

	/* Earliest-expiration cache. */
	struct task_cputime cputime_expires;

	struct list_head cpu_timers[3];

	struct pid *tty_old_pgrp;

	/* boolean value for session group leader */
	int leader;

	struct tty_struct *tty; /* NULL if no tty */

#ifdef CONFIG_SCHED_AUTOGROUP
	struct autogroup *autogroup;
#endif
	/*
	* Cumulative resource counters for dead threads in the group,
	* and for reaped dead child processes forked by this group.
	* Live threads maintain their own counters and add to these
	* in __exit_signal, except for the group leader.
	*/
	cputime_t utime, stime, cutime, cstime;
	cputime_t gtime;
	cputime_t cgtime;
#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
	struct cputime prev_cputime;
#endif
	unsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;
	unsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;
	unsigned long inblock, oublock, cinblock, coublock;
	unsigned long maxrss, cmaxrss;
	struct task_io_accounting ioac;

	/*
	* Cumulative ns of schedule CPU time fo dead threads in the
	* group, not including a zombie group leader, (This only differs
	* from jiffies_tlx_to_ns(utime + stime) if sched_clock uses something
	* other than jiffies_tlx.)
	*/
	unsigned long long sum_sched_runtime;

	/*
	* We don't bother to synchronize most readers of this at all,
	* because there is no reader checking a limit that actually needs
	* to get both rlim_cur and rlim_max atomically, and either one
	* alone is a single word that can safely be read normally.
	* getrlimit/setrlimit use task_lock(current->group_leader) to
	* protect this instead of the siglock, because they really
	* have no need to disable irqs.
	*/
	struct rlimit rlim[RLIM_NLIMITS];

#ifdef CONFIG_BSD_PROCESS_ACCT
	struct pacct_struct pacct;	/* per-process accounting information */
#endif
#ifdef CONFIG_TASKSTATS
	struct taskstats *stats;
#endif
#ifdef CONFIG_AUDIT
	unsigned audit_tty;
	unsigned audit_tty_log_passwd;
	struct tty_audit_buf *tty_audit_buf;
#endif
#ifdef CONFIG_CGROUPS
	/*
	* group_rwsem prevents new tasks from entering the threadgroup and
	* member tasks from exiting,a more specifically, setting of
	* PF_EXITING.  fork and exit paths are protected with this rwsem
	* using threadgroup_change_begin/end().  Users which require
	* threadgroup to remain stable should use threadgroup_[un]lock()
	* which also takes care of exec path.  Currently, cgroup is the
	* only user.
	*/
	struct rw_semaphore group_rwsem;
#endif

	oom_flags_t oom_flags;
	short oom_score_adj;		/* OOM kill score adjustment */
	short oom_score_adj_min;	/* OOM kill score adjustment min value.
					* Only settable by CAP_SYS_RESOURCE. */

	struct mutex cred_guard_mutex;	/* guard against foreign influences on
					* credential calculations
					* (notably. ptrace) */
};



#define PF_NO_SETAFFINITY 0x04000000    /* Userland is not allowed to meddle with cpus_allowed */

struct stat {
	unsigned long	st_dev;		/* Device.  */
	unsigned long	st_ino;		/* File serial number.  */
	unsigned int	st_mode;	/* File mode.  */
	unsigned int	st_nlink;	/* Link count.  */
	unsigned int	st_uid;		/* User ID of the file's owner.  */
	unsigned int	st_gid;		/* Group ID of the file's group. */
	unsigned long	st_rdev;	/* Device number, if device.  */
	unsigned long	__pad1;
	long		st_size;	/* Size of file, in bytes.  */
	int		st_blksize;	/* Optimal block size for I/O.  */
	int		__pad2;
	long		st_blocks;	/* Number 512-byte blocks allocated. */
	long		st_atime;	/* Time of last access.  */
	unsigned long	st_atime_nsec;
	long		st_mtime;	/* Time of last modification.  */
	unsigned long	st_mtime_nsec;
	long		st_ctime;	/* Time of last status change.  */
	unsigned long	st_ctime_nsec;
	unsigned int	__unused4;
	unsigned int	__unused5;
};

enum pageflags {
	PG_locked,		/* Page is locked. Don't touch. */
	PG_error,
	PG_referenced,
	PG_uptodate,
	PG_dirty,
	PG_lru,
	PG_active,
	PG_slab,
	PG_owner_priv_1,	/* Owner use. If pagecache, fs may use*/
	PG_arch_1,
	PG_reserved,
	PG_private,		/* If pagecache, has fs-private data */
	PG_private_2,		/* If pagecache, has fs aux data */
	PG_writeback,		/* Page is under writeback */
#ifdef CONFIG_PAGEFLAGS_EXTENDED
	PG_head,		/* A head page */
	PG_tail,		/* A tail page */
#else
	PG_compound,		/* A compound page */
#endif
	PG_swapcache,		/* Swap page: swp_entry_t in private */
	PG_mappedtodisk,	/* Has blocks allocated on-disk */
	PG_reclaim,		/* To be reclaimed asap */
	PG_swapbacked,		/* Page is backed by RAM/swap */
	PG_unevictable,		/* Page is "unevictable"  */
#ifdef CONFIG_MMU
	PG_mlocked,		/* Page is vma mlocked */
#endif
#ifdef CONFIG_ARCH_USES_PG_UNCACHED
	PG_uncached,		/* Page has been mapped as uncached */
#endif
#ifdef CONFIG_MEMORY_FAILURE
	PG_hwpoison,		/* hardware poisoned page. Don't touch */
#endif
#ifdef CONFIG_TRANSPARENT_HUGEPAGE
	PG_compound_lock,
#endif
	__NR_PAGEFLAGS,

	/* Filesystems */
	PG_checked = PG_owner_priv_1,

	/* Two page bits are conscripted by FS-Cache to maintain local caching
	* state.  These bits are set on pages belonging to the netfs's inodes
	* when those inodes are being locally cached.
	*/
	PG_fscache = PG_private_2,	/* page backed by cache */

	/* XEN */
	PG_pinned = PG_owner_priv_1,
	PG_savepinned = PG_dirty,

	/* SLOB */
	PG_slob_free = PG_private,
};




static inline int test_bit_tlx(int nr, const volatile unsigned long *addr);
static inline void __set_bit_tlx(int nr, volatile unsigned long *addr);
static inline void __clear_bit_tlx(int nr, volatile unsigned long *addr);
void set_bit_tlx(int nr, volatile unsigned long *addr);


#define __CLEARPAGEFLAG(uname, lname)                                   \
static inline void __ClearPage##uname(struct page *page)                \
												{ __clear_bit_tlx(PG_##lname, &page->flags); }

#define CLEARPAGEFLAG(uname, lname)                                     \
static inline void ClearPage##uname(struct page *page)                  \
												{ __clear_bit_tlx(PG_##lname, &page->flags); }

#define SETPAGEFLAG(uname, lname)                                       \
static inline void SetPage##uname(struct page *page)                    \
												{ set_bit_tlx(PG_##lname, &page->flags); }

#define TESTPAGEFLAG(uname, lname)                                      \
static inline int Page##uname(const struct page *page)                  \
											{ return test_bit_tlx(PG_##lname, &page->flags); }

#define PAGEFLAG(uname, lname) TESTPAGEFLAG(uname, lname)               \
				SETPAGEFLAG(uname, lname) CLEARPAGEFLAG(uname, lname)


#define __SETPAGEFLAG(uname, lname)                                     \
static inline void __SetPage##uname(struct page *page)                  \
												{ __set_bit_tlx(PG_##lname, &page->flags); }

#define __PAGEFLAG(uname, lname) TESTPAGEFLAG(uname, lname)             \
				__SETPAGEFLAG(uname, lname)  __CLEARPAGEFLAG(uname, lname)



__PAGEFLAG(Head, head) CLEARPAGEFLAG(Head, head)
__PAGEFLAG(Tail, tail)
PAGEFLAG(Active, active) __CLEARPAGEFLAG(Active, active)
__PAGEFLAG(Slab, slab)
PAGEFLAG(Reserved, reserved) __CLEARPAGEFLAG(Reserved, reserved)




struct pidmap {
				atomic_t nr_free;
				void *page;
};




#ifdef CONFIG_ZONE_DMA
#define DMA_ZONE(xx) xx##_DMA,
#else
#define DMA_ZONE(xx)
#endif

#ifdef CONFIG_ZONE_DMA32
#define DMA32_ZONE(xx) xx##_DMA32,
#else
#define DMA32_ZONE(xx)
#endif

#ifdef CONFIG_HIGHMEM
#define HIGHMEM_ZONE(xx) , xx##_HIGH
#else
#define HIGHMEM_ZONE(xx)
#endif

#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx) , xx##_MOVABLE

enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
		FOR_ALL_ZONES(PGALLOC),
		PGFREE, PGACTIVATE, PGDEACTIVATE,
		PGFAULT, PGMAJFAULT,
		FOR_ALL_ZONES(PGREFILL),
		FOR_ALL_ZONES(PGSTEAL_KSWAPD),
		FOR_ALL_ZONES(PGSTEAL_DIRECT),
		FOR_ALL_ZONES(PGSCAN_KSWAPD),
		FOR_ALL_ZONES(PGSCAN_DIRECT),
		PGSCAN_DIRECT_THROTTLE,
#ifdef CONFIG_NUMA
		PGSCAN_ZONE_RECLAIM_FAILED,
#endif
		PGINODESTEAL, SLABS_SCANNED, KSWAPD_INODESTEAL,
		KSWAPD_LOW_WMARK_HIT_QUICKLY, KSWAPD_HIGH_WMARK_HIT_QUICKLY,
		PAGEOUTRUN, ALLOCSTALL, PGROTATED,
		DROP_PAGECACHE, DROP_SLAB,
#ifdef CONFIG_NUMA_BALANCING
		NUMA_PTE_UPDATES,
		NUMA_HUGE_PTE_UPDATES,
		NUMA_HINT_FAULTS,
		NUMA_HINT_FAULTS_LOCAL,
		NUMA_PAGE_MIGRATE,
#endif
#ifdef CONFIG_MIGRATION
		PGMIGRATE_SUCCESS, PGMIGRATE_FAIL,
#endif
#ifdef CONFIG_COMPACTION
		COMPACTMIGRATE_SCANNED, COMPACTFREE_SCANNED,
		COMPACTISOLATED,
		COMPACTSTALL, COMPACTFAIL, COMPACTSUCCESS,
#endif
#ifdef CONFIG_HUGETLB_PAGE
		HTLB_BUDDY_PGALLOC, HTLB_BUDDY_PGALLOC_FAIL,
#endif
		UNEVICTABLE_PGCULLED,	/* culled to noreclaim list */
		UNEVICTABLE_PGSCANNED,	/* scanned for reclaimability */
		UNEVICTABLE_PGRESCUED,	/* rescued from noreclaim list */
		UNEVICTABLE_PGMLOCKED,
		UNEVICTABLE_PGMUNLOCKED,
		UNEVICTABLE_PGCLEARED,	/* on COW, page truncate */
		UNEVICTABLE_PGSTRANDED,	/* unable to isolate on unlock */
#ifdef CONFIG_TRANSPARENT_HUGEPAGE
		THP_FAULT_ALLOC,
		THP_FAULT_FALLBACK,
		THP_COLLAPSE_ALLOC,
		THP_COLLAPSE_ALLOC_FAILED,
		THP_SPLIT,
		THP_ZERO_PAGE_ALLOC,
		THP_ZERO_PAGE_ALLOC_FAILED,
#endif
#ifdef CONFIG_DEBUG_TLBFLUSH
#ifdef CONFIG_SMP
		NR_TLB_REMOTE_FLUSH,	/* cpu tried to flush others' tlbs */
		NR_TLB_REMOTE_FLUSH_RECEIVED,/* cpu received ipi for flush */
#endif /* CONFIG_SMP */
		NR_TLB_LOCAL_FLUSH_ALL,
		NR_TLB_LOCAL_FLUSH_ONE,
#endif /* CONFIG_DEBUG_TLBFLUSH */
#ifdef CONFIG_DEBUG_VM_VMACACHE
		VMACACHE_FIND_CALLS,
		VMACACHE_FIND_HITS,
#endif
		NR_VM_EVENT_ITEMS
};








struct shrinker {
				unsigned long (*count_objects)(struct shrinker *,
																				struct shrink_control *sc);
				unsigned long (*scan_objects)(struct shrinker *,
																			struct shrink_control *sc);

				int seeks;      /* seeks to recreate an obj */
				long batch;     /* reclaim batch size, 0 = default */
				unsigned long flags;

					/* These are for internal use */
					struct list_head list;
				/* objs pending delete, per node */
				atomic_long_t *nr_deferred;
};

struct kref {
					atomic_t refcount;
};

#define BITS_PER_PAGE           (PAGE_SIZE * 8)
#define BITS_PER_PAGE_MASK      (BITS_PER_PAGE-1)
#define PIDMAP_ENTRIES          ((PID_MAX_LIMIT+BITS_PER_PAGE-1)/BITS_PER_PAGE)

struct pid_namespace {
				struct kref kref;
				struct pidmap pidmap[PIDMAP_ENTRIES];
				struct rcu_head rcu;
				int last_pid;
				unsigned int nr_hashed;
				struct task_struct *child_reaper;
				struct kmem_cache *pid_cachep;
				unsigned int level;
				struct pid_namespace *parent;
#ifdef CONFIG_PROC_FS
				struct vfsmount *proc_mnt;
				struct dentry *proc_self;
#endif
#ifdef CONFIG_BSD_PROCESS_ACCT
				struct bsd_acct_struct *bacct;
#endif
				struct user_namespace *user_ns;
				struct work_struct proc_work;
				kgid_t pid_gid;
				int hide_pid;
				int reboot;     /* group exit code if this pidns was rebooted */
				unsigned int proc_inum;
};


struct vm_operations_struct {
	void (*open)(struct vm_area_struct * area);
	void (*close)(struct vm_area_struct * area);
	int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);
	void (*map_pages)(struct vm_area_struct *vma, struct vm_fault *vmf);

	/* notification that a previously read-only page is about to become
	* writable, if an error is returned it will cause a SIGBUS */
	int (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);

	/* called by access_process_vm when get_user_pages() fails, typically
	* for use by special VMAs that can switch between memory and hardware
	*/
	int (*access)(struct vm_area_struct *vma, unsigned long addr,
					void *buf, int len, int write);

	/* Called by the /proc/PID/maps code to ask the vma whether it
	* has a special name.  Returning non-NULL will also cause this
	* vma to be dumped unconditionally. */
	const char *(*name)(struct vm_area_struct *vma);

#ifdef CONFIG_NUMA
	/*
	* set_policy() op must add a reference to any non-NULL @new mempolicy
	* to hold the policy upon return.  Caller should pass NULL @new to
	* remove a policy and fall back to surrounding context--i.e. do not
	* install a MPOL_DEFAULT policy, nor the task or system default
	* mempolicy.
	*/
	int (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);

	/*
	* get_policy() op must add reference [mpol_get()] to any policy at
	* (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure
	* in mm/mempolicy.c will do this automatically.
	* get_policy() must NOT add a ref if the policy at (vma,addr) is not
	* marked as MPOL_SHARED. vma policies are protected by the mmap_sem.
	* If no [shared/vma] mempolicy exists at the addr, get_policy() op
	* must return NULL--i.e., do not "fallback" to task or system default
	* policy.
	*/
	struct mempolicy *(*get_policy)(struct vm_area_struct *vma,
					unsigned long addr);
	int (*migrate)(struct vm_area_struct *vma, const nodemask_t *from,
		const nodemask_t *to, unsigned long flags);
#endif
	/* called by sys_remap_file_pages() to populate non-linear mapping */
	int (*remap_pages)(struct vm_area_struct *vma, unsigned long addr,
				unsigned long size, pgoff_t pgoff);
};




struct nsproxy {
				atomic_t count;
				struct uts_namespace *uts_ns;
				struct ipc_namespace *ipc_ns;
				struct mnt_namespace *mnt_ns;
				struct pid_namespace *pid_ns_for_children;
				struct net           *net_ns;
};


#define SECTIONS_WIDTH          SECTIONS_SHIFT
#define SECTIONS_PGOFF          ((sizeof(unsigned long)*8) - SECTIONS_WIDTH)



#ifdef NODE_NOT_IN_PAGE_FLAGS
#define ZONEID_SHIFT            (SECTIONS_SHIFT + ZONES_SHIFT)
#define ZONEID_PGOFF            ((SECTIONS_PGOFF < ZONES_PGOFF)? \
																								SECTIONS_PGOFF : ZONES_PGOFF)
#else
#define ZONEID_SHIFT            (NODES_SHIFT + ZONES_SHIFT)
#define ZONEID_PGOFF            ((NODES_PGOFF < ZONES_PGOFF)? \
																								NODES_PGOFF : ZONES_PGOFF)
#endif

#define ZONEID_PGSHIFT          (ZONEID_PGOFF * (ZONEID_SHIFT != 0))
#define ZONEID_MASK             ((1UL << ZONEID_SHIFT) - 1)


#define PAGE_BUDDY_MAPCOUNT_VALUE (-128)
#define PAGE_MAPPING_ANON       1
#define PAGE_BUDDY_MAPCOUNT_VALUE (-128)

#define IDR_BITS 8
#define IDR_SIZE (1 << IDR_BITS)

#define EI_NIDENT       16

typedef __u64   Elf64_Addr;
typedef __u16   Elf64_Half;
//typedef __s16   Elf64_SHalf;
typedef __u64   Elf64_Off;
typedef __u32   Elf64_Word;
typedef __u64   Elf64_Xword;

typedef struct elf64_phdr {
	Elf64_Word p_type;
	Elf64_Word p_flags;
	Elf64_Off p_offset;           /* Segment file offset */
	Elf64_Addr p_vaddr;           /* Segment virtual address */
	Elf64_Addr p_paddr;           /* Segment physical address */
	Elf64_Xword p_filesz;         /* Segment size in file */
	Elf64_Xword p_memsz;          /* Segment size in memory */
	Elf64_Xword p_align;          /* Segment alignment, file & memory */
} Elf64_Phdr;

typedef struct elf64_hdr {
	unsigned char e_ident[EI_NIDENT];     /* ELF "magic number" */
	Elf64_Half e_type;
	Elf64_Half e_machine;
	Elf64_Word e_version;
	Elf64_Addr e_entry;           /* Entry point virtual address */
	Elf64_Off e_phoff;            /* Program header table file offset */
	Elf64_Off e_shoff;            /* Section header table file offset */
	Elf64_Word e_flags;
	Elf64_Half e_ehsize;
	Elf64_Half e_phentsize;
	Elf64_Half e_phnum;
	Elf64_Half e_shentsize;
	Elf64_Half e_shnum;
	Elf64_Half e_shstrndx;
} Elf64_Ehdr;

#define elfhdr          elf64_hdr

struct attribute {
				const char              *name;
				umode_t                 mode;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
				bool                    ignore_lockdep:1;
				struct lock_class_key   *key;
				struct lock_class_key   skey;
#endif
};


struct bin_attribute {
				struct attribute        attr;
				size_t                  size;
				void                    *private;
				ssize_t (*read)(struct file *, struct kobject *, struct bin_attribute *,
												char *, loff_t, size_t);
				ssize_t (*write)(struct file *, struct kobject *, struct bin_attribute *,
												char *, loff_t, size_t);
				int (*mmap)(struct file *, struct kobject *, struct bin_attribute *attr,
										struct vm_area_struct *vma);
};


struct kobject {
				const char              *name;
				struct list_head        entry;
				struct kobject          *parent;
				struct kset             *kset;
				struct kobj_type        *ktype;
				struct kernfs_node      *sd;
				struct kref             kref;
#ifdef CONFIG_DEBUG_KOBJECT_RELEASE
				struct delayed_work     release;
#endif
				unsigned int state_initialized:1;
				unsigned int state_in_sysfs:1;
				unsigned int state_add_uevent_sent:1;
				unsigned int state_remove_uevent_sent:1;
				unsigned int uevent_suppress:1;
};

enum kernfs_node_type {
	KERNFS_DIR		= 0x0001,
	KERNFS_FILE		= 0x0002,
	KERNFS_LINK		= 0x0004,
};


enum kernfs_root_flag {
	/*
	* kernfs_nodes are created in the deactivated state and invisible.
	* They require explicit kernfs_activate() to become visible.  This
	* can be used to make related nodes become visible atomically
	* after all nodes are created successfully.
	*/
	KERNFS_ROOT_CREATE_DEACTIVATED		= 0x0001,

	/*
	* For regular flies, if the opener has CAP_DAC_OVERRIDE, open(2)
	* succeeds regardless of the RW permissions.  sysfs had an extra
	* layer of enforcement where open(2) fails with -EACCES regardless
	* of CAP_DAC_OVERRIDE if the permission doesn't have the
	* respective read or write access at all (none of S_IRUGO or
	* S_IWUGO) or the respective operation isn't implemented.  The
	* following flag enables that behavior.
	*/
	KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK	= 0x0002,
};

struct kset {
				struct list_head list;
				spinlock_t list_lock;
				struct kobject kobj;
				const struct kset_uevent_ops *uevent_ops;
};



struct static_key {
				atomic_t enabled;
};


struct idr_layer {
         int                     prefix; /* the ID prefix of this idr_layer */
         int                     layer;  /* distance from leaf */
         struct idr_layer __rcu  *ary[1<<IDR_BITS];
         int                     count;  /* When zero, we can release it */
         union {
                 /* A zero bit means "space here" */
                 DECLARE_BITMAP(bitmap, IDR_SIZE);
                 struct rcu_head         rcu_head;
         };
 };

struct idr {
				struct idr_layer __rcu  *hint;  /* the last layer allocated from */
				struct idr_layer __rcu  *top;
				int                     layers; /* only valid w/o concurrent changes */
				int                     cur;    /* current pos for cyclic allocation */
				spinlock_t              lock;
				int                     id_free_cnt;
				struct idr_layer        *id_free;
};

struct ida {
				struct idr              idr;
				struct ida_bitmap       *free_bitmap;
};




#define TVN_BITS (CONFIG_BASE_SMALL ? 4 : 6)
#define TVR_BITS (CONFIG_BASE_SMALL ? 6 : 8)
#define TVN_SIZE (1 << TVN_BITS)
#define TVR_SIZE (1 << TVR_BITS)


struct tvec {
	struct list_head vec[TVN_SIZE];
};

struct tvec_root {
	struct list_head vec[TVR_SIZE];
};

#define MIN_NICE	-20
# define HZ             CONFIG_HZ       /* Internal kernel timer frequency */

enum {
	/*
	* worker_pool flags
	*
	* A bound pool is either associated or disassociated with its CPU.
	* While associated (!DISASSOCIATED), all workers are bound to the
	* CPU and none has %WORKER_UNBOUND set and concurrency management
	* is in effect.
	*
	* While DISASSOCIATED, the cpu may be offline and all workers have
	* %WORKER_UNBOUND set and concurrency management disabled, and may
	* be executing on any CPU.  The pool behaves as an unbound one.
	*
	* Note that DISASSOCIATED should be flipped only while holding
	* attach_mutex to avoid changing binding state while
	* worker_attach_to_pool() is in progress.
	*/
	POOL_DISASSOCIATED	= 1 << 2,	/* cpu can't serve workers */

	/* worker flags */
	WORKER_DIE		= 1 << 1,	/* die die die */
	WORKER_IDLE		= 1 << 2,	/* is idle */
	WORKER_PREP		= 1 << 3,	/* preparing to run works */
	WORKER_CPU_INTENSIVE	= 1 << 6,	/* cpu intensive */
	WORKER_UNBOUND		= 1 << 7,	/* worker is unbound */
	WORKER_REBOUND		= 1 << 8,	/* worker was rebound */

	WORKER_NOT_RUNNING	= WORKER_PREP | WORKER_CPU_INTENSIVE |
					WORKER_UNBOUND | WORKER_REBOUND,

	NR_STD_WORKER_POOLS	= 2,		/* # standard pools per cpu */

	UNBOUND_POOL_HASH_ORDER	= 6,		/* hashed by pool->attrs */
	BUSY_WORKER_HASH_ORDER	= 6,		/* 64 pointers */

	MAX_IDLE_WORKERS_RATIO	= 4,		/* 1/4 of busy can be idle */
	IDLE_WORKER_TIMEOUT	= 300 * HZ,	/* keep idle ones for 5 mins */

	MAYDAY_INITIAL_TIMEOUT  = HZ / 100 >= 2 ? HZ / 100 : 2,
						/* call for help after 10ms
							(min two ticks) */
	MAYDAY_INTERVAL		= HZ / 10,	/* and then every 100ms */
	CREATE_COOLDOWN		= HZ,		/* time to breath after fail */

	/*
	* Rescue workers are used only on emergencies and shared by
	* all cpus.  Give MIN_NICE.
	*/
	RESCUER_NICE_LEVEL	= MIN_NICE,
	HIGHPRI_NICE_LEVEL	= MIN_NICE,

	WQ_NAME_LEN		= 24,
};


struct srcu_struct_array_tlx {
				unsigned long c[2];
				unsigned long seq[2];
};

struct kernfs_elem_dir_tlx {
	unsigned long		subdirs;
	/* children rbtree starts here and goes through kn->rb */
	struct rb_root		children;

	/*
	* The kernfs hierarchy this directory belongs to.  This fits
	* better directly in kernfs_node but is here to save space.
	*/
	struct kernfs_root	*root;
};

struct kernfs_elem_symlink_tlx {
	struct kernfs_node	*target_kn;
};

struct kernfs_elem_attr_tlx {
	const struct kernfs_ops	*ops;
	struct kernfs_open_node	*open;
	loff_t			size;
	struct kernfs_node	*notify_next;	/* for kernfs_notify() */
};


struct kernfs_node_tlx {
	atomic_t		count;
	atomic_t		active;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
	/*
	* Use kernfs_get_parent() and kernfs_name/path() instead of
	* accessing the following two fields directly.  If the node is
	* never moved to a different parent, it is safe to access the
	* parent directly.
	*/
	struct kernfs_node_tlx	*parent;
	const char		*name;

	struct rb_node		rb;

	const void		*ns;	/* namespace tag */
	unsigned int		hash;	/* ns + name hash */
	union {
		struct kernfs_elem_dir_tlx		dir;
		struct kernfs_elem_symlink_tlx	symlink;
		struct kernfs_elem_attr_tlx		attr;
	};

	void			*priv;

	unsigned short		flags;
	umode_t			mode;
	unsigned int		ino;
	struct kernfs_iattrs	*iattr;
};

struct kernfs_root_tlx {
				/* published fields */
				struct kernfs_node_tlx      *kn;
				unsigned int            flags;  /* KERNFS_ROOT_* flags */

	/* private fields, do not use outside kernfs proper */
				struct ida              ino_ida;
				struct kernfs_syscall_ops *syscall_ops;

				/* list of kernfs_super_info of this root, protected by kernfs_mutex */
				struct list_head        supers;

				wait_queue_head_t       deactivate_waitq;
};


struct hrtimer_cpu_base_tlx  {
				raw_spinlock_t                  lock;
				unsigned int                    active_bases;
				unsigned int                    clock_was_set;
#ifdef CONFIG_HIGH_RES_TIMERS
				ktime_t                         expires_next;
				int                             hres_active;
				int                             hang_detected;
				unsigned long                   nr_events;
				unsigned long                   nr_retries;
				unsigned long                   nr_hangs;
				ktime_t                         max_hang_time;
#endif
				struct hrtimer_clock_base       clock_base[HRTIMER_MAX_CLOCK_BASES];
};

struct kernel_param_tlx {
				const char *name;
				const struct kernel_param_ops *ops;
				u16 perm;
				s16 level;
				union {
								void *arg;
								const struct kparam_string *str;
								const struct kparam_array *arr;
				};
	};


struct plist_head_tlx {
				struct list_head node_list;
	};

struct llist_head_tlx {
				struct llist_node *first;
};


struct workqueue_attrs_tlx {
				int                     nice;           /* nice level */
				cpumask_var_t           cpumask;        /* allowed CPUs */
				bool                    no_numa;        /* disable NUMA affinity */
};



struct blocking_notifier_head_tlx {
				struct rw_semaphore rwsem;
				struct notifier_block __rcu *head;
};

struct thread_info_tlx {
				unsigned long           flags;          /* low level flags */
				mm_segment_t            addr_limit;     /* address limit */
				struct task_struct      *task;          /* main task structure */
				struct exec_domain      *exec_domain;   /* execution domain */
				struct restart_block    restart_block;
				int                     preempt_count;  /* 0 => preemptable, <0 => bug */
				int                     cpu;            /* cpu */
};


struct vm_event_state_tlx {
				unsigned long event[NR_VM_EVENT_ITEMS];
};


unsigned long
				cpu_bit_bitmap_tlx[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)];

#define ___GFP_HIGH             0x20u
#define __GFP_HIGH      ((__force gfp_t)___GFP_HIGH)    /* Should access emergency pools? */

#define while_each_thread(g, t) \
				while ((t = next_thread_tlx(t)) != g)

struct clocksource_tlx {
	/*
	* Hotpath data, fits in a single cache line when the
	* clocksource itself is cacheline aligned.
	*/
	cycle_t (*read)(struct clocksource_tlx *cs);
	cycle_t cycle_last;
	cycle_t mask;
	u32 mult;
	u32 shift;
	u64 max_idle_ns;
	u32 maxadj;
#ifdef CONFIG_ARCH_CLOCKSOURCE_DATA
	struct arch_clocksource_data archdata;
#endif

	const char *name;
	struct list_head list;
	int rating;
	int (*enable)(struct clocksource_tlx *cs);
	void (*disable)(struct clocksource_tlx *cs);
	unsigned long flags;
	void (*suspend)(struct clocksource_tlx *cs);
	void (*resume)(struct clocksource_tlx *cs);

	/* private: */
#ifdef CONFIG_CLOCKSOURCE_WATCHDOG
	/* Watchdog related data, used by the framework */
	struct list_head wd_list;
	cycle_t cs_last;
	cycle_t wd_last;
#endif
	struct module *owner;
} ____cacheline_aligned;





struct idr_layer_tlx {
				int                     prefix; /* the ID prefix of this idr_layer_tlx */
				int                     layer;  /* distance from leaf */
				struct idr_layer_tlx __rcu  *ary[1<<IDR_BITS];
				int                     count;  /* When zero, we can release it */
				union {
								/* A zero bit means "space here" */
								DECLARE_BITMAP(bitmap, IDR_SIZE);
								struct rcu_head         rcu_head;
				};
};


struct pcpu_group_info_tlx {
				int                     nr_units;       /* aligned # of units */
				unsigned long           base_offset;    /* base address offset */
				unsigned int            *cpu_map;       /* unit->cpu map, empty
																									* entries contain NR_CPUS */
};

struct pcpu_alloc_info_tlx {
					size_t                  static_size;
					size_t                  reserved_size;
					size_t                  dyn_size;
					size_t                  unit_size;
					size_t                  atom_size;
					size_t                  alloc_size;
					size_t                  __ai_size;      /* internal, don't use */
					int                     nr_groups;      /* 0 if grouping unnecessary */
					struct pcpu_group_info_tlx  groups[];
	};

struct static_key_deferred {
				struct static_key key;
				unsigned long timeout;
				struct delayed_work work;
};




#define UID_GID_MAP_MAX_EXTENTS 5

struct uid_gid_map {
				u32 nr_extents;
				struct uid_gid_extent {
								u32 first;
								u32 lower_first;
								u32 count;
				} extent[UID_GID_MAP_MAX_EXTENTS];
};


struct user_namespace {
				struct uid_gid_map      uid_map;
				struct uid_gid_map      gid_map;
				struct uid_gid_map      projid_map;
				atomic_t                count;
				struct user_namespace   *parent;
				int                     level;
				kuid_t                  owner;
				kgid_t                  group;
				unsigned int            proc_inum;
#ifdef CONFIG_PERSISTENT_KEYRINGS
				struct key              *persistent_keyring_register;
				struct rw_semaphore     persistent_keyring_register_sem;
#endif
};




struct worker_pool {
	spinlock_t		lock;
	int			cpu;
	int			node;
	int			id;
	unsigned int		flags;

	struct list_head	worklist;
	int			nr_workers;
	int			nr_idle;

	struct list_head	idle_list;
	struct timer_list	idle_timer;
	struct timer_list	mayday_timer;
	struct hlist_head busy_hash[1 << (BUSY_WORKER_HASH_ORDER)];
	struct mutex		manager_arb;
	struct mutex		attach_mutex;
	struct list_head	workers;
	struct completion	*detach_completion;

	struct ida		worker_ida;

	struct workqueue_attrs_tlx	*attrs;
	struct hlist_node	hash_node;
	int			refcnt;
	atomic_t		nr_running ____cacheline_aligned_in_smp;
	struct rcu_head		rcu;
} ____cacheline_aligned_in_smp;




struct tvec_base {
	spinlock_t lock;
	struct timer_list *running_timer;
	unsigned long timer_jiffies_tlx;
	unsigned long next_timer;
	unsigned long active_timers;
	unsigned long all_timers;
	struct tvec_root tv1;
	struct tvec tv2;
	struct tvec tv3;
	struct tvec tv4;
	struct tvec tv5;
} ____cacheline_aligned;



struct kstat_tlx {
	u64		ino;
	dev_t		dev;
	umode_t		mode;
	unsigned int	nlink;
	kuid_t		uid;
	kgid_t		gid;
	dev_t		rdev;
	loff_t		size;
	struct timespec  atime;
	struct timespec	mtime;
	struct timespec	ctime;
	unsigned long	blksize;
	unsigned long long	blocks;
};




struct wait_opts {
	enum pid_type		wo_type;
	int			wo_flags;
	struct pid		*wo_pid;

	struct siginfo __user	*wo_info;
	int __user		*wo_stat;
	struct rusage __user	*wo_rusage;

	wait_queue_t		child_wait;
	int			notask_error;
};


typedef struct {
         struct seqcount seqcount;
         spinlock_t lock;
} seqlock_t_tlx;


#define rwlock_acquire_read(l, s, t, i)         lock_acquire_shared_recursive(l, s, t, NULL, i)


void do_raw_read_lock_tlx(rwlock_t *lock)
{
//         arch_read_lock(&lock->raw_lock);
//					static inline void arch_read_lock(
				arch_rwlock_t *rw = &lock->raw_lock;

       unsigned int tmp, tmp2;

         asm volatile(
         "       sevl\n"
         "1:     wfe\n"
         "2:     ldaxr   %w0, %2\n"
         "       add     %w0, %w0, #1\n"
         "       tbnz    %w0, #31, 1b\n"
        "       stxr    %w1, %w0, %2\n"
         "       cbnz    %w1, 2b\n"
         : "=&r" (tmp), "=&r" (tmp2), "+Q" (rw->lock)
         :
         : "memory");

}


static inline int arch_read_trylock_tlx(arch_rwlock_t *lock)
 {

         return 0;
 }

# define do_raw_read_trylock(rwlock)    arch_read_trylock_tlx(&(rwlock)->raw_lock)



static inline void __raw_read_lock_tlx(rwlock_t *lock);

void  _raw_read_lock_tlx(rwlock_t *lock)
 {
         __raw_read_lock_tlx(lock);
 }


#define round_up(x, y) ((((x)-1) | __round_mask(x, y))+1)
#define roundup_pow_of_two(n)                   \
 (                                               \
         __builtin_constant_p(n) ? (             \
                 (n == 1) ? 1 :                  \
                 (1UL << (ilog2((n) - 1) + 1))   \
                                    ) :          \
         __roundup_pow_of_two_tlx(n)                 \
  )


#define SIGURG          23
#define SIGWINCH        28
#define SIGQUEUE_PREALLOC       1
#define VM_NONE         0x00000000

#if SIGRTMIN > BITS_PER_LONG
#define rt_sigmask(sig) (1ULL << ((sig)-1))
#else
#define rt_sigmask(sig) sigmask(sig)
#endif

#define siginmask(sig, mask) (rt_sigmask(sig) & (mask))

#define elf_phdr        elf64_phdr



#define BIT_MASK(nr)            (1UL << ((nr) % BITS_PER_LONG))
#define BIT_WORD(nr)            ((nr) / BITS_PER_LONG)

#define __lockfunc __attribute__((section(".spinlock.text")))

#define _RET_IP_                (unsigned long)__builtin_return_address(0)


static inline void queue_write_lock_tlx(struct qrwlock *lock);
#define arch_write_lock(l)      queue_write_lock_tlx(l)
# define do_raw_write_lock(rwlock)      do {__acquire(lock); \
                        arch_write_lock(&(rwlock)->raw_lock); } while (0)


#define LOCK_CONTENDED(_lock, try, lock) \
         lock(_lock)

#define preempt_disable()                       barrier()

#define lock_acquire(l, s, t, r, c, n, i)      do { } while (0)

static inline void __raw_read_lock_tlx(rwlock_t *lock)
{
		preempt_disable();
//		rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
		lock_acquire(&lock->dep_map, 0, 0, 2, 1, NULL, _RET_IP_);
		LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock_tlx);

}

#define ENODEV          19      /* No such device */
#define EINTR            4      /* Interrupted system call */
int printk_tlx(const char *fmt, ...) {};

#define KERN_SOH        "\001"          /* ASCII Start Of Header */
#define KERN_INFO       KERN_SOH "6"    /* informational */

#define __ref            __section(.ref.text) noinline
#define __init          __section(.init.text) __cold notrace
#define __initdata      __section(.init.data)

#define S_IXGRP 00010
#define S_ISUID  0004000
#define S_ISGID  0002000

#ifdef CONFIG_ARM64_64K_PAGES
#define PAGE_SHIFT              16
#else
#define PAGE_SHIFT              12
#endif
#define PAGE_SIZE               (_AC(1,UL) << PAGE_SHIFT)
#define PAGE_MASK               (~(PAGE_SIZE-1))

#define local_irq_disable()     do { raw_local_irq_disable(); } while (0)





#define rwlock_acquire(l, s, t, i)              lock_acquire_exclusive(l, s, t, NULL, i)


static inline void arch_local_irq_disable_tlx(void)
{
				asm volatile(
								"msr    daifset, #2             // arch_local_irq_disable"
								:
								:
								: "memory");
}


#define raw_local_irq_disable()         arch_local_irq_disable_tlx()
#define lock_acquire_exclusive(l, s, t, n, i)           lock_acquire(l, s, t, 0, 1, n, i)
#define arch_read_lock(l)       queue_read_lock_tlx(l)


#define smp_mb()        barrier()
#define _QW_LOCKED      0xff            /* A writer holds the lock */
#define _NSIG           64



int overflowuid_tlx;

#define WEXITED         0x00000004
//  8 #define WCONTINUED      0x00000008
#define WNOWAIT         0x01000000      /* Don't reap, just poll status.*/
#define get_task_struct(tsk) do { atomic_inc_tlx(&(tsk)->usage); } while(0)

static inline void arch_read_unlock_tlx(arch_rwlock_t *rw)
{
				unsigned int tmp, tmp2;

				asm volatile(
				"1:     ldxr    %w0, %2\n"
				"       sub     %w0, %w0, #1\n"
				"       stlxr   %w1, %w0, %2\n"
				"       cbnz    %w1, 1b\n"
				: "=&r" (tmp), "=&r" (tmp2), "+Q" (rw->lock)
				:
				: "memory");
}

#define do_raw_read_unlock_tlx(rwlock)     do {arch_read_unlock_tlx(&(rwlock)->raw_lock); \
																					__release(lock); } while (0)
# define lock_release_tlx(l, n, i)                  do { } while (0)
#define rwlock_release_tlx(l, n, i)                 lock_release_tlx(l, n, i)
#define preempt_enable()                        barrier()
#define lock_acquire(l, s, t, r, c, n, i)      do { } while (0)


static inline void __raw_read_unlock_tlx(rwlock_t *lock)
{
				rwlock_release_tlx(&lock->dep_map, 1, _RET_IP_);
				do_raw_read_unlock_tlx(lock);
				preempt_enable();
}

void __lockfunc _raw_read_unlock_tlx(rwlock_t *lock)
{
				__raw_read_unlock_tlx(lock);
}

#define read_unlock(lock)               _raw_read_unlock_tlx(lock)

#define EXIT_DEAD               16
#define EXIT_ZOMBIE             32
#define EXIT_TRACE              (EXIT_ZOMBIE | EXIT_DEAD)



cputime_t task_gtime_tlx(struct task_struct *t) {};

#define RUSAGE_BOTH     (-2)            /* sys_wait4() uses this */
#define SIGNAL_GROUP_EXIT       0x00000004 /* group exit in progress */



#define _raw_write_unlock_irq(lock) __raw_write_unlock_irq_tlx(lock)

//#define _raw_write_lock_irq_tlx(lock) __raw_write_lock_irq_tlx(lock)





static inline void __raw_write_lock_irq_tlx(rwlock_t *lock)
{
				local_irq_disable();
				preempt_disable();
				rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
				LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);
}

static void __lockfunc _raw_write_lock_irq_tlx(rwlock_t *lock)
{
				__raw_write_lock_irq_tlx(lock);
}



#define write_lock_irq(lock)            _raw_write_lock_irq_tlx(lock)
#define JOBCTL_LISTENING_BIT    22      /* ptracer is listening for events */
#define __TASK_TRACED           8
#define __TASK_STOPPED          4

#define si_pid		_sifields._kill._pid
#define si_uid		_sifields._kill._uid
#define si_tid		_sifields._timer._tid
#define si_overrun	_sifields._timer._overrun
#define si_sys_private  _sifields._timer._sys_private
#define si_status	_sifields._sigchld._status
#define si_utime	_sifields._sigchld._utime
#define si_stime	_sifields._sigchld._stime
#define si_value	_sifields._rt._sigval
#define si_int		_sifields._rt._sigval.sival_int
#define si_ptr		_sifields._rt._sigval.sival_ptr
#define si_addr		_sifields._sigfault._addr
#ifdef __ARCH_SI_TRAPNO
#define si_trapno	_sifields._sigfault._trapno
#endif
#define si_addr_lsb	_sifields._sigfault._addr_lsb
#define si_band		_sifields._sigpoll._band
#define si_fd		_sifields._sigpoll._fd
#ifdef __ARCH_SIGSYS
#define si_call_addr	_sifields._sigsys._call_addr
#define si_syscall	_sifields._sigsys._syscall
#define si_arch		_sifields._sigsys._arch
#endif

#define __ARCH_SI_CLOCK_T __kernel_clock_t





#define task_cred_xxx(task, xxx)                        \
({                                                      \
				__typeof__(((struct cred *)NULL)->xxx) ___val;  \
				rcu_read_lock_tlx();                                \
				___val = __task_cred((task))->xxx;              \
				rcu_read_unlock_tlx();                              \
				___val;                                         \
})


#define __SI_CHLD       (4 << 16)
#define CLD_STOPPED     (__SI_CHLD|5)   /* child has stopped */
#define CLD_TRAPPED     (__SI_CHLD|4)   /* traced child has trapped */
#define CLD_EXITED      (__SI_CHLD|1)   /* child has exited */
#define CLD_KILLED      (__SI_CHLD|2)   /* child was killed */
#define CLD_DUMPED      (__SI_CHLD|3)   /* child terminated abnormally */



rwlock_t tasklist_lock_tlx;
# define do_raw_write_trylock(rwlock)   arch_write_trylock(&(rwlock)->raw_lock)


#define key_put(k)                      do { } while(0)

struct timeval {
         __kernel_time_t         tv_sec;         /* seconds */
         __kernel_suseconds_t    tv_usec;        /* microseconds */
};

struct  rusage {
         struct timeval ru_utime;        /* user time used */
         struct timeval ru_stime;        /* system time used */
         __kernel_long_t ru_maxrss;      /* maximum resident set size */
         __kernel_long_t ru_ixrss;       /* integral shared memory size */
         __kernel_long_t ru_idrss;       /* integral unshared data size */
         __kernel_long_t ru_isrss;       /* integral unshared stack size */
         __kernel_long_t ru_minflt;      /* page reclaims */
         __kernel_long_t ru_majflt;      /* page faults */
         __kernel_long_t ru_nswap;       /* swaps */
         __kernel_long_t ru_inblock;     /* block input operations */
         __kernel_long_t ru_oublock;     /* block output operations */
         __kernel_long_t ru_msgsnd;      /* messages sent */
         __kernel_long_t ru_msgrcv;      /* messages received */
         __kernel_long_t ru_nsignals;    /* signals received */
         __kernel_long_t ru_nvcsw;       /* voluntary context switches */
         __kernel_long_t ru_nivcsw;      /* involuntary " */
 };

#define EFAULT          14      /* Bad address */
static inline unsigned long __must_check copy_to_user(void __user *to, const void *from, unsigned long n);
int getrusage_tlx(struct task_struct *p, int who, struct rusage __user *ru)
{
       struct rusage r;
//         k_getrusage(p, who, &r);
       return copy_to_user(ru, &r, sizeof(r)) ? -EFAULT : 0;
}




#define rcu_dereference(p) rcu_dereference_check(p, 0)
#define __WNOTHREAD     0x20000000      /* Don't wait on children of other threads in this group */


#define WUNTRACED       0x00000002
#define WCONTINUED      0x00000008

#define write_unlock_irq(lock)          _raw_write_unlock_irq(lock)
#define PG_head_mask ((1L << PG_compound))
#define PG_head_tail_mask ((1L << PG_compound) | (1L << PG_reclaim))



#define SIGNAL_STOP_STOPPED     0x00000001 /* job control stop in effect */
#define JOBCTL_LISTENING        (1 << JOBCTL_LISTENING_BIT)
#define task_is_stopped_or_traced(task) \
                         ((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)

#define __WALL          0x40000000      /* Wait on all children, regardless of type */
#define __WCLONE        0x80000000      /* Wait only on non-SIGCHLD children */
#define SIGCHLD         17


#define current_cred_xxx(xxx)                   \
 ({                                              \
         current_cred()->xxx;                    \
 })

#define current_euid()          (current_cred_xxx(euid))
#define current_egid()          (current_cred_xxx(egid))


#define VM_STACK_INCOMPLETE_SETUP       (VM_RAND_READ | VM_SEQ_READ)

#ifdef CONFIG_STACK_GROWSUP
 #define VM_STACK_FLAGS  (VM_GROWSUP | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
#else
 #define VM_STACK_FLAGS  (VM_GROWSDOWN | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)
#endif

#ifdef CONFIG_MEM_SOFT_DIRTY
 # define VM_SOFTDIRTY   0x08000000      /* Not soft dirty clean area */
#else
 # define VM_SOFTDIRTY   0
 #endif

#define STACK_TOP_MAX           TASK_SIZE_64
#define PSR_MODE_EL0t   0x00000000


#define MMF_DUMP_FILTER_DEFAULT \
         ((1 << MMF_DUMP_ANON_PRIVATE) | (1 << MMF_DUMP_ANON_SHARED) |\
          (1 << MMF_DUMP_HUGETLB_PRIVATE) | MMF_DUMP_MASK_DEFAULT_ELF)

#define VM_INIT_DEF_MASK        VM_NOHUGEPAGE

#define MMF_INIT_MASK           (MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK)



#define PF_NPROC_EXCEEDED 0x00001000    /* set_user noticed that RLIMIT_NPROC was exceeded */

#define RLIM_NLIMITS            16

#define DIV_ROUND_UP(n,d) (((n) + (d) - 1) / (d))


static inline void arch_write_unlock_tlx(arch_rwlock_t *rw)
{
				asm volatile(
				"       stlr    %w1, %0\n"
				: "=Q" (rw->lock) : "r" (0) : "memory");
}


# define do_raw_write_unlock(rwlock)    do {arch_write_unlock_tlx(&(rwlock)->raw_lock); __release(lock); } while (0)

#define local_irq_enable()      do { raw_local_irq_enable(); } while (0)


struct mem_section_tlx *__nr_to_section_tlx(unsigned long nr);

/* Task command name length */

static inline void spin_lock_tlx(spinlock_t *lock);
static inline void spin_unlock_tlx(spinlock_t *lock);


#define _raw_spin_lock(lock) __raw_spin_lock_tlx(lock)
#define _raw_spin_unlock(lock) __raw_spin_unlock_tlx(lock)

#define raw_spin_lock(lock)     _raw_spin_lock(lock)
#define raw_spin_unlock(lock)           _raw_spin_unlock(lock)


#define preempt_enable()                        barrier()



# define seqcount_lockdep_reader_access(x)
#define smp_rmb()       barrier()
#define cpu_relax()                     barrier()
#define spin_acquire(l, s, t, i)                lock_acquire_exclusive(l, s, t, NULL, i)
#define spin_release(l, n, i)                   lock_release_tlx(l, n, i)

# define arch_spin_trylock(lock)        ({ barrier(); (void)(lock); 1; })

#define THREAD_SIZE             16384

static inline int atomic_sub_return_tlx(int i, atomic_t *v);
#define atomic_dec_and_test(v)  (atomic_sub_return_tlx(1, v) == 0)
#define delay_group_leader(p) \
                 (thread_group_leader_tlx(p) && !thread_group_empty_tlx(p))



#define CLD_CONTINUED   (__SI_CHLD|6)   /* stopped child has continued */
#define SIGCONT         18

#define smp_mb()        barrier()

void free_uid_tlx(struct user_struct * us) {};

#define SIGRTMIN        32


#define SIG_KERNEL_IGNORE_MASK (\
         rt_sigmask(SIGCONT)   |  rt_sigmask(SIGCHLD)   | \
         rt_sigmask(SIGWINCH)  |  rt_sigmask(SIGURG)    )

#define local_irq_restore(flags) do { raw_local_irq_restore(flags); } while (0)

#define __virt_to_phys(x)       (((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET))


#define SECTIONS_MASK           ((1UL << SECTIONS_WIDTH) - 1)
#define SECTIONS_PGSHIFT        (SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))
unsigned long page_to_section_tlx(const struct page *page)
 {
         return (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;
 }


struct page *__section_mem_map_addr_tlx(struct mem_section_tlx *section);

#define __page_to_pfn(pg)                                       \
({      const struct page *__pg = (pg);                         \
         int __sec = page_to_section_tlx(__pg);                      \
         (unsigned long)(__pg - __section_mem_map_addr_tlx(__nr_to_section_tlx(__sec))); \
})




#define __pa(x)                 __virt_to_phys((unsigned long)(x))
#define pfn_to_page __pfn_to_page
#define SECTION_SIZE_BITS	30
#define PFN_SECTION_SHIFT       (SECTION_SIZE_BITS - PAGE_SHIFT)
#define pfn_to_section_nr(pfn) ((pfn) >> PFN_SECTION_SHIFT)
static inline struct mem_section_tlx *__pfn_to_section_tlx(unsigned long pfn)
{
         return __nr_to_section_tlx(pfn_to_section_nr(pfn));
}




#define __pfn_to_page(pfn)                              \
 ({      unsigned long __pfn = (pfn);                    \
         struct mem_section_tlx *__sec = __pfn_to_section_tlx(__pfn);    \
         __section_mem_map_addr_tlx(__sec) + __pfn;          \
 })

#define PHYS_OFFSET             ({ memstart_addr_tlx; })
#define VA_BITS                 (39)
#define UL(x) _AC(x, UL)
#define SZ_64K                          0x00010000
#define VMALLOC_END             (PAGE_OFFSET - UL(0x400000000) - SZ_64K)
#define vmemmap                 ((struct page *)(VMALLOC_END + SZ_64K))
#define PAGE_OFFSET             (UL(0xffffffffffffffff) << (VA_BITS - 1))

#define typecheck(type,x) \
({      type __dummy; \
         typeof(x) __dummy2; \
         (void)(&__dummy == &__dummy2); \
         1; \
})
 #define raw_local_irq_save(flags)                       \
          do {                                            \
                  typecheck(unsigned long, flags);        \
                  flags = arch_local_irq_save_tlx();          \
           } while (0)

 #define raw_local_irq_restore(flags)                    \
          do {                                            \
                  typecheck(unsigned long, flags);        \
                  arch_local_irq_restore_tlx(flags);          \
           } while (0)


phys_addr_t              memstart_addr_tlx = 0;

#define _raw_spin_unlock_irqrestore(lock, flags) __raw_spin_unlock_irqrestore_tlx(lock, flags)


#define atomic64_inc(v)                 atomic64_add_tlx(1LL, (v))
#define virt_to_page(kaddr)     pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
#define raw_spin_unlock_irqrestore(lock, flags)         \
         do {                                                    \
                 typecheck(unsigned long, flags);                \
                 _raw_spin_unlock_irqrestore(lock, flags);       \
         } while (0)

#define NODES_PGSHIFT           (NODES_PGOFF * (NODES_WIDTH != 0))
#define NODES_MASK              ((1UL << NODES_WIDTH) - 1)
#define rcu_dereference_sparse(p, space)
#define smp_read_barrier_depends()      do { } while(0)
#define __WARN()

#define WARN_ON(condition) ({                                           \
          int __ret_warn_on = !!(condition);                              \
          if (unlikely(__ret_warn_on))                                    \
                  __WARN();                                               \
          unlikely(__ret_warn_on);                                        \
  })

#define lockdep_assert_held(l)  do {                            \
                 WARN_ON(debug_locks_tlx);    \
         } while (0)

#define SHIFT_PERCPU_PTR(__p, __offset) ({                              \
         __verify_pcpu_ptr((__p));                                       \
         RELOC_HIDE((typeof(*(__p)) __kernel __force *)(__p), (__offset)); \
})

struct thread_info_tlx *current_thread_info_tlx_tlx(void);

#define raw_smp_processor_id() (current_thread_info_tlx_tlx()->cpu)
#define __my_cpu_offset per_cpu_offset(raw_smp_processor_id())
#define my_cpu_offset __my_cpu_offset
#define raw_cpu_ptr(ptr) SHIFT_PERCPU_PTR(ptr, __my_cpu_offset)
#define this_cpu_ptr(ptr) SHIFT_PERCPU_PTR(ptr, my_cpu_offset)

#define __verify_pcpu_ptr(ptr)  do {                                    \
          const void __percpu *__vpp_verify = (typeof((ptr) + 0))NULL;    \
          (void)__vpp_verify;                                             \
  } while (0)

#define __pcpu_double_call_return_bool(stem, pcp1, pcp2, ...)           \
 ({                                                                      \
         bool pdcrb_ret__;                                               \
         __verify_pcpu_ptr(&pcp1);                                       \
         switch(sizeof(pcp1)) {                                          \
         case 1: pdcrb_ret__ = stem##1(pcp1, pcp2, __VA_ARGS__); break;  \
         case 2: pdcrb_ret__ = stem##2(pcp1, pcp2, __VA_ARGS__); break;  \
         case 4: pdcrb_ret__ = stem##4(pcp1, pcp2, __VA_ARGS__); break;  \
         case 8: pdcrb_ret__ = stem##8(pcp1, pcp2, __VA_ARGS__); break;  \
         default:                                                        \
                 __bad_size_call_parameter_tlx(); break;                     \
         }                                                               \
         pdcrb_ret__;                                                    \
 })

#define local_irq_save(flags)                                   \
         do {                                                    \
                 raw_local_irq_save(flags);                      \
         } while (0)

#define raw_cpu_generic_to_op(pcp, val, op)                             \
do {                                                                    \
         *raw_cpu_ptr(&(pcp)) op val;                                    \
} while (0)

#  define raw_cpu_read_1(pcp)   (*raw_cpu_ptr(&(pcp)))
#  define raw_cpu_read_2(pcp)   (*raw_cpu_ptr(&(pcp)))
#  define raw_cpu_read_4(pcp)   (*raw_cpu_ptr(&(pcp)))
#  define raw_cpu_read_8(pcp)   (*raw_cpu_ptr(&(pcp)))
#  define raw_cpu_write_1(pcp, val)     raw_cpu_generic_to_op((pcp), (val), =)
#  define raw_cpu_write_2(pcp, val)     raw_cpu_generic_to_op((pcp), (val), =)
#  define raw_cpu_write_4(pcp, val)     raw_cpu_generic_to_op((pcp), (val), =)
#  define raw_cpu_write_8(pcp, val)     raw_cpu_generic_to_op((pcp), (val), =)

#define __pcpu_size_call_return(stem, variable)                         \
 ({      typeof(variable) pscr_ret__;                                    \
         __verify_pcpu_ptr(&(variable));                                 \
         switch(sizeof(variable)) {                                      \
         case 1: pscr_ret__ = stem##1(variable);break;                   \
         case 2: pscr_ret__ = stem##2(variable);break;                   \
         case 4: pscr_ret__ = stem##4(variable);break;                   \
         case 8: pscr_ret__ = stem##8(variable);break;                   \
         default:                                                        \
                 __bad_size_call_parameter_tlx();break;                      \
         }                                                               \
         pscr_ret__;                                                     \
 })

#define __pcpu_size_call(stem, variable, ...)                           \
 do {                                                                    \
         __verify_pcpu_ptr(&(variable));                                 \
         switch(sizeof(variable)) {                                      \
                 case 1: stem##1(variable, __VA_ARGS__);break;           \
                 case 2: stem##2(variable, __VA_ARGS__);break;           \
                 case 4: stem##4(variable, __VA_ARGS__);break;           \
                 case 8: stem##8(variable, __VA_ARGS__);break;           \
                 default:                                                \
                         __bad_size_call_parameter_tlx();break;              \
         }                                                               \
 } while (0)

# define raw_cpu_read(pcp)      __pcpu_size_call_return(raw_cpu_read_, (pcp))
# define raw_cpu_write(pcp, val)        __pcpu_size_call(raw_cpu_write_, (pcp), (val))

#define raw_cpu_generic_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2)  \
 ({                                                                      \
         int __ret = 0;                                                  \
         if (raw_cpu_read(pcp1) == (oval1) &&                            \
                          raw_cpu_read(pcp2)  == (oval2)) {              \
                 raw_cpu_write(pcp1, (nval1));                           \
                 raw_cpu_write(pcp2, (nval2));                           \
                 __ret = 1;                                              \
         }                                                               \
         (__ret);                                                        \
 })


#define _this_cpu_generic_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2)        \
 ({                                                                      \
         int ret__;                                                      \
         unsigned long flags;                                            \
         raw_local_irq_save(flags);                                      \
         ret__ = raw_cpu_generic_cmpxchg_double(pcp1, pcp2,              \
                         oval1, oval2, nval1, nval2);                    \
         raw_local_irq_restore(flags);                                   \
         ret__;                                                          \
 })

#  define this_cpu_cmpxchg_double_1(pcp1, pcp2, oval1, oval2, nval1, nval2)     \
         _this_cpu_generic_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2)
#  define this_cpu_cmpxchg_double_2(pcp1, pcp2, oval1, oval2, nval1, nval2)     \
         _this_cpu_generic_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2)
#  define this_cpu_cmpxchg_double_4(pcp1, pcp2, oval1, oval2, nval1, nval2)     \
         _this_cpu_generic_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2)
#  define this_cpu_cmpxchg_double_8(pcp1, pcp2, oval1, oval2, nval1, nval2)     \
         _this_cpu_generic_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2)

void __bad_size_call_parameter_tlx(void) {};

# define this_cpu_cmpxchg_double(pcp1, pcp2, oval1, oval2, nval1, nval2)        \
       __pcpu_double_call_return_bool(this_cpu_cmpxchg_double_, (pcp1), (pcp2), (oval1), (oval2), (nval1), (nval2))

#define atomic64_dec(v)                 atomic64_sub_tlx(1LL, (v))

#define __phys_to_virt_tlx(x)       ((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
#define __virt_to_phys_tlx(x)       (((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET))
static inline void *phys_to_virt_tlx(phys_addr_t x)
 {
         return (void *)(__phys_to_virt_tlx(x));
 }

#define __va(x)                 ((void *)__phys_to_virt_tlx((phys_addr_t)(x)))
#define PFN_PHYS(x)     ((phys_addr_t)(x) << PAGE_SHIFT)

#define page_to_pfn __page_to_pfn
void *lowmem_page_address_tlx(const struct page *page);

#define page_address(page) lowmem_page_address_tlx(page)
#define do_raw_spin_lock_flags(lock, flags) do_raw_spin_lock_tlx(lock)





#define raw_spin_lock_irqsave(lock, flags)                      \
         do {                                            \
                 typecheck(unsigned long, flags);        \
                 flags = _raw_spin_lock_irqsave_tlx(lock);   \
        } while (0)

#define spin_lock_irqsave(lock, flags)                          \
 do {                                                            \
         raw_spin_lock_irqsave(spinlock_check_tlx(lock), flags);     \
 } while (0)

#define __rcu_dereference_check(p, c, space) \
 ({ \
         typeof(*p) *_________p1 = (typeof(*p) *__force)ACCESS_ONCE(p); \
         rcu_dereference_sparse(p, space); \
         smp_read_barrier_depends(); /* Dependency order vs. p above. */ \
         ((typeof(*p) __force __kernel *)(_________p1)); \
 })

#define rcu_dereference_check(p, c) \
         __rcu_dereference_check((p), rcu_read_lock_held_tlx() || (c), __rcu)

void call_rcu_sched_tlx(struct rcu_head *head,
                     void (*func)(struct rcu_head *rcu)) {};
#define call_rcu        call_rcu_sched_tlx








#define container_of(ptr, type, member) ({                      \
         const typeof( ((type *)0)->member ) *__mptr = (ptr);    \
         (type *)( (char *)__mptr - offsetof(type,member) );})


#define validate_creds(cred)                            \
 do {                                                    \
 } while(0)


void groups_free_tlx(struct group_info * gi) {};

#define put_group_info(group_info)                      \
 do {                                                    \
         if (atomic_dec_and_test(&(group_info)->usage))  \
                 groups_free_tlx(group_info);                \
  } while (0)
#define atomic_read(v)  (*(volatile int *)&(v)->counter)
#define atomic64_dec_return(v)          atomic64_sub_return_tlx(1LL, (v))
#define atomic64_dec_and_test(v)        (atomic64_dec_return((v)) == 0)



#define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000)
#define PID_MAX_LIMIT (CONFIG_BASE_SMALL ? PAGE_SIZE * 8 : \
          (sizeof(long) > 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT))


# define might_resched() do { } while (0)
# define might_sleep() do { might_resched(); } while (0)
#define S_IFCHR  0020000
# define this_cpu_dec(pcp)              this_cpu_sub((pcp), 1)
void kobject_put_tlx(struct kobject *kobj) {};
void module_put_tlx(struct module *module) {};


#define in_interrupt()          (irq_count())
#define PF_KTHREAD      0x00200000      /* I am a kernel thread */

static inline unsigned long __cmpxchg_mb_tlx(volatile void *ptr, unsigned long old,
																					unsigned long new, int size);
#define cmpxchg(ptr, o, n) \
({ \
         __typeof__(*(ptr)) __ret; \
         __ret = (__typeof__(*(ptr))) \
                 __cmpxchg_mb_tlx((ptr), (unsigned long)(o), (unsigned long)(n), \
                              sizeof(*(ptr))); \
         __ret; \
 })


#define __GFP_BITS_MASK ((__force gfp_t)((1 << __GFP_BITS_SHIFT) - 1))



#define get_order(n)                                            \
 (                                                               \
         __builtin_constant_p(n) ? (                             \
                 ((n) == 0UL) ? BITS_PER_LONG - PAGE_SHIFT :     \
                 (((n) < (1UL << PAGE_SHIFT)) ? 0 :              \
                  ilog2((n) - 1) - PAGE_SHIFT + 1)               \
         ) :                                                     \
         __get_order_tlx(n)                                          \
 )



#define __GFP_COMP      ((__force gfp_t)___GFP_COMP)    /* Add compound page metadata */
gfp_t gfp_allowed_mask_tlx;
#define ___GFP_WAIT             0x10u
#define __GFP_WAIT      ((__force gfp_t)___GFP_WAIT)    /* Can wait and reschedule? */
#define __GFP_HARDWALL   ((__force gfp_t)___GFP_HARDWALL) /* Enforce hardwall cpuset memory allocs */

#define ALLOC_WMARK_LOW         WMARK_LOW
#define ALLOC_CPUSET            0x40 /* check for correct cpuset */
#define ALLOC_FAIR              0x100 /* fair zone allocation */
#define ALLOC_NO_WATERMARKS     0x04 /* don't check watermarks at all */
#define ALLOC_WMARK_MASK        (ALLOC_NO_WATERMARKS-1)

#define __GFP_DMA       ((__force gfp_t)___GFP_DMA)
#define ___GFP_NOWARN           0x200u
#define ___GFP_REPEAT           0x400u
#define ___GFP_NOFAIL           0x800u
#define ___GFP_MEMALLOC         0x2000u
//#define ___GFP_COMP             0x4000u
//#define ___GFP_ZERO             0x8000u
#define ___GFP_NOMEMALLOC       0x10000u
#define ___GFP_HARDWALL         0x20000u
#define ___GFP_NORETRY          0x1000u
#define ___GFP_THISNODE         0x40000u
#define ___GFP_NOTRACK          0x200000u
#define ___GFP_MOVABLE          0x08u
#define ___GFP_HIGHMEM          0x02u
#define ___GFP_DMA              0x01u

#define __GFP_REPEAT    ((__force gfp_t)___GFP_REPEAT)  /* See above */
#define __GFP_MEMALLOC  ((__force gfp_t)___GFP_MEMALLOC)/* Allow access to emergency reserves */
#define __GFP_NOMEMALLOC ((__force gfp_t)___GFP_NOMEMALLOC) /* Don't use emergency reserves.
																													* This takes precedence over the
																													* __GFP_MEMALLOC flag if both are
																													* set
																													*/
#define __GFP_HARDWALL   ((__force gfp_t)___GFP_HARDWALL) /* Enforce hardwall cpuset memory allocs */
#define __GFP_THISNODE  ((__force gfp_t)___GFP_THISNODE)/* No fallback, no policies */
#define __GFP_MOVABLE   ((__force gfp_t)___GFP_MOVABLE)  /* Page is movable */
#define __GFP_HIGHMEM   ((__force gfp_t)___GFP_HIGHMEM)
#define OPT_ZONE_DMA ZONE_NORMAL
#define OPT_ZONE_DMA32 ZONE_NORMAL
#define OPT_ZONE_HIGHMEM ZONE_NORMAL
#define ___GFP_WRITE            0x1000000u
#define __GFP_WRITE     ((__force gfp_t)___GFP_WRITE)   /* Allocator intends to dirty page */
#define ___GFP_DMA32            0x04u
#define __GFP_DMA32     ((__force gfp_t)___GFP_DMA32)
#define GFP_ZONEMASK    (__GFP_DMA|__GFP_HIGHMEM|__GFP_DMA32|__GFP_MOVABLE)
#define GFP_ZONE_TABLE ( \
         (ZONE_NORMAL << 0 * ZONES_SHIFT)                                      \
         | (OPT_ZONE_DMA << ___GFP_DMA * ZONES_SHIFT)                          \
         | (OPT_ZONE_HIGHMEM << ___GFP_HIGHMEM * ZONES_SHIFT)                  \
         | (OPT_ZONE_DMA32 << ___GFP_DMA32 * ZONES_SHIFT)                      \
         | (ZONE_NORMAL << ___GFP_MOVABLE * ZONES_SHIFT)                       \
         | (OPT_ZONE_DMA << (___GFP_MOVABLE | ___GFP_DMA) * ZONES_SHIFT)       \
         | (ZONE_MOVABLE << (___GFP_MOVABLE | ___GFP_HIGHMEM) * ZONES_SHIFT)   \
         | (OPT_ZONE_DMA32 << (___GFP_MOVABLE | ___GFP_DMA32) * ZONES_SHIFT)   \
)


enum node_states_tlx {
	N_POSSIBLE,		/* The node could become online at some point */
	N_ONLINE,		/* The node is online */
	N_NORMAL_MEMORY,	/* The node has regular memory */
#ifdef CONFIG_HIGHMEM
	N_HIGH_MEMORY,		/* The node has regular or high memory */
#else
	N_HIGH_MEMORY = N_NORMAL_MEMORY,
#endif
#ifdef CONFIG_MOVABLE_NODE
	N_MEMORY,		/* The node has memory(regular, high, movable) */
#else
	N_MEMORY = N_HIGH_MEMORY,
#endif
	N_CPU,		/* The node has one or more cpus */
	NR_node_states_tlx
};


#define cpuset_current_mems_allowed (node_states_tlx[N_MEMORY])
nodemask_t node_states_tlx[NR_node_states_tlx];



struct zoneref *next_zones_zonelist_tlx(struct zoneref *z,
                                         enum zone_type highest_zoneidx,
                                         nodemask_t *nodes,
                                         struct zone **zone)
{
         /*
          * Find the next suitable zone to use for the allocation.
          * Only filter based on nodemask if it's set
          */
     if (likely(nodes == NULL))
             while (z->zone_idx > highest_zoneidx)
                         z++;
         else
                 while (z->zone_idx > highest_zoneidx)
                         z++;

         *zone = z->zone;
         return z;
}

static inline struct zoneref *first_zones_zonelist_tlx(struct zonelist *zonelist,
                                         enum zone_type highest_zoneidx,
                                         nodemask_t *nodes,
                                         struct zone **zone)
{
       return next_zones_zonelist_tlx(zonelist->_zonerefs, highest_zoneidx, nodes,
                                                                 zone);
 }

#define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \
         for (z = first_zones_zonelist_tlx(zlist, highidx, nodemask, &zone); \
                 zone;                                                   \
                 z = next_zones_zonelist_tlx(++z, highidx, nodemask, &zone)) \





#define atomic_set(v,i) (((v)->counter) = (i))
extern void clear_page_tlx(void *to);
unsigned long __per_cpu_offset_tlx[NR_CPUS];
#define per_cpu_offset(x) (__per_cpu_offset_tlx[x])

#define ___GFP_COLD             0x100u
#define ___GFP_ZERO             0x8000u
#define __GFP_COLD      ((__force gfp_t)___GFP_COLD)    /* Cache-cold page required */
#define __GFP_ZERO      ((__force gfp_t)___GFP_ZERO)    /* Return zeroed page on success */

static inline void expand(struct zone *zone, struct page *page,
	int low, int high, struct free_area *area,
	int migratetype)
{
	unsigned long size = 1 << high;

	while (high > low) {
		area--;
		high--;
		size >>= 1;
		list_add(&page[size].lru, &area->free_list[migratetype]);
		area->nr_free++;
		(&page[size])->private = high;
	}
}

int fallbacks_tlx[MIGRATE_TYPES][4] = {
	[MIGRATE_UNMOVABLE]   = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE,     MIGRATE_RESERVE },
	[MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE,   MIGRATE_MOVABLE,     MIGRATE_RESERVE },
#ifdef CONFIG_CMA
	[MIGRATE_MOVABLE]     = { MIGRATE_CMA,         MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_RESERVE },
	[MIGRATE_CMA]         = { MIGRATE_RESERVE }, /* Never used */
#else
	[MIGRATE_MOVABLE]     = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE,   MIGRATE_RESERVE },
#endif
	[MIGRATE_RESERVE]     = { MIGRATE_RESERVE }, /* Never used */
#ifdef CONFIG_MEMORY_ISOLATION
	[MIGRATE_ISOLATE]     = { MIGRATE_RESERVE }, /* Never used */
#endif
};

#define pageblock_order         (MAX_ORDER-1)
#define pageblock_nr_pages      (1UL << pageblock_order)


int move_freepages_tlx(struct zone *zone,
			  struct page *start_page, struct page *end_page,
			  int migratetype)
{
	struct page *page;
	unsigned long order;
	int pages_moved = 0;
	for (page = start_page; page <= end_page;) {
		/* Make sure we are not inadvertently changing nodes */
		order = page->private;
		list_move(&page->lru,
			  &zone->free_area[order].free_list[migratetype]);
		page += 1 << order;
		pages_moved += 1 << order;
	}

	return pages_moved;
}


int move_freepages_block_tlx(struct zone *zone, struct page *page,
				int migratetype)
{
	unsigned long start_pfn, end_pfn;
	struct page *start_page, *end_page;

	start_pfn = page_to_pfn(page);
	start_pfn = start_pfn & ~(pageblock_nr_pages-1);
	start_page = pfn_to_page(start_pfn);
	end_page = start_page + pageblock_nr_pages - 1;
	end_pfn = start_pfn + pageblock_nr_pages - 1;

	return move_freepages_tlx(zone, start_page, end_page, migratetype);
}

int page_group_by_mobility_disabled_tlx;


struct page *
__rmqueue_fallback_tlx(struct zone *zone, unsigned int order, int start_migratetype)
{
	struct free_area *area;
	unsigned int current_order;
	struct page *page;
	int migratetype, new_type, i;
	for (current_order = MAX_ORDER-1;
				current_order >= order && current_order <= MAX_ORDER-1;
				--current_order) {
		for (i = 0;; i++) {
			migratetype = fallbacks_tlx[start_migratetype][i];
			if (migratetype == MIGRATE_RESERVE)
				break;
			area = &(zone->free_area[current_order]);
			if (list_empty(&area->free_list[migratetype]))
				continue;
			page = list_entry(area->free_list[migratetype].next,
					struct page, lru);
			area->nr_free--;
//			new_type = try_to_steal_freepages(zone, page,
//							  start_migratetype,
//							  migratetype);
//			struct zone *zone, struct page *page,
								int start_type = start_migratetype;
								int fallback_type = migratetype;
//			{
				int current_order = page->private;
				if (current_order >= pageblock_order / 2 ||
						start_type == MIGRATE_RECLAIMABLE ||
						page_group_by_mobility_disabled_tlx) {
					int pages;
					pages = move_freepages_block_tlx(zone, page, start_type);
					if (pages >= (1 << (pageblock_order-1)) ||
							page_group_by_mobility_disabled_tlx) {
//						set_pageblock_migratetype(page, start_type);
						new_type =start_type;
						goto have_type;
					}

				}
				new_type = fallback_type;
have_type:
			list_del(&page->lru);
//			rmv_page_order(page);
			expand(zone, page, order, current_order, area,
			       new_type);
//			set_freepage_migratetype(page, new_type);
			return page;
		}
	}

	return NULL;
}

struct page *__rmqueue_smallest_tlx(struct zone *zone, unsigned int order,
						int migratetype)
{
	unsigned int current_order;
	struct free_area *area;
	struct page *page;

	/* Find a page of the appropriate size in the preferred list */
	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
		area = &(zone->free_area[current_order]);
		if (list_empty(&area->free_list[migratetype]))
			continue;

		page = list_entry(area->free_list[migratetype].next,
							struct page, lru);
		list_del(&page->lru);
	//	rmv_page_order(page);
		area->nr_free--;
		expand(zone, page, order, current_order, area, migratetype);
//		set_freepage_migratetype(page, migratetype);
		return page;
	}

	return NULL;
}

struct page *__rmqueue_tlx(struct zone *zone, unsigned int order,
						int migratetype)
{
	struct page *page;

retry_reserve:
	page = __rmqueue_smallest_tlx(zone, order, migratetype);

	if (unlikely(!page) && migratetype != MIGRATE_RESERVE) {
		page = __rmqueue_fallback_tlx(zone, order, migratetype);
		if (!page) {
			migratetype = MIGRATE_RESERVE;
			goto retry_reserve;
		}
	}
	return page;
}


int rmqueue_bulk_tlx(struct zone *zone, unsigned int order,
			unsigned long count, struct list_head *list,
			int migratetype, bool cold)
{
	int i;
	for (i = 0; i < count; ++i) {
		struct page *page = __rmqueue_tlx(zone, order, migratetype);
		if (likely(!cold))
			list_add(&page->lru, list);
		else
			list_add_tail(&page->lru, list);
		list = &page->lru;
	}
	return i;
}

struct page *buffered_rmqueue_tlx(struct zone *preferred_zone,
			struct zone *zone, unsigned int order,
			gfp_t gfp_flags, int migratetype)
{
	unsigned long flags;
	struct page *page;
	bool cold = ((gfp_flags & __GFP_COLD) != 0);
again:
	if (likely(order == 0)) {
		struct per_cpu_pages *pcp;
		struct list_head *list;
//		local_irq_save(flags);
		pcp = &this_cpu_ptr(zone->pageset)->pcp;
		list = &pcp->lists[migratetype];
		if (list_empty(list)) {
			pcp->count += rmqueue_bulk_tlx(zone, 0,
					pcp->batch, list,
					migratetype, cold);
		}
		if (cold)
			page = list_entry(list->prev, struct page, lru);
		else
			page = list_entry(list->next, struct page, lru);
		list_del(&page->lru);
		pcp->count--;
	} else {
//		spin_lock_irqsave(&zone->lock, flags);
		page = __rmqueue_tlx(zone, order, migratetype);
//		spin_unlock(&zone->lock);
//		__mod_zone_freepage_state(zone, -(1 << order),
//					  page->index);
	}
//	__mod_zone_page_state(zone, NR_ALLOC_BATCH, -(1 << order));
//	__count_zone_vm_events(PGALLOC, zone, 1 << order);
//	local_irq_restore(flags);
//	prep_new_page(page, order, gfp_flags);
		int i;
		for (i = 0; i < (1 << order); i++) {
			struct page *p = page + i;
		}
//		set_page_private(page, 0);
		(page)->private = 0;
		atomic_set(&page->_count, 1);
		if (gfp_flags & __GFP_ZERO) {
				int i;
				for (i = 0; i < (1 << order); i++) {
					struct page *__page = page + i;
	//				pagefault_disable();
					void *kaddr =  page_address(page);
					clear_page_tlx(kaddr);
		//			pagefault_enable();
				}
		}
	return page;
}


struct page *
__alloc_pages_nodemask_tlx(gfp_t gfp_mask, unsigned int order,
			struct zonelist *zonelist, nodemask_t *nodemask)
{
enum zone_type z_;
	int bit = (__force int) (gfp_mask & GFP_ZONEMASK);
	z_ = (GFP_ZONE_TABLE >> (bit * ZONES_SHIFT)) &
																					((1 << ZONES_SHIFT) - 1);
	enum zone_type high_zoneidx = z_;
//	gfp_zone(gfp_mask);
	struct zone *preferred_zone;
	struct zoneref *preferred_zoneref;
	struct page *page = NULL;
	int migratetype = MIGRATE_UNMOVABLE;
//int migratetype = (((gfp_mask & __GFP_MOVABLE) != 0) << 1) |
//							((gfp_mask & __GFP_RECLAIMABLE) != 0);
	unsigned int cpuset_mems_cookie;
	int alloc_flags = ALLOC_WMARK_LOW|ALLOC_CPUSET|ALLOC_FAIR;
	int classzone_idx;

	gfp_mask &= gfp_allowed_mask_tlx;
	if(gfp_mask & __GFP_WAIT) might_sleep();
retry_cpuset:
//	cpuset_mems_cookie = read_mems_allowed_begin();
//	preferred_zoneref = first_zones_zonelist(zonelist, high_zoneidx,
//				nodemask ? : &cpuset_current_mems_allowed,
//				&preferred_zone);
	preferred_zoneref =next_zones_zonelist_tlx(zonelist->_zonerefs, high_zoneidx,
		nodemask ? : &cpuset_current_mems_allowed, &preferred_zone);

	classzone_idx = preferred_zoneref->zone_idx;
retry:
		gfp_mask = gfp_mask|__GFP_HARDWALL;
		struct zoneref *z;
		page = NULL;
		struct zone *zone;
		nodemask_t *allowednodes = NULL;/* zonelist_cache approximation */
		int zlc_active = 0;		/* set if using zonelist_cache */
		int did_zlc_setup = 0;		/* just call zlc_setup() one time */
		bool consider_zone_dirty = (alloc_flags & ALLOC_WMARK_LOW) &&
					(gfp_mask & __GFP_WRITE);
zonelist_scan:
		for_each_zone_zonelist_nodemask(zone, z, zonelist,
							high_zoneidx, nodemask) {
			unsigned long mark;
			mark = zone->watermark[alloc_flags & ALLOC_WMARK_MASK];
try_this_zone:
			page = buffered_rmqueue_tlx(preferred_zone, zone, order,
							gfp_mask, migratetype);
			if (page)
				break;
		}
		if (page)
			page->pfmemalloc = !!(alloc_flags & ALLOC_NO_WATERMARKS);

		return page;
}

#define __refdata        __section(.ref.data)

struct pglist_data contig_page_data_tlx;

#define NODE_DATA(nid)          (&contig_page_data_tlx)
#define __set_current_state(state_value)                        \
       do { current->state = (state_value); } while (0)

#define TASK_PARKED             512
#define ZONES_WIDTH             ZONES_SHIFT
#define NODES_PGOFF             (SECTIONS_PGOFF - NODES_WIDTH)
#define ZONES_PGOFF             (NODES_PGOFF - ZONES_WIDTH)
#define ZONES_MASK              ((1UL << ZONES_WIDTH) - 1)
#define ZONES_PGSHIFT           (ZONES_PGOFF * (ZONES_WIDTH != 0))




#define SIG_DFL ((__force __sighandler_t)0)     /* default signal handling */
#define SIG_IGN ((__force __sighandler_t)1)     /* ignore signal */

#define __for_each_thread(signal, t)    \
         list_for_each_entry_rcu(t, &(signal)->thread_head, thread_node)

#define for_each_thread(p, t)           \
         __for_each_thread((p)->signal, t)

#define ATOMIC_INIT(i)  { (i) }
#define KUIDT_INIT(value) (kuid_t){ value }
#define KGIDT_INIT(value) (kgid_t){ value }
#define GLOBAL_ROOT_UID KUIDT_INIT(0)
#define GLOBAL_ROOT_GID KGIDT_INIT(0)

enum {
	PROC_ROOT_INO		= 1,
	PROC_IPC_INIT_INO	= 0xEFFFFFFFU,
	PROC_UTS_INIT_INO	= 0xEFFFFFFEU,
	PROC_USER_INIT_INO	= 0xEFFFFFFDU,
	PROC_PID_INIT_INO	= 0xEFFFFFFCU,
};

struct user_namespace init_user_ns_tlx = {
	.uid_map = {
		.nr_extents = 1,
		.extent[0] = {
			.first = 0,
			.lower_first = 0,
			.count = 4294967295U,
		},
	},
	.gid_map = {
		.nr_extents = 1,
		.extent[0] = {
			.first = 0,
			.lower_first = 0,
			.count = 4294967295U,
		},
	},
	.projid_map = {
		.nr_extents = 1,
		.extent[0] = {
			.first = 0,
			.lower_first = 0,
			.count = 4294967295U,
		},
	},
	.count = ATOMIC_INIT(3),
	.owner = GLOBAL_ROOT_UID,
	.group = GLOBAL_ROOT_GID,
	.proc_inum = PROC_USER_INIT_INO,
};

#define __convert_gid(size, gid) (gid)
#define __convert_uid(size, uid) (uid)

int inotify_max_queued_events_tlx_tlx ;
#define current_user_ns()       (&init_user_ns_tlx)

#define SET_UID(var, uid) do { (var) = __convert_uid(sizeof(var), (uid)); } while (0)
#define SET_GID(var, gid) do { (var) = __convert_gid(sizeof(var), (gid)); } while (0)
#define MAX_ERRNO       4095
#define IS_ERR_VALUE(x) unlikely((x) >= (unsigned long)-MAX_ERRNO)



struct thread_info_tlx *current_thread_info_tlx_tlx(void)
{
  register unsigned long sp asm ("sp");
  return (struct thread_info_tlx *)(sp & ~(THREAD_SIZE - 1));
}

#define get_current() (current_thread_info_tlx_tlx()->task)
#define current get_current()
#define lockdep_is_held(lock)
#define sig_kernel_ignore(sig) \
         (((sig) < SIGRTMIN) && siginmask(sig, SIG_KERNEL_IGNORE_MASK))

#define UINT_MAX        (~0U)



#define get_current_user()                              \
 ({                                                      \
         struct user_struct *__u;                        \
         const struct cred *__cred;                      \
         __cred = current_cred();                        \
         __u = get_uid_tlx(__cred->user);                    \
         __u;                                            \
 })

#define atomic_inc_return(v)    (atomic_add_return_tlx(1, v))

#define NORETURN __attribute__((__noreturn__))

#define ENAMETOOLONG    36      /* File name too long */

#define ARCH_HAS_PREFETCH



//#define ___GFP_HIGH             0x20u
#define ___GFP_IO               0x40u
#define ___GFP_FS               0x80u

#define __GFP_WAIT      ((__force gfp_t)___GFP_WAIT)    /* Can wait and reschedule? */
//#define __GFP_HIGH      ((__force gfp_t)___GFP_HIGH)    /* Should access emergency pools? */
#define __GFP_IO        ((__force gfp_t)___GFP_IO)      /* Can start physical IO? */
#define __GFP_FS        ((__force gfp_t)___GFP_FS)      /* Can call down to low-level FS? */



#define GFP_KERNEL      (__GFP_WAIT | __GFP_IO | __GFP_FS)


#define ENOENT           2      /* No such file or directory */
#define EFAULT          14      /* Bad address */
#define ERANGE          34      /* Math result not representable */
#define ENOMEM          12      /* Out of memory */
#define EINVAL          22      /* Invalid argument */
#define EBADF            9      /* Bad file number */
#define SIG_SETMASK        2    /* for setting the signal mask */
#define MAXQUOTAS 2
#define DCACHE_AUTODIR_TYPE             0x00200000 /* Lookupless directory (presumed automount) */
#define DCACHE_ENTRY_TYPE               0x00700000
#define sigmask(sig)    (1UL << ((sig) - 1))
#define SIGSTOP         19
#define SIGKILL          9

typedef long long qsize_t;      /* Type in which we store sizes */

struct mem_dqinfo {
         struct quota_format_type *dqi_format;
         int dqi_fmt_id;         /* Id of the dqi_format - used when turning
                                  * quotas on after remount RW */
         struct list_head dqi_dirty_list;        /* List of dirty dquots */
         unsigned long dqi_flags;
         unsigned int dqi_bgrace;
         unsigned int dqi_igrace;
         qsize_t dqi_maxblimit;
         qsize_t dqi_maxilimit;
         void *dqi_priv;
 };

enum {
  SB_UNFROZEN = 0,		/* FS is unfrozen */
  SB_FREEZE_WRITE	= 1,		/* Writes, dir ops, ioctls frozen */
  SB_FREEZE_PAGEFAULT = 2,	/* Page faults stopped as well */
  SB_FREEZE_FS = 3,		/* For internal FS use (e.g. to stop
           * internal threads if needed) */
  SB_FREEZE_COMPLETE = 4,		/* ->freeze_fs finished successfully */
};

#define SB_FREEZE_LEVELS (SB_FREEZE_COMPLETE - 1)

struct dentry;

unsigned __d_entry_type_tlx(const struct dentry *dentry);
bool d_is_autodir(const struct dentry *dentry);

struct radix_tree_root {
         unsigned int            height;
         gfp_t                   gfp_mask;
         struct radix_tree_node  __rcu *rnode;
 };

struct quota_info {
         unsigned int flags;                     /* Flags for diskquotas on this device */
         struct mutex dqio_mutex;                /* lock device while I/O in progress */
         struct mutex dqonoff_mutex;             /* Serialize quotaon & quotaoff */
         struct rw_semaphore dqptr_sem;          /* serialize ops using quota_info struct, pointers from inode to dquots */
         struct inode *files[MAXQUOTAS];         /* inodes of quotafiles */
         struct mem_dqinfo info[MAXQUOTAS];      /* Information for each quota type */
         const struct quota_format_ops *ops[MAXQUOTAS];  /* Operations for each type */
 };

struct sb_writers {
  /* Counters for counting writers at each level */
  struct percpu_counter	counter[SB_FREEZE_LEVELS];
  wait_queue_head_t	wait;		/* queue for waiting for
               writers / faults to finish */
  int			frozen;		/* Is sb frozen? */
  wait_queue_head_t	wait_unfrozen;	/* queue for waiting for
               sb to be thawed */
#ifdef CONFIG_DEBUG_LOCK_ALLOC
  struct lockdep_map	lock_map[SB_FREEZE_LEVELS];
#endif
};

struct list_lru {
         struct list_lru_node    *node;
         nodemask_t              active_nodes;
};

struct nfs_lock_info {
         u32             state;
         struct nlm_lockowner *owner;
        struct list_head list;
};

struct nfs4_lock_info {
         struct nfs4_lock_state *owner;
  };

/* Possible states of 'frozen' field */

#define VM_DENYWRITE    0x00000800      /* ETXTBSY on write attempts.. */


#define __SWP_TYPE_SHIFT        3
#define __SWP_TYPE_BITS         6
#define __SWP_OFFSET_BITS       49
#define __SWP_TYPE_MASK         ((1 << __SWP_TYPE_BITS) - 1)
#define __SWP_OFFSET_SHIFT      (__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)
#define __SWP_OFFSET_MASK       ((1UL << __SWP_OFFSET_BITS) - 1)
#define PTE_FILE                (_AT(pteval_t, 1) << 2) /* only when !pte_present() */
#define pte_unmap(pte)                  do { } while (0)
#define pgd_index_tlx(addr)         (((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
# define VM_GROWSUP     VM_NONE
# define DNAME_INLINE_LEN 32 /* 192 bytes */
#define USE_CMPXCHG_LOCKREF \
         (IS_ENABLED(CONFIG_ARCH_USE_CMPXCHG_LOCKREF) && \
          IS_ENABLED(CONFIG_SMP) && SPINLOCK_SIZE <= 4)

#define LIST_BL_LOCKMASK        1UL
#define SB_FREEZE_LEVELS (SB_FREEZE_COMPLETE - 1)
void lockref_get_tlx(struct lockref * l) {};

#define QSTR_INIT(n,l) { { { .len = l } }, .name = n }
#define fops_put(fops) \
         do { if (fops) module_put_tlx((fops)->owner); } while(0)

#define ACL_NOT_CACHED ((void *)(-1))

#define kmemcheck_bitfield_begin(name)
#define kmemcheck_bitfield_end(name)
#define IOP_NOFOLLOW    0x0004
#define IOP_LOOKUP      0x0002
#define FASYNC          00020000        /* fcntl, for BSD compatibility */


#define DCACHE_MISS_TYPE                0x00000000 /* Negative dentry */
#define DCACHE_DIRECTORY_TYPE           0x00100000 /* Normal directory */

#define DCACHE_SYMLINK_TYPE             0x00300000 /* Symlink */
#define DCACHE_FILE_TYPE                0x00400000 /* Other file type */
#define DCACHE_OP_PRUNE                 0x00000010
#define DCACHE_OP_DELETE                0x00000008
#define DCACHE_OP_WEAK_REVALIDATE       0x00000800

#define DCACHE_MOUNTED                  0x00010000 /* is a mountpoint */
#define DCACHE_NEED_AUTOMOUNT           0x00020000 /* handle automount on this dir */
#define DCACHE_MANAGE_TRANSIT           0x00040000 /* manage transit from this dirent */
#define DCACHE_MANAGED_DENTRY \
         (DCACHE_MOUNTED|DCACHE_NEED_AUTOMOUNT|DCACHE_MANAGE_TRANSIT)

#define DCACHE_OP_REVALIDATE            0x00000004
#define O_TRUNC         00001000        /* not fcntl */
#define hashlen_hash(hashlen) ((u32) (hashlen))
#define DCACHE_OP_COMPARE               0x00000002
#define O_DIRECTORY      040000 /* must be a directory */
#define IS_POSIXACL(inode)      __IS_FLG(inode, MS_POSIXACL)
#define MS_POSIXACL     (1<<16) /* VFS does not apply the umask */
#define MS_BORN         (1<<29)
#define MS_ACTIVE       (1<<30)
#define MS_NOUSER       (1<<31)

#define MS_SILENT       32768
#define MAX_LFS_FILESIZE        ((loff_t)0x7fffffffffffffffLL)

#define __IS_FLG(inode, flg)    ((inode)->i_sb->s_flags & (flg))
#define DCACHE_OP_HASH                  0x00000001
#define SIGNAL_STOP_CONTINUED   0x00000002 /* SIGCONT since WCONTINUED reap */

#define __task_cred(task)       \
        rcu_dereference((task)->real_cred)

int debug_locks_tlx;
void __free_pages_tlx(struct page *page, unsigned int order);





void mod_zone_page_state_tlx(struct zone * z, enum zone_stat_item zi, int i) {};
struct kmem_cache *cred_jar_tlx_tlx;
static inline void might_fault_tlx(void) { };
void put_page_tlx(struct page *page) {};

#define RADIX_TREE_MAX_TAGS 3

#ifdef __KERNEL__
#define RADIX_TREE_MAP_SHIFT	(CONFIG_BASE_SMALL ? 4 : 6)
#else
#define RADIX_TREE_MAP_SHIFT	3	/* For more stressful testing */
#endif

#define RADIX_TREE_MAP_SIZE	(1UL << RADIX_TREE_MAP_SHIFT)
#define RADIX_TREE_MAP_MASK	(RADIX_TREE_MAP_SIZE-1)
#define RADIX_TREE_TAG_LONGS	\
	((RADIX_TREE_MAP_SIZE + BITS_PER_LONG - 1) / BITS_PER_LONG)


void touch_atime_tlx(const struct path * a) {};

struct radix_tree_node {
	unsigned int	path;	/* Offset in parent & height from the bottom */
	unsigned int	count;
	union {
		struct {
			/* Used when ascending tree */
			struct radix_tree_node *parent;
			/* For tree user */
			void *private_data;
		};
		/* Used when freeing node */
		struct rcu_head	rcu_head;
	};
	/* For tree user */
	struct list_head private_list;
	void __rcu	*slots[RADIX_TREE_MAP_SIZE];
	unsigned long	tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS];
};


#define RADIX_TREE_HEIGHT_SHIFT	(RADIX_TREE_MAX_PATH + 1)
#define RADIX_TREE_HEIGHT_MASK	((1UL << RADIX_TREE_HEIGHT_SHIFT) - 1)

#define RADIX_TREE_TAG_LONGS	\
					((RADIX_TREE_MAP_SIZE + BITS_PER_LONG - 1) / BITS_PER_LONG)

#define RADIX_TREE_INDEX_BITS  (8 /* CHAR_BIT */ * sizeof(unsigned long))
#define RADIX_TREE_MAX_PATH (DIV_ROUND_UP(RADIX_TREE_INDEX_BITS, \
					  RADIX_TREE_MAP_SHIFT))

struct kmem_cache *radix_tree_node_cachep_tlx;
unsigned long height_to_maxindex_tlx_tlx[RADIX_TREE_MAX_PATH + 1] ;
#define BLOCKS_PER_PAGE  (PAGE_CACHE_SIZE/512)
#define RADIX_TREE_EXCEPTIONAL_ENTRY    2
#define RADIX_TREE_INDIRECT_PTR         1
#define rcu_dereference_raw(p) rcu_dereference_check(p, 1) /*@@@ needed? @@@*/

void *__radix_tree_lookup_tlx_tlx(struct radix_tree_root *root, unsigned long index,
			  struct radix_tree_node **nodep, void ***slotp)
{
	struct radix_tree_node *node, *parent;
	unsigned int height, shift;
	void **slot;

	node = rcu_dereference_raw(root->rnode);
	if (node == NULL)
		return NULL;

	if (!(int)((unsigned long)node & RADIX_TREE_INDIRECT_PTR)) {
		if (index > 0)
			return NULL;

		if (nodep)
			*nodep = NULL;
		if (slotp)
			*slotp = (void **)&root->rnode;
		return node;
	}
	node = (void *)((unsigned long)node & ~RADIX_TREE_INDIRECT_PTR);

	height = node->path & RADIX_TREE_HEIGHT_MASK;
	if (index > height_to_maxindex_tlx_tlx[height])
		return NULL;

	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	do {
		parent = node;
		slot = node->slots + ((index >> shift) & RADIX_TREE_MAP_MASK);
		node = rcu_dereference_raw(*slot);
		if (node == NULL)
			return NULL;

		shift -= RADIX_TREE_MAP_SHIFT;
		height--;
	} while (height > 0);

	if (nodep)
		*nodep = parent;
	if (slotp)
		*slotp = slot;
	return node;
}


void *radix_tree_delete_item_tlx(struct radix_tree_root *root,
			     unsigned long index, void *item)
{
	struct radix_tree_node *node;
	unsigned int offset;
	void **slot;
	void *entry;
	int tag;

	entry = __radix_tree_lookup_tlx_tlx(root, index, &node, &slot);
	if (!entry)
		return NULL;

	if (item && entry != item)
		return NULL;

	if (!node) {
//		root_tag_clear_all(root);
		root->rnode = NULL;
		return entry;
	}

	offset = index & RADIX_TREE_MAP_MASK;
	node->slots[offset] = NULL;
	node->count--;
	return entry;
}


void *radix_tree_delete_tlx(struct radix_tree_root *root, unsigned long index)
{
	return radix_tree_delete_item_tlx(root, index, NULL);
}




#define RCU_INITIALIZER(v) (typeof(*(v)) __force __rcu *)(v)
#define smp_store_release(p, v)                                         \
do {                                                                    \
				compiletime_assert_atomic_type(*p);                             \
				switch (sizeof(*p)) {                                           \
				case 4:                                                         \
								asm volatile ("stlr %w1, %0"                            \
																: "=Q" (*p) : "r" (v) : "memory");      \
								break;                                                  \
				case 8:                                                         \
								asm volatile ("stlr %1, %0"                             \
																: "=Q" (*p) : "r" (v) : "memory");      \
								break;                                                  \
				}                                                               \
	} while (0)

#define rcu_assign_pointer(p, v) smp_store_release(&p, RCU_INITIALIZER(v));

/* Internally used bits of node->count */
#define RADIX_TREE_COUNT_SHIFT	(RADIX_TREE_MAP_SHIFT + 1)
#define RADIX_TREE_COUNT_MASK	((1UL << RADIX_TREE_COUNT_SHIFT) - 1)

#define RADIX_TREE_INDIRECT_PTR		1
/*
 * A common use of the radix tree is to store pointers to struct pages;
 * but shmem/tmpfs needs also to store swap entries in the same tree:
 * those are marked as exceptional entries to distinguish them.
 * EXCEPTIONAL_ENTRY tests the bit, EXCEPTIONAL_SHIFT shifts content past it.
 */
#define RADIX_TREE_EXCEPTIONAL_ENTRY	2
#define RADIX_TREE_EXCEPTIONAL_SHIFT	2



static inline int radix_tree_is_indirect_ptr(void *ptr)
{
	return (int)((unsigned long)ptr & RADIX_TREE_INDIRECT_PTR);
}

void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
}

unsigned long radix_tree_maxindex(unsigned int height)
{
	return height_to_maxindex_tlx_tlx[height];
}

static inline void *ptr_to_indirect(void *ptr)
{
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void tag_set(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__set_bit_tlx(offset, node->tags[tag]);
}

#define __GFP_BITS_SHIFT 25     /* Room for N __GFP_FOO bits */

static inline void root_tag_set(struct radix_tree_root *root, unsigned int tag)
{
	root->gfp_mask |= (__force gfp_t)(1 << (tag + __GFP_BITS_SHIFT));
}

static inline int root_tag_get(struct radix_tree_root *root, unsigned int tag)
{
	return (__force unsigned)root->gfp_mask & (1 << (tag + __GFP_BITS_SHIFT));
}


#define RADIX_TREE_PRELOAD_SIZE (RADIX_TREE_MAX_PATH * 2 - 1)

#define PREEMPT_BITS    8
#define SOFTIRQ_BITS    8
#define HARDIRQ_BITS    4
#define NMI_BITS        1
#define PREEMPT_SHIFT   0
#define SOFTIRQ_SHIFT   (PREEMPT_SHIFT + PREEMPT_BITS)
#define HARDIRQ_SHIFT   (SOFTIRQ_SHIFT + SOFTIRQ_BITS)
#define NMI_SHIFT       (HARDIRQ_SHIFT + HARDIRQ_BITS)
#define __IRQ_MASK(x)   ((1UL << (x))-1)
#define NMI_MASK        (__IRQ_MASK(NMI_BITS)     << NMI_SHIFT)
#define SOFTIRQ_MASK    (__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
#define HARDIRQ_MASK    (__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)

#define irq_count()     (preempt_count_tlx() & (HARDIRQ_MASK | SOFTIRQ_MASK \
                                   | NMI_MASK))


static inline gfp_t root_gfp_mask(struct radix_tree_root *root)
{
	return root->gfp_mask & __GFP_BITS_MASK;
}

void *slab_alloc_tlx(struct kmem_cache *s,
	 gfp_t gfpflags, unsigned long addr);

struct radix_tree_preload {
	int nr;
	struct radix_tree_node *nodes[RADIX_TREE_PRELOAD_SIZE];
};

static __always_inline int preempt_count_tlx(void)
{
				return current_thread_info_tlx_tlx()->preempt_count;
}


static DEFINE_PER_CPU(struct radix_tree_preload, radix_tree_preloads) = { 0, };

struct radix_tree_node *
radix_tree_node_alloc_tlx(struct radix_tree_root *root)
{
	struct radix_tree_node *ret = NULL;
	gfp_t gfp_mask = root_gfp_mask(root);
	if (!(gfp_mask & __GFP_WAIT) && !in_interrupt()) {
		struct radix_tree_preload *rtp;
		rtp = this_cpu_ptr(&radix_tree_preloads);
		if (rtp->nr) {
			ret = rtp->nodes[rtp->nr - 1];
			rtp->nodes[rtp->nr - 1] = NULL;
			rtp->nr--;
		}
	}
	if (ret == NULL)
		ret =  slab_alloc_tlx(radix_tree_node_cachep_tlx, gfp_mask | __GFP_ZERO, _RET_IP_);
//		 kmem_cache_alloc(radix_tree_node_cachep_tlx, gfp_mask);
	return ret;
}


static int radix_tree_extend(struct radix_tree_root *root, unsigned long index)
{
	struct radix_tree_node *node;
	struct radix_tree_node *slot;
	unsigned int height;
	int tag;

	/* Figure out what the height should be.  */
	height = root->height + 1;
	while (index > radix_tree_maxindex(height))
		height++;

	if (root->rnode == NULL) {
		root->height = height;
		goto out;
	}

	do {
		unsigned int newheight;
		if (!(node = radix_tree_node_alloc_tlx(root)))
			return -ENOMEM;

		/* Propagate the aggregated tag info into the new root */
		for (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++) {
			if (root_tag_get(root, tag))
				tag_set(node, tag, 0);
		}

		/* Increase the height.  */
		newheight = root->height+1;
		node->path = newheight;
		node->count = 1;
		node->parent = NULL;
		slot = root->rnode;
		if (newheight > 1) {
			slot = indirect_to_ptr(slot);
			slot->parent = node;
		}
		node->slots[0] = slot;
		node = ptr_to_indirect(node);
		rcu_assign_pointer(root->rnode, node);
		root->height = newheight;
	} while (height > root->height);
out:
	return 0;
}


int __radix_tree_create_tlx(struct radix_tree_root *root, unsigned long index,
			struct radix_tree_node **nodep, void ***slotp)
{
	struct radix_tree_node *node = NULL, *slot;
	unsigned int height, shift, offset;
	int error;

	/* Make sure the tree is high enough.  */
	if (index > radix_tree_maxindex(root->height)) {
		error = radix_tree_extend(root, index);
		if (error)
			return error;
	}

	slot = indirect_to_ptr(root->rnode);

	height = root->height;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	offset = 0;			/* uninitialised var warning */
	while (height > 0) {
		if (slot == NULL) {
			/* Have to add a child node.  */
			if (!(slot = radix_tree_node_alloc_tlx(root)))
				return -ENOMEM;
			slot->path = height;
			slot->parent = node;
			if (node) {
				rcu_assign_pointer(node->slots[offset], slot);
				node->count++;
				slot->path |= offset << RADIX_TREE_HEIGHT_SHIFT;
			} else
				rcu_assign_pointer(root->rnode, ptr_to_indirect(slot));
		}

		/* Go a level down */
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
		node = slot;
		slot = node->slots[offset];
		shift -= RADIX_TREE_MAP_SHIFT;
		height--;
	}

	if (nodep)
		*nodep = node;
	if (slotp)
		*slotp = node ? node->slots + offset : (void **)&root->rnode;
	return 0;
}


int radix_tree_insert_tlx(struct radix_tree_root *root,
			unsigned long index, void *item)
{
	struct radix_tree_node *node;
	void **slot;
	int error;

	error = __radix_tree_create_tlx(root, index, &node, &slot);
	rcu_assign_pointer(*slot, item);
	return 0;
}



void *radix_tree_lookup_tlx(struct radix_tree_root *root, unsigned long index)
{
	return __radix_tree_lookup_tlx_tlx(root, index, NULL, NULL);
}




struct kobject *fs_kobj_tlx_tlx;




#define small_const_nbits(nbits) \
				(__builtin_constant_p(nbits) && (nbits) <= BITS_PER_LONG)

static inline void bitmap_zero_tlx(unsigned long *dst, int nbits)
 {
       if (small_const_nbits(nbits))
                 *dst = 0UL;
       else {
                 int len = BITS_TO_LONGS(nbits) * sizeof(unsigned long);
                 memset_tlx(dst, 0, len);
         }
 }
#define cpumask_bits(maskp) ((maskp)->bits)
static inline u32 __attribute_const__ read_cpuid_cachetype_tlx(void)
{
				return read_cpuid(CTR_EL0);
}

#define CTR_CWG_SHIFT           24
#define CTR_CWG_MASK            15

static inline u32 cache_type_cwg_tlx(void)
 {
          return (read_cpuid_cachetype_tlx() >> CTR_CWG_SHIFT) & CTR_CWG_MASK;
 }


int atomic_cmpxchg_tlx(atomic_t *ptr, int old, int new)
 {
         unsigned long tmp;
         int oldval;

         smp_mb();

         asm volatile("// atomic_cmpxchg\n"
 "1:     ldxr    %w1, %2\n"
 "       cmp     %w1, %w3\n"
 "       b.ne    2f\n"
 "       stxr    %w0, %w4, %2\n"
 "       cbnz    %w0, 1b\n"
 "2:"
         : "=&r" (tmp), "=&r" (oldval), "+Q" (ptr->counter)
         : "Ir" (old), "r" (new)
         : "cc");

         smp_mb();
         return oldval;
 }


#define page_to_phys(page)      (__pfn_to_phys(page_to_pfn(page)))


static inline cpumask_t *mm_cpumask_tlx(struct mm_struct *mm)
 {
         return mm->cpu_vm_mask_var;
 }

#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)
#define max_t(type, x, y) ({                    \
         type __max1 = (x);                      \
         type __max2 = (y);                      \
         __max1 > __max2 ? __max1: __max2; })

static inline void cpumask_clear_tlx(struct cpumask *dstp)
{
         bitmap_zero_tlx(cpumask_bits(dstp), nr_cpumask_bits);
}

#define cpu_switch_mm(pgd,mm) cpu_do_switch_mm_tlx(__virt_to_phys_tlx(pgd),mm)
#define __pa_symbol(x)  __pa(RELOC_HIDE((unsigned long)(x), 0))
#define clamp(val, min, max) ({                 \
         typeof(val) __val = (val);              \
         typeof(min) __min = (min);              \
         typeof(max) __max = (max);              \
         (void) (&__val == &__min);              \
         (void) (&__val == &__max);              \
         __val = __val < __min ? __min: __val;   \
         __val > __max ? __max: __val; })

#define round_down(x, y) ((x) & ~__round_mask(x, y))

#define _QW_WAITING     1               /* A writer is waiting     */
#define _QW_WMASK       0xff            /* Writer mask             */


static inline void arch_spin_lock_tlx(arch_spinlock_t *lock)
	{
					unsigned int tmp;
					arch_spinlock_t lockval, newval;

					asm volatile(
					/* Atomically increment the next ticket. */
	"       prfm    pstl1strm, %3\n"
	"1:     ldaxr   %w0, %3\n"
	"       add     %w1, %w0, %w5\n"
	"       stxr    %w2, %w1, %3\n"
	"       cbnz    %w2, 1b\n"
					/* Did we get the lock? */
	"       eor     %w1, %w0, %w0, ror #16\n"
	"       cbz     %w1, 3f\n"
					/*
					* No: spin on the owner. Send a local event to avoid missing an
					* unlock before the exclusive load.
					*/
	"       sevl\n"
	"2:     wfe\n"
	"       ldaxrh  %w2, %4\n"
	"       eor     %w1, %w2, %w0, lsr #16\n"
	"       cbnz    %w1, 2b\n"
					/* We got the lock. Critical section starts here. */
	"3:"
				: "=&r" (lockval), "=&r" (newval), "=&r" (tmp), "+Q" (*lock)
				: "Q" (lock->owner), "I" (1 << TICKET_SHIFT)
				: "memory");
	}


static inline void arch_spin_unlock_tlx(arch_spinlock_t *lock)
{
					asm volatile(
	"       stlrh   %w1, %0\n"
					: "=Q" (lock->owner)
					: "r" (lock->owner + 1)
					: "memory");
}

static inline void queue_write_lock_tlx(struct qrwlock *lock)
 {
         /* Optimize for the unfair lock case where the fair flag is 0. */
         if (atomic_cmpxchg_tlx(&lock->cnts, 0, _QW_LOCKED) == 0)
                 return;

//         queue_write_lock_slowpath(lock);
					u32 cnts;
					arch_spin_lock_tlx(&lock->lock);
					if (!atomic_read(&lock->cnts) &&
							(atomic_cmpxchg_tlx(&lock->cnts, 0, _QW_LOCKED) == 0))
						goto unlock;
					for (;;) {
						cnts = atomic_read(&lock->cnts);
						if (!(cnts & _QW_WMASK) &&
								(atomic_cmpxchg_tlx(&lock->cnts, cnts,
										cnts | _QW_WAITING) == cnts))
							break;
						cpu_relax();
					}
					for (;;) {
						cnts = atomic_read(&lock->cnts);
						if ((cnts == _QW_WAITING) &&
								(atomic_cmpxchg_tlx(&lock->cnts, _QW_WAITING,
										_QW_LOCKED) == _QW_WAITING))
							break;
						cpu_relax();
					}
				unlock:
					arch_spin_unlock_tlx(&lock->lock);


 };




static void put_pid_ns_tlx(struct pid_namespace *ns)
{
}

static void security_cred_free_tlx(struct cred *cred)
 {
 }

static  void kernel_map_pages_tlx(struct page *page, int numpages, int enable)
{
}

#define atomic64_read(v)        (*(volatile long *)&(v)->counter)

static inline long atomic_long_read_tlx(atomic_long_t *l)
 {
         atomic64_t *v = (atomic64_t *)l;

         return (long)atomic64_read(v);
 }

#define NSEC_PER_SEC    1000000000L
atomic_long_t vm_stat_tlx[NR_VM_ZONE_STAT_ITEMS];


# ifdef __BIG_ENDIAN
#define __cpu_to_be32(x) ((__force __be32)(__u32)(x))
#else
#define __cpu_to_be32(x) ((__force __be32)__swab32((x)))
#endif


#define __swab32(x)                             \
         (__builtin_constant_p((__u32)(x)) ?     \
         ___constant_swab32(x) :                 \
         __fswab32_tlx(x))


#define ___constant_swab32(x) ((__u32)(                         \
         (((__u32)(x) & (__u32)0x000000ffUL) << 24) |            \
         (((__u32)(x) & (__u32)0x0000ff00UL) <<  8) |            \
         (((__u32)(x) & (__u32)0x00ff0000UL) >>  8) |            \
         (((__u32)(x) & (__u32)0xff000000UL) >> 24)))

static inline __attribute_const__ __u32 __fswab32_tlx(__u32 val)
 {
 #ifdef __HAVE_BUILTIN_BSWAP32__
         return __builtin_bswap32(val);
 #elif defined(__arch_swab32)
         return __arch_swab32(val);
 #else
         return ___constant_swab32(val);
 #endif
  }




#ifdef __LITTLE_ENDIAN
  #define HASH_LEN_DECLARE u32 hash; u32 len;
//  #define bytemask_from_count(cnt)       (~(~0ul << (cnt)*8))
#else
  #define HASH_LEN_DECLARE u32 len; u32 hash;
//  #define bytemask_from_count(cnt)       (~(~0ul >> (cnt)*8))
#endif


#ifdef __LITTLE_ENDIAN
static inline __u32 __be32_to_cpup_tlx(const __be32 *p)
{
         return __swab32p((__u32 *)p);
}

#define __le32_to_cpu(x) ((__force __u32)(__le32)(x))
#define __be32_to_cpu(x) __swab32((__force __u32)(__be32)(x))
#else
static inline __u32 __be32_to_cpup_tlx(const __be32 *p)
 {
         return (__force __u32)*p;
}

#define __le32_to_cpu(x) __swab32((__force __u32)(__le32)(x))
#define __be32_to_cpu(x) ((__force __u32)(__be32)(x))
#endif




# define __DEBUG_MUTEX_INITIALIZER(lockname)
# define __DEP_MAP_MUTEX_INITIALIZER(lockname)


#define __MUTEX_INITIALIZER(lockname) \
                 { .count = ATOMIC_INIT(1) \
                 , .wait_lock = __SPIN_LOCK_UNLOCKED(lockname.wait_lock) \
                 , .wait_list = LIST_HEAD_INIT(lockname.wait_list) \
                 __DEBUG_MUTEX_INITIALIZER(lockname) \
                 __DEP_MAP_MUTEX_INITIALIZER(lockname) }

#define DEFINE_MUTEX(mutexname) \
         struct mutex mutexname = __MUTEX_INITIALIZER(mutexname)


#define GFP_ATOMIC      (__GFP_HIGH)
#define GFP_NOWAIT      (GFP_ATOMIC & ~__GFP_HIGH)



# define mutex_init(mutex) \
 do {                                                    \
         static struct lock_class_key __key;             \
                                                         \
         __mutex_init_tlx((mutex), #mutex, &__key);          \
 } while (0)


#define __meminitdata    __section(.meminit.data)


static inline int cache_line_size_tlx(void)
 {
       u32 cwg = cache_type_cwg_tlx();
         return cwg ? 4 << cwg : L1_CACHE_BYTES;
 }

#define __ALIGN_KERNEL_MASK(x, mask)    (((x) + (mask)) & ~(mask))
#define __ALIGN_KERNEL(x, a)            __ALIGN_KERNEL_MASK(x, (typeof(x))(a) - 1)
#define ALIGN(x, a)             __ALIGN_KERNEL((x), (a))

#define TIMER_IRQSAFE                   0x2LU
struct tvec_base boot_tvec_bases_tlx;
static inline unsigned long global_page_state_tlx(enum zone_stat_item item)
{
         long x = atomic_long_read_tlx(&vm_stat_tlx[item]);
 #ifdef CONFIG_SMP
         if (x < 0)
                 x = 0;
 #endif
         return x;
}



#define BITMAP_LAST_WORD_MASK(nbits)                                    \
(                                                                       \
				((nbits) % BITS_PER_LONG) ?                                     \
								(1UL<<((nbits) % BITS_PER_LONG))-1 : ~0UL               \
)




static inline unsigned int __arch_hweight32_tlx(unsigned int w)
{
//			return __sw_hweight32(w);
 #ifdef ARCH_HAS_FAST_MULTIPLIER
         w -= (w >> 1) & 0x55555555;
         w =  (w & 0x33333333) + ((w >> 2) & 0x33333333);
         w =  (w + (w >> 4)) & 0x0f0f0f0f;
         return (w * 0x01010101) >> 24;
 #else
         unsigned int res = w - ((w >> 1) & 0x55555555);
         res = (res & 0x33333333) + ((res >> 2) & 0x33333333);
         res = (res + (res >> 4)) & 0x0F0F0F0F;
         res = res + (res >> 8);
         return (res + (res >> 16)) & 0x000000FF;
  #endif
}
static inline unsigned long __arch_hweight64_tlx(__u64 w)
{
 #if BITS_PER_LONG == 32
         return __sw_hweight32((unsigned int)(w >> 32)) +
                __sw_hweight32((unsigned int)w);
 #elif BITS_PER_LONG == 64
 #ifdef ARCH_HAS_FAST_MULTIPLIER
         w -= (w >> 1) & 0x5555555555555555ul;
         w =  (w & 0x3333333333333333ul) + ((w >> 2) & 0x3333333333333333ul);
         w =  (w + (w >> 4)) & 0x0f0f0f0f0f0f0f0ful;
         return (w * 0x0101010101010101ul) >> 56;
 #else
         __u64 res = w - ((w >> 1) & 0x5555555555555555ul);
         res = (res & 0x3333333333333333ul) + ((res >> 2) & 0x3333333333333333ul);
         res = (res + (res >> 4)) & 0x0F0F0F0F0F0F0F0Ful;
         res = res + (res >> 8);
         res = res + (res >> 16);
         return (res + (res >> 32)) & 0x00000000000000FFul;
 #endif
 #endif
}


#define __const_hweight8(w)             \
			((unsigned int)                 \
				((!!((w) & (1ULL << 0))) +     \
					(!!((w) & (1ULL << 1))) +     \
					(!!((w) & (1ULL << 2))) +     \
					(!!((w) & (1ULL << 3))) +     \
					(!!((w) & (1ULL << 4))) +     \
					(!!((w) & (1ULL << 5))) +     \
					(!!((w) & (1ULL << 6))) +     \
					(!!((w) & (1ULL << 7)))))

#define __const_hweight16(w) (__const_hweight8(w)  + __const_hweight8((w)  >> 8 ))
#define __const_hweight32(w) (__const_hweight16(w) + __const_hweight16((w) >> 16))
#define __const_hweight64(w) (__const_hweight32(w) + __const_hweight32((w) >> 32))

#define hweight32(w) (__builtin_constant_p(w) ? __const_hweight32(w) : __arch_hweight32_tlx(w))
#define hweight64(w) (__builtin_constant_p(w) ? __const_hweight64(w) : __arch_hweight64_tlx(w))

static inline unsigned long hweight_long_tlx(unsigned long w)
{
				return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
}


static inline int bitmap_weight_tlx(const unsigned long *src, int nbits)
 {
         if (small_const_nbits(nbits))
                 return hweight_long_tlx(*src & BITMAP_LAST_WORD_MASK(nbits));
//         return __bitmap_weight(src, nbits);
				const unsigned long *bitmap = src;
				int bits = nbits;
         int k, w = 0, lim = bits/BITS_PER_LONG;
         for (k = 0; k < lim; k++)
               w += hweight_long_tlx(bitmap[k]);

         if (bits % BITS_PER_LONG)
                 w += hweight_long_tlx(bitmap[k] & BITMAP_LAST_WORD_MASK(bits));
         return w;

 }

static inline void bitmap_copy_tlx(unsigned long *dst, const unsigned long *src,
                         int nbits)
 {
         if (small_const_nbits(nbits))
                 *dst = *src;
         else {
               int len = BITS_TO_LONGS(nbits) * sizeof(unsigned long);
                 memcpy_tlx(dst, src, len);
         }
 }


#define TIF_32BIT               22      /* 32bit process */
static __always_inline bool arch_static_branch_tlx(struct static_key *key)
{
         return true;
 }

#define atomic_sub_and_test(i, v) (atomic_sub_return_tlx(i, v) == 0)

static inline int kref_sub_tlx(struct kref *kref, unsigned int count,
              void (*release)(struct kref *kref))
 {
         WARN_ON(release == NULL);

         if (atomic_sub_and_test((int) count, &kref->refcount)) {
                 release(kref);
                 return 1;
         }
          return 0;
 }


static inline bool rcu_is_watching_tlx(void)
 {
         return true;
 }

//uid_t from_kuid(struct user_namespace *to, kuid_t uid) {};

#define ATOMIC64_INIT(i) { (i) }
static inline bool kuid_has_mapping_tlx(struct user_namespace *ns, kuid_t uid)
{
       return false;
//			from_kuid(ns, uid) != (uid_t) -1;
}



static inline unsigned long vma_pages_tlx(struct vm_area_struct *vma)
 {
         return (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
 }
#define rb_entry(ptr, type, member) container_of(ptr, type, member)

#define __pgprot(x)     (x)

#define PTE_TYPE_PAGE           (_AT(pteval_t, 3) << 0)
#define PTE_SHARED              (_AT(pteval_t, 3) << 8)         /* SH[1:0], inner shareable */

#define PROT_DEFAULT            (PTE_TYPE_PAGE | PTE_AF | PTE_SHARED)
#define MT_NORMAL               4
#define PTE_ATTRINDX(t)         (_AT(pteval_t, (t)) << 2)

#define _PAGE_DEFAULT           (PROT_DEFAULT | PTE_ATTRINDX(MT_NORMAL))
#define PTE_TYPE_MASK           (_AT(pteval_t, 3) << 0)
#define PTE_PXN                 (_AT(pteval_t, 1) << 53)        /* Privileged XN */
#define PTE_NG                  (_AT(pteval_t, 1) << 11)        /* nG */

#define PAGE_NONE               __pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
#define PAGE_SHARED             __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
#define PAGE_SHARED_EXEC        __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
#define PAGE_COPY               __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
#define PAGE_COPY_EXEC          __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
#define PAGE_READONLY           __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
#define PAGE_READONLY_EXEC      (_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)

#define __P000  PAGE_NONE
#define __P001  PAGE_READONLY
#define __P010  PAGE_COPY
#define __P011  PAGE_COPY
#define __P100  PAGE_READONLY_EXEC
#define __P101  PAGE_READONLY_EXEC
#define __P110  PAGE_COPY_EXEC
#define __P111  PAGE_COPY_EXEC

#define __S000  PAGE_NONE
#define __S001  PAGE_READONLY
#define __S010  PAGE_SHARED
#define __S011  PAGE_SHARED
#define __S100  PAGE_READONLY_EXEC
#define __S101  PAGE_READONLY_EXEC
#define __S110  PAGE_SHARED_EXEC
#define __S111  PAGE_SHARED_EXEC

pgprot_t protection_map_tlx[16] = {
         __P000, __P001, __P010, __P011, __P100, __P101, __P110, __P111,
         __S000, __S001, __S010, __S011, __S100, __S101, __S110, __S111
};

#define VM_READ         0x00000001      /* currently active flags */
#define VM_WRITE        0x00000002
#define VM_EXEC         0x00000004
#define VM_SHARED       0x00000008
static inline void rb_link_node_tlx(struct rb_node * node, struct rb_node * parent,
                                 struct rb_node ** rb_link)
 {
         node->__rb_parent_color = (unsigned long)parent;
         node->rb_left = node->rb_right = NULL;

          *rb_link = node;
 }

#define VM_LOCKED       0x00002000
#define EIO              5      /* I/O error */
void down_write_tlx(struct rw_semaphore *sem) {};
#define VM_EXEC         0x00000004
#define PT_LOOS    0x60000000      /* OS-specific */
#define MMF_DUMPABLE_BITS 2
#define VM_DATA_DEFAULT_FLAGS \
         (((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
          VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
#define MMF_DUMP_FILTER_SHIFT   MMF_DUMPABLE_BITS
#define MMF_DUMP_FILTER_BITS    7




#define pgprot_val(x)   (x)
pgprot_t vm_get_page_prot_tlx(unsigned long vm_flags)
{
         return __pgprot(pgprot_val(protection_map_tlx[vm_flags &
                               (VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]) |
                         pgprot_val(__pgprot(0)));
}

#define VM_SEQ_READ     0x00008000      /* App will access data sequentially */
#define VM_RAND_READ    0x00010000      /* App will not benefit from clustered reads */
#define VM_ACCOUNT      0x00100000      /* Is a VM accounted object */
#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
#define VM_GROWSDOWN    0x00000100      /* general info on the segment */
#define TASK_SIZE_64            (UL(1) << VA_BITS)
# define MMF_DUMP_MASK_DEFAULT_ELF      0
#define MMF_DUMP_HUGETLB_PRIVATE 7
#define MMF_DUMP_ANON_SHARED    3
#define MMF_DUMP_ANON_PRIVATE   2
#define VM_NOHUGEPAGE   0x40000000      /* MADV_NOHUGEPAGE marked this vma */
#define ET_EXEC   2
#define ET_DYN    3
#define PF_R            0x4
#define PF_W            0x2
#define PT_LOAD    1
#ifdef CONFIG_COMPAT
  #define AARCH32_VECTORS_BASE    0xffff0000
  #define STACK_TOP               (test_thread_flag(TIF_32BIT) ? \
                                 AARCH32_VECTORS_BASE : STACK_TOP_MAX)
#else
 #define STACK_TOP               STACK_TOP_MAX
#endif /* CONFIG_COMPAT */


#ifdef CONFIG_COMPAT
#define TASK_SIZE_32            UL(0x100000000)
#define TASK_SIZE               (test_thread_flag(TIF_32BIT) ? \
                                 TASK_SIZE_32 : TASK_SIZE_64)
#define TASK_SIZE_OF(tsk)       (test_tsk_thread_flag(tsk, TIF_32BIT) ? \
                                 TASK_SIZE_32 : TASK_SIZE_64)
#else
#define TASK_SIZE               TASK_SIZE_64
#endif /* CONFIG_COMPAT */




#define TASK_UNMAPPED_BASE      (PAGE_ALIGN(TASK_SIZE / 4))

int randomize_va_space_tlx;
#define TIF_FOREIGN_FPSTATE     3       /* CPU's FP state is not current's */
#define set_thread_flag(flag) \
          set_ti_thread_flag_tlx(current_thread_info_tlx_tlx(), flag)
#define PF_FORKNOEXEC   0x00000040      /* forked but didn't exec */
#define PF_RANDOMIZE    0x00400000      /* randomize virtual address space */
#define TASK_SIZE_64            (UL(1) << VA_BITS)
void mmput_tlx(struct mm_struct *mm) {};
static inline void mm_update_next_owner_tlx(struct mm_struct *mm)
{
}
void up_read_tlx(struct rw_semaphore *sem) {};
void down_read_tlx(struct rw_semaphore *sem) {};
void sync_mm_rss_tlx(struct mm_struct *mm) {};
void mm_release_tlx(struct task_struct * t, struct mm_struct * m) {};
#define PF_X            0x1
#define PT_GNU_STACK    (PT_LOOS + 0x474e551)



static inline int is_compat_thread_tlx(struct thread_info_tlx *thread)
{
         return 0;
}


static inline pid_t task_pid_nr_tlx(struct task_struct *tsk)
{
         return tsk->pid;
}
extern struct task_struct *cpu_switch_to_tlx(struct task_struct *prev,
                                          struct task_struct *next);

static inline int test_and_set_bit_tlx(int nr, volatile unsigned long *addr)
{
				unsigned long mask = BIT_MASK(nr);
				unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
				unsigned long old;
				unsigned long flags;

//         _atomic_spin_lock_irqsave(p, flags);
				old = *p;
				*p = old | mask;
//         _atomic_spin_unlock_irqrestore(p, flags);

				return (old & mask) != 0;
}

#define cpumask_bits(maskp) ((maskp)->bits)


static inline unsigned int cpumask_check_tlx(unsigned int cpu)
{
				return cpu;
}


static inline int cpumask_test_and_set_cpu_tlx(int cpu, struct cpumask *cpumask)
{
         return test_and_set_bit_tlx(cpumask_check_tlx(cpu), cpumask_bits(cpumask));
}
static inline void set_ti_thread_flag_tlx(struct thread_info_tlx *ti, int flag)
 {
         set_bit_tlx(flag, (unsigned long *)&ti->flags);
}


#define task_thread_info(task)  ((struct thread_info_tlx *)(task)->stack)

static inline unsigned int task_cpu_tlx(const struct task_struct *p)
 {
         return task_thread_info(p)->cpu;
 }


static inline void clear_ti_thread_flag_tlx(struct thread_info_tlx *ti, int flag)
{
				__clear_bit_tlx(flag, (unsigned long *)&ti->flags);
}

static inline void clear_tsk_thread_flag_tlx(struct task_struct *tsk, int flag)
 {
         clear_ti_thread_flag_tlx(task_thread_info(tsk), flag);
 }




#define raw_spin_trylock(lock)  __cond_lock(lock, _raw_spin_trylock(lock))


static inline int sigismember_tlx(sigset_t *set, int _sig)
{
				unsigned long sig = _sig - 1;
				if (_NSIG_WORDS == 1)
								return 1 & (set->sig[0] >> sig);
				else
								return 1 & (set->sig[sig / _NSIG_BPW] >> (sig % _NSIG_BPW));
}

static inline int test_bit_tlx(int nr, const volatile unsigned long *addr)
{
				return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
}

static inline int test_ti_thread_flag_tlx(struct thread_info_tlx *ti, int flag)
{
					return test_bit_tlx(flag, (unsigned long *)&ti->flags);
}

static inline int test_tsk_thread_flag_tlx(struct task_struct *tsk, int flag)
{
				return test_ti_thread_flag_tlx(task_thread_info(tsk), flag);
}

int signal_pending_tlx(struct task_struct *p)
{
				return unlikely(test_tsk_thread_flag_tlx(p,TIF_SIGPENDING));
}



int __fatal_signal_pending_tlx(struct task_struct *p)
{
				return unlikely(sigismember_tlx(&p->pending.signal, SIGKILL));
}

static inline int signal_pending_state_tlx(long state, struct task_struct *p)
 {
         if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
                 return 0;
         if (!signal_pending_tlx(p))
                 return 0;

         return (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending_tlx(p);
 }




#define ___GFP_RECLAIMABLE      0x80000u

#define __GFP_NOWARN    ((__force gfp_t)___GFP_NOWARN)  /* Suppress page allocation failure warning */
#define __GFP_NOFAIL    ((__force gfp_t)___GFP_NOFAIL)  /* See above */
#define __GFP_NORETRY   ((__force gfp_t)___GFP_NORETRY) /* See above */
#define __GFP_NOTRACK   ((__force gfp_t)___GFP_NOTRACK)  /* Don't track with kmemcheck */

#define GFP_DMA         __GFP_DMA
#define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE) /* Page is reclaimable */



#define GFP_RECLAIM_MASK (__GFP_WAIT|__GFP_HIGH|__GFP_IO|__GFP_FS|\
                         __GFP_NOWARN|__GFP_REPEAT|__GFP_NOFAIL|\
                         __GFP_NORETRY|__GFP_MEMALLOC|__GFP_NOMEMALLOC)

#define GFP_CONSTRAINT_MASK (__GFP_HARDWALL|__GFP_THISNODE)


#define FMODE_EXEC              ((__force fmode_t)0x20)
#define READ                    0
#define MAX_RW_COUNT (INT_MAX & PAGE_CACHE_MASK)

#define O_RDONLY        00000000
#define __FMODE_EXEC            ((__force int) FMODE_EXEC)
#define MAY_EXEC                0x00000001
#define MAY_OPEN                0x00000020
#define AT_FDCWD                -100    /* Special value used to indicate
                                            openat should use the current
                                             working directory. */

#define O_RDWR          00000002
#define MKDEV(ma,mi)    (((ma) << MINORBITS) | (mi))
#define MS_KERNMOUNT    (1<<22) /* this is a kern_mount call */


int numa_node_id_tlx(void)
{
				return 0;
//         raw_cpu_read(numa_node);
}


static inline struct page *
__alloc_pages_tlx(gfp_t gfp_mask, unsigned int order,
								struct zonelist *zonelist)
{
				return __alloc_pages_nodemask_tlx(gfp_mask, order, zonelist, NULL);
}

static inline int gfp_zonelist_tlx(gfp_t flags)
{
				return 0;
}


static inline struct zonelist *node_zonelist_tlx(int nid, gfp_t flags)
{
				return NODE_DATA(nid)->node_zonelists + gfp_zonelist_tlx(flags);
}

struct page *alloc_pages_node_tlx(int nid, gfp_t gfp_mask,
                                                 unsigned int order)
{
         /* Unknown node is current node */
         if (nid < 0)
                 nid = numa_node_id_tlx();

         return __alloc_pages_tlx(gfp_mask, order, node_zonelist_tlx(nid, gfp_mask));
}



#define roundup(x, y) (                                 \
 {                                                       \
         const typeof(y) __y = y;                        \
         (((x) + (__y - 1)) / __y) * __y;                \
  }                                                       \
 )

#define INT_MAX         ((int)(~0U>>1))


#define PTR_ALIGN(p, a)         ((typeof(p))ALIGN((unsigned long)(p), (a)))

void kobject_init_tlx(struct kobject *kobj, struct kobj_type *ktype)
{
//			kref_init(&kobj->kref);
			atomic_set(&(&kobj->kref)->refcount, 1);
			INIT_LIST_HEAD(&kobj->entry);
			kobj->state_in_sysfs = 0;
			kobj->state_add_uevent_sent = 0;
			kobj->state_remove_uevent_sent = 0;
			kobj->state_initialized = 1;
			kobj->ktype = ktype;

};

#define be32_to_cpup __be32_to_cpup_tlx
# define lockdep_init_map(lock, name, key, sub) \
               do { (void)(name); (void)(key); } while (0)


static inline void __seqcount_init_tlx(seqcount_t *s, const char *name,
                                           struct lock_class_key *key)
 {
         /*
          * Make sure we are not reinitializing a held lock:
         */
         lockdep_init_map(&s->dep_map, name, key, 0);
         s->sequence = 0;
 }


#define NSEC_PER_USEC   1000L
#define MAXPHASE 500000000L     /* max phase error (ns) */
#define STA_UNSYNC      0x0040  /* clock unsynchronized (rw) */
#define NTP_PHASE_LIMIT ((MAXPHASE / NSEC_PER_USEC) << 5) /* beyond max. dispersion */
unsigned long tick_usec_tlx;         /* USER_HZ period (usec) */
unsigned long tick_nsec_tlx;         /* SHIFTED_HZ period (nsec) */
# define USER_HZ        100             /* some user interfaces are */
#define NTP_SCALE_SHIFT         32
#define hlist_first_rcu(head)   (*((struct hlist_node __rcu **)(&(head)->first)))
#define hlist_next_rcu(node)    (*((struct hlist_node __rcu **)(&(node)->next)))


static inline u64 div_u64_rem_tlx2(u64 dividend, u32 divisor, u32 *remainder)
{
				*remainder = dividend % divisor;
				return dividend / divisor;
}

static inline u64 div_u64_tlx(u64 dividend, u32 divisor)
{
         u32 remainder;
         return div_u64_rem_tlx2(dividend, divisor, &remainder);
}
#define NTP_INTERVAL_FREQ  (HZ)




struct cpumask *const cpu_possible_mask_tlx;

#define SECTION_SHIFT           21
#define SECTION_SIZE            (_AC(1, UL) << SECTION_SHIFT)
#define SECTION_MASK            (~(SECTION_SIZE-1))

#define HPAGE_SHIFT             PMD_SHIFT
#define HUGETLB_PAGE_ORDER      (HPAGE_SHIFT - PAGE_SHIFT)



#define PMD_TYPE_SECT           (_AT(pmdval_t, 1) << 0)
#define PMD_SECT_S              (_AT(pmdval_t, 3) << 8)
#define PMD_SECT_AF             (_AT(pmdval_t, 1) << 10)


#define PTE_ATTRINDX(t)         (_AT(pteval_t, (t)) << 2)
#define PGDIR_SHIFT             30
#define PMD_TYPE_SECT           (_AT(pmdval_t, 1) << 0)
#define PMD_SECT_S              (_AT(pmdval_t, 3) << 8)
#define PMD_SECT_AF             (_AT(pmdval_t, 1) << 10)
#define PHYS_MASK_SHIFT         (48)
#define PHYS_MASK               ((UL(1) << PHYS_MASK_SHIFT) - 1)
#define pte_index(addr)         (((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))




#define MT_NORMAL               4
#define PTE_DIRTY               (_AT(pteval_t, 1) << 55)
#define PTE_WRITE               (_AT(pteval_t, 1) << 57)
#define _PAGE_DEFAULT           (PROT_DEFAULT | PTE_ATTRINDX(MT_NORMAL))
#define PTE_PXN                 (_AT(pteval_t, 1) << 53)        /* Privileged XN */

#define PROT_DEFAULT            (PTE_TYPE_PAGE | PTE_AF | PTE_SHARED)
#define MT_DEVICE_nGnRE         1
#define PMD_ATTRINDX(t)         (_AT(pmdval_t, (t)) << 2)
#define PMD_SECT_PXN            (_AT(pmdval_t, 1) << 53)
#define PMD_SECT_UXN            (_AT(pmdval_t, 1) << 54)
#define PROT_SECT_DEFAULT       (PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)
#define PUD_SHIFT       PGDIR_SHIFT
#define PTRS_PER_PUD    1
#define PUD_SIZE        (1UL << PUD_SHIFT)
#define PROT_SECT_DEFAULT       (PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)
#define pmd_val(x)      (x)
static inline pte_t *pmd_page_vaddr_tlx(pmd_t pmd)
 {
         return __va(pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK);
 }
#define pte_offset_kernel(dir,addr)     (pmd_page_vaddr_tlx(*(dir)) + pte_index(addr))






static inline void set_pte_tlx(pte_t *ptep, pte_t pte)
 {
         *ptep = pte;
 }
#define pfn_pte(pfn,prot)       (__pte(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))


#define PROT_DEVICE_nGnRE       (PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_DEVICE_nGnRE))
//#define PROT_NORMAL_NC          (PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL_NC))
//#define PROT_NORMAL             (PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL))

#define PROT_SECT_DEVICE_nGnRE  (PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_DEVICE_nGnRE))
#define PROT_SECT_NORMAL_EXEC   (PROT_SECT_DEFAULT | PMD_SECT_UXN | PMD_ATTRINDX(MT_NORMAL))
#define PAGE_KERNEL_EXEC        __pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE)
#define PROT_SECT_NORMAL_EXEC   (PROT_SECT_DEFAULT | PMD_SECT_UXN | PMD_ATTRINDX(MT_NORMAL))
#define EPROBE_DEFER    517     /* Driver requests probe retry */
#define PMD_SHIFT               21


#define pud_none(pud)           (!pud_val(pud))

 #define pud_bad(pud)            (!(pud_val(pud) & 2))
#define pgd_val(x)      (x)
#define pmd_none(pmd)           (!pmd_val(pmd))
#define PTRS_PER_PTE            512
#define PTRS_PER_PMD            512
#define __pud(x)                                ((pud_t) { __pgd(x) })
#define PMD_TYPE_TABLE          (_AT(pmdval_t, 3) << 0)
#define pmd_index(addr)         (((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
#define pud_val(x)                              (pgd_val((x).pgd))
#define PHYS_MASK_SHIFT         (48)
#define PHYS_MASK               ((UL(1) << PHYS_MASK_SHIFT) - 1)




static inline pmd_t *pud_page_vaddr_tlx(pud_t pud)
 {
         return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
 }

#define KERNEL_DS       (-1UL)
#define get_ds()        (KERNEL_DS)
#define USER_DS         TASK_SIZE_64
#define get_fs()        (current_thread_info_tlx_tlx()->addr_limit)
#define VERIFY_READ 0
#define VERIFY_WRITE 1
#define _sig_andn(x,y)  ((x) & ~(y))
#define _sig_and(x,y)   ((x) & (y))

#define _SIG_SET_BINOP(name, op)					\
static inline void name(sigset_t *r, const sigset_t *a, const sigset_t *b) \
{									\
	unsigned long a0, a1, a2, a3, b0, b1, b2, b3;			\
									\
	switch (_NSIG_WORDS) {						\
	    case 4:							\
		a3 = a->sig[3]; a2 = a->sig[2];				\
		b3 = b->sig[3]; b2 = b->sig[2];				\
		r->sig[3] = op(a3, b3);					\
		r->sig[2] = op(a2, b2);					\
	    case 2:							\
		a1 = a->sig[1]; b1 = b->sig[1];				\
		r->sig[1] = op(a1, b1);					\
	    case 1:							\
		a0 = a->sig[0]; b0 = b->sig[0];				\
		r->sig[0] = op(a0, b0);					\
		break;							\
	    default:	\
		break;				\
	}								\
}

_SIG_SET_BINOP(sigandsets_tlx, _sig_and)
_SIG_SET_BINOP(sigandnsets_tlx, _sig_andn)


#define PMD_SHIFT               21
#define PMD_SIZE                (_AC(1, UL) << PMD_SHIFT)
#define PMD_MASK                (~(PMD_SIZE-1))
#define VM_MAYEXEC      0x00000040
#define VM_MAYWRITE     0x00000020
#define VM_MAYREAD      0x00000010      /* limits for mprotect() etc */

#define AT_EXECFD 2     /* file descriptor of program */
unsigned long elf_hwcap_tlx;
#ifdef __AARCH64EB__
#define ELF_PLATFORM            ("aarch64_be")
#else
#define ELF_PLATFORM            ("aarch64")
#endif
#define ESTALE          116     /* Stale file handle */
#define ECHILD          10      /* No child processes */
#define EOPENSTALE      518     /* open found a stale dentry */
#define ESRCH            3      /* No such process */
#define ELOOP           40      /* Too many symbolic links encountered */
# define rwlock_init(lock)                                      \
         do { *(lock) = __RW_LOCK_UNLOCKED(lock); } while (0)



bool llist_add_batch_tlx(struct llist_node *new_first, struct llist_node *new_last,
                      struct llist_head_tlx *head)
 {
         struct llist_node *first;

         do {
                 new_last->next = first = ACCESS_ONCE(head->first);
         } while (cmpxchg(&head->first, first, new_first) != first);

         return !first;
 }


int atomic_add_return_tlx(int i, atomic_t *v)
{
				unsigned long tmp;
				int result;

				asm volatile("// atomic_add_return\n"
"1:     ldxr    %w0, %2\n"
"       add     %w0, %w0, %w3\n"
"       stlxr   %w1, %w0, %2\n"
"       cbnz    %w1, 1b"
				: "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
				: "Ir" (i)
				: "memory");

				smp_mb();
				return result;
	}


static inline void atomic_inc_tlx(atomic_t *v)
{
				atomic_add_return_tlx(1, v);
}

struct kobject *kobject_get_tlx(struct kobject *kobj)
{
				if (kobj)
								atomic_inc_tlx(&(&kobj->kref)->refcount);
			return kobj;
}

static inline struct cred *div_u64_rem_tlx(struct cred *cred)
 {
         atomic_inc_tlx(&cred->usage);
         return cred;
 }

static inline spinlock_t *ptlock_ptr_tlx(struct page *page)
 {
       return NULL;
//       *page->ptl;
 }

#define pte_clear(mm,addr,ptep) set_pte_tlx(ptep, __pte(0))

static inline pte_t ptep_get_and_clear_tlx(struct mm_struct *mm,
                                        unsigned long address,
                                        pte_t *ptep)
 {
         pte_t pte = *ptep;
         pte_clear(mm, address, ptep);
         return pte;
 }

static inline pte_t __ptep_modify_prot_start_tlx(struct mm_struct *mm,
                                              unsigned long addr,
                                              pte_t *ptep)
 {
         /*
          * Get the current pte state, but zero it out to make it
          * non-present, preventing the hardware from asynchronously
          * updating it.
          */
         return ptep_get_and_clear_tlx(mm, addr, ptep);
 }
#define PTRS_PER_PGD            512
#define dmb(opt)        asm volatile("dmb " #opt : : : "memory")



#define pte_valid_user(pte) \
         ((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))

//#define pte_special(pte)        (!!(pte_val(pte) & PTE_SPECIAL))
#define pte_exec(pte)           (!(pte_val(pte) & PTE_UXN))
#define pte_dirty(pte)          (!!(pte_val(pte) & PTE_DIRTY))
#define pte_write(pte)          (!!(pte_val(pte) & PTE_WRITE))

void set_pte_at_tlx(struct mm_struct *mm, unsigned long addr,
                               pte_t *ptep, pte_t pte)
 {
         if (pte_valid_user(pte)) {
//                 if (!pte_special(pte) && pte_exec(pte))
//                         __sync_icache_dcache(pte, addr);
                 if (pte_dirty(pte) && pte_write(pte))
                         pte_val(pte) &= ~PTE_RDONLY;
                 else
                         pte_val(pte) |= PTE_RDONLY;
         }

         set_pte_tlx(ptep, pte);
 }

static inline void __ptep_modify_prot_commit_tlx(struct mm_struct *mm,
                                              unsigned long addr,
                                              pte_t *ptep, pte_t pte)
 {
         /*
          * The pte is non-present, so there's no hardware state to
          * preserve.
          */
         set_pte_at_tlx(mm, addr, ptep, pte);
 }






# define RW_DEP_MAP_INIT(lockname)
#define __ARCH_RW_LOCK_UNLOCKED         { 0 }
#define pmd_page(pmd)           pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
static struct page *pmd_to_page_tlx(pmd_t *pmd)
 {
         unsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);
         return virt_to_page((void *)((unsigned long) pmd & mask));
 }

struct cred *get_cred_tlx(const struct cred *cred)
{
         struct cred *nonconst_cred = (struct cred *) cred;
         validate_creds(cred);
         return div_u64_rem_tlx(nonconst_cred);
}

static inline spinlock_t *pmd_lockptr_tlx(struct mm_struct *mm, pmd_t *pmd)
 {
         return ptlock_ptr_tlx(pmd_to_page_tlx(pmd));
 }

int percpu_counter_batch_tlx;

void percpu_counter_add_tlx(struct percpu_counter *fbc, s64 amount);

void percpu_counter_inc_tlx(struct percpu_counter *fbc)
{
       percpu_counter_add_tlx(fbc, 1);
}
static inline bool llist_add_tlx(struct llist_node *new, struct llist_head_tlx *head)
 {
         return llist_add_batch_tlx(new, new, head);
 }
static inline int waitqueue_active_tlx(wait_queue_head_t *q)
{
         return !list_empty(&q->task_list);
}



#define S_ISREG(m)      (((m) & S_IFMT) == S_IFREG)
#define EISDIR          21      /* Is a directory */
#define ECHILD          10      /* No child processes */
static inline unsigned raw_seqcount_begin_tlx(const seqcount_t *s)
 {
         unsigned ret = ACCESS_ONCE(s->sequence);

         seqcount_lockdep_reader_access(s);
         smp_rmb();
         return ret & ~1;
 }
static inline void hlist_del_init_rcu_tlx(struct hlist_node *n)
 {
       if (!hlist_unhashed(n)) {
                 __hlist_del(n);
                 n->pprev = NULL;
       }
 }
static inline void write_seqlock_tlx(seqlock_t *sl)
 {
         spin_lock_tlx(&sl->lock);
//         write_seqcount_begin(&sl->seqcount);
 }

static inline void write_sequnlock_tlx(seqlock_t *sl)
{
//         write_seqcount_end(&sl->seqcount);
         spin_unlock_tlx(&sl->lock);
}


#define elf_addr_t      Elf64_Off
#define AT_BASE_PLATFORM 24     /* string identifying real platform, may
                                  * differ from AT_PLATFORM. */
#define AT_PLATFORM 15  /* string identifying CPU for optimizations */
#define AT_EXECFN  31   /* filename of program */
#define AT_RANDOM 25    /* address of 16 random bytes */
#define AT_SECURE 23   /* secure mode boolean */
#define AT_UID    11    /* real uid */
#define AT_EUID   12    /* effective uid */
#define AT_GID    13    /* real gid */
#define AT_EGID   14    /* effective gid */
uid_t from_kuid_munged_tlx(struct user_namespace *to, kuid_t uid) {};
gid_t from_kgid_munged_tlx(struct user_namespace *to, kgid_t gid) {};
#define AT_FLAGS  8     /* flags */
#define AT_ENTRY  9     /* entry point of program */
#define AT_PHNUM  5     /* number of program headers */
#define AT_PAGESZ 6     /* system page size */
#define AT_BASE   7     /* base address of interpreter */
#define AT_PHDR   3     /* program headers for program */
#define AT_PHENT  4     /* size of program header entry */
# define CLOCKS_PER_SEC (USER_HZ)       /* in "ticks" like times() */
#define AT_CLKTCK 17    /* frequency at which times() increments */
#define AT_PAGESZ 6     /* system page size */
#define ELF_EXEC_PAGESIZE       PAGE_SIZE
#define elf_hwcap_tlx               (elf_hwcap_tlx)
#define AT_HWCAP  16    /* arch dependent hints at CPU capabilities */
static inline int is_highmem_tlx(struct zone *zone)
{
     return 0;
}
#define PageHighMem(__p) is_highmem_tlx(page_zone_tlx(__p))

static inline void hlist_add_after_rcu_tlx(struct hlist_node *prev,
                                        struct hlist_node *n)
 {
         n->next = prev->next;
         n->pprev = &prev->next;
         rcu_assign_pointer(hlist_next_rcu(prev), n);
         if (n->next)
                 n->next->pprev = &n->next;
 }
static inline void hlist_add_head_rcu_tlx(struct hlist_node *n,
                                         struct hlist_head *h)
 {
         struct hlist_node *first = h->first;

         n->next = first;
         n->pprev = &h->first;
         rcu_assign_pointer(hlist_first_rcu(h), n);
         if (first)
                 first->pprev = &n->next;
 }







#define xchg(ptr,x) \
({ \
	__typeof__(*(ptr)) __ret; \
	__ret = (__typeof__(*(ptr))) \
		__xchg_tlx((unsigned long)(x), (ptr), sizeof(*(ptr))); \
	__ret; \
})

#define xchg(ptr,x) \
 ({ \
         __typeof__(*(ptr)) __ret; \
         __ret = (__typeof__(*(ptr))) \
                 __xchg_tlx((unsigned long)(x), (ptr), sizeof(*(ptr))); \
         __ret; \
 })



#define TASK_RUNNING            0

#define TASK_UNINTERRUPTIBLE    2
static inline void set_task_comm_tlx(struct task_struct *tsk, const char *from)
 {
//         __set_task_comm(tsk, from, false);
//		     trace_task_rename(tsk, from);
         strlcpy_tlx(tsk->comm, from, sizeof(tsk->comm));
//				 perf_event_comm(tsk, exec);
 }
void ignore_signals_tlx(struct task_struct * t) {};
int set_cpus_allowed_ptr_tlx(struct task_struct *p,
                                 const struct cpumask *new_mask) {};

#define cpu_all_mask to_cpumask(cpu_all_bits_tlx)
#define PF_NOFREEZE     0x00008000      /* this thread should not be frozen */
#define set_current_state(state_value)          \
         set_mb(current->state, (state_value))

#define CLONE_FS        0x00000200      /* set if fs info shared between processes */
#define CLONE_FILES     0x00000400      /* set if open files shared between processes */
#define CLONE_VM        0x00000100      /* set if VM shared between processes */
#define CLONE_UNTRACED          0x00800000      /* set if the tracing process can't force CLONE_PTRACE on this clone */


struct hlist_bl_node {
         struct hlist_bl_node *next, **pprev;
  };

static inline int hlist_bl_unhashed_tlx(const struct hlist_bl_node *h)
 {
         return !h->pprev;
 }

static inline void INIT_HLIST_BL_NODE_tlx(struct hlist_bl_node *h)
{
         h->next = NULL;
         h->pprev = NULL;
}







#define ilog2(n)				\
(						\
	__builtin_constant_p(n) ? (		\
		(n) < 1 ? ____ilog2_NaN() :	\
		(n) & (1ULL << 63) ? 63 :	\
		(n) & (1ULL << 62) ? 62 :	\
		(n) & (1ULL << 61) ? 61 :	\
		(n) & (1ULL << 60) ? 60 :	\
		(n) & (1ULL << 59) ? 59 :	\
		(n) & (1ULL << 58) ? 58 :	\
		(n) & (1ULL << 57) ? 57 :	\
		(n) & (1ULL << 56) ? 56 :	\
		(n) & (1ULL << 55) ? 55 :	\
		(n) & (1ULL << 54) ? 54 :	\
		(n) & (1ULL << 53) ? 53 :	\
		(n) & (1ULL << 52) ? 52 :	\
		(n) & (1ULL << 51) ? 51 :	\
		(n) & (1ULL << 50) ? 50 :	\
		(n) & (1ULL << 49) ? 49 :	\
		(n) & (1ULL << 48) ? 48 :	\
		(n) & (1ULL << 47) ? 47 :	\
		(n) & (1ULL << 46) ? 46 :	\
		(n) & (1ULL << 45) ? 45 :	\
		(n) & (1ULL << 44) ? 44 :	\
		(n) & (1ULL << 43) ? 43 :	\
		(n) & (1ULL << 42) ? 42 :	\
		(n) & (1ULL << 41) ? 41 :	\
		(n) & (1ULL << 40) ? 40 :	\
		(n) & (1ULL << 39) ? 39 :	\
		(n) & (1ULL << 38) ? 38 :	\
		(n) & (1ULL << 37) ? 37 :	\
		(n) & (1ULL << 36) ? 36 :	\
		(n) & (1ULL << 35) ? 35 :	\
		(n) & (1ULL << 34) ? 34 :	\
		(n) & (1ULL << 33) ? 33 :	\
		(n) & (1ULL << 32) ? 32 :	\
		(n) & (1ULL << 31) ? 31 :	\
		(n) & (1ULL << 30) ? 30 :	\
		(n) & (1ULL << 29) ? 29 :	\
		(n) & (1ULL << 28) ? 28 :	\
		(n) & (1ULL << 27) ? 27 :	\
		(n) & (1ULL << 26) ? 26 :	\
		(n) & (1ULL << 25) ? 25 :	\
		(n) & (1ULL << 24) ? 24 :	\
		(n) & (1ULL << 23) ? 23 :	\
		(n) & (1ULL << 22) ? 22 :	\
		(n) & (1ULL << 21) ? 21 :	\
		(n) & (1ULL << 20) ? 20 :	\
		(n) & (1ULL << 19) ? 19 :	\
		(n) & (1ULL << 18) ? 18 :	\
		(n) & (1ULL << 17) ? 17 :	\
		(n) & (1ULL << 16) ? 16 :	\
		(n) & (1ULL << 15) ? 15 :	\
		(n) & (1ULL << 14) ? 14 :	\
		(n) & (1ULL << 13) ? 13 :	\
		(n) & (1ULL << 12) ? 12 :	\
		(n) & (1ULL << 11) ? 11 :	\
		(n) & (1ULL << 10) ? 10 :	\
		(n) & (1ULL <<  9) ?  9 :	\
		(n) & (1ULL <<  8) ?  8 :	\
		(n) & (1ULL <<  7) ?  7 :	\
		(n) & (1ULL <<  6) ?  6 :	\
		(n) & (1ULL <<  5) ?  5 :	\
		(n) & (1ULL <<  4) ?  4 :	\
		(n) & (1ULL <<  3) ?  3 :	\
		(n) & (1ULL <<  2) ?  2 :	\
		(n) & (1ULL <<  1) ?  1 :	\
		(n) & (1ULL <<  0) ?  0 :	\
		____ilog2_NaN()			\
				   ) :		\
	(sizeof(n) <= 4) ?			\
	__ilog2_u32_tlx(n) :			\
	__ilog2_u64_tlx(n)				\
 )


static inline int sigisemptyset_tlx(sigset_t *set)
{
	switch (_NSIG_WORDS) {
	case 4:
		return (set->sig[3] | set->sig[2] |
			set->sig[1] | set->sig[0]) == 0;
	case 2:
		return (set->sig[1] | set->sig[0]) == 0;
	case 1:
		return set->sig[0] == 0;
	default:
		return 0;
	}
}

void pud_clear_bad_tlx(pud_t * p) {};




static __always_inline unsigned long __fls_tlx(unsigned long word)
{
					return (sizeof(word) * 8) - 1 - __builtin_clzl(word);
}


static __always_inline int fls64_tlx(__u64 x)
{
				if (x == 0)
								return 0;
				return __fls_tlx(x) + 1;
	}

unsigned long find_last_bit_tlx(const unsigned long *addr, unsigned long size)
{
	unsigned long words;
	unsigned long tmp;

	/* Start at final word. */
	words = size / BITS_PER_LONG;

	/* Partial final word? */
	if (size & (BITS_PER_LONG-1)) {
		tmp = (addr[words] & (~0UL >> (BITS_PER_LONG
					- (size & (BITS_PER_LONG-1)))));
		if (tmp)
			goto found;
	}

	while (words) {
		tmp = addr[--words];
		if (tmp) {
found:
			return words * BITS_PER_LONG + __fls_tlx(tmp);
		}
	}

	/* Not found */
	return size;
}

int __get_order_tlx(unsigned long size)
 {
         int order;

         size--;
         size >>= PAGE_SHIFT;
#if BITS_PER_LONG == 32
         order = fls_tlx(size);
#else
       order = fls64_tlx(size);
#endif
          return order;
 }

static inline unsigned long __cmpxchg_tlx(volatile void *ptr, unsigned long old,
				      unsigned long new, int size)
{
	unsigned long oldval = 0, res;

	switch (size) {
	case 1:
		do {
			asm volatile("// __cmpxchg1\n"
			"	ldxrb	%w1, %2\n"
			"	mov	%w0, #0\n"
			"	cmp	%w1, %w3\n"
			"	b.ne	1f\n"
			"	stxrb	%w0, %w4, %2\n"
			"1:\n"
				: "=&r" (res), "=&r" (oldval), "+Q" (*(u8 *)ptr)
				: "Ir" (old), "r" (new)
				: "cc");
		} while (res);
		break;

	case 2:
		do {
			asm volatile("// __cmpxchg2\n"
			"	ldxrh	%w1, %2\n"
			"	mov	%w0, #0\n"
			"	cmp	%w1, %w3\n"
			"	b.ne	1f\n"
			"	stxrh	%w0, %w4, %2\n"
			"1:\n"
				: "=&r" (res), "=&r" (oldval), "+Q" (*(u16 *)ptr)
				: "Ir" (old), "r" (new)
				: "cc");
		} while (res);
		break;

	case 4:
		do {
			asm volatile("// __cmpxchg4\n"
			"	ldxr	%w1, %2\n"
			"	mov	%w0, #0\n"
			"	cmp	%w1, %w3\n"
			"	b.ne	1f\n"
			"	stxr	%w0, %w4, %2\n"
			"1:\n"
				: "=&r" (res), "=&r" (oldval), "+Q" (*(u32 *)ptr)
				: "Ir" (old), "r" (new)
				: "cc");
		} while (res);
		break;

	case 8:
		do {
			asm volatile("// __cmpxchg8\n"
			"	ldxr	%1, %2\n"
			"	mov	%w0, #0\n"
			"	cmp	%1, %3\n"
			"	b.ne	1f\n"
			"	stxr	%w0, %4, %2\n"
			"1:\n"
				: "=&r" (res), "=&r" (oldval), "+Q" (*(u64 *)ptr)
				: "Ir" (old), "r" (new)
				: "cc");
		} while (res);
		break;

	default:
		BUILD_BUG();
	}

	return oldval;
}

static inline unsigned long __cmpxchg_mb_tlx(volatile void *ptr, unsigned long old,
                                          unsigned long new, int size)
 {
         unsigned long ret;

         smp_mb();
         ret = __cmpxchg_tlx(ptr, old, new, size);
         smp_mb();

         return ret;
 }

static inline int page_zone_id_tlx(struct page *page)
 {
         return (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;
 }
static inline int PageBuddy_tlx(struct page *page)
 {
         return atomic_read(&page->_mapcount) == PAGE_BUDDY_MAPCOUNT_VALUE;
 }

static inline int put_page_testzero_tlx(struct page *page)
 {
         return atomic_dec_and_test(&page->_count);
 }
static inline int PageAnon_tlx(struct page *page)
 {
         return ((unsigned long)page->mapping & PAGE_MAPPING_ANON) != 0;
 }
static inline void
 debug_check_no_locks_freed_tlx(const void *from, unsigned long len)
{
}


#  define raw_cpu_add_1(pcp, val)       raw_cpu_generic_to_op((pcp), (val), +=)
#  define raw_cpu_add_2(pcp, val)       raw_cpu_generic_to_op((pcp), (val), +=)
#  define raw_cpu_add_4(pcp, val)       raw_cpu_generic_to_op((pcp), (val), +=)
#  define raw_cpu_add_8(pcp, val)       raw_cpu_generic_to_op((pcp), (val), +=)
# define raw_cpu_add(pcp, val)  __pcpu_size_call(raw_cpu_add_, (pcp), (val))

struct kmem_cache *filp_cachep_tlx ;
struct workqueue_struct *system_wq_tlx;
struct vm_event_state_tlx vm_event_states_tlx;
#  define is_migrate_cma(migratetype) false

# define SPIN_DEBUG_INIT(lockname)
unsigned int m_hash_mask_tlx;
unsigned int m_hash_shift_tlx;
 struct hlist_head *mount_hashtable_tlx ;
 unsigned int mp_hash_mask_tlx ;
 unsigned int mp_hash_shift_tlx ;
struct kmem_cache *mnt_cache_tlx ;
 unsigned int d_hash_mask_tlx ;
 unsigned int d_hash_shift_tlx ;
struct hlist_bl_head *dentry_hashtable_tlx ;


#define __RAW_SPIN_LOCK_INITIALIZER(lockname)   \
         {                                       \
         .raw_lock = __ARCH_SPIN_LOCK_UNLOCKED,  \
         SPIN_DEP_MAP_INIT(lockname) }

#define __RAW_SPIN_LOCK_UNLOCKED(lockname)      \
          (raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)

#define __SPIN_LOCK_INITIALIZER(lockname) \
       { { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }

#define __SPIN_LOCK_UNLOCKED(lockname) \
         (spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)

static inline void __count_vm_events_tlx(enum vm_event_item item, long delta)
 {
         raw_cpu_add(vm_event_states_tlx.event[item], delta);
  }


static inline void set_freepage_migratetype_tlx(struct page *page, int migratetype)
 {
         page->index = migratetype;
 }

#define set_page_private(page, v)       ((page)->private = (v))
static inline void __mod_zone_freepage_state_tlx(struct zone *zone, int nr_pages,
                                              int migratetype)
 {
  //       __mod_zone_page_state_tlx(zone, NR_FREE_PAGES, nr_pages);
//         if (is_migrate_cma(migratetype))
//                 __mod_zone_page_state_tlx(zone, NR_FREE_CMA_PAGES, nr_pages);
 }



static inline void
debug_check_no_obj_freed_tlx(const void *address, unsigned long size) { }




#ifdef CONFIG_ZONE_DMA
#define DMA_ZONE(xx) xx##_DMA,
#else
#define DMA_ZONE(xx)
#endif

 #ifdef CONFIG_ZONE_DMA32
 #define DMA32_ZONE(xx) xx##_DMA32,
 #else
 #define DMA32_ZONE(xx)
 #endif

 #ifdef CONFIG_HIGHMEM
 #define HIGHMEM_ZONE(xx) , xx##_HIGH
 #else
 #define HIGHMEM_ZONE(xx)
#endif

#define FOR_ALL_ZONES(xx) DMA_ZONE(xx) DMA32_ZONE(xx) xx##_NORMAL HIGHMEM_ZONE(xx) , xx##_MOVABLE
#define __ARCH_SPIN_LOCK_UNLOCKED       { 0 , 0 }
# define SPIN_DEP_MAP_INIT(lockname)
#define set_mb(var, value)      do { var = value; smp_mb(); } while (0)







static inline unsigned long __xchg_tlx(unsigned long x, volatile void *ptr, int size)
{
	unsigned long ret, tmp;

	switch (size) {
	case 1:
		asm volatile("//	__xchg1\n"
		"1:	ldxrb	%w0, %2\n"
		"	stlxrb	%w1, %w3, %2\n"
		"	cbnz	%w1, 1b\n"
			: "=&r" (ret), "=&r" (tmp), "+Q" (*(u8 *)ptr)
			: "r" (x)
			: "memory");
		break;
	case 2:
		asm volatile("//	__xchg2\n"
		"1:	ldxrh	%w0, %2\n"
		"	stlxrh	%w1, %w3, %2\n"
		"	cbnz	%w1, 1b\n"
			: "=&r" (ret), "=&r" (tmp), "+Q" (*(u16 *)ptr)
			: "r" (x)
			: "memory");
		break;
	case 4:
		asm volatile("//	__xchg4\n"
		"1:	ldxr	%w0, %2\n"
		"	stlxr	%w1, %w3, %2\n"
		"	cbnz	%w1, 1b\n"
			: "=&r" (ret), "=&r" (tmp), "+Q" (*(u32 *)ptr)
			: "r" (x)
			: "memory");
		break;
	case 8:
		asm volatile("//	__xchg8\n"
		"1:	ldxr	%0, %2\n"
		"	stlxr	%w1, %3, %2\n"
		"	cbnz	%w1, 1b\n"
			: "=&r" (ret), "=&r" (tmp), "+Q" (*(u64 *)ptr)
			: "r" (x)
			: "memory");
		break;
	default:
		BUILD_BUG();
	}

	smp_mb();
	return ret;
}


long do_fork_tlx(unsigned long clone_flags,
				unsigned long stack_start,
				unsigned long stack_size,
				int __user *parent_tidptr,
				int __user *child_tidptr);

static pid_t kernel_thread_tlx(int (*fn)(void *), void *arg, unsigned long flags)
 {
         return do_fork_tlx(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
                 (unsigned long)arg, NULL, NULL);
 }



#define __stringify_1(x...)     #x
#define __stringify(x...)       __stringify_1(x)


static inline void * __must_check ERR_PTR_tlx(long error)
 {
         return (void *) error;
 }

#define DECLARE_COMPLETION(work) \
          struct completion work = COMPLETION_INITIALIZER(work)

# define DECLARE_COMPLETION_ONSTACK(work) DECLARE_COMPLETION(work)



#define EXPORT_SYMBOL(sym)
#define EXPORT_SYMBOL_GPL(sym)

#define __WAIT_QUEUE_HEAD_INITIALIZER(name) {                           \
         .lock           = __SPIN_LOCK_UNLOCKED(name.lock),              \
          .task_list      = { &(name).task_list, &(name).task_list } }

#define COMPLETION_INITIALIZER(work) \
          { 0, __WAIT_QUEUE_HEAD_INITIALIZER((work).wait) }

#define RB_ROOT (struct rb_root) { NULL, }

#define pfn_valid_within(pfn) (1)

static inline int __check_is_bitmap_tlx(const unsigned long *bitmap)
 {
         return 1;
 }

#define to_cpumask(bitmap)                                              \
         ((struct cpumask *)(1 ? (bitmap)                                \
                             : (void *)sizeof(__check_is_bitmap_tlx(bitmap))))

const DECLARE_BITMAP(cpu_all_bits_tlx, NR_CPUS);






#define _this_cpu_generic_to_op(pcp, val, op)                           \
 do {                                                                    \
         unsigned long flags;                                            \
         raw_local_irq_save(flags);                                      \
         *raw_cpu_ptr(&(pcp)) op val;                                    \
         raw_local_irq_restore(flags);                                   \
 } while (0)

#  define this_cpu_add_1(pcp, val)      _this_cpu_generic_to_op((pcp), (val), +=)
#  define this_cpu_add_2(pcp, val)      _this_cpu_generic_to_op((pcp), (val), +=)
#  define this_cpu_add_4(pcp, val)      _this_cpu_generic_to_op((pcp), (val), +=)
#  define this_cpu_add_8(pcp, val)      _this_cpu_generic_to_op((pcp), (val), +=)





static inline void iounmap_tlx(void __iomem *addr)
{
}
#define readl(c)                ({ u32 __v = readl_relaxed(c); __iormb(); __v; })
#define atomic64_inc_not_zero(v)        atomic64_add_unless_tlx((v), 1LL, 0LL)
#define __iormb()               rmb()
#define readl_relaxed(c)        ({ u32 __v = le32_to_cpu((__force __le32)__raw_readl_tlx(c)); __v; })


#define atomic_long_inc_not_zero(l) atomic64_inc_not_zero((atomic64_t *)(l))

#define TIMER_FLAG_MASK                 0x3LU
#define TIMER_NOT_PINNED        0
#define S_IALLUGO       (S_ISUID|S_ISGID|S_ISVTX|S_IRWXUGO)
#define EROFS           30      /* Read-only file system */
#define ENOTDIR         20      /* Not a directory */
#define EEXIST          17      /* File exists */
#define TASK_NORMAL             (TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)



void complete_tlx(struct completion *x)
{
				x->done++;
//       __wake_up_locked(&x->wait, TASK_NORMAL, 1);
//				__wake_up_common(&x->wait, TASK_NORMAL, 1, 0, NULL);
					wait_queue_head_t *q = &x->wait;
					unsigned int mode = TASK_NORMAL;
					int nr_exclusive = 1;
					int wake_flags = 0;
					void *key = NULL;
					wait_queue_t *curr, *next;

					list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
								unsigned flags = curr->flags;

							if (curr->func(curr, mode, wake_flags, key) &&
																(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
												break;
				}
};
#define pmd_addr_end(addr, end)                                         \
 ({      unsigned long __boundary = ((addr) + PMD_SIZE) & PMD_MASK;      \
         (__boundary - 1 < (end) - 1)? __boundary: (end);                \
})

#define __phys_to_pfn(paddr)    ((unsigned long)((paddr) >> PAGE_SHIFT))
#define pud_addr_end(addr, end)                 (end)
#define PUD_MASK        (~(PUD_SIZE-1))



static inline pud_t * pud_offset_tlx(pgd_t * pgd, unsigned long address)
 {
          return (pud_t *)pgd;
 }


static inline pmd_t *pmd_offset_tlx(pud_t *pud, unsigned long addr)
{
         return (pmd_t *)pud_page_vaddr_tlx(*pud) + pmd_index(addr);
}



static inline void set_pmd_tlx(pmd_t *pmdp, pmd_t pmd)
 {
         *pmdp = pmd;
         dsb(ishst);
 }




static inline u32 icache_policy_tlx(void)
{
         return (read_cpuid_cachetype_tlx() >> CTR_L1IP_SHIFT) & CTR_L1IP_MASK;
}




static inline void __ClearPageBuddy_tlx(struct page *page)
{
//       VM_BUG_ON_PAGE(!PageBuddy(page), page);
				atomic_set(&page->_mapcount, -1);
}

static inline void __SetPageBuddy_tlx(struct page *page)
{
//         VM_BUG_ON_PAGE(atomic_read(&page->_mapcount) != -1, page);
				atomic_set(&page->_mapcount, PAGE_BUDDY_MAPCOUNT_VALUE);
}

static inline void set_pud_tlx(pud_t *pudp, pud_t pud)
 {
         *pudp = pud;
         dsb(ishst);
 }

static inline unsigned int cpumask_next_tlx(int n, const struct cpumask *srcp)
{
         return n+1;
}

#define per_cpu_ptr(ptr, cpu)   SHIFT_PERCPU_PTR((ptr), per_cpu_offset((cpu)))
#define per_cpu(var, cpu) \
          (*SHIFT_PERCPU_PTR(&(var), per_cpu_offset(cpu)))


int nr_cpu_ids_tlx_tlx = NR_CPUS;
#define pmd_index(addr)         (((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
#define for_each_cpu(cpu, mask)                         \
         for ((cpu) = -1;                                \
                 (cpu) = cpumask_next_tlx((cpu), (mask)),    \
                 (cpu) < nr_cpu_ids_tlx_tlx;)

#define lockdep_set_class_and_name(lock, key, name) \
                 lockdep_init_map(&(lock)->dep_map, name, key, 0)







#define for_each_online_pgdat(pgdat)                    \
         for (pgdat = first_online_pgdat_tlx();              \
            pgdat;                                     \
              pgdat = next_online_pgdat_tlx(pgdat))

#define min_t(type, x, y) ({                    \
        type __min1 = (x);                      \
         type __min2 = (y);                      \
         __min1 < __min2 ? __min1: __min2; })

unsigned long totalram_pages_tlx;



#define IDR_BITS 8
#define IDR_SIZE (1 << IDR_BITS)

int do_raw_spin_trylock_tlx(raw_spinlock_t *lock) {};
#define TICKET_SHIFT    16
#  define ATOMIC_HASH_SIZE 4

#define CLOCK_MONOTONIC                 1
struct address_space_operations {
	int (*writepage)(struct page *page, struct writeback_control *wbc);
	int (*readpage)(struct file *, struct page *);

	/* Write back some dirty pages from this mapping. */
	int (*writepages)(struct address_space *, struct writeback_control *);

	/* Set a page dirty.  Return true if this dirtied it */
	int (*set_page_dirty)(struct page *page);

	int (*readpages)(struct file *filp, struct address_space *mapping,
			struct list_head *pages, unsigned nr_pages);

	int (*write_begin)(struct file *, struct address_space *mapping,
				loff_t pos, unsigned len, unsigned flags,
				struct page **pagep, void **fsdata);
	int (*write_end)(struct file *, struct address_space *mapping,
				loff_t pos, unsigned len, unsigned copied,
				struct page *page, void *fsdata);

	/* Unfortunately this kludge is needed for FIBMAP. Don't use it */
	sector_t (*bmap)(struct address_space *, sector_t);
	void (*invalidatepage) (struct page *, unsigned int, unsigned int);
	int (*releasepage) (struct page *, gfp_t);
	void (*freepage)(struct page *);
	ssize_t (*direct_IO)(int, struct kiocb *, struct iov_iter *iter, loff_t offset);
	int (*get_xip_mem)(struct address_space *, pgoff_t, int,
						void **, unsigned long *);
	/*
	 * migrate the contents of a page to the specified target. If
	 * migrate_mode is MIGRATE_ASYNC, it must not block.
	 */
	int (*migratepage) (struct address_space *,
			struct page *, struct page *, enum migrate_mode);
	int (*launder_page) (struct page *);
	int (*is_partially_uptodate) (struct page *, unsigned long,
					unsigned long);
	void (*is_dirty_writeback) (struct page *, bool *, bool *);
	int (*error_remove_page)(struct address_space *, struct page *);

	/* swapfile support */
	int (*swap_activate)(struct swap_info_struct *sis, struct file *file,
				sector_t *span);
	void (*swap_deactivate)(struct file *file);
};

const struct address_space_operations empty_aops_tlx = {
 };


#define ALLOC_HARDER            0x10 /* try to alloc harder */
#define ALLOC_WMARK_MIN         WMARK_MIN

bool gfp_pfmemalloc_allowed_tlx(gfp_t gfp_mask)
{
	int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;
	alloc_flags |= (__force int) (gfp_mask & __GFP_HIGH);
	alloc_flags |= ALLOC_HARDER;
	return !!(alloc_flags & ALLOC_NO_WATERMARKS);;
}

# define seqcount_init(s)                               \
         do {                                            \
                 static struct lock_class_key __key;     \
                 __seqcount_init_tlx((s), #s, &__key);       \
         } while (0)

#define this_cpu_inc(pcp)               this_cpu_add(pcp, 1)
#define CURRENT_TIME            (current_kernel_time_tlx())

static void kfree_tlx(const void *x__);



#define IDA_CHUNK_SIZE          128     /* 128 bytes per chunk */
#define IDA_BITMAP_LONGS        (IDA_CHUNK_SIZE / sizeof(long) - 1)
#define IDA_BITMAP_BITS         (IDA_BITMAP_LONGS * sizeof(long) * 8)

struct ida_bitmap {
         long                    nr_busy;
         unsigned long           bitmap[IDA_BITMAP_LONGS];
};

#define MAX_IDR_SHIFT		(sizeof(int) * 8 - 1)
#define MAX_IDR_BIT		(1U << MAX_IDR_SHIFT)

/* Leave the possibility of an incomplete final layer */
#define MAX_IDR_LEVEL ((MAX_IDR_SHIFT + IDR_BITS - 1) / IDR_BITS)

/* Number of id_layer structs to leave in free list */
#define MAX_IDR_FREE (MAX_IDR_LEVEL * 2)

static void __move_to_free_list(struct idr *idp, struct idr_layer *p)
{
	p->ary[0] = idp->id_free;
	idp->id_free = p;
	idp->id_free_cnt++;
}

static void move_to_free_list(struct idr *idp, struct idr_layer *p)
{
	unsigned long flags;

	/*
	 * Depends on the return element being zeroed.
	 */
//	spin_lock_irqsave(&idp->lock, flags);
	__move_to_free_list(idp, p);
//	spin_unlock_irqrestore(&idp->lock, flags);
}


static inline void *kmem_cache_zalloc_tlx(struct kmem_cache *k, gfp_t flags);

static struct kmem_cache *idr_layer_cache;//0

int __idr_pre_get_tlx(struct idr *idp, gfp_t gfp_mask)
{
	while (idp->id_free_cnt < MAX_IDR_FREE) {
		struct idr_layer *new;
		new = kmem_cache_zalloc_tlx(idr_layer_cache, gfp_mask);
		if (new == NULL)
			return (0);
		move_to_free_list(idp, new);
	}
	return 1;
}

int ida_pre_get_tlx(struct ida *ida, gfp_t gfp_mask)
{
	/* allocate idr_layers */
	if (!__idr_pre_get_tlx(&ida->idr, gfp_mask))
		return 0;

	/* allocate free_bitmap */
	if (!ida->free_bitmap) {
		struct ida_bitmap *bitmap;

		bitmap = kmalloc_tlx(sizeof(struct ida_bitmap), gfp_mask);
		if (!bitmap)
			return 0;

				unsigned long flags;

				if (!ida->free_bitmap) {
//					spin_lock_irqsave(&ida->idr.lock, flags);
					if (!ida->free_bitmap) {
						ida->free_bitmap = bitmap;
						bitmap = NULL;
					}
//					spin_unlock_irqrestore(&ida->idr.lock, flags);
				}

				kfree_tlx(bitmap);
	}

	return 1;
}

static int idr_max(int layers)
{
	int bits = min_t(int, layers * IDR_BITS, MAX_IDR_SHIFT);

	return (1 << bits) - 1;
}

int __bitmap_full_tlx(const unsigned long *bitmap, int bits)
 {
         int k, lim = bits/BITS_PER_LONG;
         for (k = 0; k < lim; ++k)
                 if (~bitmap[k])
                         return 0;

         if (bits % BITS_PER_LONG)
                 if (~bitmap[k] & BITMAP_LAST_WORD_MASK(bits))
                         return 0;

         return 1;
 }

static inline int bitmap_full(const unsigned long *src, int nbits)
{
         if (small_const_nbits(nbits))
                 return ! (~(*src) & BITMAP_LAST_WORD_MASK(nbits));
         else
                 return __bitmap_full_tlx(src, nbits);
}

#define IDR_MASK ((1 << IDR_BITS)-1)
unsigned long find_next_zero_bit_tlx(const unsigned long *addr, unsigned long size,
				unsigned long offset);

void kmem_cache_free_tlx(struct kmem_cache *s, void *x);


static void idr_mark_full(struct idr_layer **pa, int id)
{
	struct idr_layer *p = pa[0];
	int l = 0;

	__set_bit_tlx(id & IDR_MASK, p->bitmap);
	/*
	 * If this layer is full mark the bit in the layer above to
	 * show that this part of the radix tree is full.  This may
	 * complete the layer above and require walking up the radix
	 * tree.
	 */
	while (bitmap_full(p->bitmap, IDR_SIZE)) {
		if (!(p = pa[++l]))
			break;
		id = id >> IDR_BITS;
		__set_bit_tlx((id & IDR_MASK), p->bitmap);
	}
}

#define EAGAIN          11      /* Try again */

static struct idr_layer *get_from_free_list(struct idr *idp)
{
	struct idr_layer *p;
	unsigned long flags;

//	spin_lock_irqsave(&idp->lock, flags);
	if ((p = idp->id_free)) {
		idp->id_free = p->ary[0];
		idp->id_free_cnt--;
		p->ary[0] = NULL;
	}
//	spin_unlock_irqrestore(&idp->lock, flags);
	return(p);
}

int ida_get_new_above_tlx(struct ida *ida, int starting_id, int *p_id)
{
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct ida_bitmap *bitmap;
	unsigned long flags;
	int idr_id = starting_id / IDA_BITMAP_BITS;
	int offset = starting_id % IDA_BITMAP_BITS;
	int t, id;
	struct idr *idp = &ida->idr;
	starting_id = idr_id;
	gfp_t gfp_mask = 0;
	struct idr *layer_idr = &ida->idr;
	struct idr_layer *p, *new;
	int layers, v;

 restart:
		id = starting_id;
 build_up:
		p = idp->top;
		layers = idp->layers;
		if (unlikely(!p)) {
			if (!(p = kmem_cache_zalloc_tlx(idr_layer_cache, gfp_mask)))
				return -ENOMEM;
			p->layer = 0;
			layers = 1;
		}
		while (id > idr_max(layers)) {
			layers++;
			if (!p->count) {
				p->layer++;
				continue;
			}
			new = kmem_cache_zalloc_tlx(idr_layer_cache, gfp_mask);;
			new->ary[0] = p;
			new->count = 1;
			new->layer = layers-1;
			new->prefix = id & ~idr_max(new->layer + 1);
			if (bitmap_full(p->bitmap, IDR_SIZE))
				__set_bit_tlx(0, new->bitmap);
			p = new;
		}
		asm volatile ("stlr %1, %0" : "=Q" (idp->top) : "r" (RCU_INITIALIZER(p)) : "memory");
		idp->layers = layers;
			int *starting_id0 = &id;
				int n, m, sh;
				int l, oid;

				id = *starting_id0;
	restart0:
				p = idp->top;
				l = idp->layers;
				pa[l--] = NULL;
				while (1) {
					n = (id >> (IDR_BITS*l)) & IDR_MASK;
					m = find_next_zero_bit_tlx(p->bitmap, IDR_SIZE, n);
					if (m == IDR_SIZE) {
						l++;
						oid = id;
						id = (id | ((1 << (IDR_BITS * l)) - 1)) + 1;
						p = pa[l];
						sh = IDR_BITS * (l + 1);
						if (oid >> sh == id >> sh)
							continue;
						else
							goto restart0;
					}
					if (m != n) {
						sh = IDR_BITS*l;
						id = ((id >> sh) ^ n ^ m) << sh;
					}
					if (l == 0)
						break;
					if (!p->ary[m]) {
						new = kmem_cache_zalloc_tlx(idr_layer_cache, gfp_mask);
						new->layer = l-1;
						p->count++;
					}
					pa[l--] = p;
					p = p->ary[m];
				}
				pa[l] = p;
		if (id == -EAGAIN)
			goto build_up;
	t= id;

	if (t != idr_id)
		offset = 0;
	idr_id = t;
	bitmap = (void *)pa[0]->ary[idr_id & IDR_MASK];
	if (!bitmap) {
		bitmap = ida->free_bitmap;
		ida->free_bitmap = NULL;
		if (!bitmap)
			return -EAGAIN;
		memset_tlx(bitmap, 0, sizeof(struct ida_bitmap));
		pa[0]->count++;
	}
	t = find_next_zero_bit_tlx(bitmap->bitmap, IDA_BITMAP_BITS, offset);
	if (t == IDA_BITMAP_BITS) {
		idr_id++;
		offset = 0;
		goto restart;
	}

	id = idr_id * IDA_BITMAP_BITS + t;
	__set_bit_tlx(t, bitmap->bitmap);
	if (++bitmap->nr_busy == IDA_BITMAP_BITS)
		idr_mark_full(pa, idr_id);

	*p_id = id;
	if (ida->idr.id_free_cnt || ida->free_bitmap) {
		struct idr_layer *p = get_from_free_list(&ida->idr);
		if (p)
			kmem_cache_free_tlx(idr_layer_cache, p);
	}

	return 0;
}


void *pcpu_alloc_tlx(size_t size, size_t align, bool reserved);
#define alloc_percpu(type)                                              \
         (typeof(type) __percpu *)pcpu_alloc_tlx(sizeof(type),           \
                                                 __alignof__(type), false)

struct pglist_data *first_online_pgdat_tlx(void)
{
         return NODE_DATA(first_online_node);
}

struct pglist_data *next_online_pgdat_tlx(struct pglist_data *pgdat)
{
         int nid = MAX_NUMNODES;

         if (nid == MAX_NUMNODES)
                 return NULL;
         return NODE_DATA(nid);
}




void __rcu_read_lock_tlx(void)
 {
 }

void __rcu_read_unlock_tlx(void)
{
}

static inline long __must_check PTR_ERR_tlx(__force const void *ptr)
  {
          return (long) ptr;
 }




#define NTP_INTERVAL_LENGTH (NSEC_PER_SEC/NTP_INTERVAL_FREQ)
# define do_div(n,base) ({                                      \
         uint32_t __base = (base);                               \
         uint32_t __rem;                                         \
         __rem = ((uint64_t)(n)) % __base;                       \
         (n) = ((uint64_t)(n)) / __base;                         \
         __rem;                                                  \
   })

struct task_group root_task_group_tlx;

struct task_struct init_task_tlx;

#define PIDNS_HASH_ADDING (1U << 31)

struct pid_namespace init_pid_ns_tlx = {
	.kref = {
		.refcount       = ATOMIC_INIT(2),
	},
	.pidmap = {
		[ 0 ... PIDMAP_ENTRIES-1] = { ATOMIC_INIT(BITS_PER_PAGE), NULL }
	},
	.last_pid = 0,
	.nr_hashed = PIDNS_HASH_ADDING,
	.level = 0,
	.child_reaper = &init_task_tlx,
	.user_ns = &init_user_ns_tlx,
	.proc_inum = PROC_PID_INIT_INO,
};

int sigprocmask_tlx(int i, sigset_t * a, sigset_t * b) {};
struct pid *get_task_pid_tlx(struct task_struct *task, enum pid_type type)
{
         struct pid *pid;
//       rcu_read_lock();
         if (type != PIDTYPE_PID)
               task = task->group_leader;
         pid = task->pids[type].pid;
					if (pid)
											atomic_inc_tlx(&pid->count);
//         rcu_read_unlock();
         return pid;
 }



static inline void
 init_waitqueue_func_entry_tlx(wait_queue_t *q, wait_queue_func_t func)
{
         q->flags        = 0;
         q->private      = NULL;
         q->func         = func;
}

void add_wait_queue_tlx(wait_queue_head_t *q, wait_queue_t *wait)
 {
         unsigned long flags;

         wait->flags &= ~WQ_FLAG_EXCLUSIVE;
  //       spin_lock_irqsave(&q->lock, flags);
//         __add_wait_queue_tlx(q, wait);
					list_add(&wait->task_list, &q->task_list);
//         spin_unlock_irqrestore(&q->lock, flags);
}


#define read_lock(lock)         _raw_read_lock_tlx(lock)
#define WNOHANG         0x00000001
#define ERESTARTSYS     512

void remove_wait_queue_tlx(wait_queue_head_t *q, wait_queue_t *wait)
 {
         unsigned long flags;

//         spin_lock_irqsave(&q->lock, flags);
//         __remove_wait_queue_tlx(q, wait);
				list_del(&wait->task_list);
//         spin_unlock_irqrestore(&q->lock, flags);
 }


static inline spinlock_t *pte_lockptr_tlx(struct mm_struct *mm, pmd_t *pmd)
{
         return ptlock_ptr_tlx(pmd_page(*pmd));
}
#define pte_offset_map(dir,addr)        pte_offset_kernel((dir), (addr))
static inline spinlock_t *pmd_lock_tlx(struct mm_struct *mm, pmd_t *pmd)
{
         spinlock_t *ptl = pmd_lockptr_tlx(mm, pmd);
         spin_lock_tlx(ptl);
         return ptl;
}
static inline int pte_swp_soft_dirty_tlx(pte_t pte)
{
         return 0;
}
static inline pte_t pte_swp_clear_soft_dirty_tlx(pte_t pte)
{
         return pte;
}
#define __pte_to_swp_entry(pte) ((swp_entry_t) { pte_val(pte) })
#define __swp_type(x)           (((x).val >> __SWP_TYPE_SHIFT) & __SWP_TYPE_MASK)
#define __swp_entry(type,offset) ((swp_entry_t) { ((type) << __SWP_TYPE_SHIFT) | ((offset) << __SWP_OFFSET_SHIFT) })
#define __swp_offset(x)         (((x).val >> __SWP_OFFSET_SHIFT) & __SWP_OFFSET_MASK)
#define __swp_entry_to_pte(swp) ((pte_t) { (swp).val })
#define pte_present(pte)        (!!(pte_val(pte) & (PTE_VALID | PTE_PROT_NONE)))
static inline pte_t ptep_modify_prot_start_tlx(struct mm_struct *mm,
                                            unsigned long addr,
                                            pte_t *ptep)
 {
         return __ptep_modify_prot_start_tlx(mm, addr, ptep);
 }

int pte_numa_tlx(pte_t pte) {};
pte_t pte_mknonnuma_tlx(pte_t pte) {};

static inline pte_t pte_modify_tlx(pte_t pte, pgprot_t newprot)
{
         const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
                               PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
         pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
         return pte;
}
#define pte_dirty(pte)          (!!(pte_val(pte) & PTE_DIRTY))
static inline pte_t pte_mkwrite_tlx(pte_t pte)
{
         pte_val(pte) |= PTE_WRITE;
         return pte;
}

static inline void ptep_modify_prot_commit_tlx(struct mm_struct *mm,
                                            unsigned long addr,
                                            pte_t *ptep, pte_t pte)
{
         __ptep_modify_prot_commit_tlx(mm, addr, ptep, pte);
}

#define pte_file(pte)           (pte_val(pte) & PTE_FILE)

static inline pte_t pte_swp_mksoft_dirty_tlx(pte_t pte)
 {
         return pte;
 }



#define arch_leave_lazy_mmu_mode()      do {} while (0)
#define pte_unmap_unlock(pte, ptl)      do {            \
         spin_unlock_tlx(ptl);                               \
         pte_unmap(pte);                                 \
} while (0)


static inline int pud_none_or_clear_bad_tlx(pud_t *pud)
 {
         if (pud_none(*pud))
                 return 1;
         if (unlikely(pud_bad(*pud))) {
                 pud_clear_bad_tlx(pud);
         return 1;
         }
         return 0;
 }

#define VM_HUGETLB      0x00400000      /* Huge TLB Page VM */



#define pgd_offset(mm, addr)    ((mm)->pgd+pgd_index_tlx(addr))
#define smp_wmb()       dmb(ishst)
#define smp_mb__before_spinlock()       smp_wmb()

static inline void set_tlb_flush_pending_tlx(struct mm_struct *mm)
 {
         mm->tlb_flush_pending = true;

         /*
          * Guarantee that the tlb_flush_pending store does not leak into the
          * critical section updating the page tables
          */
         smp_mb__before_spinlock();
 }

#define pgd_addr_end(addr, end)                                         \
 ({      unsigned long __boundary = ((addr) + PGDIR_SIZE) & PGDIR_MASK;  \
         (__boundary - 1 < (end) - 1)? __boundary: (end);                \
 })
static inline int pgd_none_tlx(pgd_t pgd)           { return 0; }
#define ASID(mm)        ((mm)->context.id & 0xffff)


#define NR_IRQS 64
int nr_irqs_tlx = NR_IRQS;

#define BOOTMEM_LOW_LIMIT 0
#define BOOTMEM_ALLOC_ACCESSIBLE        0
unsigned long max_low_pfn_tlx;
unsigned long max_pfn_tlx;
#define MAX_DMA_ADDRESS PAGE_OFFSET


static inline void *alloc_remap_tlx(int nid, unsigned long size)
{
         return NULL;
}






const unsigned long *pcpu_unit_offsets_tlx;
#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof((arr)[0]) + __must_be_array(arr))
void *pcpu_base_addr_tlx;
#define zone_idx(zone)          ((zone) - (zone)->zone_pgdat->node_zones)



#define SWAPPER_DIR_SIZE        (3 * PAGE_SIZE)
#define IDMAP_DIR_SIZE          (2 * PAGE_SIZE)
#define PGDIR_SIZE              (_AC(1, UL) << PGDIR_SHIFT)
#define PGDIR_MASK              (~(PGDIR_SIZE-1))
#define cpu_to_be32 __cpu_to_be32
#ifdef CONFIG_MODULES
#define PERCPU_MODULE_RESERVE           (8 << 10)
#else
#define PERCPU_MODULE_RESERVE           0
#endif
#if BITS_PER_LONG > 32
#define PERCPU_DYNAMIC_RESERVE          (20 << 10)
#else
#define PERCPU_DYNAMIC_RESERVE          (12 << 10)
#endif
#define ULONG_MAX       (~0UL)
#define PFN_ALIGN(x)    (((unsigned long)(x) + (PAGE_SIZE - 1)) & PAGE_MASK)
#define PERCPU_DYNAMIC_EARLY_SIZE       (12 << 10)
#define PCPU_MIN_UNIT_SIZE              PFN_ALIGN(32 << 10)
#define num_possible_cpus()     1U
#define PERCPU_DYNAMIC_EARLY_SLOTS      128


static inline int cpumask_scnprintf_tlx(char *buf, int len,
                                     const struct cpumask *srcp)
 {
//         return bitmap_scnprintf(buf, len, cpumask_bits(srcp), nr_cpumask_bits);
 }



//struct timespec {
//         __kernel_time_t tv_sec;                 /* seconds */
//         long            tv_nsec;                /* nanoseconds */
//};







extern pgd_t swapper_pg_dir_tlx[PTRS_PER_PGD];
extern pgd_t idmap_pg_dir_tlx[PTRS_PER_PGD];


#define S_IRUSR 00400
#define S_IWUSR 00200
#define S_IFDIR  0040000
#define S_ISDIR(m)      (((m) & S_IFMT) == S_IFDIR)
#define lockdep_set_class(lock, key) \
                 lockdep_init_map(&(lock)->dep_map, #key, key, 0)
#define GFP_HIGHUSER_MOVABLE    (__GFP_WAIT | __GFP_IO | __GFP_FS | \
                                  __GFP_HARDWALL | __GFP_HIGHMEM | \
                                  __GFP_MOVABLE)

#define prefetchw(x) __builtin_prefetch(x,1)

static inline void spin_lock_prefetch_tlx(const void *x)
{
         prefetchw(x);
}

static inline void atomic64_add_tlx(u64 i, atomic64_t *v)
{
				long result;
				unsigned long tmp;

				asm volatile("// atomic64_add\n"
"1:     ldxr    %0, %2\n"
"       add     %0, %0, %3\n"
"       stxr    %w1, %0, %2\n"
"       cbnz    %w1, 1b"
				: "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
				: "Ir" (i));
}

void atomic_long_add_tlx(long i, atomic_long_t *l)
{
         atomic64_t *v = (atomic64_t *)l;

         atomic64_add_tlx(i, v);
}

static inline int PageSlabPfmemalloc_tlx(struct page *page)
{
         return PageActive(page);
}

static inline void SetPageSlabPfmemalloc_tlx(struct page *page)
 {
         SetPageActive(page);
 }

#define alloc_pages(gfp_mask, order) \
                 alloc_pages_node_tlx(numa_node_id_tlx(), gfp_mask, order)

static inline int __raw_spin_trylock_tlx(raw_spinlock_t *lock)
  {
          preempt_disable();
          if (do_raw_spin_trylock_tlx(lock)) {
                  spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
                  return 1;
          }
          preempt_enable();
          return 0;
  }

















#define _atomic_spin_lock_irqsave(l,f) do {     \
} while(0)

#define _atomic_spin_unlock_irqrestore(l,f) do {        \
 } while(0)




void up_write_tlx(struct rw_semaphore *sem) {};
void free_pages_tlx(unsigned long addr, unsigned int order)
{
       if (addr != 0) {
               __free_pages_tlx(virt_to_page((void *)addr), order);
         }
}
#define free_page(addr) free_pages_tlx((addr), 0)
#define S_IRWXU 00700
#define S_IRWXG 00070
#define S_IRWXO 00007


#define __rcu_dereference_protected(p, c, space) \
 ({ \
         rcu_lockdep_assert(c, "suspicious rcu_dereference_protected() usage"); \
         rcu_dereference_sparse(p, space); \
         ((typeof(*p) __force __kernel *)(p)); \
 })

#define rcu_dereference_protected(p, c) \
         __rcu_dereference_protected((p), (c), __rcu)



#define rcu_lock_acquire(a)            do { } while (0)
#define rcu_lockdep_assert(c, s) do { } while (0)
#define rcu_lock_release_tlx(a)

static inline void rcu_read_lock_tlx(void)
 {
         __rcu_read_lock_tlx();
         __acquire(RCU);
         rcu_lock_acquire(&rcu_lock_map);
         rcu_lockdep_assert(rcu_is_watching_tlx(),
                            "rcu_read_lock_tlx() used illegally while idle");
 }

static inline void rcu_read_unlock_tlx(void)
 {
         rcu_lockdep_assert(rcu_is_watching_tlx(),
                            "rcu_read_unlock_tlx() used illegally while idle");
         rcu_lock_release_tlx(&rcu_lock_map);
         __release(RCU);
         __rcu_read_unlock_tlx();
 }






#define list_entry_rcu(ptr, type, member) \
({ \
         typeof(*ptr) __rcu *__ptr = (typeof(*ptr) __rcu __force *)ptr; \
         container_of((typeof(ptr))rcu_dereference_raw(__ptr), type, member); \
})


struct task_struct *next_thread_tlx(const struct task_struct *p)
{
				return list_entry_rcu(p->thread_group.next,
															struct task_struct, thread_group);
}


static inline int test_and_clear_bit_tlx(int nr, volatile unsigned long *addr)
{
				unsigned long mask = BIT_MASK(nr);
				unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
				unsigned long old;
				unsigned long flags;

				_atomic_spin_lock_irqsave(p, flags);
				old = *p;
				*p = old & ~mask;
				_atomic_spin_unlock_irqrestore(p, flags);

}

static inline int test_and_clear_ti_thread_flag_tlx(struct thread_info_tlx *ti, int flag)
  {
         return test_and_clear_bit_tlx(flag, (unsigned long *)&ti->flags);
  }


static inline void __flush_icache_all_tlx(void)
 {
         asm("ic ialluis");
         dsb(ish);
 }

static inline int icache_is_aivivt_tlx(void)
{
         return icache_policy_tlx() == ICACHE_POLICY_AIVIVT;
}











#define S_IRWXUGO       (S_IRWXU|S_IRWXG|S_IRWXO)
#define S_ISVTX  0001000
#define current_fsuid()         (current_cred_xxx(fsuid))
#define current_fsgid()         (current_cred_xxx(fsgid))
#define percpu_counter_init(fbc, value)                                 \
         ({                                                              \
                 static struct lock_class_key __key;                     \
                                                                         \
                 __percpu_counter_init_tlx(fbc, value, &__key);              \
         })
#define VM_NORESERVE    0x00200000      /* should the VM suppress accounting */

#define get_cpu_var(var) (*({                           \
          preempt_disable();                              \
         this_cpu_ptr(&var); }))

#define put_cpu_var(var) do {                           \
         (void)&(var);                                   \
         preempt_enable();                               \
} while (0)

#define current_cred() \
         rcu_dereference_protected(current->cred, 1)


#define S_IFMT  00170000
#define S_IFSOCK 0140000
#define S_IFLNK  0120000
#define S_IFREG  0100000
#define S_IFBLK  0060000
#define S_IFIFO  0010000

#define S_ISBLK(m)      (((m) & S_IFMT) == S_IFBLK)
#define S_ISFIFO(m)     (((m) & S_IFMT) == S_IFIFO)
#define S_ISSOCK(m)     (((m) & S_IFMT) == S_IFSOCK)

#define MAX_USER_RT_PRIO        100
#define MAX_RT_PRIO             MAX_USER_RT_PRIO
static inline struct task_struct *next_thread_tlx_tlx(const struct task_struct *p)
 {
         return list_entry_rcu(p->thread_group.next,
                               struct task_struct, thread_group);
 }


#define _QW_WMASK       0xff            /* Writer mask             */
#define _QR_SHIFT       8               /* Reader count shift      */
#define _QR_BIAS        (1U << _QR_SHIFT)

void queue_read_lock_slowpath_tlx(struct qrwlock *lock) {};



static inline void queue_read_lock_tlx(struct qrwlock *lock)
 {
         u32 cnts;

         cnts = atomic_add_return_tlx(_QR_BIAS, &lock->cnts);
         if (likely(!(cnts & _QW_WMASK)))
                 return;

         /* The slowpath will decrement the reader count, if necessary. */
         queue_read_lock_slowpath_tlx(lock);
 }

#define PTE_RDONLY              (_AT(pteval_t, 1) << 7)         /* AP[2] */
#define PTE_SPECIAL             (_AT(pteval_t, 1) << 56)
#define pte_young(pte)          (!!(pte_val(pte) & PTE_AF))
#define pte_valid(pte)          (pte_val(pte) & L_PTE_VALID)
#define pte_pfn(pte)            ((pte_val(pte) & PHYS_MASK) >> PAGE_SHIFT)
#define pte_page(pte)           (pfn_to_page(pte_pfn(pte)))
#define PG_dcache_clean PG_arch_1
typedef __u16   Elf64_Half;
struct thread_info {
         unsigned long           flags;          /* low level flags */
         mm_segment_t            addr_limit;     /* address limit */
         struct task_struct      *task;          /* main task structure */
         struct exec_domain      *exec_domain;   /* execution domain */
         struct restart_block    restart_block;
         int                     preempt_count;  /* 0 => preemptable, <0 => bug */
         int                     cpu;            /* cpu */
};


struct group_info init_groups_tlx = { .usage = ATOMIC_INIT(2) };

struct user_struct root_user_tlx = {
				.__count        = ATOMIC_INIT(1),
				.processes      = ATOMIC_INIT(1),
				.sigpending     = ATOMIC_INIT(0),
				.locked_shm     = 0,
};

#define INIT_USER (&root_user_tlx)
# define CAP_FULL_SET     ((kernel_cap_t){{ ~0, ~0 }})
# define CAP_EMPTY_SET    ((kernel_cap_t){{ 0, 0 }})
#define SECUREBITS_DEFAULT 0x00000000

struct cred init_cred_tlx  = {
 .usage			= ATOMIC_INIT(4),
 .uid			= GLOBAL_ROOT_UID,
 .gid			= GLOBAL_ROOT_GID,
 .suid			= GLOBAL_ROOT_UID,
 .sgid			= GLOBAL_ROOT_GID,
 .euid			= GLOBAL_ROOT_UID,
 .egid			= GLOBAL_ROOT_GID,
 .fsuid			= GLOBAL_ROOT_UID,
 .fsgid			= GLOBAL_ROOT_GID,
 .securebits		= SECUREBITS_DEFAULT,
 .cap_inheritable	= CAP_EMPTY_SET,
 .cap_permitted		= CAP_FULL_SET,
 .cap_effective		= CAP_FULL_SET,
 .cap_bset		= CAP_FULL_SET,
 .user			= INIT_USER,
 .user_ns		= &init_user_ns_tlx,
 .group_info		= &init_groups_tlx,
};

union thread_union {
         struct thread_info thread_info;
         unsigned long stack[THREAD_SIZE/sizeof(long)];
};

#define __RWSEM_OPT_INIT(lockname)
# define __RWSEM_DEP_MAP_INIT(lockname)

#define INIT_MM_CONTEXT(name) \
          .context.id_lock = __RAW_SPIN_LOCK_UNLOCKED(name.context.id_lock),

#define RWSEM_UNLOCKED_VALUE            0x00000000

#define __RWSEM_INITIALIZER(name)                               \
         { .count = RWSEM_UNLOCKED_VALUE,                        \
         .wait_list = LIST_HEAD_INIT((name).wait_list),        \
         .wait_lock = __RAW_SPIN_LOCK_UNLOCKED(name.wait_lock) \
         __RWSEM_OPT_INIT(name)                                \
         __RWSEM_DEP_MAP_INIT(name) }

struct mm_struct init_mm_tlx = {
         .mm_rb          = RB_ROOT,
         .pgd            = swapper_pg_dir_tlx,
         .mm_users       = ATOMIC_INIT(2),
         .mm_count       = ATOMIC_INIT(1),
         .mmap_sem       = __RWSEM_INITIALIZER(init_mm_tlx.mmap_sem),
         .page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm_tlx.page_table_lock),
         .mmlist         = LIST_HEAD_INIT(init_mm_tlx.mmlist),
         INIT_MM_CONTEXT(init_mm_tlx)
 };



 long do_no_restart_syscall_tlx(struct restart_block *parm) {};

 #define PREEMPT_ENABLED (0)
 #define PREEMPT_DISABLED        PREEMPT_ENABLED

 #define PF_WQ_WORKER    0x00000020      /* I'm a workqueue worker */
 #define PF_FROZEN       0x00010000      /* frozen for system suspend */




 #define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)
 #define MMF_DUMP_FILTER_MASK \
          (((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)




 #define PREEMPT_ACTIVE_BITS     1
 #define PREEMPT_ACTIVE_SHIFT    (NMI_SHIFT + NMI_BITS)

 #define PREEMPT_ACTIVE  (__IRQ_MASK(PREEMPT_ACTIVE_BITS) << PREEMPT_ACTIVE_SHIFT)

 #define INIT_PREEMPT_COUNT      (PREEMPT_DISABLED + PREEMPT_ACTIVE)

  typedef void (*handler_t)(int, struct pt_regs *);

 struct exec_domain {
          const char              *name;          /* name of the execdomain */
          handler_t               handler;        /* handler for syscalls */
          unsigned char           pers_low;       /* lowest personality */
          unsigned char           pers_high;      /* highest personality */
          unsigned long           *signal_map;    /* signal mapping */
          unsigned long           *signal_invmap; /* reverse signal mapping */
          struct map_segment      *err_map;       /* error mapping */
          struct map_segment      *socktype_map;  /* socket type mapping */
          struct map_segment      *sockopt_map;   /* socket option mapping */
          struct map_segment      *af_map;        /* address family mapping */
          struct module           *module;        /* module context of the ed. */
          struct exec_domain      *next;          /* linked list (internal) */
  };

 struct exec_domain       default_exec_domain_tlx;

 #define INIT_THREAD_INFO(tsk)                                           \
  {                                                                       \
          .task           = &tsk,                                         \
          .exec_domain    = &default_exec_domain_tlx,                         \
          .flags          = 0,                                            \
          .preempt_count  = INIT_PREEMPT_COUNT,                           \
          .addr_limit     = KERNEL_DS,                                    \
          .restart_block  = {                                             \
                  .fn     = do_no_restart_syscall_tlx,                        \
           },                                                              \
  }





 //union thread_union init_thread_union;
 // = { INIT_THREAD_INFO(init_task_tlx) };



#define PLIST_NODE_INIT(node, __prio)                   \
{                                                       \
         .prio  = (__prio),                              \
         .prio_list = LIST_HEAD_INIT((node).prio_list),  \
         .node_list = LIST_HEAD_INIT((node).node_list),  \
}


union thread_union init_thread_union;

#define init_thread_info        (init_thread_union.thread_info)

#define CPU_MASK_LAST_WORD BITMAP_LAST_WORD_MASK(NR_CPUS)
#define MAX_NICE        19
#define MIN_NICE        -20
#define NICE_WIDTH      (MAX_NICE - MIN_NICE + 1)

#define MAX_USER_RT_PRIO        100
#define MAX_RT_PRIO             MAX_USER_RT_PRIO

#define MAX_PRIO                (MAX_RT_PRIO + NICE_WIDTH)

#define CPU_MASK_ALL                                                    \
 (cpumask_t) { {                                                         \
         [BITS_TO_LONGS(NR_CPUS)-1] = CPU_MASK_LAST_WORD                 \
 } }

#define SCHED_NORMAL            0
#define RR_TIMESLICE            (100 * HZ / 1000)
#define RCU_POINTER_INITIALIZER(p, v) \
                 .p = RCU_INITIALIZER(v)

# define INIT_PUSHABLE_TASKS(tsk)					\
	.pushable_tasks = PLIST_NODE_INIT(tsk.pushable_tasks, MAX_PRIO),




#define INIT_TASK_COMM "swapper"

#define INIT_IDS
#define INIT_THREAD  {  }

struct fs_struct init_fs_tlx; //
struct files_struct init_files_tlx;



#define INIT_CPUTIME

#define INIT_CPU_TIMERS(cpu_timers)                                     \
{                                                                       \
				LIST_HEAD_INIT(cpu_timers[0]),                                  \
				LIST_HEAD_INIT(cpu_timers[1]),                                  \
				LIST_HEAD_INIT(cpu_timers[2]),                                  \
}


# define INIT_GROUP_RWSEM

#define INIT_SIGNALS(sig) {                                             \
         .thread_head    = LIST_HEAD_INIT(init_task_tlx.thread_node),        \
         .wait_chldexit  = __WAIT_QUEUE_HEAD_INITIALIZER(sig.wait_chldexit),\
         .shared_pending = {                                             \
                 .list = LIST_HEAD_INIT(sig.shared_pending.list),        \
                 .signal =  {{0}}},                                      \
         .posix_timers    = LIST_HEAD_INIT(sig.posix_timers),            \
       .cpu_timers     = INIT_CPU_TIMERS(sig.cpu_timers),              \
       .cputimer       = {                                             \
                 .running = 0,                                           \
         },                                                              \
         .cred_guard_mutex =                                             \
                  __MUTEX_INITIALIZER(sig.cred_guard_mutex),             \
         INIT_GROUP_RWSEM(sig)                                           \
}

#define INIT_SIGHAND(sighand) {                                         \
         .count          = ATOMIC_INIT(1),                               \
         .action         = { { { .sa_handler = SIG_DFL, } }, },          \
         .siglock        = __SPIN_LOCK_UNLOCKED(sighand.siglock),        \
         .signalfd_wqh   = __WAIT_QUEUE_HEAD_INITIALIZER(sighand.signalfd_wqh),  \
 }


struct signal_struct init_signals_tlx
 = INIT_SIGNALS(init_signals_tlx);
struct sighand_struct init_sighand_tlx;
//= INIT_SIGHAND(init_sighand_tlx);

struct uts_namespace init_uts_ns_tlx;

struct nsproxy init_nsproxy_tlx = {
         .count                  = ATOMIC_INIT(1),
         .uts_ns                 = &init_uts_ns_tlx,
         .mnt_ns                 = NULL,
         .pid_ns_for_children    = &init_pid_ns_tlx,
 };



struct pid init_struct_pid_tlx; //= INIT_STRUCT_PID;

#define INIT_PID_LINK(type)                                     \
 {                                                               \
         .node = {                                               \
                 .next = NULL,                                   \
                 .pprev = NULL,                                  \
         },                                                      \
         .pid = &init_struct_pid_tlx,                                \
 }

# define INIT_PERF_EVENTS(tsk)

# define INIT_LOCKDEP
# define INIT_FTRACE_GRAPH
# define INIT_TRACE_RECURSION
# define INIT_CPUSET_SEQ
# define INIT_RT_MUTEXES (tsk)
# define INIT_VTIME (tsk)
# define INIT_TASK_RCU_PREEMPT




#define RCU_INITIALIZER_TLX(v) (typeof(*(v)) __force __rcu *)(v)
#define RCU_POINTER_INITIALIZER_TLX(p, v) \
								.p = RCU_INITIALIZER_TLX(v)

# define INIT_CGROUP_SCHED_TLX(tsk)						\
	.sched_task_group = &root_task_group_tlx,

#define __ARCH_SPIN_LOCK_UNLOCKED_TLX       { 0 , 0 }

# define SPIN_DEP_MAP_INIT_TLX(lockname)

#define __RAW_SPIN_LOCK_INITIALIZER_TLX(lockname)   \
				{                                       \
				.raw_lock = __ARCH_SPIN_LOCK_UNLOCKED_TLX,  \
				SPIN_DEP_MAP_INIT_TLX(lockname) }

#define __RAW_SPIN_LOCK_UNLOCKED_TLX(lockname)      \
					(raw_spinlock_t) __RAW_SPIN_LOCK_INITIALIZER(lockname)

#define __SPIN_LOCK_INITIALIZER_TLX(lockname) \
			{ { .rlock = __RAW_SPIN_LOCK_INITIALIZER(lockname) } }

#define __SPIN_LOCK_UNLOCKED_TLX(lockname) \
				(spinlock_t ) __SPIN_LOCK_INITIALIZER(lockname)


#define INIT_PID_LINK_TLX(type)                                     \
{                                                               \
				.node = {                                               \
								.next = NULL,                                   \
								.pprev = NULL,                                  \
				},                                                      \
				.pid = &init_struct_pid_tlx,                                \
}

#define ATOMIC_INIT_TLX(i)  { (i) }

#define INIT_CPU_TIMERS_TLX(cpu_timers)                                     \
{                                                                       \
				LIST_HEAD_INIT(cpu_timers[0]),                                  \
				LIST_HEAD_INIT(cpu_timers[1]),                                  \
				LIST_HEAD_INIT(cpu_timers[2]),                                  \
}



#define INIT_TASK_TLX(tsk)	\
{									\
	.state		= 0,						\
	.stack		= &init_thread_info,				\
	.usage		= ATOMIC_INIT_TLX(2),				\
	.flags		= PF_KTHREAD,					\
	.prio		= MAX_PRIO-20,					\
	.static_prio	= MAX_PRIO-20,					\
	.normal_prio	= MAX_PRIO-20,					\
	.policy		= SCHED_NORMAL,					\
	.cpus_allowed	= CPU_MASK_ALL,					\
	.nr_cpus_allowed= NR_CPUS,					\
	.mm		= NULL,						\
	.active_mm	= &init_mm_tlx,					\
	.se		= {						\
		.group_node 	= LIST_HEAD_INIT(tsk.se.group_node),	\
	},								\
	.rt		= {						\
		.run_list	= LIST_HEAD_INIT(tsk.rt.run_list),	\
		.time_slice	= RR_TIMESLICE,				\
	},								\
	.tasks		= LIST_HEAD_INIT(tsk.tasks),				\
	INIT_CGROUP_SCHED_TLX(tsk)						\
	.ptraced	= LIST_HEAD_INIT(tsk.ptraced),			\
	.ptrace_entry	= LIST_HEAD_INIT(tsk.ptrace_entry),		\
	.real_parent	= &tsk,						\
	.parent		= &tsk,						\
	.children	= LIST_HEAD_INIT(tsk.children),			\
	.sibling	= LIST_HEAD_INIT(tsk.sibling),			\
	.group_leader	= &tsk,						\
	.vfork_done	= NULL,						\
	RCU_POINTER_INITIALIZER_TLX(real_cred, &init_cred_tlx),			\
	RCU_POINTER_INITIALIZER_TLX(cred, &init_cred_tlx),			\
	.comm		= "swapper",				\
	.thread		= { },					\
	.fs		= &init_fs_tlx,					\
	.files		= &init_files_tlx,					\
	.signal		= &init_signals_tlx,				\
	.sighand	= &init_sighand_tlx,				\
	.nsproxy	= &init_nsproxy_tlx,				\
	.pending	= {						\
		.list = LIST_HEAD_INIT(tsk.pending.list),		\
		.signal = {{0}}},					\
	.blocked	= {{0}},					\
	.alloc_lock	= __SPIN_LOCK_UNLOCKED_TLX(tsk.alloc_lock),		\
	.journal_info	= NULL,						\
	.cpu_timers	= INIT_CPU_TIMERS_TLX(tsk.cpu_timers),		\
	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED_TLX(tsk.pi_lock),	\
	.timer_slack_ns = 50000, /* 50 usec default slack */		\
	.pids = {							\
		[PIDTYPE_PID]  = INIT_PID_LINK_TLX(PIDTYPE_PID),		\
		[PIDTYPE_PGID] = INIT_PID_LINK_TLX(PIDTYPE_PGID),		\
		[PIDTYPE_SID]  = INIT_PID_LINK_TLX(PIDTYPE_SID),		\
	},								\
	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
	.thread_node	= LIST_HEAD_INIT(init_signals_tlx.thread_head),	\
}


struct task_struct init_task_tlx = INIT_TASK_TLX(init_task_tlx);

struct task_struct init_task = INIT_TASK_TLX(init_task_tlx);
#define __init_task_data __attribute__((__section__(".data..init_task")))
union thread_union init_thread_union __init_task_data =
	{ INIT_THREAD_INFO(init_task) };



#define ENOEXEC          8      /* Exec format error */
#define PT_INTERP  3
#define SELFMAG         4
#define ELFMAG          "\177ELF"
#define THREAD_START_SP         (THREAD_SIZE - 16)
#define RLIMIT_STACK            3       /* max stack size */



#define S_IXUGO         (S_IXUSR|S_IXGRP|S_IXOTH)
#define S_IRUGO         (S_IRUSR|S_IRGRP|S_IROTH)
#define INT_MIN         (-INT_MAX - 1)
static inline unsigned long task_rlimit_tlx(const struct task_struct *tsk,
                 unsigned int limit)
{
       return ACCESS_ONCE(tsk->signal->rlim[limit].rlim_cur);
}

static inline unsigned long rlimit_tlx(unsigned int limit)
{
         return task_rlimit_tlx(current, limit);
}
#define VM_SEQ_READ     0x00008000      /* App will access data sequentially */

#define TIF_SWITCH_MM           23      /* deferred switch_mm */
#define __get_cpu_var(var) (*this_cpu_ptr(&(var)))
static __always_inline bool static_key_false_tlx(struct static_key *key)
{
         return true;
}


#define list_for_each_entry_rcu(pos, head, member) \
         for (pos = list_entry_rcu((head)->next, typeof(*pos), member); \
                 &pos->member != (head); \
                 pos = list_entry_rcu(pos->member.next, typeof(*pos), member))





struct kmem_cache *mm_cachep_tlx;




static void kfree_tlx(const void *x__);

#define PGD_SIZE        (PTRS_PER_PGD * sizeof(pgd_t))
void pgd_free_tlx(struct mm_struct *mm, pgd_t *pgd)
{
         if (PGD_SIZE == PAGE_SIZE)
                 free_page((unsigned long)pgd);
         else
                 kfree_tlx(pgd);
}

void __mmdrop_tlx(struct mm_struct *mm)
 {
         pgd_free_tlx(mm, mm->pgd);
//         destroy_context(mm);
//         free_mm(mm);
				 kmem_cache_free_tlx(mm_cachep_tlx, (mm));
 }

void mmdrop_tlx(struct mm_struct * mm)
 {
         if (unlikely(atomic_dec_and_test(&mm->mm_count)))
                 __mmdrop_tlx(mm);
 }
void rcu_note_context_switch_tlx(int cpu) {};


#define PF_WQ_WORKER    0x00000020      /* I'm a workqueue worker */
#define PF_FROZEN       0x00010000      /* frozen for system suspend */




#define MMF_DUMPABLE_MASK ((1 << MMF_DUMPABLE_BITS) - 1)
#define MMF_DUMP_FILTER_MASK \
         (((1 << MMF_DUMP_FILTER_BITS) - 1) << MMF_DUMP_FILTER_SHIFT)




#define PREEMPT_ACTIVE_BITS     1
#define PREEMPT_ACTIVE_SHIFT    (NMI_SHIFT + NMI_BITS)





void __might_sleep_tlx(const char *file, int line, int preempt_offset)
{
}





#define test_thread_flag(flag) \
         test_ti_thread_flag_tlx(current_thread_info_tlx_tlx(), flag)

#define tif_need_resched_tlx() test_thread_flag(TIF_NEED_RESCHED)



void __schedule_tlx(void);

void _cond_resched_tlx(void)
{
	if (test_thread_flag(TIF_NEED_RESCHED)) {
//         __preempt_count_add(PREEMPT_ACTIVE);
				__schedule_tlx();
//         __preempt_count_sub(PREEMPT_ACTIVE);
						return 1;
	}
	return 0;
}


struct vm_area_struct *find_vma_tlx(struct mm_struct *mm, unsigned long addr)
{
	struct rb_node *rb_node;
	struct vm_area_struct *vma;

	/* Check the cache first. */
//	vma = vmacache_find(mm, addr);
//	struct mm_struct *mm, unsigned long addr)
//	{
		int i;
		for (i = 0; i < VMACACHE_SIZE; i++) {
			struct vm_area_struct *vma = current->vmacache[i];

			if (!vma)
				continue;
			if (vma->vm_mm != mm)
				break;
			if (vma->vm_start <= addr && vma->vm_end > addr) {
				break;
			}
		}

	if (likely(vma))
		return vma;

	rb_node = mm->mm_rb.rb_node;
	vma = NULL;

	while (rb_node) {
		struct vm_area_struct *tmp;

		tmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);

		if (tmp->vm_end > addr) {
			vma = tmp;
			if (tmp->vm_start <= addr)
				break;
			rb_node = rb_node->rb_left;
		} else
			rb_node = rb_node->rb_right;
	}

//	if (vma)
//		vmacache_update(addr, vma);
	return vma;
}


#define MAP_FIXED       0x10            /* Interpret addr exactly */

struct vm_unmapped_area_info {
#define VM_UNMAPPED_AREA_TOPDOWN 1
				unsigned long flags;
				unsigned long length;
				unsigned long low_limit;
				unsigned long high_limit;
				unsigned long align_mask;
				unsigned long align_offset;
};

#define rb_parent(r)   ((struct rb_node *)((r)->__rb_parent_color & ~3))

unsigned long unmapped_area_topdown_tlx(struct vm_unmapped_area_info *info)
{
	struct mm_struct *mm = current->mm;
	struct vm_area_struct *vma;
	unsigned long length, low_limit, high_limit, gap_start, gap_end;
	length = info->length + info->align_mask;
	gap_end = info->high_limit;
	high_limit = gap_end - length;
	low_limit = info->low_limit + length;
	gap_start = mm->highest_vm_end;
	if (gap_start <= high_limit)
		goto found_highest;
	vma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);
	while (true) {
		gap_start = vma->vm_prev ? vma->vm_prev->vm_end : 0;
		if (gap_start <= high_limit && vma->vm_rb.rb_right) {
			struct vm_area_struct *right =
				rb_entry(vma->vm_rb.rb_right,
					struct vm_area_struct, vm_rb);
			if (right->rb_subtree_gap >= length) {
				vma = right;
				continue;
			}
		}

check_current:
		gap_end = vma->vm_start;
		if (gap_start <= high_limit && gap_end - gap_start >= length)
			goto found;
		if (vma->vm_rb.rb_left) {
			struct vm_area_struct *left =
				rb_entry(vma->vm_rb.rb_left,
					struct vm_area_struct, vm_rb);
			if (left->rb_subtree_gap >= length) {
				vma = left;
				continue;
			}
		}
		while (true) {
			struct rb_node *prev = &vma->vm_rb;
			vma = rb_entry(rb_parent(prev),
							struct vm_area_struct, vm_rb);
			if (prev == vma->vm_rb.rb_right) {
				gap_start = vma->vm_prev ?
					vma->vm_prev->vm_end : 0;
				goto check_current;
			}
		}
	}

found:
	if (gap_end > info->high_limit)
		gap_end = info->high_limit;

found_highest:
	gap_end -= info->length;
	gap_end -= (gap_end - info->align_offset) & info->align_mask;
	return gap_end;
}

#define RB_EMPTY_ROOT(root)  ((root)->rb_node == NULL)

unsigned long unmapped_area_tlx(struct vm_unmapped_area_info *info)
{
	struct mm_struct *mm = current->mm;
	struct vm_area_struct *vma;
	unsigned long length, low_limit, high_limit, gap_start, gap_end;
	length = info->length + info->align_mask;
	high_limit = info->high_limit - length;
	low_limit = info->low_limit + length;
	if (RB_EMPTY_ROOT(&mm->mm_rb))
		goto check_highest;
	vma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);
	if (vma->rb_subtree_gap < length)
		goto check_highest;
	while (true) {
		/* Visit left subtree if it looks promising */
		gap_end = vma->vm_start;
		if (gap_end >= low_limit && vma->vm_rb.rb_left) {
			struct vm_area_struct *left =
				rb_entry(vma->vm_rb.rb_left,
					struct vm_area_struct, vm_rb);
			if (left->rb_subtree_gap >= length) {
				vma = left;
				continue;
			}
		}
		gap_start = vma->vm_prev ? vma->vm_prev->vm_end : 0;
check_current:
		if (gap_end >= low_limit && gap_end - gap_start >= length)
			goto found;

		/* Visit right subtree if it looks promising */
		if (vma->vm_rb.rb_right) {
			struct vm_area_struct *right =
				rb_entry(vma->vm_rb.rb_right,
					struct vm_area_struct, vm_rb);
			if (right->rb_subtree_gap >= length) {
				vma = right;
				continue;
			}
		}

		/* Go back up the rbtree to find next candidate node */
		while (true) {
			struct rb_node *prev = &vma->vm_rb;
			if (!rb_parent(prev))
				goto check_highest;
			vma = rb_entry(rb_parent(prev),
							struct vm_area_struct, vm_rb);
			if (prev == vma->vm_rb.rb_left) {
				gap_start = vma->vm_prev->vm_end;
				gap_end = vma->vm_start;
				goto check_current;
			}
		}
	}

check_highest:
	/* Check highest gap, which does not precede any rbtree node */
	gap_start = mm->highest_vm_end;
	gap_end = ULONG_MAX;  /* Only for VM_BUG_ON below */

found:
	/* We found a suitable gap. Clip it with the original low_limit. */
	if (gap_start < info->low_limit)
		gap_start = info->low_limit;

	/* Adjust gap address to the desired alignment */
	gap_start += (info->align_offset - gap_start) & info->align_mask;
	return gap_start;
}


static inline unsigned long
vm_unmapped_area_tlx(struct vm_unmapped_area_info *info)
{
				if (!(info->flags & VM_UNMAPPED_AREA_TOPDOWN))
								return unmapped_area_tlx(info);
				else
								return unmapped_area_topdown_tlx(info);
}


unsigned long mmap_min_addr_tlx;


unsigned long
arch_get_unmapped_area_tlx(struct file *filp, unsigned long addr,
		unsigned long len, unsigned long pgoff, unsigned long flags)
{
	struct mm_struct *mm = current->mm;
	struct vm_area_struct *vma;
	struct vm_unmapped_area_info info;

	if (len > TASK_SIZE - mmap_min_addr_tlx)
		return -ENOMEM;

	if (flags & MAP_FIXED)
		return addr;

	if (addr) {
		addr = PAGE_ALIGN(addr);
		vma = find_vma_tlx(mm, addr);
		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr_tlx &&
				(!vma || addr + len <= vma->vm_start))
			return addr;
	}

	info.flags = 0;
	info.length = len;
	info.low_limit = mm->mmap_base;
	info.high_limit = TASK_SIZE;
	info.align_mask = 0;
	return vm_unmapped_area_tlx(&info);
}

static __always_inline bool need_resched_tlx(void)
 {
         return unlikely(tif_need_resched_tlx());
 }

static inline int is_compat_task_tlx(void)
{
         return test_thread_flag(TIF_32BIT);
}

#define TASK_DEAD               64
int kref_put_tlx(struct kref *kref, void (*release)(struct kref *kref))
 {
         return kref_sub_tlx(kref, 1, release);
 }

#define __round_mask(x, y) ((__typeof__(x))((y)-1))
#define BUILD_BUG_ON_ZERO(e) (0)



#define PSR_I_BIT       0x00000080
#define TIF_NEED_RESCHED        1
extern void cpu_do_idle_tlx(void);
#define RB_CLEAR_NODE(node)  \
          ((node)->__rb_parent_color = (unsigned long)(node))

#define CLOCK_REALTIME                  0
#define MAX_CLOCKS                      16
#define CLOCK_BOOTTIME                  7
#define CLOCK_TAI                       11

int hrtimer_clock_to_base_table_tlx[MAX_CLOCKS] = {
	[CLOCK_REALTIME]	= HRTIMER_BASE_REALTIME,
	[CLOCK_MONOTONIC]	= HRTIMER_BASE_MONOTONIC,
	[CLOCK_BOOTTIME]	= HRTIMER_BASE_BOOTTIME,
	[CLOCK_TAI]		= HRTIMER_BASE_TAI,
};

struct hrtimer_cpu_base {
         raw_spinlock_t                  lock;
         unsigned int                    active_bases;
         unsigned int                    clock_was_set;
#ifdef CONFIG_HIGH_RES_TIMERS
         ktime_t                         expires_next;
         int                             hres_active;
         int                             hang_detected;
         unsigned long                   nr_events;
         unsigned long                   nr_retries;
         unsigned long                   nr_hangs;
         ktime_t                         max_hang_time;
#endif
				struct hrtimer_clock_base       clock_base[HRTIMER_MAX_CLOCK_BASES];
};


struct hrtimer_cpu_base hrtimer_bases_tlx;
void __hrtimer_init_tlx(struct hrtimer *timer, clockid_t clock_id,
				enum hrtimer_mode mode)
{
	struct hrtimer_cpu_base *cpu_base;
	int base;

	memset_tlx(timer, 0, sizeof(struct hrtimer));

	cpu_base = SHIFT_PERCPU_PTR(&(hrtimer_bases_tlx), __my_cpu_offset);

	if (clock_id == CLOCK_REALTIME && mode != HRTIMER_MODE_ABS)
		clock_id = CLOCK_MONOTONIC;

	base = hrtimer_clock_to_base_table_tlx[clock_id];
	timer->base = &cpu_base->clock_base[base];
//	timerqueue_init(&timer->node);
	RB_CLEAR_NODE(&(&timer->node)->node);
}


#define __sched         __attribute__((__section__(".sched.text")))

static inline void kref_init_tlx(struct kref *kref)
 {
         atomic_set(&kref->refcount, 1);
 }





static inline void cpumask_copy_tlx(struct cpumask *dstp,
                                 const struct cpumask *srcp)
 {
         bitmap_copy_tlx(cpumask_bits(dstp), cpumask_bits(srcp), nr_cpumask_bits);
 }
static inline unsigned int cpumask_weight_tlx(const struct cpumask *srcp)
 {
         return bitmap_weight_tlx(cpumask_bits(srcp), nr_cpumask_bits);
 }


static inline void __set_bit_tlx(int nr, volatile unsigned long *addr)
 {
         unsigned long mask = BIT_MASK(nr);
         unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

          *p  |= mask;
 }


static inline void arch_local_irq_enable_tlx(void)
 {
         asm volatile(
                 "msr    daifclr, #2             // arch_local_irq_enable"
                 :
                 :
                 : "memory");
 }

#define raw_local_irq_enable()          arch_local_irq_enable_tlx()




static inline void __raw_write_unlock_irq_tlx(rwlock_t *lock)
 {
         rwlock_release_tlx(&lock->dep_map, 1, _RET_IP_);
         do_raw_write_unlock(lock);
         local_irq_enable();
         preempt_enable();
 }




//static  void do_raw_spin_lock_tlx(raw_spinlock_t *lock)
// {
//           arch_spin_trylock(&lock->raw_lock);
// }


unsigned long loops_per_jiffy = (1<<12);



void do_raw_spin_lock_tlx(raw_spinlock_t *lock)
 {

 }




static void do_raw_spin_unlock_tlx(raw_spinlock_t *lock)
{
//         arch_spin_unlock_tlx(&lock->raw_lock);
}


static inline unsigned __read_seqcount_begin_tlx(const seqcount_t *s)
 {
         unsigned ret;

 repeat:
         ret = ACCESS_ONCE(s->sequence);
         if (unlikely(ret & 1)) {
                 cpu_relax();
                 goto repeat;
         }
         return ret;
 }

void __raw_spin_lock_tlx(raw_spinlock_t *lock)
 {
         preempt_disable();
         spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
         LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock_tlx);
 }

void __raw_spin_unlock_tlx(raw_spinlock_t *lock)
 {
         spin_release(&lock->dep_map, 1, _RET_IP_);
         do_raw_spin_unlock_tlx(lock);
         preempt_enable();
 }

static inline int __read_seqcount_retry_tlx(const seqcount_t *s, unsigned start)
 {
         return unlikely(s->sequence != start);
 }

static inline unsigned raw_read_seqcount_begin_tlx(const seqcount_t *s)
 {
         unsigned ret = __read_seqcount_begin_tlx(s);
         smp_rmb();
         return ret;
 }

static inline int read_seqcount_retry_tlx(const seqcount_t *s, unsigned start)
 {
         smp_rmb();
         return __read_seqcount_retry_tlx(s, start);
 }

static inline int atomic_sub_return_tlx(int i, atomic_t *v)
 {
         unsigned long tmp;
         int result;

         asm volatile("// atomic_sub_return\n"
 "1:     ldxr    %w0, %2\n"
 "       sub     %w0, %w0, %w3\n"
 "       stlxr   %w1, %w0, %2\n"
 "       cbnz    %w1, 1b"
         : "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
         : "Ir" (i)
         : "memory");

         smp_mb();
         return result;
 }

static __always_inline int fls_tlx(int x)
{
				return x ? sizeof(x) * 8 - __builtin_clz(x) : 0;
	}



extern int ____ilog2_NaN(void);
int __ilog2_u32_tlx(u32 n)
{
	return fls_tlx(n) - 1;
}
int __ilog2_u64_tlx(u64 n)
{
	return fls64_tlx(n) - 1;
}


pid_t task_pid_vnr_tlx(struct task_struct *tsk)
 {
//         return __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);
//					 struct task_struct *task, enum pid_type type,
//516
//				struct pid_namespace *ns;
         pid_t nr = 0;
//         ns = ns_of_pid(current->pids[PIDTYPE_PID].pid);
				struct pid *pid = current->pids[PIDTYPE_PID].pid;
        struct pid_namespace *ns = NULL;
        if (pid)
                 ns = pid->numbers[pid->level].ns;
        if (likely( tsk->pids[PIDTYPE_PID].pid != NULL)) {
//                 nr = pid_nr_ns(tsk->pids[PIDTYPE_PID].pid, ns);
									 pid = tsk->pids[PIDTYPE_PID].pid;
         					 struct upid *upid;
       						 if (pid && ns->level <= pid->level) {
                 		upid = &pid->numbers[ns->level];
										 if (upid->ns == ns)
                         nr = upid->nr;
       							}
         }
				return nr;
 }


static inline void put_task_struct_tlx(struct task_struct *t)
 {
//         if (atomic_dec_and_test(&t->usage))
//                 __put_task_struct(t);
 }

static inline bool kgid_has_mapping_tlx(struct user_namespace *ns, kgid_t gid)
 {
         return true;
 }

static inline bool thread_group_leader_tlx(struct task_struct *p)
{
				return p->exit_signal >= 0;
}

static inline int thread_group_empty_tlx(struct task_struct *p)
{
         return list_empty(&p->thread_group);
}



static inline void ptrace_unlink_tlx(struct task_struct *child)
{

}







static inline unsigned long arch_local_irq_save_tlx(void)
   {
           unsigned long flags;
           asm volatile(
                   "mrs    %0, daif                // arch_local_irq_save\n"
                   "msr    daifset, #2"
                   : "=r" (flags)
                   :
                   : "memory");
           return flags;
   }

static inline void arch_local_irq_restore_tlx(unsigned long flags)
   {
           asm volatile(
                   "msr    daif, %0                // arch_local_irq_restore"
           :
           : "r" (flags)
           : "memory");
   }


static inline void __raw_spin_unlock_irqrestore_tlx(raw_spinlock_t *lock,
                                             unsigned long flags)
 {
         spin_release(&lock->dep_map, 1, _RET_IP_);
         do_raw_spin_unlock_tlx(lock);
         local_irq_restore(flags);
         preempt_enable();
 }





static inline struct page *compound_head_by_tail_tlx(struct page *tail)
 {
         struct page *head = tail->first_page;
         smp_rmb();
         if (likely(PageTail(tail)))
                 return head;
         return tail;
 }






static inline struct page *compound_head_tlx(struct page *page)
{
         if (unlikely(PageTail(page)))
                 return compound_head_by_tail_tlx(page);
         return page;
}








static inline void atomic_dec_tlx(atomic_t *v)
 {
         atomic_sub_return_tlx(1, v);
 }


 static inline void atomic_long_inc_tlx(atomic_long_t *l)
  {
          atomic64_t *v = (atomic64_t *)l;

          atomic64_inc(v);
  }

static inline int rcu_read_lock_held_tlx(void)
 {
                 return 1;
 }





 struct page *virt_to_head_page_tlx(const void *x)
 {
         struct page *page = virt_to_page(x);
         return compound_head_tlx(page);
 }


 void spin_unlock_irqrestore_tlx(spinlock_t *lock, unsigned long flags)
 {
         raw_spin_unlock_irqrestore(&lock->rlock, flags);
 }

static inline int page_to_nid_tlx(const struct page *page)
 {
         return (page->flags >> NODES_PGSHIFT) & NODES_MASK;
 }



static inline void atomic64_sub_tlx(u64 i, atomic64_t *v)
 {
         long result;
         unsigned long tmp;

         asm volatile("// atomic64_sub\n"
 "1:     ldxr    %0, %2\n"
 "       sub     %0, %0, %3\n"
 "       stxr    %w1, %0, %2\n"
 "       cbnz    %w1, 1b"
         : "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
         : "Ir" (i));
 }

void atomic_long_dec_tlx(atomic_long_t *l)
 {
         atomic64_t *v = (atomic64_t *)l;

         atomic64_dec(v);
  }


static inline void atomic_long_sub_tlx(long i, atomic_long_t *l)
 {
         atomic64_t *v = (atomic64_t *)l;

         atomic64_sub_tlx(i, v);
  }



raw_spinlock_t *spinlock_check_tlx(spinlock_t *lock)
 {
         return &lock->rlock;
 }


static inline int compound_order_tlx(struct page *page)
 {
         if (!PageHead(page))
                 return 0;
         return (unsigned long)page[1].lru.prev;
 }

void *lowmem_page_address_tlx(const struct page *page)
 {
         return __va(PFN_PHYS(page_to_pfn(page)));
 }

static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
						pte_t *ptep, pte_t pte)
{
	if (pte_valid_user(pte)) {
		if (!pte_special(pte) && pte_exec(pte)) {
			struct page *page = pte_page(pte);

					/* no flushing needed for anonymous pages */
//					if (!page_mapping(page))
//								return;
				struct address_space *mapping = page->mapping;
         if (unlikely(PageSlab(page)))
                 return;
 					if ((unsigned long)mapping & PAGE_MAPPING_ANON)
                 mapping = NULL;
         if  (!mapping) return;
					if (!test_and_set_bit_tlx(PG_dcache_clean, &page->flags)) {
	//							__flush_dcache_area(page_address(page),
	//															PAGE_SIZE << compound_order_tlx(page));
								__flush_icache_all_tlx();
					} else if (icache_is_aivivt_tlx()) {
									__flush_icache_all_tlx();
					}


		}
//			__sync_icache_dcache(pte, addr);
		if (pte_dirty(pte) && pte_write(pte))
			pte_val(pte) &= ~PTE_RDONLY;
		else
			pte_val(pte) |= PTE_RDONLY;
	}

	set_pte(ptep, pte);
}


static inline unsigned long __raw_spin_lock_irqsave_tlx(raw_spinlock_t *lock)
 {
         unsigned long flags;

         local_irq_save(flags);
         preempt_disable();
         spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);

#ifdef CONFIG_LOCKDEP
         LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock_tlx);
#else
         do_raw_spin_lock_flags(lock, &flags);
#endif
         return flags;
 }

static unsigned long __lockfunc _raw_spin_lock_irqsave_tlx(raw_spinlock_t *lock)
 {
         return __raw_spin_lock_irqsave_tlx(lock);
 }


#define cpu_rq(cpu)             (&per_cpu(runqueues_tlx, (cpu)))




static inline int bit_spin_is_locked_tlx(int bitnum, unsigned long *addr)
  {
  #if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
          return test_bit_tlx(bitnum, addr);
  #elif defined CONFIG_PREEMPT_COUNT
          return preempt_count_tlx();
  #else
          return 1;
  #endif
 }

static inline bool hlist_bl_is_locked_tlx(struct hlist_bl_head *b)
 {
         return bit_spin_is_locked_tlx(0, (unsigned long *)b);
 }

static inline void put_user_ns_tlx(struct user_namespace *ns)
 {
 }



static void put_cred_rcu_tlx(struct rcu_head *rcu)
 {
         struct cred *cred = container_of(rcu, struct cred, rcu);
         security_cred_free_tlx(cred);
         key_put(cred->session_keyring);
         key_put(cred->process_keyring);
         key_put(cred->thread_keyring);
         key_put(cred->request_key_auth);
         if (cred->group_info)
                 put_group_info(cred->group_info);
         free_uid_tlx(cred->user);
         put_user_ns_tlx(cred->user_ns);
         kmem_cache_free_tlx(cred_jar_tlx_tlx, cred);
 }

static void __put_cred_tlx(struct cred *cred)
{
       call_rcu(&cred->rcu, put_cred_rcu_tlx);
}

long atomic64_sub_return_tlx(long i, atomic64_t *v)
 {
         long result;
         unsigned long tmp;

         asm volatile("// atomic64_sub_return\n"
 "1:     ldxr    %0, %2\n"
 "       sub     %0, %0, %3\n"
 "       stlxr   %w1, %0, %2\n"
 "       cbnz    %w1, 1b"
         : "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
         : "Ir" (i)
         : "memory");

         smp_mb();
         return result;
 }



 static inline s64 percpu_counter_read_positive_tlx(struct percpu_counter *fbc)
 {
         s64 ret = fbc->count;

         barrier();              /* Prevent reloads of fbc->count */
         if (ret >= 0)
                 return ret;
         return 0;
 }

static inline void put_cred_tlx(const struct cred *_cred)
 {
         struct cred *cred = (struct cred *) _cred;

         validate_creds(cred);
         if (atomic_dec_and_test(&(cred)->usage))
                 __put_cred_tlx(cred);
 }


static void put_pid_tlx(struct pid *pid)
 {
         struct pid_namespace *ns;

         if (!pid)
                 return;

         ns = pid->numbers[pid->level].ns;
         if ((atomic_read(&pid->count) == 1) ||
              atomic_dec_and_test(&pid->count)) {
                 kmem_cache_free_tlx(ns->pid_cachep, pid);
                 put_pid_ns_tlx(ns);
         }
 }

static inline s64 percpu_counter_sum_positive_tlx(struct percpu_counter *fbc)
{
         return percpu_counter_read_positive_tlx(fbc);
}
int atomic_long_dec_and_test_tlx(atomic_long_t *l)
 {
         atomic64_t *v = (atomic64_t *)l;

         return atomic64_dec_and_test(v);
 }

static inline enum zone_type page_zonenum_tlx(const struct page *page)
 {
         return (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;
 }

struct zone *page_zone_tlx(const struct page *page)
 {
         return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum_tlx(page)];
 }



#define PAGES_PER_WAITQUEUE     256
#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)

unsigned long get_pfnblock_flags_mask_tlx(struct page *page, unsigned long pfn,
					unsigned long end_bitidx,
					unsigned long mask)
{
	struct zone *zone;
	unsigned long *bitmap;
	unsigned long bitidx, word_bitidx;
	unsigned long word;

	zone = page_zone_tlx(page);
	bitmap = __pfn_to_section_tlx(pfn)->pageblock_flags;
//	bitidx = pfn_to_bitidx(zone, pfn);
	pfn &= (PAGES_PER_SECTION-1);
	bitidx = (pfn >> pageblock_order) * NR_PAGEBLOCK_BITS;
	word_bitidx = bitidx / BITS_PER_LONG;
	bitidx &= (BITS_PER_LONG-1);

	word = bitmap[word_bitidx];
	bitidx += end_bitidx;
	return (word >> (BITS_PER_LONG - bitidx - 1)) & mask;
}

static inline int get_pfnblock_migratetype_tlx(struct page *page, unsigned long pfn)
{
//         BUILD_BUG_ON(PB_migrate_end - PB_migrate != 2);
				return get_pfnblock_flags_mask_tlx(page, pfn, PB_migrate_end,
																				MIGRATETYPE_MASK);
	}



void __clear_bit_tlx(int nr, volatile unsigned long *addr)
 {
         unsigned long mask = BIT_MASK(nr);
         unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

          *p &= ~mask;
 }

static inline void __ClearPageSlabPfmemalloc_tlx(struct page *page)
{
	__ClearPageActive(page);
}

static inline void page_mapcount_reset_tlx(struct page *page)
 {
         atomic_set(&(page)->_mapcount, -1);
 }

static inline void spin_lock_tlx(spinlock_t *lock)
{
//         raw_spin_lock(&lock->rlock);
}

static inline void spin_unlock_tlx(spinlock_t *lock)
{
//         raw_spin_unlock(&lock->rlock);
}

void debug_spin_lock_after_tlx(raw_spinlock_t *lock);
void debug_spin_lock_before_tlx(raw_spinlock_t *lock);
void __spin_lock_debug_tlx(raw_spinlock_t *lock);

void do_raw_spin_lock2(raw_spinlock_t *lock)
{
//		__acquire(lock);
	arch_spin_lock_tlx(&lock->raw_lock);
}

void __raw_spin_lock_irq_tlx(raw_spinlock_t *lock)
 {
         local_irq_disable();
         preempt_disable();
         spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
         LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock2);
}

void __raw_spin_unlock_irq_tlx(raw_spinlock_t *lock)
 {
       spin_release(&lock->dep_map, 1, _RET_IP_);
//       do_raw_spin_unlock_tl(lock);
				arch_spin_unlock_tlx(&lock->raw_lock);
       local_irq_enable();
       preempt_enable();
}

#define _raw_spin_unlock_irq(lock) __raw_spin_unlock_irq_tlx(lock)
#define _raw_spin_lock_irq(lock) __raw_spin_lock_irq_tlx(lock)
#define raw_spin_unlock_irq(lock)       _raw_spin_unlock_irq(lock)
#define raw_spin_lock_irq(lock)         _raw_spin_lock_irq(lock)
void spin_lock_irq_tlx(spinlock_t *lock)
{
         raw_spin_lock_irq(&lock->rlock);
}

void spin_unlock_irq_tlx(spinlock_t *lock)
{
         raw_spin_unlock_irq(&lock->rlock);
}


#define MAX_SCHEDULE_TIMEOUT    LONG_MAX
#define TASK_KILLABLE           (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)

#define LONG_MAX        ((long)(~0UL>>1))
#define MAX_SCHEDULE_TIMEOUT    LONG_MAX
#define TASK_KILLABLE           (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)

signed long schedule_timeout_tlx(signed long timeout)
{
	__schedule_tlx();
	return timeout < 0 ? 0 : timeout;
}


int default_wake_function_tlx(wait_queue_t *curr, unsigned mode, int wake_flags,
													void *key);

#define __WAITQUEUE_INITIALIZER(name, tsk) {                            \
         .private        = tsk,                                          \
         .func           = default_wake_function_tlx,                        \
         .task_list      = { NULL, NULL } }

#define DECLARE_WAITQUEUE(name, tsk)                                    \
         wait_queue_t name = __WAITQUEUE_INITIALIZER(name, tsk)

long
do_wait_for_common_tlx(struct completion *x,
			long (*action)(long), long timeout, int state)
{
	if (!x->done) {
		DECLARE_WAITQUEUE(wait, current);
		 (&wait)->flags |= WQ_FLAG_EXCLUSIVE;
//		 __add_wait_queue_tail(&x->wait, &wait);
//		__add_wait_queue_tail_exclusive(&x->wait, &wait);
			 list_add_tail(&(&wait)->task_list, &(&x->wait)->task_list);
		do {
			__set_current_state(state);
			spin_unlock_irq_tlx(&x->wait.lock);
			timeout = action(timeout);
			spin_lock_irq_tlx(&x->wait.lock);
		} while (!x->done && timeout);
//		__remove_wait_queue_tlx(&x->wait, &wait);
			list_del(&(&wait)->task_list);
		if (!x->done)
			return timeout;
	}
	x->done--;
	return timeout ?: 1;
}

long
wait_for_common_tlx(struct completion *x, long timeout, int state)
{
//	return __wait_for_common(x, schedule_timeout, timeout, state);
	might_sleep();

	spin_lock_irq_tlx(&x->wait.lock);
	timeout = do_wait_for_common_tlx(x, schedule_timeout_tlx, timeout, state);
	spin_unlock_irq_tlx(&x->wait.lock);
	return timeout;
}

int wait_for_completion_killable_tlx(struct completion *x)
{
				long t = wait_for_common_tlx(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);
				return 0;
}


void wait_for_completion_tlx(struct completion *x)
{
			wait_for_common_tlx(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);
}



static inline struct user_struct *get_uid_tlx(struct user_struct *u)
{
				atomic_inc_tlx(&u->__count);
				return u;
}


static inline unsigned read_seqcount_begin_tlx(const seqcount_t *s)
{
         seqcount_lockdep_reader_access(s);
         return raw_read_seqcount_begin_tlx(s);
}

static inline unsigned read_seqbegin_tlx(const seqlock_t_tlx *sl)
 {
         return read_seqcount_begin_tlx(&sl->seqcount);
 }


static inline void read_seqlock_excl_tlx(seqlock_t_tlx *sl)
 {
         spin_lock_tlx(&sl->lock);
 }

static inline void read_sequnlock_excl_tlx(seqlock_t_tlx *sl)
 {
         spin_unlock_tlx(&sl->lock);
 }


static inline unsigned read_seqretry_tlx(const seqlock_t_tlx *sl, unsigned start)
{
         return read_seqcount_retry_tlx(&sl->seqcount, start);
}

static void check_flags_tlx(unsigned long flags)
 {
 }


static int __lock_is_held_tlx(struct lockdep_map *lock)
 {
         return 0;
 }

int lock_is_held_tlx(struct lockdep_map *lock)
 {
         unsigned long flags;
         int ret = 0;
         return ret;
 }



static inline void sigdelsetmask_tlx(sigset_t *set, unsigned long mask)
 {
         set->sig[0] &= ~mask;
 }


static inline void sigaddset_tlx(sigset_t *set, int _sig)
{
				unsigned long sig = _sig - 1;
				if (_NSIG_WORDS == 1)
								set->sig[0] |= 1UL << sig;
				else
								set->sig[sig / _NSIG_BPW] |= 1UL << (sig % _NSIG_BPW);
}


static inline void sigemptyset_tlx(sigset_t *set)
{
         switch (_NSIG_WORDS) {
         default:
                 memset_tlx(set, 0, sizeof(sigset_t));
                 break;
         case 2: set->sig[1] = 0;
         case 1: set->sig[0] = 0;
               break;
         }
 }



static inline void read_seqbegin_or_lock_tlx(seqlock_t_tlx *lock, int *seq)
 {
         if (!(*seq & 1))        /* Even */
                 *seq = read_seqbegin_tlx(lock);
         else                    /* Odd */
                 read_seqlock_excl_tlx(lock);
 }



static inline void prefetch_tlx(const void *ptr)
 {
         asm volatile("prfm pldl1keep, %a0\n" : : "p" (ptr));
 }

static inline bool __must_check IS_ERR_OR_NULL_tlx(__force const void *ptr)
 {
         return !ptr || IS_ERR_VALUE((unsigned long)ptr);
  }

static inline void done_seqretry_tlx(seqlock_t_tlx *lock, int seq)
{
         if (seq & 1)
                 read_sequnlock_excl_tlx(lock);
}

static inline int need_seqretry_tlx(seqlock_t_tlx *lock, int seq)
 {
         return !(seq & 1) && read_seqretry_tlx(lock, seq);
 }

#define raw_smp_processor_id() (current_thread_info_tlx_tlx()->cpu)



static inline int arch_irqs_disabled_flags_tlx(unsigned long flags)
{
					return flags & PSR_I_BIT;
}

static inline unsigned long arch_local_save_flags_tlx(void)
{
				unsigned long flags;
				asm volatile(
								"mrs    %0, daif                // arch_local_save_flags"
								: "=r" (flags)
								:
								: "memory");
				return flags;
}

static inline int arch_irqs_disabled_tlx(void)
{
					return arch_irqs_disabled_flags_tlx(arch_local_save_flags_tlx());
}

#define raw_irqs_disabled()             (arch_irqs_disabled_tlx())
#define irqs_disabled()         (raw_irqs_disabled())

int __bitmap_equal_tlx(const unsigned long *bitmap1,
		const unsigned long *bitmap2, int bits)
{
	int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] != bitmap2[k])
			return 0;

	if (bits % BITS_PER_LONG)
		if ((bitmap1[k] ^ bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
			return 0;

	return 1;
}

static inline int bitmap_equal_tlx(const unsigned long *src1,
												const unsigned long *src2, int nbits)
{
				if (small_const_nbits(nbits))
								return ! ((*src1 ^ *src2) & BITMAP_LAST_WORD_MASK(nbits));
				else
								return __bitmap_equal_tlx(src1, src2, nbits);
}


static inline bool cpumask_equal_tlx(const struct cpumask *src1p,
																const struct cpumask *src2p)
{
			return bitmap_equal_tlx(cpumask_bits(src1p), cpumask_bits(src2p),
																									nr_cpumask_bits);
}

static inline const struct cpumask *get_cpu_mask_tlx(unsigned int cpu)
{
				const unsigned long *p = cpu_bit_bitmap_tlx[1 + cpu % BITS_PER_LONG];
				p -= cpu / BITS_PER_LONG;
				return to_cpumask(p);
}


#define tsk_cpus_allowed(tsk) (&(tsk)->cpus_allowed)
#define cpumask_of(cpu) (get_cpu_mask_tlx(cpu))
#define preempt_disable_notrace()               barrier()
#define preempt_enable_no_resched_notrace()     barrier()

notrace static unsigned int check_preemption_disabled_tlx(const char *what1,
							const char *what2)
{
	int this_cpu = raw_smp_processor_id();

	if (likely(preempt_count_tlx()))
		goto out;

	if (irqs_disabled())
		goto out;

	/*
	 * Kernel threads bound to a single CPU can safely use
	 * smp_processor_id():
	 */
	if (cpumask_equal_tlx(tsk_cpus_allowed(current), cpumask_of(this_cpu)))
		goto out;

	/*
	 * It is valid to assume CPU-locality during early bootup:
	 */
	if (system_state != SYSTEM_RUNNING)
		goto out;

	/*
	 * Avoid recursion:
	 */
	preempt_disable_notrace();

//	if (!printk_ratelimit())
//		goto out_enable;


out_enable:
	preempt_enable_no_resched_notrace();
out:
	return this_cpu;
}


void __this_cpu_preempt_check_tlx(const char *op)
 {
          check_preemption_disabled_tlx("__this_cpu_", op);
 }

# define __this_cpu_read(pcp) \
         (__this_cpu_preempt_check_tlx("read"),__pcpu_size_call_return(raw_cpu_read_, (pcp)))



#define time_after_eq(a,b)      \
         (typecheck(unsigned long, a) && \
          typecheck(unsigned long, b) && \
          ((long)((a) - (b)) >= 0))

#define __jiffy_data  __attribute__((section(".data")))
unsigned long volatile __jiffy_data jiffies_tlx;
u64 __jiffy_data jiffies_64;
struct clocksource_tlx clocksource_jiffies_tlx;


#define isb()           asm volatile("isb" : : : "memory")
#define __pfn_to_phys(pfn)      ((phys_addr_t)(pfn) << PAGE_SHIFT)
# define smp_processor_id() raw_smp_processor_id()


#define ATOMIC_LONG_INIT(i)     ATOMIC64_INIT(i)
static inline void clear_tlb_flush_pending_tlx(struct mm_struct *mm)
{
}
struct page *empty_zero_page_tlx;



#define S_IFMT  00170000
#define ___GFP_COMP             0x4000u


#define S_ISCHR(m)      (((m) & S_IFMT) == S_IFCHR)

#define O_NOCTTY        00000400        /* not fcntl */
/* Write access to underlying fs */
#define FMODE_WRITER            ((__force fmode_t)0x10000)
 /* Has read method(s) */
#define FMODE_CAN_READ          ((__force fmode_t)0x20000)
 /* Has write method(s) */
#define FMODE_CAN_WRITE         ((__force fmode_t)0x40000)
#define MS_RDONLY        1      /* Mount read-only */
#define special_file(m) (S_ISCHR(m)||S_ISBLK(m)||S_ISFIFO(m)||S_ISSOCK(m))
#define FMODE_PREAD             ((__force fmode_t)0x8)
/* file can be accessed using pwrite */
#define FMODE_PWRITE            ((__force fmode_t)0x10)
/* file is seekable */
#define FMODE_LSEEK             ((__force fmode_t)0x4)
#define OPEN_FMODE(flag) ((__force fmode_t)(((flag + 1) & O_ACCMODE) | \
                                                (flag & __FMODE_NONOTIFY)))
#define O_WRONLY        00000001
#define O_EXCL          00000200        /* not fcntl */
#define FMODE_NONOTIFY          ((__force fmode_t)0x1000000)
#define O_ACCMODE       00000003
#define O_CREAT         00000100        /* not fcntl */
#define O_PATH          010000000

#define __FMODE_NONOTIFY        ((__force int) FMODE_NONOTIFY)


int d_invalidate_tlx(struct dentry * dentry)
{
//	__d_drop(dentry);
	return 0;
}


#define _THIS_IP_  ({ __label__ __here; __here: (unsigned long)&&__here; })
#define lock_acquire_shared_recursive(l, s, t, n, i)    lock_acquire(l, s, t, 2, 1, n, i)
#define __init_or_module __init
#define S_IXOTH 00001
#define S_IXUSR 00100
#define S_IROTH 00004
#define S_IRGRP 00040


static inline struct kset *to_kset_tlx(struct kobject *kobj)
{
         return kobj ? container_of(kobj, struct kset, kobj) : NULL;
}

static inline long atomic64_cmpxchg_tlx(atomic64_t *ptr, long old, long new)
 {
         long oldval;
         unsigned long res;

         smp_mb();

         asm volatile("// atomic64_cmpxchg\n"
 "1:     ldxr    %1, %2\n"
 "       cmp     %1, %3\n"
 "       b.ne    2f\n"
 "       stxr    %w0, %4, %2\n"
 "       cbnz    %w0, 1b\n"
 "2:"
         : "=&r" (res), "=&r" (oldval), "+Q" (ptr->counter)
         : "Ir" (old), "r" (new)
         : "cc");

         smp_mb();
         return oldval;
 }


#define ULLONG_MAX      (~0ULL)

#define min(x, y) ({                            \
         typeof(x) _min1 = (x);                  \
         typeof(y) _min2 = (y);                  \
         (void) (&_min1 == &_min2);              \
         _min1 < _min2 ? _min1 : _min2; })

#define max(x, y) ({                            \
         typeof(x) _max1 = (x);                  \
         typeof(y) _max2 = (y);                  \
         (void) (&_max1 == &_max2);              \
         _max1 > _max2 ? _max1 : _max2; })

static inline bool page_is_guard_tlx(struct page *page) { return false; }
#define page_private(page)              ((page)->private)
#define NR_MIGRATETYPE_BITS (PB_migrate_end - PB_migrate + 1)



#define MIGRATETYPE_MASK ((1UL << NR_MIGRATETYPE_BITS) - 1)



#define RADIX_TREE_INDEX_BITS  (8 /* CHAR_BIT */ * sizeof(unsigned long))
#define RADIX_TREE_MAP_SHIFT    (CONFIG_BASE_SMALL ? 4 : 6)
#define RADIX_TREE_MAX_PATH (DIV_ROUND_UP(RADIX_TREE_INDEX_BITS, \
                                            RADIX_TREE_MAP_SHIFT))

#define INIT_RADIX_TREE(root, mask)                                     \
do {                                                                    \
         (root)->height = 0;                                             \
         (root)->gfp_mask = (mask);                                      \
         (root)->rnode = NULL;                                           \
} while (0)

#define i_size_ordered_init(inode) do { } while (0)



struct hlist_bl_head {
         struct hlist_bl_node *first;
  };







#define O_NOCTTY        00000400        /* not fcntl */
/* Write access to underlying fs */
#define FMODE_WRITER            ((__force fmode_t)0x10000)
 /* Has read method(s) */
#define FMODE_CAN_READ          ((__force fmode_t)0x20000)
 /* Has write method(s) */
#define FMODE_CAN_WRITE         ((__force fmode_t)0x40000)
#define MS_RDONLY        1      /* Mount read-only */
#define special_file(m) (S_ISCHR(m)||S_ISBLK(m)||S_ISFIFO(m)||S_ISSOCK(m))
#define FMODE_PREAD             ((__force fmode_t)0x8)
/* file can be accessed using pwrite */
#define FMODE_PWRITE            ((__force fmode_t)0x10)
/* file is seekable */
#define FMODE_LSEEK             ((__force fmode_t)0x4)
#define OPEN_FMODE(flag) ((__force fmode_t)(((flag + 1) & O_ACCMODE) | \
                                                (flag & __FMODE_NONOTIFY)))
#define O_WRONLY        00000001
#define O_EXCL          00000200        /* not fcntl */
#define FMODE_NONOTIFY          ((__force fmode_t)0x1000000)
#define O_ACCMODE       00000003
#define O_CREAT         00000100        /* not fcntl */
#define O_PATH          010000000

#define __FMODE_NONOTIFY        ((__force int) FMODE_NONOTIFY)

static inline bool d_can_lookup_tlx(const struct dentry *dentry)
{
         return __d_entry_type_tlx(dentry) == DCACHE_DIRECTORY_TYPE;
}



enum {
  FILE_CREATED = 1,
  FILE_OPENED = 2
};


struct file_ra_state {
  pgoff_t start;			/* where readahead started */
  unsigned int size;		/* # of readahead pages */
  unsigned int async_size;	/* do asynchronous readahead when
             there are only # of pages ahead */

  unsigned int ra_pages;		/* Maximum readahead window */
  unsigned int mmap_miss;		/* Cache miss stat for mmap accesses */
  loff_t prev_pos;		/* Cache last read() position */
};

struct address_space {
  struct inode		*host;		/* owner: inode, block_device */
  struct radix_tree_root	page_tree;	/* radix tree of all pages */
  spinlock_t		tree_lock;	/* and lock protecting it */
  unsigned int		i_mmap_writable;/* count VM_SHARED mappings */
  struct rb_root		i_mmap;		/* tree of private and shared mappings */
  struct list_head	i_mmap_nonlinear;/*list VM_NONLINEAR mappings */
  struct mutex		i_mmap_mutex;	/* protect tree, count, list */
  /* Protected by tree_lock together with the radix tree */
  unsigned long		nrpages;	/* number of total pages */
  unsigned long		nrshadows;	/* number of shadow entries */
  pgoff_t			writeback_index;/* writeback starts here */
  const struct address_space_operations *a_ops;	/* methods */
  unsigned long		flags;		/* error bits/gfp mask */
  struct backing_dev_info *backing_dev_info; /* device readahead, etc */
  spinlock_t		private_lock;	/* for use by the address_space */
  struct list_head	private_list;	/* ditto */
  void			*private_data;	/* ditto */
} __attribute__((aligned(sizeof(long))));




struct super_operations {
     struct inode *(*alloc_inode)(struct super_block *sb);
  void (*destroy_inode)(struct inode *);

     void (*dirty_inode) (struct inode *, int flags);
  int (*write_inode) (struct inode *, struct writeback_control *wbc);
  int (*drop_inode) (struct inode *);
  void (*evict_inode) (struct inode *);
  void (*put_super) (struct super_block *);
  int (*sync_fs)(struct super_block *sb, int wait);
  int (*freeze_fs) (struct super_block *);
  int (*unfreeze_fs) (struct super_block *);
  int (*statfs) (struct dentry *, struct kstat_tlxfs *);
  int (*remount_fs) (struct super_block *, int *, char *);
  void (*umount_begin) (struct super_block *);

  int (*show_options)(struct seq_file *, struct dentry *);
  int (*show_devname)(struct seq_file *, struct dentry *);
  int (*show_path)(struct seq_file *, struct dentry *);
  int (*show_stats)(struct seq_file *, struct dentry *);
#ifdef CONFIG_QUOTA
  ssize_t (*quota_read)(struct super_block *, int, char *, size_t, loff_t);
  ssize_t (*quota_write)(struct super_block *, int, const char *, size_t, loff_t);
#endif
  int (*bdev_try_to_free_page)(struct super_block*, struct page*, gfp_t);
  long (*nr_cached_objects)(struct super_block *, int);
  long (*free_cached_objects)(struct super_block *, long, int);
};

struct super_block {
  struct list_head	s_list;		/* Keep this first */
  dev_t			s_dev;		/* search index; _not_ kdev_t */
  unsigned char		s_blocksize_bits;
  unsigned long		s_blocksize;
  loff_t			s_maxbytes;	/* Max file size */
  struct file_system_type	*s_type;
  const struct super_operations	*s_op;
  const struct dquot_operations	*dq_op;
  const struct quotactl_ops	*s_qcop;
  const struct export_operations *s_export_op;
  unsigned long		s_flags;
  unsigned long		s_magic;
  struct dentry		*s_root;
  struct rw_semaphore	s_umount;
  int			s_count;
  atomic_t		s_active;
#ifdef CONFIG_SECURITY
  void                    *s_security;
#endif
  const struct xattr_handler **s_xattr;

  struct list_head	s_inodes;	/* all inodes */
  struct hlist_bl_head	s_anon;		/* anonymous dentries for (nfs) exporting */
  struct list_head	s_mounts;	/* list of mounts; _not_ for fs use */
  struct block_device	*s_bdev;
  struct backing_dev_info *s_bdi;
  struct mtd_info		*s_mtd;
  struct hlist_node	s_instances;
  struct quota_info	s_dquot;	/* Diskquota specific options */

  struct sb_writers	s_writers;

  char s_id[32];				/* Informational name */
  u8 s_uuid[16];				/* UUID */

  void 			*s_fs_info;	/* Filesystem private info */
  unsigned int		s_max_links;
  fmode_t			s_mode;

  /* Granularity of c/m/atime in ns.
     Cannot be worse than a second */
  u32		   s_time_gran;

  /*
   * The next field is for VFS *only*. No filesystems have any business
   * even looking at it. You had been warned.
   */
  struct mutex s_vfs_rename_mutex;	/* Kludge */

  /*
   * Filesystem subtype.  If non-empty the filesystem type field
   * in /proc/mounts will be "type.subtype"
   */
  char *s_subtype;

  /*
   * Saved mount options for lazy filesystems using
   * generic_show_options()
   */
  char __rcu *s_options;
  const struct dentry_operations *s_d_op; /* default d_op for dentries */

  /*
   * Saved pool identifier for cleancache (-1 means none)
   */
  int cleancache_poolid;

  struct shrinker s_shrink;	/* per-sb shrinker handle */

  /* Number of inodes with nlink == 0 but still referenced */
  atomic_long_t s_remove_count;

  /* Being remounted read-only */
  int s_readonly_remount;

  /* AIO completions deferred from interrupt context */
  struct workqueue_struct *s_dio_done_wq;

  /*
   * Keep the lru lists last in the structure so they always sit on their
   * own individual cachelines.
   */
  struct list_lru		s_dentry_lru ____cacheline_aligned_in_smp;
  struct list_lru		s_inode_lru ____cacheline_aligned_in_smp;
  struct rcu_head		rcu;
};


struct filename {
  const char		*name;	/* pointer to actual string */
  const __user char	*uptr;	/* original userland pointer */
  struct audit_names	*aname;
  bool			separate; /* should "name" be freed? */
};

struct backing_dev_info;
struct backing_dev_info noop_backing_dev_info_tlx;

#define MINORBITS       20
#define MINORMASK       ((1U << MINORBITS) - 1)
#define MAJOR(dev)      ((unsigned int) ((dev) >> MINORBITS))
#define MINOR(dev)      ((unsigned int) ((dev) & MINORMASK))

int unnamed_dev_start_tlx = 1;
struct ida unnamed_dev_ida_tlx;

int get_anon_bdev_tlx(dev_t *p)
{
	int dev;
	int error;

retry:
	if (ida_pre_get_tlx(&unnamed_dev_ida_tlx, GFP_ATOMIC) == 0)
		return -ENOMEM;
	error = ida_get_new_above_tlx(&unnamed_dev_ida_tlx, unnamed_dev_start_tlx, &dev);
	if (!error)
		unnamed_dev_start_tlx = dev + 1;
	*p = MKDEV(0, dev & MINORMASK);
	return 0;
}

int set_anon_super_tlx(struct super_block *s, void *data)
{
				int error = get_anon_bdev_tlx(&s->s_dev);
				if (!error)
								s->s_bdi = &noop_backing_dev_info_tlx;
				return error;
}

void *slab_alloc_tlx(struct kmem_cache *s,
		gfp_t gfpflags, unsigned long addr);

#define PATH_MAX        4096    /* # chars in a path name including nul */
#define EMBEDDED_NAME_MAX       (PATH_MAX - sizeof(struct filename))
struct kmem_cache *names_cachep_tlx;

struct filename *
getname_kernel_tlx(const char * filename)
 {
         struct filename *result;
         char *kname;
         int len;

         len = strlen_tlx(filename);
         result =  slab_alloc_tlx(names_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
         kname = (char *)result + sizeof(*result);
         result->name = kname;
         result->uptr = NULL;
         result->aname = NULL;
         result->separate = false;

         strlcpy_tlx(kname, filename, EMBEDDED_NAME_MAX);
       return result;
 }

struct inode_operations {
  struct dentry * (*lookup) (struct inode *,struct dentry *, unsigned int);
  void * (*follow_link) (struct dentry *, struct nameidata *);
  int (*permission) (struct inode *, int);
  struct posix_acl * (*get_acl)(struct inode *, int);

  int (*readlink) (struct dentry *, char __user *,int);
  void (*put_link) (struct dentry *, struct nameidata *, void *);

  int (*create) (struct inode *,struct dentry *, umode_t, bool);
  int (*link) (struct dentry *,struct inode *,struct dentry *);
  int (*unlink) (struct inode *,struct dentry *);
  int (*symlink) (struct inode *,struct dentry *,const char *);
  int (*mkdir) (struct inode *,struct dentry *,umode_t);
  int (*rmdir) (struct inode *,struct dentry *);
  int (*mknod) (struct inode *,struct dentry *,umode_t,dev_t);
  int (*rename) (struct inode *, struct dentry *,
      struct inode *, struct dentry *);
  int (*rename2) (struct inode *, struct dentry *,
      struct inode *, struct dentry *, unsigned int);
  int (*setattr) (struct dentry *, struct iattr *);
  int (*getattr) (struct vfsmount *mnt, struct dentry *, struct kstat_tlx *);
  int (*setxattr) (struct dentry *, const char *,const void *,size_t,int);
  ssize_t (*getxattr) (struct dentry *, const char *, void *, size_t);
  ssize_t (*listxattr) (struct dentry *, char *, size_t);
  int (*removexattr) (struct dentry *, const char *);
  int (*fiemap)(struct inode *, struct fiemap_extent_info *, u64 start,
          u64 len);
  int (*update_time)(struct inode *, struct timespec *, int);
  int (*atomic_open)(struct inode *, struct dentry *,
         struct file *, unsigned open_flag,
         umode_t create_mode, int *opened);
  int (*tmpfile) (struct inode *, struct dentry *, umode_t);
  int (*set_acl)(struct inode *, struct posix_acl *, int);
} ____cacheline_aligned;

struct inode {
  umode_t			i_mode;
  unsigned short		i_opflags;
  kuid_t			i_uid;
  kgid_t			i_gid;
  unsigned int		i_flags;

#ifdef CONFIG_FS_POSIX_ACL
  struct posix_acl	*i_acl;
  struct posix_acl	*i_default_acl;
#endif

  const struct inode_operations	*i_op;
  struct super_block	*i_sb;
  struct address_space	*i_mapping;

#ifdef CONFIG_SECURITY
  void			*i_security;
#endif

  /* Stat data, not accessed from path walking */
  unsigned long		i_ino;
  /*
   * Filesystems may only read i_nlink directly.  They shall use the
   * following functions for modification:
   *
   *    (set|clear|inc|drop)_nlink
   *    inode_(inc|dec)_link_count
   */
  union {
    const unsigned int i_nlink;
    unsigned int __i_nlink;
  };
  dev_t			i_rdev;
  loff_t			i_size;
  struct timespec		i_atime;
  struct timespec		i_mtime;
  struct timespec		i_ctime;
  spinlock_t		i_lock;	/* i_blocks, i_bytes, maybe i_size */
  unsigned short          i_bytes;
  unsigned int		i_blkbits;
  blkcnt_t		i_blocks;

#ifdef __NEED_I_SIZE_ORDERED
  seqcount_t		i_size_seqcount;
#endif

  /* Misc */
  unsigned long		i_state;
  struct mutex		i_mutex;

  unsigned long		dirtied_when;	/* jiffies_tlx of first dirtying */

  struct hlist_node	i_hash;
  struct list_head	i_wb_list;	/* backing dev IO list */
  struct list_head	i_lru;		/* inode LRU list */
  struct list_head	i_sb_list;
  union {
    struct hlist_head	i_dentry;
    struct rcu_head		i_rcu;
  };
  u64			i_version;
  atomic_t		i_count;
  atomic_t		i_dio_count;
  atomic_t		i_writecount;
#ifdef CONFIG_IMA
  atomic_t		i_readcount; /* struct files open RO */
#endif
  const struct file_operations	*i_fop;	/* former ->i_op->default_file_ops */
  struct file_lock	*i_flock;
  struct address_space	i_data;
#ifdef CONFIG_QUOTA
  struct dquot		*i_dquot[MAXQUOTAS];
#endif
  struct list_head	i_devices;
  union {
    struct pipe_inode_info	*i_pipe;
    struct block_device	*i_bdev;
    struct cdev		*i_cdev;
  };

  __u32			i_generation;

#ifdef CONFIG_FSNOTIFY
  __u32			i_fsnotify_mask; /* all events this inode cares about */
  struct hlist_head	i_fsnotify_marks;
#endif

  void			*i_private; /* fs or device private pointer */
};


typedef struct files_struct *fl_owner_t;

typedef int (*filldir_t)(void *, const char *, int, loff_t, u64, unsigned);

struct dir_context {
  const filldir_t actor;
  loff_t pos;
};



struct file_lock {
  struct file_lock *fl_next;	/* singly linked list for this inode  */
  struct hlist_node fl_link;	/* node in global lists */
  struct list_head fl_block;	/* circular list of blocked processes */
  fl_owner_t fl_owner;
  unsigned int fl_flags;
  unsigned char fl_type;
  unsigned int fl_pid;
  int fl_link_cpu;		/* what cpu's list is this on? */
  struct pid *fl_nspid;
  wait_queue_head_t fl_wait;
  struct file *fl_file;
  loff_t fl_start;
  loff_t fl_end;

  struct fasync_struct *	fl_fasync; /* for lease break notifications */
  /* for lease breaks: */
  unsigned long fl_break_time;
  unsigned long fl_downgrade_time;

  const struct file_lock_operations *fl_ops;	/* Callbacks for filesystems */
  const struct lock_manager_operations *fl_lmops;	/* Callbacks for lockmanagers */
  union {
    struct nfs_lock_info	nfs_fl;
    struct nfs4_lock_info	nfs4_fl;
    struct {
      struct list_head link;	/* link in AFS vnode's pending_locks list */
      int state;		/* state of grant or error if -ve */
    } afs;
  } fl_u;
};




struct file_operations {
  struct module *owner;
  loff_t (*llseek) (struct file *, loff_t, int);
  ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
  ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
  ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t);
  ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t);
  ssize_t (*read_iter) (struct kiocb *, struct iov_iter *);
  ssize_t (*write_iter) (struct kiocb *, struct iov_iter *);
  int (*iterate) (struct file *, struct dir_context *);
  unsigned int (*poll) (struct file *, struct poll_table_struct *);
  long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
  long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
  int (*mmap) (struct file *, struct vm_area_struct *);
  int (*open) (struct inode *, struct file *);
  int (*flush) (struct file *, fl_owner_t id);
  int (*release) (struct inode *, struct file *);
  int (*fsync) (struct file *, loff_t, loff_t, int datasync);
  int (*aio_fsync) (struct kiocb *, int datasync);
  int (*fasync) (int, struct file *, int);
  int (*lock) (struct file *, int, struct file_lock *);
  ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int);
  unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
  int (*check_flags)(int);
  int (*flock) (struct file *, int, struct file_lock *);
  ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, loff_t *, size_t, unsigned int);
  ssize_t (*splice_read)(struct file *, loff_t *, struct pipe_inode_info *, size_t, unsigned int);
  int (*setlease)(struct file *, long, struct file_lock **);
  long (*fallocate)(struct file *file, int mode, loff_t offset,
        loff_t len);
  int (*show_fdinfo)(struct seq_file *m, struct file *f);
};



struct fown_struct {
  rwlock_t lock;          /* protects pid, uid, euid fields */
  struct pid *pid;	/* pid or -pgrp where SIGIO should be sent */
  enum pid_type pid_type;	/* Kind of process group SIGIO should be sent to */
  kuid_t uid, euid;	/* uid/euid of process setting the owner */
  int signum;		/* posix.1b rt signal to be delivered on IO */
};

struct path {
       struct vfsmount *mnt;
       struct dentry *dentry;
 };



struct file {
  union {
    struct llist_node	fu_llist;
    struct rcu_head 	fu_rcuhead;
  } f_u;
  struct path		f_path;
#define f_dentry	f_path.dentry
  struct inode		*f_inode;	/* cached value */
  const struct file_operations	*f_op;

  /*
   * Protects f_ep_links, f_flags.
   * Must not be taken from IRQ context.
   */
  spinlock_t		f_lock;
  atomic_long_t		f_count;
  unsigned int 		f_flags;
  fmode_t			f_mode;
  struct mutex		f_pos_lock;
  loff_t			f_pos;
  struct fown_struct	f_owner;
  const struct cred	*f_cred;
  struct file_ra_state	f_ra;

  u64			f_version;
#ifdef CONFIG_SECURITY
  void			*f_security;
#endif
  /* needed for tty driver, and maybe others */
  void			*private_data;

#ifdef CONFIG_EPOLL
  /* Used by fs/eventpoll.c to link all the hooks to this file */
  struct list_head	f_ep_links;
  struct list_head	f_tfile_llink;
#endif /* #ifdef CONFIG_EPOLL */
  struct address_space	*f_mapping;
} __attribute__((aligned(4)));	/* lest something weird decides that 2 is OK */

#ifdef __LITTLE_ENDIAN
  #define HASH_LEN_DECLARE u32 hash; u32 len;
//  #define bytemask_from_count(cnt)       (~(~0ul << (cnt)*8))
#else
  #define HASH_LEN_DECLARE u32 len; u32 hash;
//  #define bytemask_from_count(cnt)       (~(~0ul >> (cnt)*8))
#endif

struct qstr {
         union {
                 struct {
                         HASH_LEN_DECLARE;
                 };
                 u64 hash_len;
         };
         const unsigned char *name;
 };



struct dentry_operations {
  int (*d_revalidate)(struct dentry *, unsigned int);
  int (*d_weak_revalidate)(struct dentry *, unsigned int);
  int (*d_hash)(const struct dentry *, struct qstr *);
  int (*d_compare)(const struct dentry *, const struct dentry *,
      unsigned int, const char *, const struct qstr *);
  int (*d_delete)(const struct dentry *);
  void (*d_release)(struct dentry *);
  void (*d_prune)(struct dentry *);
  void (*d_iput)(struct dentry *, struct inode *);
  char *(*d_dname)(struct dentry *, char *, int);
  struct vfsmount *(*d_automount)(struct path *);
  int (*d_manage)(struct dentry *, bool);
} ____cacheline_aligned;

#define d_lock	d_lockref.lock
struct lockref {
         union {
 #if USE_CMPXCHG_LOCKREF
                 aligned_u64 lock_count;
 #endif
                 struct {
                         spinlock_t lock;
                         unsigned int count;
                 };
         };
  };

int lockref_get_not_dead_tlx(struct lockref *lockref) {
	int retval;
	retval = 0;
	if ((int) lockref->count >= 0) {
		lockref->count++;
		retval = 1;
	}
	return retval;
};

struct dentry {
  /* RCU lookup touched fields */
  unsigned int d_flags;		/* protected by d_lock */
  seqcount_t d_seq;		/* per dentry seqlock */
  struct hlist_bl_node d_hash;	/* lookup hash list */
  struct dentry *d_parent;	/* parent directory */
  struct qstr d_name;
  struct inode *d_inode;		/* Where the name belongs to - NULL is
           * negative */
  unsigned char d_iname[DNAME_INLINE_LEN];	/* small names */

  /* Ref lookup also touches following */
  struct lockref d_lockref;	/* per-dentry lock and refcount */
  const struct dentry_operations *d_op;
  struct super_block *d_sb;	/* The root of the dentry tree */
  unsigned long d_time;		/* used by d_revalidate */
  void *d_fsdata;			/* fs-specific data */

  struct list_head d_lru;		/* LRU list */
  /*
   * d_child and d_rcu can share memory
   */
  union {
    struct list_head d_child;	/* child of parent list */
     struct rcu_head d_rcu;
  } d_u;
  struct list_head d_subdirs;	/* our children */
  struct hlist_node d_alias;	/* inode alias list */
};


struct dentry *dget_parent_tlx(struct dentry *dentry) {
	return dentry->d_parent;
};

#define IS_ROOT(x) ((x) == (x)->d_parent)

int is_subdir_tlx(struct dentry *new_dentry, struct dentry *old_dentry)
{
	int result;
	if (new_dentry == old_dentry)
						return 1;

	struct dentry *p1 = old_dentry;
	struct dentry *p2 = new_dentry;
	struct dentry *p;
	for (p = p2; !IS_ROOT(p); p = p->d_parent) {
								if (p->d_parent == p1)
												goto have_p;
	}
	p = NULL;
have_p:
	if (p)
												result = 1;
								else
												result = 0;
	return result;

};


struct file_system_type {
  const char *name;
  int fs_flags;
#define FS_REQUIRES_DEV		1
#define FS_BINARY_MOUNTDATA	2
#define FS_HAS_SUBTYPE		4
#define FS_USERNS_MOUNT		8	/* Can be mounted by userns root */
#define FS_USERNS_DEV_MOUNT	16 /* A userns mount does not imply MNT_NODEV */
#define FS_RENAME_DOES_D_MOVE	32768	/* FS will handle d_move() during rename() internally. */
  struct dentry *(*mount) (struct file_system_type *, int,
           const char *, void *);
  void (*kill_sb) (struct super_block *);
  struct module *owner;
  struct file_system_type * next;
  struct hlist_head fs_supers;

  struct lock_class_key s_lock_key;
  struct lock_class_key s_umount_key;
  struct lock_class_key s_vfs_rename_key;
  struct lock_class_key s_writers_key[SB_FREEZE_LEVELS];

  struct lock_class_key i_lock_key;
  struct lock_class_key i_mutex_key;
  struct lock_class_key i_mutex_dir_key;
};



#define FMODE_EXEC              ((__force fmode_t)0x20)
#define READ                    0
#define MAX_RW_COUNT (INT_MAX & PAGE_CACHE_MASK)

#define O_RDONLY        00000000
#define __FMODE_EXEC            ((__force int) FMODE_EXEC)
#define MAY_EXEC                0x00000001
#define MAY_OPEN                0x00000020
#define AT_FDCWD                -100    /* Special value used to indicate
                                            openat should use the current
                                             working directory. */

#define O_RDWR          00000002
#define MKDEV(ma,mi)    (((ma) << MINORBITS) | (mi))
#define MS_KERNMOUNT    (1<<22) /* this is a kern_mount call */






static inline void i_readcount_inc_tlx(struct inode *inode)
{
         return;
}

static inline void i_readcount_dec_tlx(struct inode *inode)
{
        return;
}


static inline void i_uid_write_tlx(struct inode *inode, uid_t uid)
 {
//         inode->i_uid = make_kuid(&init_user_ns_tlx, uid);
 }

 static inline void i_gid_write_tlx(struct inode *inode, gid_t gid)
 {
//         inode->i_gid = make_kgid(&init_user_ns_tlx, gid);
 }

static inline void put_write_access_tlx(struct inode * inode)
 {
         atomic_dec_tlx(&inode->i_writecount);
 }

seqlock_t_tlx rename_lock_tlx;

struct dentry *d_alloc_tlx(struct dentry * parent, const struct qstr *name);

int dcache_dir_open_tlx(struct inode *inode, struct file *file)
{
	static struct qstr cursor_name = QSTR_INIT(".", 1);

	file->private_data = d_alloc_tlx(file->f_path.dentry, &cursor_name);

	return file->private_data ? 0 : -ENOMEM;
}

int dcache_readdir_tlx(struct file *file, struct dir_context *ctx)
{
	struct dentry *dentry = file->f_path.dentry;
	struct dentry *cursor = file->private_data;
	struct list_head *p, *q = &cursor->d_u.d_child;
//	spin_lock(&dentry->d_lock);
	if (ctx->pos == 2)
		list_move(q, &dentry->d_subdirs);

	for (p = q->next; p != &dentry->d_subdirs; p = p->next) {
		struct dentry *next = list_entry(p, struct dentry, d_u.d_child);
//		spin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);


		if (!(next->d_inode)) {
//			spin_unlock(&next->d_lock);
			continue;
		}

//		spin_unlock(&next->d_lock);
//		spin_unlock(&dentry->d_lock);
//		if (!dir_emit(ctx, next->d_name.name, next->d_name.len,
//			      next->d_inode->i_ino, dt_type(next->d_inode)))
//			struct dir_context *ctx,
			const char *name = next->d_name.name;
			int namelen = next->d_name.len;
			u64 ino = next->d_inode->i_ino;
			unsigned type = ((next->d_inode)->i_mode >> 12) & 15;
			if(!(ctx->actor(ctx, name, namelen, ctx->pos, ino, type) == 0))
						return 0;
//		spin_lock(&dentry->d_lock);
//		spin_lock_nested(&next->d_lock, DENTRY_D_LOCK_NESTED);
		/* next is still alive */
		list_move(q, p);
//		spin_unlock(&next->d_lock);
		p = q;
		ctx->pos++;
	}
//	spin_unlock(&dentry->d_lock);
	return 0;
}

struct file_operations simple_dir_operations_tlx = {
       .open           = dcache_dir_open_tlx,
       .release        = NULL,
       .llseek         = NULL,
       .read           = NULL,
       .iterate        = dcache_readdir_tlx,
       .fsync          = NULL,
};



struct file_operations bad_sock_fops_tlx = {};
struct file_operations def_blk_fops_tlx = {};



struct cdev {
				struct kobject kobj;
				struct module *owner;
				const struct file_operations *ops;
				struct list_head list;
				dev_t dev;
				unsigned int count;
};

#define ENXIO            6      /* No such device or address */
struct kobj_map *cdev_map_tlx;

struct kobject *kobj_lookup_tlx(struct kobj_map *domain, dev_t dev, int *index);

int chrdev_open_tlx(struct inode *inode, struct file *filp)
{
	const struct file_operations *fops;
	struct cdev *p;
	struct cdev *new = NULL;
	int ret = 0;

//	spin_lock(&cdev_lock);
	p = inode->i_cdev;
	if (!p) {
		struct kobject *kobj;
		int idx;
	//	spin_unlock(&cdev_lock);
		kobj = kobj_lookup_tlx(cdev_map_tlx, inode->i_rdev, &idx);
		new = container_of(kobj, struct cdev, kobj);
//		spin_lock(&cdev_lock);
		/* Check i_cdev again in case somebody beat us to it while
			we dropped the lock. */
		p = inode->i_cdev;
		if (!p) {
			inode->i_cdev = p = new;
			list_add(&inode->i_devices, &p->list);
			new = NULL;
		}
	}
//	spin_unlock(&cdev_lock);
//	cdev_put(new);
	ret = -ENXIO;
	fops = p->ops;
//	replace_fops(filp, fops);
	struct file *__file = (filp); \
//	fops_put(__file->f_op);
	__file->f_op = (fops);
	if (filp->f_op->open) {
		ret = filp->f_op->open(inode, filp);
	}

	return 0;}

struct file_operations def_chr_fops_tlx = {
	.open = chrdev_open_tlx,
	.llseek = NULL,
};

int do_vfs_ioctl_tlx(struct file *filp, unsigned int fd, unsigned int cmd,
                     unsigned long arg) {};

void locks_free_lock_tlx(struct file_lock *fl) {};
int flock_lock_file_wait_tlx(struct file *filp, struct file_lock *fl) {};
int register_chrdev_region_tlx(dev_t from, unsigned count, const char *name) {};


#define RADIX_TREE_EXCEPTIONAL_SHIFT    2
#define O_LARGEFILE     0400000
#define FMODE_ATOMIC_POS        ((__force fmode_t)0x8000)



char *strncpy_tlx(char *dest, const char *src, size_t count)
{
         char *tmp = dest;

         while (count) {
                 if ((*tmp = *src) != 0)
                         src++;
                 tmp++;
                 count--;
         }
       return dest;
 }

struct filename *getname_tlx(const char __user * filename)
{
	int flags = 0;
	int *empty = NULL;
	struct filename *result, *err;
	int len;
	long max;
	char *kname;
	result = slab_alloc_tlx(names_cachep_tlx, GFP_KERNEL, _RET_IP_);
		kname = (char *)result + sizeof(*result);
	result->name = kname;
	result->separate = false;
	max = EMBEDDED_NAME_MAX;
//	len = __strncpy_from_user(kname, filename, max);
	char *dst = kname;
	char __user *src = filename;
	long count = max;
		char *tmp;
			strncpy_tlx(dst, (const char __force *)src, count);
			for (tmp = dst; *tmp && count > 0; tmp++, count--)
								;

	result->uptr = filename;
	result->aname = NULL;
	return result;

}

static inline u32 new_encode_dev_tlx(dev_t dev)
{
         unsigned major = MAJOR(dev);
         unsigned minor = MINOR(dev);
         return (minor & 0xff) | (major << 8) | ((minor & ~0xff) << 12);
}


#define FMODE_PATH              ((__force fmode_t)0x4000)

#define file_count(x)   atomic_long_read_tlx(&(x)->f_count)
#define force_o_largefile() (BITS_PER_LONG != 32)

int iterate_dir_tlx(struct file *file, struct dir_context *ctx)
{
		ctx->pos = file->f_pos;
		file->f_op->iterate(file, ctx);
		file->f_pos = ctx->pos;

	return 0;
}
ssize_t vfs_read_tlx(struct file *file, char __user *buf, size_t count, loff_t *pos)
{
	ssize_t ret;
	ret = count;
	if (ret >= 0) {
		count = ret;
		if (file->f_op->read)
			ret = file->f_op->read(file, buf, count, pos);
	//	inc_syscr(current);
	}

	return ret;
}


ssize_t vfs_write_tlx(struct file *file, const char __user *buf, size_t count, loff_t *pos)
{
	ssize_t ret;

	ret = count;
	if (ret >= 0) {
		count = ret;
//		file_start_write(file);
		if (file->f_op->write)
			ret = file->f_op->write(file, buf, count, pos);
//		inc_syscw(current);
//		file_end_write(file);
	}

	return ret;
}




#define LOCK_SH         1       /* shared lock */
#define LOCK_EX         2       /* exclusive lock */
#define LOCK_RW         192     /* which allows concurrent read & write ops */

#define F_RDLCK         0
#define F_WRLCK         1
#define F_UNLCK         2

#define INT_LIMIT(x)    (~((x)1 << (sizeof(x)*8 - 1)))
#define OFFSET_MAX      INT_LIMIT(loff_t)


#define LOCK_NB         4       /* or'd with one of the above to prevent
                                    blocking */
#define LOCK_UN         8       /* remove lock */
#define LOCK_MAND       32      /* This is a mandatory flock ... */
#define F_SETLK         6
#define F_SETLKW        7
#define FL_SLEEP        128     /* A blocking lock */
/* file is open for reading */
#define FMODE_READ              ((__force fmode_t)0x1)
/* file is open for writing */
#define FMODE_WRITE             ((__force fmode_t)0x2)
#define FL_FLOCK        2


static inline struct file *get_file_tlx(struct file *f)
{
         atomic_long_inc_tlx(&f->f_count);
         return f;
}


unsigned __d_entry_type_tlx(const struct dentry *dentry)
 {
         return dentry->d_flags & DCACHE_ENTRY_TYPE;
 }

static bool d_is_autodir_tlx(const struct dentry *dentry)
 {
         return __d_entry_type_tlx(dentry) == DCACHE_AUTODIR_TYPE;
 }

static inline bool d_is_dir_tlx(const struct dentry *dentry)
{
         return d_can_lookup_tlx(dentry) || d_is_autodir_tlx(dentry);
}

#define hlist_bl_entry(ptr, type, member) container_of(ptr,type,member)




static inline struct hlist_bl_node *hlist_bl_first_rcu_tlx(struct hlist_bl_head *h)
{
         return (struct hlist_bl_node *)
                 ((unsigned long)rcu_dereference_check(h->first, hlist_bl_is_locked_tlx(h)) & ~LIST_BL_LOCKMASK);
}

#define hlist_bl_for_each_entry_rcu(tpos, pos, head, member)            \
         for (pos = hlist_bl_first_rcu_tlx(head);                            \
                 pos &&                                                  \
                 ({ tpos = hlist_bl_entry(pos, typeof(*tpos), member); 1; }); \
                 pos = rcu_dereference_raw(pos->next))


static inline bool d_mountpoint_tlx(const struct dentry *dentry)
 {
         return dentry->d_flags & DCACHE_MOUNTED;
 }


static inline int d_unhashed_tlx(const struct dentry *dentry)
 {
         return hlist_bl_unhashed_tlx(&dentry->d_hash);
 }

static inline int d_unlinked_tlx(const struct dentry *dentry)
 {
         return d_unhashed_tlx(dentry) && !IS_ROOT(dentry);
 }


static inline bool d_is_symlink_tlx(const struct dentry *dentry)
 {
         return __d_entry_type_tlx(dentry) == DCACHE_SYMLINK_TYPE;
 }

static inline bool d_is_negative_tlx(const struct dentry *dentry)
 {
         return __d_entry_type_tlx(dentry) == DCACHE_MISS_TYPE;
 }




#define le32_to_cpu __le32_to_cpu
#define ioremap(addr, size)             __ioremap_tlx((addr), (size), __pgprot(PROT_DEVICE_nGnRE))
#define BITMAP_FIRST_WORD_MASK(start) (~0UL << ((start) % BITS_PER_LONG))

void bitmap_clear_tlx(unsigned long *map, int start, int nr)
{
	unsigned long *p = map + BIT_WORD(start);
	const int size = start + nr;
	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
	unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);

	while (nr - bits_to_clear >= 0) {
		*p &= ~mask_to_clear;
		nr -= bits_to_clear;
		bits_to_clear = BITS_PER_LONG;
		mask_to_clear = ~0UL;
		p++;
	}
	if (nr) {
		mask_to_clear &= BITMAP_LAST_WORD_MASK(size);
		*p &= ~mask_to_clear;
	}
}

void free_percpu_tlx(void __percpu *__pdata) {};
void free_cpumask_var_tlx(cpumask_var_t mask) {};

bool zalloc_cpumask_var_node_tlx(cpumask_var_t *mask, gfp_t flags, int node) {};


void bitmap_set_tlx(unsigned long *map, int start, int nr)
{
	unsigned long *p = map + BIT_WORD(start);
	const int size = start + nr;
	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
	unsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);

	while (nr - bits_to_set >= 0) {
		*p |= mask_to_set;
		nr -= bits_to_set;
		bits_to_set = BITS_PER_LONG;
		mask_to_set = ~0UL;
		p++;
	}
	if (nr) {
		mask_to_set &= BITMAP_LAST_WORD_MASK(size);
		*p |= mask_to_set;
	}
}


#define BITOP_WORD(nr)          ((nr) / BITS_PER_LONG)

unsigned long __ffs_tlx(unsigned long word)
{
				return __builtin_ctzl(word);
	}

unsigned long find_next_bit_tlx(const unsigned long *addr, unsigned long size,
					unsigned long offset)
{
	const unsigned long *p = addr + BITOP_WORD(offset);
	unsigned long result = offset & ~(BITS_PER_LONG-1);
	unsigned long tmp;

	if (offset >= size)
		return size;
	size -= result;
	offset %= BITS_PER_LONG;
	if (offset) {
		tmp = *(p++);
		tmp &= (~0UL << offset);
		if (size < BITS_PER_LONG)
			goto found_first;
		if (tmp)
			goto found_middle;
		size -= BITS_PER_LONG;
		result += BITS_PER_LONG;
	}
	while (size & ~(BITS_PER_LONG-1)) {
		if ((tmp = *(p++)))
			goto found_middle;
		result += BITS_PER_LONG;
		size -= BITS_PER_LONG;
	}
	if (!size)
		return result;
	tmp = *p;

found_first:
	tmp &= (~0UL >> (BITS_PER_LONG - size));
	if (tmp == 0UL)		/* Are any bits set? */
		return result + size;	/* Nope. */
found_middle:
	return result + __ffs_tlx(tmp);
}



unsigned long bitmap_find_next_zero_area_tlx(unsigned long *map,
                                          unsigned long size,
                                          unsigned long start,
                                          unsigned int nr,
                                          unsigned long align_mask)
{
	        unsigned long index, end, i;
again:
         index = find_next_zero_bit_tlx(map, size, start);

         /* Align allocation */
         index = (((index) + (align_mask)) & ~(align_mask)); //(index, align_mask);

         end = index + nr;
         if (end > size)
                 return end;
         i = find_next_bit_tlx(map, end, index);
         if (i < end) {
                 start = i + 1;
                 goto again;
         }
         return index;

}

//#define THIS_MODULE (&__this_module)
#define atomic64_set(v,i)       (((v)->counter) = (i))
void atomic_long_set_tlx(atomic_long_t *l, long i)
{
         atomic64_t *v = (atomic64_t *)l;

         atomic64_set(v, i);
}




int kset_register_tlx(struct kset *k);

int kobject_set_name_vargs_tlx(struct kobject *kobj, const char *fmt,
					va_list vargs);

int kobject_set_name_tlx(struct kobject *kobj, const char *fmt, ...)
{
	va_list vargs;
	int retval;

	va_start(vargs, fmt);
	retval = kobject_set_name_vargs_tlx(kobj, fmt, vargs);
	va_end(vargs);

	return retval;
}

void *kzalloc_tlx(size_t size, gfp_t flags);

struct kobj_type;
struct kobj_type kset_ktype_tlx;
struct kset *kset_create_and_add_tlx(const char *name,
				const struct kset_uevent_ops *uevent_ops,
				struct kobject *parent_kobj)
{
	struct kset *kset;
	int error;

//	kset = kset_create(name, uevent_ops, parent_kobj);
	kset = kzalloc_tlx(sizeof(*kset), GFP_KERNEL);
	kobject_set_name_tlx(&kset->kobj, "%s", name);
	kset->uevent_ops = uevent_ops;
	kset->kobj.parent = parent_kobj;
	kset->kobj.ktype = &kset_ktype_tlx;
	kset->kobj.kset = NULL;
	error = kset_register_tlx(kset);
	return kset;
}

#define BLOCKING_INIT_NOTIFIER_HEAD(name) do {  \
                  init_rwsem(&(name)->rwsem);     \
                  (name)->head = NULL;            \
          } while (0)



#define MAX_PHYSMEM_BITS	40
#define SECTIONS_SHIFT  (MAX_PHYSMEM_BITS - SECTION_SIZE_BITS)
#define NR_MEM_SECTIONS         (1UL << SECTIONS_SHIFT)

#define NR_SECTION_ROOTS        DIV_ROUND_UP(NR_MEM_SECTIONS, SECTIONS_PER_ROOT)

#define dmb(opt)        asm volatile("dmb " #opt : : : "memory")
#ifdef CONFIG_SPARSEMEM_EXTREME
 struct mem_section_tlx *mem_section_tlx[NR_SECTION_ROOTS];
#else
 struct mem_section_tlx mem_section_tlx[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];
#endif

#define SECTION_NR_TO_ROOT(sec) ((sec) / SECTIONS_PER_ROOT)
struct mem_section_tlx *__nr_to_section_tlx(unsigned long nr)
{
				if (!mem_section_tlx[SECTION_NR_TO_ROOT(nr)])
								return NULL;
				return &mem_section_tlx[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
}


static inline void cpumask_set_cpu_tlx(unsigned int cpu, struct cpumask *dstp)
{
         set_bit_tlx(cpumask_check_tlx(cpu), cpumask_bits(dstp));
}


extern void cpu_do_switch_mm_tlx(unsigned long pgd_phys, struct mm_struct *mm);







#define KERN_ERR        KERN_SOH "3"    /* error conditions */


#define __RW_LOCK_UNLOCKED(lockname) \
         (rwlock_t)      {       .raw_lock = __ARCH_RW_LOCK_UNLOCKED,    \
                                  RW_DEP_MAP_INIT(lockname) }


static inline u32 cache_type_cwg_tlx_tlx(void)
 {
          return (read_cpuid_cachetype_tlx() >> CTR_CWG_SHIFT) & CTR_CWG_MASK;
 }



static inline int atomic64_add_unless_tlx(atomic64_t *v, long a, long u)
 {
         long c, old;

         c = atomic64_read(v);
         while (c != u && (old = atomic64_cmpxchg_tlx((v), c, c + a)) != c)
                 c = old;

         return c != u;
 }
#define rmb()           dsb(ld)
static inline u32 __raw_readl_tlx(const volatile void __iomem *addr)
 {
         u32 val;
         asm volatile("ldr %w0, [%1]" : "=r" (val) : "r" (addr));
         return val;
 }



static inline bool __must_check IS_ERR_tlx(__force const void *ptr)
 {
         return IS_ERR_VALUE((unsigned long)ptr);
 }
#define hlist_for_each_entry_rcu(pos, head, member)                     \
         for (pos = hlist_entry_safe (rcu_dereference_raw(hlist_first_rcu(head)),\
                         typeof(*(pos)), member);                        \
                 pos;                                                    \
                 pos = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(\
                         &(pos)->member)), typeof(*(pos)), member))

int kobject_add_tlx(struct kobject *kobj, struct kobject *parent,
		const char *fmt, ...);

struct kobj_type;
struct kobj_type dynamic_kobj_ktype_tlx;


struct kobject *kobject_create_and_add_tlx(const char *name, struct kobject *parent)
{
	struct kobject *kobj;
	int retval;

//	kobj = kobject_create();
	kobj = kzalloc_tlx(sizeof(*kobj), GFP_KERNEL);
	kobject_init_tlx(kobj, &dynamic_kobj_ktype_tlx);
	retval = kobject_add_tlx(kobj, parent, "%s", name);
	return kobj;
}




# define raw_spin_lock_init(lock)                               \
         do { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)


int __percpu_counter_init_tlx(struct percpu_counter *fbc, s64 amount,
													struct lock_class_key *key)
{
			raw_spin_lock_init(&fbc->lock);
			fbc->count = amount;
			fbc->counters = alloc_percpu(s32);
			return 0;
}

static void __init_rwsem_tlx(struct rw_semaphore *sem, const char *name,
                   struct lock_class_key *key)
{
         sem->count = 0;
         raw_spin_lock_init(&sem->wait_lock);
         INIT_LIST_HEAD(&sem->wait_list);
}



#define spin_lock_init(_lock)                           \
 do {                                                    \
         spinlock_check_tlx(_lock);                          \
         raw_spin_lock_init(&(_lock)->rlock);            \
 } while (0)

#define init_rwsem(sem)                                         \
 do {                                                            \
         static struct lock_class_key __key;                     \
                                                                 \
         __init_rwsem_tlx((sem), #sem, &__key);                      \
 } while (0)





void __init_waitqueue_head_tlx(wait_queue_head_t *q, const char *name, struct lock_class_key *key)
{
			spin_lock_init(&q->lock);
//			lockdep_set_class_and_name(&q->lock, key, name);
			INIT_LIST_HEAD(&q->task_list);
}


#define init_waitqueue_head(q)                          \
				do {                                            \
								static struct lock_class_key __key;     \
																												\
								__init_waitqueue_head_tlx((q), #q, &__key); \
				} while (0)


LIST_HEAD(super_blocks_tlx);

#define SHRINKER_NUMA_AWARE (1 << 0)
#define DEFAULT_SEEKS 2 /* A good number if you don't know better. */
#define MAX_NON_LFS     ((1UL<<31) - 1)
#define SINGLE_DEPTH_NESTING                    1
#define GFP_USER        (__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)

char *sb_writers_name_tlx[SB_FREEZE_LEVELS] ={
         "sb_writers",
         "sb_pagefaults",
         "sb_internal",
 };
struct backing_dev_info;
struct backing_dev_info default_backing_dev_info_tlx;
spinlock_t sb_lock_tlx;

void
__mutex_init_tlx(struct mutex *lock, const char *name, struct lock_class_key *key);




struct list_lru_node {
         spinlock_t              lock;
         struct list_head        list;
         /* kept as signed so we can catch imbalance bugs */
         long                    nr_items;
} ____cacheline_aligned_in_smp;

int list_lru_init_key_tlx(struct list_lru *lru, struct lock_class_key *key)
{
				int i;
				size_t size = sizeof(*lru->node) * nr_node_ids;

				lru->node = kzalloc_tlx(size, GFP_KERNEL);
//         nodes_clear(lru->active_nodes);
				for (i = 0; i < nr_node_ids; i++) {
//								spin_lock_init(&lru->node[i].lock);
									spinlock_check_tlx(&lru->node[i].lock);
//									raw_spin_lock_init(&(&lru->node[i].lock)->rlock);
//								if (key)
	//											lockdep_set_class(&lru->node[i].lock, key);
//								INIT_LIST_HEAD(&lru->node[i].list);
									(&lru->node[i].list)->next = &lru->node[i].list;
									(&lru->node[i].list)->prev = &lru->node[i].list;
								lru->node[i].nr_items = 0;
				}
				return 0;
}



struct super_block *sget_tlx(struct file_system_type *type,
			int (*test)(struct super_block *,void *),
			int (*set)(struct super_block *,void *),
			int flags,
			void *data)
{
	struct super_block *s = NULL;
	struct super_block *old;
	int err;

retry:
	spin_lock_tlx(&sb_lock_tlx);

	if (!s) {
		spin_unlock_tlx(&sb_lock_tlx);
//		s = alloc_super(type, flags);
//		(struct file_system_type *type, int flags)
//		{
			s = kzalloc_tlx(sizeof(struct super_block),  GFP_USER);
			static const struct super_operations default_op;
			int i;
			INIT_LIST_HEAD(&s->s_mounts);
			for (i = 0; i < SB_FREEZE_LEVELS; i++) {
//				if (
					percpu_counter_init(&s->s_writers.counter[i], 0);
					// < 0)
//					goto fail;
				lockdep_init_map(&s->s_writers.lock_map[i], sb_writers_name_tlx[i],
						&type->s_writers_key[i], 0);
			}
			init_waitqueue_head(&s->s_writers.wait);
			init_waitqueue_head(&s->s_writers.wait_unfrozen);
			s->s_flags = flags;
			s->s_bdi = &default_backing_dev_info_tlx;
			INIT_HLIST_NODE(&s->s_instances);
	///		INIT_HLIST_BL_HEAD(&s->s_anon);
			INIT_LIST_HEAD(&s->s_inodes);
			list_lru_init_key_tlx(&s->s_dentry_lru, NULL);
			list_lru_init_key_tlx(&s->s_inode_lru, NULL);
			init_rwsem(&s->s_umount);
			lockdep_set_class(&s->s_umount, &type->s_umount_key);
//			down_write_nested(&s->s_umount, SINGLE_DEPTH_NESTING);
			s->s_count = 1;
			atomic_set(&s->s_active, 1);
			mutex_init(&s->s_vfs_rename_mutex);
			lockdep_set_class(&s->s_vfs_rename_mutex, &type->s_vfs_rename_key);
			mutex_init(&s->s_dquot.dqio_mutex);
			mutex_init(&s->s_dquot.dqonoff_mutex);
			init_rwsem(&s->s_dquot.dqptr_sem);
			s->s_maxbytes = MAX_NON_LFS;
			s->s_op = &default_op;
			s->s_time_gran = 1000000000;
			s->cleancache_poolid = -1;
			s->s_shrink.seeks = DEFAULT_SEEKS;
//			s->s_shrink.scan_objects = super_cache_scan;
//			s->s_shrink.count_objects = super_cache_count;
			s->s_shrink.batch = 1024;
			s->s_shrink.flags = SHRINKER_NUMA_AWARE;
//			return s;
		goto retry;
	}
	err = set(s, data);
	s->s_type = type;
	strlcpy_tlx(s->s_id, type->name, sizeof(s->s_id));
	list_add_tail(&s->s_list, &super_blocks_tlx);
	hlist_add_head(&s->s_instances, &type->fs_supers);
	spin_unlock_tlx(&sb_lock_tlx);
//	get_filesystem(type);
//	register_shrinker(&s->s_shrink);
	return s;
}


static inline void init_completion_tlx(struct completion *x)
{
				x->done = 0;
				init_waitqueue_head(&x->wait);
}


void
__mutex_init_tlx(struct mutex *lock, const char *name, struct lock_class_key *key)
{
				atomic_set(&lock->count, 1);
				spin_lock_init(&lock->wait_lock);
				INIT_LIST_HEAD(&lock->wait_list);
//         mutex_clear_owner(lock);
}


void address_space_init_once_tlx(struct address_space *mapping)
{
				memset_tlx(mapping, 0, sizeof(*mapping));
				INIT_RADIX_TREE(&mapping->page_tree, GFP_ATOMIC);
				spin_lock_init(&mapping->tree_lock);
				mutex_init(&mapping->i_mmap_mutex);
				INIT_LIST_HEAD(&mapping->private_list);
				spin_lock_init(&mapping->private_lock);
				mapping->i_mmap = RB_ROOT;
				INIT_LIST_HEAD(&mapping->i_mmap_nonlinear);
}








#define PREEMPT_ACTIVE  (__IRQ_MASK(PREEMPT_ACTIVE_BITS) << PREEMPT_ACTIVE_SHIFT)
#define _raw_spin_trylock(lock) __raw_spin_trylock_tlx(lock)







#define THIS_MODULE ((struct module *)0)

#define be32_to_cpu __be32_to_cpu



#define WARN_ON_ONCE(condition) WARN_ON(condition)



static inline struct kset *kset_get_tlx(struct kset *k)
 {
         return k ? to_kset_tlx(kobject_get_tlx(&k->kobj)) : NULL;
}

#define work_data_bits(work) ((unsigned long *)(&(work)->data))
#define cond_resched() ({                       \
         __might_sleep_tlx(__FILE__, __LINE__, 0);   \
         _cond_resched_tlx();                        \
})
#define lock_map_release(l)                     lock_release_tlx(l, 1, _THIS_IP_)
#define lock_map_acquire(l)                     lock_acquire_exclusive(l, 0, 0, NULL, _THIS_IP_)
#define lock_map_acquire_read(l)                lock_acquire_shared_recursive(l, 0, 0, NULL, _THIS_IP_)





#define kmemcheck_bitfield_begin(name)
#define kmemcheck_bitfield_end(name)



#define __putname(name)         kmem_cache_free_tlx(names_cachep_tlx, (void *)(name))


#define __range_ok(addr, size)						\
({									\
  unsigned long flag, roksum;					\
  __chk_user_ptr(addr);						\
  asm("adds %1, %1, %3; ccmp %1, %4, #2, cc; cset %0, ls"		\
    : "=&r" (flag), "=&r" (roksum)				\
    : "1" (addr), "Ir" (size),				\
      "r" (current_thread_info_tlx_tlx()->addr_limit)		\
    : "cc");						\
  flag;								\
})

#define access_ok(type, addr, size)	__range_ok(addr, size)





static void kmemcheck_free_shadow(struct page *page, int order) {};

typedef struct pm_message {
         int event;
 } pm_message_t;

struct dev_pm_info {
  pm_message_t		power_state;
  unsigned int		can_wakeup:1;
  unsigned int		async_suspend:1;
  bool			is_prepared:1;	/* Owned by the PM core */
  bool			is_suspended:1;	/* Ditto */
  bool			is_noirq_suspended:1;
  bool			is_late_suspended:1;
  bool			ignore_children:1;
  bool			early_init:1;	/* Owned by the PM core */
  bool			direct_complete:1;	/* Owned by the PM core */
  spinlock_t		lock;
#ifdef CONFIG_PM_SLEEP
  struct list_head	entry;
  struct completion	completion;
  struct wakeup_source	*wakeup;
  bool			wakeup_path:1;
  bool			syscore:1;
#else
  unsigned int		should_wakeup:1;
#endif
#ifdef CONFIG_PM_RUNTIME
  struct timer_list	suspend_timer;
  unsigned long		timer_expires;
  struct work_struct	work;
  wait_queue_head_t	wait_queue;
  atomic_t		usage_count;
  atomic_t		child_count;
  unsigned int		disable_depth:3;
  unsigned int		idle_notification:1;
  unsigned int		request_pending:1;
  unsigned int		deferred_resume:1;
  unsigned int		run_wake:1;
  unsigned int		runtime_auto:1;
  unsigned int		no_callbacks:1;
  unsigned int		irq_safe:1;
  unsigned int		use_autosuspend:1;
  unsigned int		timer_autosuspends:1;
  unsigned int		memalloc_noio:1;
  enum rpm_request	request;
  enum rpm_status		runtime_status_tlx;
  int			runtime_error;
  int			autosuspend_delay;
  unsigned long		last_busy;
  unsigned long		active_jiffies_tlx;
  unsigned long		suspended_jiffies_tlx;
  unsigned long		accounting_timestamp;
#endif
  struct pm_subsys_data	*subsys_data;  /* Owned by the subsystem. */
  void (*set_latency_tolerance)(struct device *, s32);
  struct dev_pm_qos	*qos;
};


struct dev_archdata {
         struct dma_map_ops *dma_ops;
 #ifdef CONFIG_IOMMU_API
         void *iommu;                    /* private IOMMU data */
 #endif
};

struct acpi_dev_node {
#ifdef CONFIG_ACPI
  struct acpi_device *companion;
#endif
};

struct static_key_deferred perf_sched_events_tlx;




struct perf_cpu_context {
  struct perf_event_context	ctx;
  struct perf_event_context	*task_ctx;
  int				active_oncpu;
  int				exclusive;
  struct hrtimer			hrtimer;
  ktime_t				hrtimer_interval;
  struct list_head		rotation_list_tlx;
  struct pmu			*unique_pmu;
  struct perf_cgroup		*cgrp;
};



struct pmu {
  struct list_head		entry;

  struct module			*module;
  struct device			*dev;
  const struct attribute_group	**attr_groups;
  const char			*name;
  int				type;

  /*
   * various common per-pmu feature flags
   */
  int				capabilities;

  int * __percpu			pmu_disable_count;
  struct perf_cpu_context * __percpu pmu_cpu_context;
  int				task_ctx_nr;
  int				hrtimer_interval_ms;

  /*
   * Fully disable/enable this PMU, can be used to protect from the PMI
   * as well as for lazy/batch writing of the MSRs.
   */
  void (*pmu_enable)		(struct pmu *pmu); /* optional */
  void (*pmu_disable)		(struct pmu *pmu); /* optional */

  /*
   * Try and initialize the event for this PMU.
   * Should return -ENOENT when the @event doesn't match this PMU.
   */
  int (*event_init)		(struct perf_event *event);

#define PERF_EF_START	0x01		/* start the counter when adding    */
#define PERF_EF_RELOAD	0x02		/* reload the counter when starting */
#define PERF_EF_UPDATE	0x04		/* update the counter when stopping */

  /*
   * Adds/Removes a counter to/from the PMU, can be done inside
   * a transaction, see the ->*_txn() methods.
   */
  int  (*add)			(struct perf_event *event, int flags);
  void (*del)			(struct perf_event *event, int flags);

  /*
   * Starts/Stops a counter present on the PMU. The PMI handler
   * should stop the counter when perf_event_overflow() returns
   * !0. ->start() will be used to continue.
   */
  void (*start)			(struct perf_event *event, int flags);
  void (*stop)			(struct perf_event *event, int flags);

  /*
   * Updates the counter value of the event.
   */
  void (*read)			(struct perf_event *event);

  /*
   * Group events scheduling is treated as a transaction, add
   * group events as a whole and perform one schedulability test.
   * If the test fails, roll back the whole group
   *
   * Start the transaction, after this ->add() doesn't need to
   * do schedulability tests.
   */
  void (*start_txn)		(struct pmu *pmu); /* optional */
  /*
   * If ->start_txn() disabled the ->add() schedulability test
   * then ->commit_txn() is required to perform one. On success
   * the transaction is closed. On error the transaction is kept
   * open until ->cancel_txn() is called.
   */
  int  (*commit_txn)		(struct pmu *pmu); /* optional */
  /*
   * Will cancel the transaction, assumes ->del() is called
   * for each successful ->add() during the transaction.
   */
  void (*cancel_txn)		(struct pmu *pmu); /* optional */

  /*
   * Will return the value for perf_event_mmap_page::index for this event,
   * if no implementation is provided it will default to: event->hw.idx + 1.
   */
  int (*event_idx)		(struct perf_event *event); /*optional */

  /*
   * flush branch stack on context-switches (needed in cpu-wide mode)
   */
  void (*flush_branch_stack)	(void);
};






struct klist {
         spinlock_t              k_lock;
         struct list_head        k_list;
         void                    (*get)(struct klist_node *);
         void                    (*put)(struct klist_node *);
} __attribute__ ((aligned (sizeof(void *))));

struct klist_node {
         void                    *n_klist;       /* never access directly */
         struct list_head        n_node;
         struct kref             n_ref;
};

#define IORESOURCE_IO           0x00000100      /* PCI/ISA I/O ports */
struct resource {
         resource_size_t start;
         resource_size_t end;
         const char *name;
         unsigned long flags;
         struct resource *parent, *sibling, *child;
};


#define segment_eq(a,b) ((a) == (b))

enum {
         ITER_IOVEC = 0,
         ITER_KVEC = 2,
         ITER_BVEC = 4,
  };
struct iovec
 {
         void __user *iov_base;  /* BSD uses caddr_t (1003.1g requires void *) */
         __kernel_size_t iov_len; /* Must be size_t (1003.1g) */
 };

struct iov_iter {
         int type;
         size_t iov_offset;
         size_t count;
         union {
                 const struct iovec *iov;
                 const struct bio_vec *bvec;
         };
          unsigned long nr_segs;
 };

#define KERNEL_DS       (-1UL)
#define get_ds()        (KERNEL_DS)
#define USER_DS         TASK_SIZE_64
#define get_fs()        (current_thread_info_tlx_tlx()->addr_limit)
#define VERIFY_READ 0
#define VERIFY_WRITE 1
#define for_each_possible_cpu(cpu) for_each_cpu((cpu), cpu_possible_mask_tlx)


static inline __must_check unsigned long
__clear_user(void __user *to, unsigned long n)
{
  memset_tlx((void __force *)to, 0, n);
  return 0;
}

long __copy_from_user_tlx(void *to,
    const void __user * from, unsigned long n)
{
  if (__builtin_constant_p(n)) {
    switch(n) {
    case 1:
      *(u8 *)to = *(u8 __force *)from;
      return 0;
    case 2:
      *(u16 *)to = *(u16 __force *)from;
      return 0;
    case 4:
      *(u32 *)to = *(u32 __force *)from;
      return 0;
#ifdef CONFIG_64BIT
    case 8:
      *(u64 *)to = *(u64 __force *)from;
      return 0;
#endif
    default:
      break;
    }
  }

  memcpy_tlx(to, (const void __force *)from, n);
  return 0;
}

extern unsigned long __must_check __copy_to_user_tlx(void __user *to, const void *from, unsigned long n);
#define __get_user_asm(instr, reg, x, addr, err)			\
  asm volatile(							\
  "1:	" instr "	" reg "1, [%2]\n"			\
  "2:\n"								\
  "	.section .fixup, \"ax\"\n"				\
  "	.align	2\n"						\
  "3:	mov	%w0, %3\n"					\
  "	mov	%1, #0\n"					\
  "	b	2b\n"						\
  "	.previous\n"						\
  "	.section __ex_table,\"a\"\n"				\
  "	.align	3\n"						\
  "	.quad	1b, 3b\n"					\
  "	.previous"						\
  : "+r" (err), "=&r" (x)						\
  : "r" (addr), "i" (-EFAULT))

#define __get_user_err(x, ptr, err)					\
do {									\
  unsigned long __gu_val;						\
  __chk_user_ptr(ptr);						\
  switch (sizeof(*(ptr))) {					\
  case 1:								\
    __get_user_asm("ldrb", "%w", __gu_val, (ptr), (err));	\
    break;							\
  case 2:								\
    __get_user_asm("ldrh", "%w", __gu_val, (ptr), (err));	\
    break;							\
  case 4:								\
    __get_user_asm("ldr", "%w", __gu_val, (ptr), (err));	\
    break;							\
  case 8:								\
    __get_user_asm("ldr", "%",  __gu_val, (ptr), (err));	\
    break;							\
  default:							\
    BUILD_BUG();						\
  }								\
  (x) = (__typeof__(*(ptr)))__gu_val;				\
} while (0)

#define __get_user(x, ptr)						\
({									\
  int __gu_err = 0;						\
  __get_user_err((x), (ptr), __gu_err);				\
  __gu_err;							\
})


#define __put_user_asm(instr, reg, x, addr, err)			\
  asm volatile(							\
  "1:	" instr "	" reg "1, [%2]\n"			\
  "2:\n"								\
  "	.section .fixup,\"ax\"\n"				\
  "	.align	2\n"						\
  "3:	mov	%w0, %3\n"					\
  "	b	2b\n"						\
  "	.previous\n"						\
  "	.section __ex_table,\"a\"\n"				\
  "	.align	3\n"						\
  "	.quad	1b, 3b\n"					\
  "	.previous"						\
  : "+r" (err)							\
  : "r" (x), "r" (addr), "i" (-EFAULT))

#define __put_user_err(x, ptr, err)					\
do {									\
  __typeof__(*(ptr)) __pu_val = (x);				\
  __chk_user_ptr(ptr);						\
  switch (sizeof(*(ptr))) {					\
  case 1:								\
    __put_user_asm("strb", "%w", __pu_val, (ptr), (err));	\
    break;							\
  case 2:								\
    __put_user_asm("strh", "%w", __pu_val, (ptr), (err));	\
    break;							\
  case 4:								\
    __put_user_asm("str",  "%w", __pu_val, (ptr), (err));	\
    break;							\
  case 8:								\
    __put_user_asm("str",  "%", __pu_val, (ptr), (err));	\
    break;							\
  default:							\
    BUILD_BUG();						\
  }								\
} while (0)

#define __put_user(x, ptr)						\
({									\
  int __pu_err = 0;						\
  __put_user_err((x), (ptr), __pu_err);				\
  __pu_err;							\
})

static inline void set_fs(mm_segment_t fs)
{
  current_thread_info_tlx_tlx()->addr_limit = fs;
}


;

void __wake_up_sync_key_tlx(wait_queue_head_t *q, unsigned int mode,
                         int nr_exclusive, void *key)
 {
         unsigned long flags;
         int wake_flags = 1; /* XXX WF_SYNC */

         if (unlikely(!q))
                 return;

         if (unlikely(nr_exclusive != 1))
                 wake_flags = 0;

//         spin_lock_irqsave(&q->lock, flags);
//         __wake_up_common(q, mode, nr_exclusive, wake_flags, key);
         wait_queue_t *curr, *next;

       list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
               unsigned flags = curr->flags;

               if (curr->func(curr, mode, wake_flags, key) &&
                               (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
                         break;
         }
//         spin_unlock_irqrestore(&q->lock, flags);
 }

#define PF_EXITPIDONE   0x00000008      /* pi exit done on shut down */

void do_exit_tlx(long code)
{
		struct task_struct *tsk = current;
		int group_dead;
		set_fs(USER_DS);
		smp_mb();
		tsk->exit_code = code;
//		exit_notify(tsk, group_dead);
		if (tsk->exit_signal >= 0) {
//				__wake_up_parent(tsk, tsk->parent);
//				do_notify_parent(tsk, tsk->exit_signal);
				__wake_up_sync_key_tlx(&(tsk->parent)->signal->wait_chldexit,
                                 TASK_INTERRUPTIBLE, 1, tsk);
		}

		tsk->exit_state =  EXIT_ZOMBIE;
		tsk->flags |= PF_EXITPIDONE;
//		if (tsk->nr_dirtied)
//			__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);
		smp_mb();
		tsk->state = TASK_DEAD;
		tsk->flags |= PF_NOFREEZE;	/* tell freezer to ignore us */
		__schedule_tlx();
		for (;;)
			cpu_relax();	/* For when BUG is null */
};


#define get_user(x, ptr)                                                \
 ({                                                                      \
         __typeof__(*(ptr)) __user *__p = (ptr);                         \
         might_fault_tlx();                                                  \
         access_ok(VERIFY_READ, __p, sizeof(*__p)) ?                     \
                 __get_user((x), __p) :                                  \
                 ((x) = 0, -EFAULT);                                     \
 })

static inline unsigned long __must_check clear_user(void __user *to, unsigned long n)
 {
         if (access_ok(VERIFY_WRITE, to, n))
                 n = __clear_user(to, n);
         return n;
 }

static inline unsigned long __must_check copy_from_user(void *to, const void __user *from, unsigned long n)
 {
         if (access_ok(VERIFY_READ, from, n))
                 n = __copy_from_user_tlx(to, from, n);
         else /* security hole - plug it */
                 memset_tlx(to, 0, n);
         return n;
 }

static inline unsigned long __must_check copy_to_user(void __user *to, const void *from, unsigned long n)
{
  if (access_ok(VERIFY_WRITE, to, n))
    n = __copy_to_user_tlx(to, from, n);
  return n;
}

size_t strnlen(const char *s, size_t count)
{
         const char *sc;

         for (sc = s; count-- && *sc != '\0'; ++sc)
                 /* nothing */;
         return sc - s;
}

#define __strnlen_user(s, n) (strnlen((s), (n)) + 1)

static inline long strnlen_user(const char __user *src, long n)
 {
         return __strnlen_user(src, n);
 }

void device_initialize_tlx(struct device *dev) {};



#define put_user(x, ptr)                                                \
 ({                                                                      \
         __typeof__(*(ptr)) __user *__p = (ptr);                         \
         might_fault_tlx();                                                  \
         access_ok(VERIFY_WRITE, __p, sizeof(*__p)) ?                    \
                 __put_user((x), __p) :                                  \
               -EFAULT;                                                \
 })

int poll_select_set_timeout_tlx(struct timespec *to, long sec, long nsec)
{
         return 0;
}






static inline unsigned fls_long_tlx(unsigned long l)
 {
         if (sizeof(l) == 4)
                 return fls_tlx(l);
         return fls64_tlx(l);
 }

unsigned long __roundup_pow_of_two_tlx(unsigned long n)
 {
         return 1UL << fls_long_tlx(n - 1);
  }


#define put_user(x, ptr)                                                \
 ({                                                                      \
         __typeof__(*(ptr)) __user *__p = (ptr);                         \
         might_fault_tlx();                                                  \
         access_ok(VERIFY_WRITE, __p, sizeof(*__p)) ?                    \
                 __put_user((x), __p) :                                  \
               -EFAULT;                                                \
 })
# define  _protect(n, ret, args...)    do { } while (0)






# define this_cpu_add(pcp, val)         __pcpu_size_call(this_cpu_add_, (pcp), (val))



# define this_cpu_sub(pcp, val)         this_cpu_add((pcp), -(typeof(pcp))(val))

void percpu_counter_add_tlx(struct percpu_counter *fbc, s64 amount)
{
			 s32 batch =  percpu_counter_batch_tlx;
				s64 count;
				count = __this_cpu_read(*fbc->counters) + amount;
				if (count >= batch || count <= -batch) {
									fbc->count += count;
								this_cpu_sub(*fbc->counters, count - amount);
				 } else {
								 this_cpu_add(*fbc->counters, amount);
				 }
}




static inline void percpu_counter_dec_tlx(struct percpu_counter *fbc)
 {
         percpu_counter_add_tlx(fbc, -1);
 }







static inline const char *kobject_name_tlx(const struct kobject *kobj)
 {
         return kobj->name;
  }

void * high_memory_tlx;



#define for_each_lru(lru) for (lru = 0; lru < NR_LRU_LISTS; lru++)
#define for_each_migratetype_order(order, type) \
         for (order = 0; order < MAX_ORDER; order++) \
                 for (type = 0; type < MIGRATE_TYPES; type++)

unsigned long  nr_kernel_pages_tlx;
unsigned long  nr_all_pages_tlx;
unsigned long  dma_reserve_tlx;

char * const zone_names[MAX_NR_ZONES] = {
#ifdef CONFIG_ZONE_DMA
	"DMA",
#endif
#ifdef CONFIG_ZONE_DMA32
	"DMA32",
#endif
	"Normal",
#ifdef CONFIG_HIGHMEM
	"HighMem",
#endif
	"Movable",
};

enum memmap_context
 {
         MEMMAP_EARLY,
         MEMMAP_HOTPLUG,
};

DEFINE_PER_CPU(struct per_cpu_pageset, boot_pageset_tlx);

unsigned long highest_memmap_pfn_tlx;



void * __init memblock_tlx_virt_alloc_internal_tlx(
				phys_addr_t size, phys_addr_t align,
				phys_addr_t min_addr, phys_addr_t max_addr,
				int nid);


void  free_area_init_node_tlx(int nid, unsigned long *zones_size,
		unsigned long node_start_pfn, unsigned long *zholes_size)
{
	pg_data_t *pgdat = (&contig_page_data_tlx);
	unsigned long start_pfn = 0;
	unsigned long end_pfn = 0;
	unsigned long realtotalpages, totalpages = 0;
	pgdat->node_id = nid;
	pgdat->node_start_pfn = node_start_pfn;
		enum zone_type i;
		for (i = 0; i < MAX_NR_ZONES; i++)
			totalpages += zones_size[i];
		pgdat->node_spanned_pages = totalpages;
		realtotalpages = totalpages;
		if (zholes_size)
			for (i = 0; i < MAX_NR_ZONES; i++)
				realtotalpages -=	zholes_size[i];
		pgdat->node_present_pages = realtotalpages;

	node_start_pfn = start_pfn;
	unsigned long node_end_pfn = end_pfn;
		enum zone_type j;
		nid = pgdat->node_id;
		unsigned long zone_start_pfn = pgdat->node_start_pfn;
		int ret;
		static struct lock_class_key __key1;

		__init_waitqueue_head_tlx((&pgdat->kswapd_wait), "&pgdat->kswapd_wait", &__key1);
//		init_waitqueue_head(&pgdat->kswapd_wait);
//		init_waitqueue_head(&pgdat->pfmemalloc_wait);
		static struct lock_class_key __key2;
		__init_waitqueue_head_tlx((&pgdat->pfmemalloc_wait), "&pgdat->pfmemalloc_wait", &__key1);
		for (j = 0; j < MAX_NR_ZONES; j++) {
			struct zone *zone = pgdat->node_zones + j;
			unsigned long size, realsize, freesize, memmap_pages;
			size = zones_size[j];
			realsize = freesize = size - zholes_size[j];
				unsigned long spanned_pages = size;
				unsigned long present_pages = realsize;
				unsigned long pages = spanned_pages;
				if (spanned_pages > present_pages + (present_pages >> 4) &&
						IS_ENABLED(CONFIG_SPARSEMEM))
					pages = present_pages;
					memmap_pages = PAGE_ALIGN(pages * sizeof(struct page)) >> PAGE_SHIFT;
			if (freesize >= memmap_pages) {
				freesize -= memmap_pages;
			};
			if (j == 0 && freesize > dma_reserve_tlx) {
				freesize -= dma_reserve_tlx;
			}
				nr_kernel_pages_tlx += freesize;
				nr_kernel_pages_tlx -= memmap_pages;
			nr_all_pages_tlx += freesize;
			zone->spanned_pages = size;
			zone->present_pages = realsize;
			zone->managed_pages =  freesize;
			zone->name = zone_names[j];
			spin_lock_init(&zone->lock);
			spin_lock_init(&zone->lru_lock);
			zone->zone_pgdat = pgdat;
			zone->pageset = &boot_pageset_tlx;
	//		mod_zone_page_state(zone, NR_ALLOC_BATCH, zone->managed_pages);
			struct lruvec *lruvec = &zone->lruvec;
			enum lru_list lru;
			memset_tlx(lruvec, 0, sizeof(struct lruvec));
			for_each_lru(lru)
								INIT_LIST_HEAD(&lruvec->lists[lru]);

			if (!size)
				continue;
				enum memmap_context context = MEMMAP_EARLY;
				struct pglist_data *pgdat = zone->zone_pgdat;
				unsigned long zone_size_pages = size;
					int i;
					size_t alloc_size;
					unsigned long pages_ = zone_size_pages;
					unsigned long size_ = 1;
					pages_ /= PAGES_PER_WAITQUEUE;
					while (size_ < pages_)
								size_ <<= 1;
					size_ = min(size_, 4096UL);
					zone->wait_table_hash_nr_entries = max(size_, 4UL);
					zone->wait_table_bits =
						__ffs_tlx(~(~(zone->wait_table_hash_nr_entries)));
					alloc_size = zone->wait_table_hash_nr_entries
									* sizeof(wait_queue_head_t);
						zone->wait_table = (wait_queue_head_t *)
							memblock_tlx_virt_alloc_internal_tlx(
								alloc_size, 0, BOOTMEM_LOW_LIMIT,
											BOOTMEM_ALLOC_ACCESSIBLE, zone->zone_pgdat->node_id);
					for (i = 0; i < zone->wait_table_hash_nr_entries; ++i)
						init_waitqueue_head(zone->wait_table + i);
				pgdat->nr_zones = zone_idx(zone) + 1;
				zone->zone_start_pfn = zone_start_pfn;
				unsigned int order, t;
				for_each_migratetype_order(order, t) {
								INIT_LIST_HEAD(&zone->free_area[order].free_list[t]);
								zone->free_area[order].nr_free = 0;
				}
			unsigned long zone0 = j;
			start_pfn = zone_start_pfn;
				struct page *page;
				unsigned long end_pfn = start_pfn + size;
				unsigned long pfn;
				struct zone *z;
				if (highest_memmap_pfn_tlx < end_pfn - 1)
					highest_memmap_pfn_tlx = end_pfn - 1;
				z = &((&contig_page_data_tlx))->node_zones[zone0];
				for (pfn = start_pfn; pfn < end_pfn; pfn++) {
					page = pfn_to_page(pfn);
					page->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);
					page->flags |= (zone0 & ZONES_MASK) << ZONES_PGSHIFT;
				page->flags &= ~(NODES_MASK << NODES_PGSHIFT);
				page->flags |= (nid & NODES_MASK) << NODES_PGSHIFT;
///				set_page_section(page, pfn_to_section_nr(pfn));
//						struct page *page, unsigned long section)

         page->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);
         page->flags |= (pfn_to_section_nr(pfn) & SECTIONS_MASK) << SECTIONS_PGSHIFT;
					SetPageReserved(page);
					if ((z->zone_start_pfn <= pfn)
							&& (pfn < z->zone_start_pfn + z->spanned_pages)) {
													unsigned long flags =(unsigned long)MIGRATE_MOVABLE;
																		unsigned long pfn = page_to_pfn(page);
																		unsigned long end_bitidx = PB_migrate_end;
																		unsigned long mask = (1 << (PB_migrate_end - PB_migrate + 1)) - 1;
														struct zone *zone;
														unsigned long *bitmap;
														unsigned long bitidx, word_bitidx;
														unsigned long old_word, word;
														zone = &NODE_DATA(page_to_nid(page))->node_zones[(page->flags >> ZONES_PGSHIFT) & ZONES_MASK];
														#ifdef CONFIG_SPARSEMEM
															bitmap =  __nr_to_section_tlx(pfn_to_section_nr(pfn))->pageblock_flags;
														#else
															bitmap = zone->pageblock_flags;
														#endif
														#ifdef CONFIG_SPARSEMEM
															pfn &= (PAGES_PER_SECTION-1);
															bitidx = (pfn >> pageblock_order) * NR_PAGEBLOCK_BITS;
														#else
															pfn = pfn - round_down(zone->zone_start_pfn, pageblock_nr_pages);
															bitidx = (pfn >> pageblock_order) * NR_PAGEBLOCK_BITS;
														#endif /* CONFIG_SPARSEMEM */
														word_bitidx = bitidx / BITS_PER_LONG;
														bitidx &= (BITS_PER_LONG-1);
														bitidx += end_bitidx;
														mask <<= (BITS_PER_LONG - bitidx - 1);
														flags <<= (BITS_PER_LONG - bitidx - 1);
														word = ACCESS_ONCE(bitmap[word_bitidx]);
														for (;;) {
															old_word = cmpxchg(&bitmap[word_bitidx], word, (word & ~mask) | flags);
															if (word == old_word)
																break;
															word = old_word;
														}
							}
					INIT_LIST_HEAD(&page->lru);
				}
			zone_start_pfn += size;
		}

}

#define SECTION_HAS_MEM_MAP     (1UL<<1)
#define PA_SECTION_SHIFT        (SECTION_SIZE_BITS)

#define SECTION_MARKED_PRESENT  (1UL<<0)
#define SECTION_NID_SHIFT       2
#define SECTION_BLOCKFLAGS_BITS \
         ((1UL << (PFN_SECTION_SHIFT - pageblock_order)) * NR_PAGEBLOCK_BITS)

#define SECTION_MAP_LAST_BIT    (1UL<<2)
#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)
#define SECTION_MAP_MASK        (~(SECTION_MAP_LAST_BIT-1))

struct page *__section_mem_map_addr_tlx(struct mem_section_tlx *section)
{
				unsigned long map = section->section_mem_map;
				map &= SECTION_MAP_MASK;
				return (struct page *)map;
}

#define PFN_UP(x)       (((x) + PAGE_SIZE-1) >> PAGE_SHIFT)
#define PFN_DOWN(x)     ((x) >> PAGE_SHIFT)

#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)
#define PAGE_SECTION_MASK       (~(PAGES_PER_SECTION-1))








#define __SC_DECL(t, a)	t a
#define __TYPE_IS_L(t)	(__same_type((t)0, 0L))
#define __TYPE_IS_UL(t)	(__same_type((t)0, 0UL))
#define __TYPE_IS_LL(t) (__same_type((t)0, 0LL) || __same_type((t)0, 0ULL))
#define __SC_LONG(t, a) __typeof(__builtin_choose_expr(__TYPE_IS_LL(t), 0LL, 0L)) a
#define __SC_CAST(t, a)	(t) a
#define __SC_ARGS(t, a)	a
#define __SC_TEST(t, a) (void)BUILD_BUG_ON_ZERO(!__TYPE_IS_LL(t) && sizeof(t) > sizeof(long))

#define __MAP0(m,...)
#define __MAP1(m,t,a) m(t,a)
#define __MAP2(m,t,a,...) m(t,a), __MAP1(m,__VA_ARGS__)
#define __MAP3(m,t,a,...) m(t,a), __MAP2(m,__VA_ARGS__)
#define __MAP4(m,t,a,...) m(t,a), __MAP3(m,__VA_ARGS__)
#define __MAP5(m,t,a,...) m(t,a), __MAP4(m,__VA_ARGS__)
#define __MAP6(m,t,a,...) m(t,a), __MAP5(m,__VA_ARGS__)
#define __MAP(n,...) __MAP##n(__VA_ARGS__)


#define SYSCALL_DEFINE0(sname)					\
    long sys_##sname(void)

#define SYSCALL_DEFINE1(name, ...) SYSCALL_DEFINEx(1, _##name, __VA_ARGS__)
#define SYSCALL_DEFINE2(name, ...) SYSCALL_DEFINEx(2, _##name, __VA_ARGS__)
#define SYSCALL_DEFINE3(name, ...) SYSCALL_DEFINEx(3, _##name, __VA_ARGS__)
#define SYSCALL_DEFINE4(name, ...) SYSCALL_DEFINEx(4, _##name, __VA_ARGS__)
#define SYSCALL_DEFINE5(name, ...) SYSCALL_DEFINEx(5, _##name, __VA_ARGS__)
#define SYSCALL_DEFINE6(name, ...) SYSCALL_DEFINEx(6, _##name, __VA_ARGS__)

#define SYSCALL_DEFINEx(x, sname, ...)				\
  __SYSCALL_DEFINEx(x, sname, __VA_ARGS__)

#define __PROTECT(...)  _protect(__VA_ARGS__)
#define __SYSCALL_DEFINEx(x, name, ...)					\
    long sys##name(__MAP(x,__SC_DECL,__VA_ARGS__))	\
    __attribute__((alias(__stringify(SyS##name))));		\
  static inline long SYSC##name(__MAP(x,__SC_DECL,__VA_ARGS__));	\
    long SyS##name(__MAP(x,__SC_LONG,__VA_ARGS__));	\
    long SyS##name(__MAP(x,__SC_LONG,__VA_ARGS__))	\
  {								\
    long ret = SYSC##name(__MAP(x,__SC_CAST,__VA_ARGS__));	\
    __MAP(x,__SC_TEST,__VA_ARGS__);				\
    __PROTECT(x, ret,__MAP(x,__SC_ARGS,__VA_ARGS__));	\
    return ret;						\
  }								\
  static inline long SYSC##name(__MAP(x,__SC_DECL,__VA_ARGS__))





#define BOOTMEM_LOW_LIMIT 0
#define BOOTMEM_ALLOC_ACCESSIBLE        0
#define MAX_DMA_ADDRESS PAGE_OFFSET



struct softirq_action
{
         void    (*action)(struct softirq_action *);
};

enum
{
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	BLOCK_IOPOLL_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
	HRTIMER_SOFTIRQ,
	RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */

	NR_SOFTIRQS
};

cpumask_var_t irq_default_affinity_tlx;


#define NR_LDISCS               30


struct tty_ldisc_ops {
  int	magic;
  char	*name;
  int	num;
  int	flags;

  /*
   * The following routines are called from above.
   */
  int	(*open)(struct tty_struct *);
  void	(*close)(struct tty_struct *);
  void	(*flush_buffer)(struct tty_struct *tty);
  ssize_t	(*chars_in_buffer)(struct tty_struct *tty);
  ssize_t	(*read)(struct tty_struct *tty, struct file *file,
      unsigned char __user *buf, size_t nr);
  ssize_t	(*write)(struct tty_struct *tty, struct file *file,
       const unsigned char *buf, size_t nr);
  int	(*ioctl)(struct tty_struct *tty, struct file *file,
       unsigned int cmd, unsigned long arg);
  long	(*compat_ioctl)(struct tty_struct *tty, struct file *file,
        unsigned int cmd, unsigned long arg);
  void	(*set_termios)(struct tty_struct *tty, struct ktermios *old);
  unsigned int (*poll)(struct tty_struct *, struct file *,
           struct poll_table_struct *);
  int	(*hangup)(struct tty_struct *tty);

  /*
   * The following routines are called from below.
   */
  void	(*receive_buf)(struct tty_struct *, const unsigned char *cp,
             char *fp, int count);
  void	(*write_wakeup)(struct tty_struct *);
  void	(*dcd_change)(struct tty_struct *, unsigned int);
  void	(*fasync)(struct tty_struct *tty, int on);
  int	(*receive_buf2)(struct tty_struct *, const unsigned char *cp,
        char *fp, int count);

  struct  module *owner;

  int refcount;
};

struct class *tty_class_tlx;



void *kmalloc_tlx(size_t size, gfp_t flags);
#define N_TTY_BUF_SIZE 4096

struct n_tty_data {
	/* producer-published */
	size_t read_head;
	size_t canon_head;
	size_t echo_head;
	size_t echo_commit;
	size_t echo_mark;
	DECLARE_BITMAP(char_map, 256);

	/* private to n_tty_receive_overrun (single-threaded) */
	unsigned long overrun_time;
	int num_overrun;

	/* non-atomic */
	bool no_room;

	/* must hold exclusive termios_rwsem to reset these */
	unsigned char lnext:1, erasing:1, raw:1, real_raw:1, icanon:1;
	unsigned char push:1;

	/* shared by producer and consumer */
	char read_buf[N_TTY_BUF_SIZE];
	DECLARE_BITMAP(read_flags, N_TTY_BUF_SIZE);
	unsigned char echo_buf[N_TTY_BUF_SIZE];

	int minimum_to_wake;

	/* consumer-published */
	size_t read_tail;
	size_t line_start;

	/* protected by output lock */
	unsigned int column;
	unsigned int canon_column;
	size_t echo_tail;

	struct mutex atomic_read_lock;
	struct mutex output_lock;
};

struct winsize {
         unsigned short ws_row;
         unsigned short ws_col;
         unsigned short ws_xpixel;
         unsigned short ws_ypixel;
};

#define NCCS 19
typedef unsigned int    tcflag_t;
typedef unsigned char   cc_t;
typedef unsigned int    speed_t;

struct ktermios {
         tcflag_t c_iflag;               /* input mode flags */
         tcflag_t c_oflag;               /* output mode flags */
         tcflag_t c_cflag;               /* control mode flags */
         tcflag_t c_lflag;               /* local mode flags */
         cc_t c_line;                    /* line discipline */
         cc_t c_cc[NCCS];                /* control characters */
         speed_t c_ispeed;               /* input speed */
         speed_t c_ospeed;               /* output speed */
 };

struct ld_semaphore {
         long                    count;
         raw_spinlock_t          wait_lock;
         unsigned int            wait_readers;
         struct list_head        read_wait;
         struct list_head        write_wait;
 };

struct tty_struct {
	int	magic;
	struct kref kref;
	struct device *dev;
	struct tty_driver *driver;
	const struct tty_operations *ops;
	int index;

	/* Protects ldisc changes: Lock tty not pty */
	struct ld_semaphore ldisc_sem;
	struct tty_ldisc *ldisc;

	struct mutex atomic_write_lock;
	struct mutex legacy_mutex;
	struct mutex throttle_mutex;
	struct rw_semaphore termios_rwsem;
	struct mutex winsize_mutex;
	spinlock_t ctrl_lock;
	/* Termios values are protected by the termios rwsem */
	struct ktermios termios, termios_locked;
	struct termiox *termiox;	/* May be NULL for unsupported */
	char name[64];
	struct pid *pgrp;		/* Protected by ctrl lock */
	struct pid *session;
	unsigned long flags;
	int count;
	struct winsize winsize;		/* winsize_mutex */
	unsigned char stopped:1, hw_stopped:1, flow_stopped:1, packet:1;
	unsigned char ctrl_status;	/* ctrl_lock */
	unsigned int receive_room;	/* Bytes free for queue */
	int flow_change;

	struct tty_struct *link;
	struct fasync_struct *fasync;
	int alt_speed;		/* For magic substitution of 38400 bps */
	wait_queue_head_t write_wait;
	wait_queue_head_t read_wait;
	struct work_struct hangup_work;
	void *disc_data;
	void *driver_data;
	struct list_head tty_files;

#define N_TTY_BUF_SIZE 4096

	unsigned char closing:1;
	unsigned char *write_buf;
	int write_cnt;
	/* If the tty has a pending do_SAK, queue it here - akpm */
	struct work_struct SAK_work;
	struct tty_port *port;
};


/*
int n_tty_open_tlx(struct tty_struct *tty)
{
	struct n_tty_data *ldata;

	ldata =  kmalloc_tlx(sizeof(*ldata), GFP_KERNEL);
//	vmalloc(sizeof(*ldata));
	ldata->overrun_time = jiffies_tlx;
	mutex_init(&ldata->atomic_read_lock);
	mutex_init(&ldata->output_lock);
	tty->disc_data = ldata;
	ldata->column = 0;
	ldata->canon_column = 0;
	ldata->minimum_to_wake = 1;
	ldata->num_overrun = 0;
	ldata->no_room = 0;
	ldata->lnext = 0;
	tty->closing = 0;

	set_bit_tlx('\r', ldata->char_map);

	return 0;
}
*/
void mutex_lock_tlx(struct mutex *lock) {};
void mutex_unlock_tlx(struct mutex *lock) {};

#define _O_FLAG(tty, f) ((tty)->termios.c_oflag & (f))
#define OPOST   0000001
#define ECHO_OP_START 0xff
#define ECHO_OP_MOVE_BACK_COL 0x80
#define ECHO_OP_SET_CANON_COL 0x81
#define ECHO_OP_ERASE_TAB 0x82
#define PARMRK  0000010
#define I_PARMRK(tty)   _I_FLAG((tty), PARMRK)
#define L_EXTPROC(tty)  _L_FLAG((tty), EXTPROC)
#define _I_FLAG(tty, f) ((tty)->termios.c_iflag & (f))

#define TIME_CHAR(tty) ((tty)->termios.c_cc[VTIME])
#define MIN_CHAR(tty) ((tty)->termios.c_cc[VMIN])
#define SIGTTIN         21
#define EXTPROC 0200000
#define _L_FLAG(tty, f) ((tty)->termios.c_lflag & (f))
#define TIOCPKT_DATA             0

#define O_NONBLOCK      00004000
#define TTY_OTHER_CLOSED        2       /* Other side (if any) has closed */
#define VTIME 5
#define VMIN 6
#define TTY_DO_WRITE_WAKEUP     5       /* Call write_wakeup after queuing new */
#define O_OPOST(tty)    _O_FLAG((tty), OPOST)

struct tty_operations {
	struct tty_struct * (*lookup)(struct tty_driver *driver,
			struct inode *inode, int idx);
	int  (*install)(struct tty_driver *driver, struct tty_struct *tty);
	void (*remove)(struct tty_driver *driver, struct tty_struct *tty);
	int  (*open)(struct tty_struct * tty, struct file * filp);
	void (*close)(struct tty_struct * tty, struct file * filp);
	void (*shutdown)(struct tty_struct *tty);
	void (*cleanup)(struct tty_struct *tty);
	int  (*write)(struct tty_struct * tty,
					const unsigned char *buf, int count);
	int  (*put_char)(struct tty_struct *tty, unsigned char ch);
	void (*flush_chars)(struct tty_struct *tty);
	int  (*write_room)(struct tty_struct *tty);
	int  (*chars_in_buffer)(struct tty_struct *tty);
	int  (*ioctl)(struct tty_struct *tty,
				unsigned int cmd, unsigned long arg);
	long (*compat_ioctl)(struct tty_struct *tty,
					unsigned int cmd, unsigned long arg);
	void (*set_termios)(struct tty_struct *tty, struct ktermios * old);
	void (*throttle)(struct tty_struct * tty);
	void (*unthrottle)(struct tty_struct * tty);
	void (*stop)(struct tty_struct *tty);
	void (*start)(struct tty_struct *tty);
	void (*hangup)(struct tty_struct *tty);
	int (*break_ctl)(struct tty_struct *tty, int state);
	void (*flush_buffer)(struct tty_struct *tty);
	void (*set_ldisc)(struct tty_struct *tty);
	void (*wait_until_sent)(struct tty_struct *tty, int timeout);
	void (*send_xchar)(struct tty_struct *tty, char ch);
	int (*tiocmget)(struct tty_struct *tty);
	int (*tiocmset)(struct tty_struct *tty,
			unsigned int set, unsigned int clear);
	int (*resize)(struct tty_struct *tty, struct winsize *ws);
	int (*set_termiox)(struct tty_struct *tty, struct termiox *tnew);
	int (*get_icount)(struct tty_struct *tty,
				struct serial_icounter_struct *icount);
#ifdef CONFIG_CONSOLE_POLL
	int (*poll_init)(struct tty_driver *driver, int line, char *options);
	int (*poll_get_char)(struct tty_driver *driver, int line);
	void (*poll_put_char)(struct tty_driver *driver, int line, char ch);
#endif
	const struct file_operations *proc_fops;
};

int tty_put_char_tlx(struct tty_struct *tty, unsigned char ch)
 {
         if (tty->ops->put_char)
                 return tty->ops->put_char(tty, ch);
         return tty->ops->write(tty, &ch, 1);
 }

#define ICANON  0000002
#define ISTRIP  0000040
#define INLCR   0000100
#define IUCLC   0001000
#define IGNCR   0000200
#define ICRNL   0000400
#define IXON    0002000
#define ISIG    0000001
#define ECHO    0000010
#define VERASE 2
#define VKILL 3
#define VEOF 4
#define VEOL 11
#define VSTART 8
#define VSTOP 9

#define __DISABLED_CHAR '\0'
#define STOP_CHAR(tty) ((tty)->termios.c_cc[VSTOP])
#define START_CHAR(tty) ((tty)->termios.c_cc[VSTART])
#define EOL_CHAR(tty) ((tty)->termios.c_cc[VEOL])
#define ERASE_CHAR(tty) ((tty)->termios.c_cc[VERASE])
#define KILL_CHAR(tty) ((tty)->termios.c_cc[VKILL])
#define EOF_CHAR(tty) ((tty)->termios.c_cc[VEOF])
#define L_ECHO(tty)     _L_FLAG((tty), ECHO)
#define L_ISIG(tty)     _L_FLAG((tty), ISIG)
#define I_INLCR(tty)    _I_FLAG((tty), INLCR)
#define I_IXON(tty)     _I_FLAG((tty), IXON)
#define I_ICRNL(tty)    _I_FLAG((tty), ICRNL)
#define I_IGNCR(tty)    _I_FLAG((tty), IGNCR)
#define I_IUCLC(tty)    _I_FLAG((tty), IUCLC)
#define I_ISTRIP(tty)   _I_FLAG((tty), ISTRIP)
#define L_ICANON(tty)   _L_FLAG((tty), ICANON)
#define TTY_LDISC_HALTED        22      /* Line discipline is halted */

int n_tty_open_tlx(struct tty_struct *tty)
{
	struct n_tty_data *ldata;

	/* Currently a malloc failure here can panic */
	ldata = kmalloc_tlx(sizeof(*ldata), GFP_KERNEL);
	ldata->overrun_time = jiffies_tlx;
	mutex_init(&ldata->atomic_read_lock);
	mutex_init(&ldata->output_lock);
	tty->disc_data = ldata;
//	reset_buffer_flags(tty->disc_data);
	ldata->column = 0;
	ldata->canon_column = 0;
	ldata->minimum_to_wake = 1;
	ldata->num_overrun = 0;
	ldata->no_room = 0;
	ldata->lnext = 0;
	tty->closing = 0;
	/* indicate buffer work may resume */
	__clear_bit_tlx(TTY_LDISC_HALTED, &tty->flags);
//	n_tty_set_termios(tty, NULL);
//	tty_unthrottle(tty);
//	struct n_tty_data *ldata = tty->disc_data;

//	if (!old || (old->c_lflag ^ tty->termios.c_lflag) & ICANON) {
		bitmap_zero_tlx(ldata->read_flags, N_TTY_BUF_SIZE);
		ldata->line_start = ldata->read_tail;
		if (!L_ICANON(tty) || !(ldata->read_head - ldata->read_tail)) {
			ldata->canon_head = ldata->read_tail;
			ldata->push = 0;
		} else {
			set_bit_tlx((ldata->read_head - 1) & (N_TTY_BUF_SIZE - 1),
				ldata->read_flags);
			ldata->canon_head = ldata->read_head;
			ldata->push = 1;
		}
		ldata->erasing = 0;
		ldata->lnext = 0;
//	}

	ldata->icanon = (L_ICANON(tty) != 0);

	if (I_ISTRIP(tty) || I_IUCLC(tty) || I_IGNCR(tty) ||
			I_ICRNL(tty) || I_INLCR(tty) || L_ICANON(tty) ||
			I_IXON(tty) || L_ISIG(tty) || L_ECHO(tty) ||
			I_PARMRK(tty)) {
		bitmap_zero_tlx(ldata->char_map, 256);

		if (I_IGNCR(tty) || I_ICRNL(tty))
			set_bit_tlx('\r', ldata->char_map);
		if (I_INLCR(tty))
			set_bit_tlx('\n', ldata->char_map);

		if (L_ICANON(tty)) {
			set_bit_tlx(ERASE_CHAR(tty), ldata->char_map);
			set_bit_tlx(KILL_CHAR(tty), ldata->char_map);
			set_bit_tlx(EOF_CHAR(tty), ldata->char_map);
			set_bit_tlx('\n', ldata->char_map);
			set_bit_tlx(EOL_CHAR(tty), ldata->char_map);
		}
		if (I_IXON(tty)) {
			set_bit_tlx(START_CHAR(tty), ldata->char_map);
			set_bit_tlx(STOP_CHAR(tty), ldata->char_map);
		}
		__clear_bit_tlx(__DISABLED_CHAR, ldata->char_map);
		ldata->raw = 0;
		ldata->real_raw = 0;
	}
//	n_tty_set_room(tty);

	/* The termios change make the tty ready for I/O
	if (waitqueue_active(&tty->write_wait))
		wake_up_interruptible(&tty->write_wait);
	if (waitqueue_active(&tty->read_wait))
		wake_up_interruptible(&tty->read_wait);
*/
	return 0;

}


#define POLLIN          0x0001
#define POLLOUT         0x0004



size_t __process_echoes_tlx(struct tty_struct *tty);

#define ECHOE   0000020
#define ECHOK   0000040
#define ECHOKE  0004000
#define ECHOPRT 0002000

#define VWERASE 14
#define VLNEXT 15
#define VREPRINT 12
#define VEOL2 16
#define ECHONL  0000100
#define L_ECHONL(tty)   _L_FLAG((tty), ECHONL)
#define EOL2_CHAR(tty) ((tty)->termios.c_cc[VEOL2])
#define TTY_NORMAL      0
#define L_IEXTEN(tty)   _L_FLAG((tty), IEXTEN)
#define IEXTEN  0100000
#define I_IXANY(tty)    _I_FLAG((tty), IXANY)
#define IXANY   0004000
#define ECHOCTL 0001000
#define L_ECHOCTL(tty)  _L_FLAG((tty), ECHOCTL)


#define I_IUTF8(tty)    _I_FLAG((tty), IUTF8)
#define O_ONLRET(tty)   _O_FLAG((tty), ONLRET)
#define O_ONLCR(tty)    _O_FLAG((tty), ONLCR)
#define O_ONOCR(tty)    _O_FLAG((tty), ONOCR)
#define O_OCRNL(tty)    _O_FLAG((tty), OCRNL)
#define O_TABDLY(tty)   _O_FLAG((tty), TABDLY)
#define O_OLCUC(tty)    _O_FLAG((tty), OLCUC)
#define   XTABS 0014000

#define TABDLY  0014000
#define OCRNL   0000010
#define ONOCR   0000020
#define ONLCR   0000004
#define ONLRET  0000040
#define IUTF8   0040000
#define ECHO_COMMIT_WATERMARK   256
#define ECHO_BLOCK              256
#define REPRINT_CHAR(tty) ((tty)->termios.c_cc[VREPRINT])
#define LNEXT_CHAR(tty) ((tty)->termios.c_cc[VLNEXT])
#define WERASE_CHAR(tty) ((tty)->termios.c_cc[VWERASE])
#define L_ECHOPRT(tty)  _L_FLAG((tty), ECHOPRT)
#define isalnum(c)      ((__ismask(c)&(_U|_L|_D)) != 0)
#define L_ECHOE(tty)    _L_FLAG((tty), ECHOE)
#define L_ECHOK(tty)    _L_FLAG((tty), ECHOK)
#define L_ECHOKE(tty)   _L_FLAG((tty), ECHOKE)


void put_tty_queue_tlx(unsigned char c, struct n_tty_data *ldata)
{
	*(&ldata->read_buf[ldata->read_head++ & (N_TTY_BUF_SIZE - 1)])= c;
}

void add_echo_byte_tlx(unsigned char c, struct n_tty_data *ldata)
{
	*(&ldata->echo_buf[ldata->echo_head++ & (N_TTY_BUF_SIZE - 1)]) = c;
}

void echo_char_raw_tlx(unsigned char c, struct n_tty_data *ldata)
{
	if (c == ECHO_OP_START) {
		add_echo_byte_tlx(ECHO_OP_START, ldata);
		add_echo_byte_tlx(ECHO_OP_START, ldata);
	} else {
		add_echo_byte_tlx(c, ldata);
	}
}

void echo_char_tlx(unsigned char c, struct tty_struct *tty)
{
	struct n_tty_data *ldata = tty->disc_data;

	if (c == ECHO_OP_START) {
		add_echo_byte_tlx(ECHO_OP_START, ldata);
		add_echo_byte_tlx(ECHO_OP_START, ldata);
	} else {
		if (L_ECHOCTL(tty) && ((__ismask(c)&(_U|_L)) != 0) && c != '\t')
			add_echo_byte_tlx(ECHO_OP_START, ldata);
		add_echo_byte_tlx(c, ldata);
	}
}

int is_continuation_tlx(unsigned char c, struct tty_struct *tty)
{
	return I_IUTF8(tty) && ((c & 0xc0) == 0x80);
}

void eraser_tlx(unsigned char c, struct tty_struct *tty)
{
	struct n_tty_data *ldata = tty->disc_data;
	enum { ERASE, WERASE, KILL } kill_type;
	size_t head;
	size_t cnt;
	int seen_alnums;

	if (ldata->read_head == ldata->canon_head) {
		/* process_output('\a', tty); */ /* what do you think? */
		return;
	}
	if (c == ERASE_CHAR(tty))
		kill_type = ERASE;
	else if (c == WERASE_CHAR(tty))
		kill_type = WERASE;
	else {
		if (!L_ECHO(tty)) {
			ldata->read_head = ldata->canon_head;
			return;
		}
		if (!L_ECHOK(tty) || !L_ECHOKE(tty) || !L_ECHOE(tty)) {
			ldata->read_head = ldata->canon_head;
			if (ldata->erasing) {
				echo_char_raw_tlx('/', ldata);
				ldata->erasing = 0;
			}
			echo_char_tlx(KILL_CHAR(tty), tty);
			/* Add a newline if ECHOK is on and ECHOKE is off. */
			if (L_ECHOK(tty))
				echo_char_raw_tlx('\n', ldata);
			return;
		}
		kill_type = KILL;
	}

	seen_alnums = 0;
	while (ldata->read_head != ldata->canon_head) {
		head = ldata->read_head;

		/* erase a single possibly multibyte character */
		do {
			head--;
			c =  ldata->read_buf[head & (N_TTY_BUF_SIZE - 1)];
		} while (is_continuation_tlx(c, tty) && head != ldata->canon_head);

		/* do not partially erase */
		if (is_continuation_tlx(c, tty))
			break;

		if (kill_type == WERASE) {
			/* Equivalent to BSD's ALTWERASE. */
			if (isalnum(c) || c == '_')
				seen_alnums++;
			else if (seen_alnums)
				break;
		}
		cnt = ldata->read_head - head;
		ldata->read_head = head;
		if (L_ECHO(tty)) {
			if (L_ECHOPRT(tty)) {
				if (!ldata->erasing) {
					echo_char_raw_tlx('\\', ldata);
					ldata->erasing = 1;
				}
				/* if cnt > 1, output a multi-byte character */
				echo_char_tlx(c, tty);
				while (--cnt > 0) {
					head++;
					echo_char_raw_tlx(ldata->read_buf[head & (N_TTY_BUF_SIZE - 1)], ldata);
//					echo_move_back_col(ldata);
						add_echo_byte_tlx(ECHO_OP_START, ldata);
						add_echo_byte_tlx(ECHO_OP_MOVE_BACK_COL, ldata);
				}
			} else if (kill_type == ERASE && !L_ECHOE(tty)) {
				echo_char_tlx(ERASE_CHAR(tty), tty);
			} else if (c == '\t') {
				unsigned int num_chars = 0;
				int after_tab = 0;
				size_t tail = ldata->read_head;

				/*
				* Count the columns used for characters
				* since the start of input or after a
				* previous tab.
				* This info is used to go back the correct
				* number of columns.
				*/
				while (tail != ldata->canon_head) {
					tail--;
					c =  ldata->read_buf[tail & (N_TTY_BUF_SIZE - 1)];
					if (c == '\t') {
						after_tab = 1;
						break;
					} else if (((__ismask(c)&(_C)) != 0)) {
						if (L_ECHOCTL(tty))
							num_chars += 2;
					} else if (!is_continuation_tlx(c, tty)) {
						num_chars++;
					}
				}
//				echo_erase_tab(num_chars, after_tab, ldata);
					add_echo_byte_tlx(ECHO_OP_START, ldata);
					add_echo_byte_tlx(ECHO_OP_ERASE_TAB, ldata);

					/* We only need to know this modulo 8 (tab spacing) */
					num_chars &= 7;

					/* Set the high bit as a flag if num_chars is after a previous tab */
					if (after_tab)
						num_chars |= 0x80;

					add_echo_byte_tlx(num_chars, ldata);
			} else {
				if (((__ismask(c)&(_C)) != 0) && L_ECHOCTL(tty)) {
					echo_char_raw_tlx('\b', ldata);
					echo_char_raw_tlx(' ', ldata);
					echo_char_raw_tlx('\b', ldata);
				}
				if (!((__ismask(c)&(_C)) != 0) || L_ECHOCTL(tty)) {
					echo_char_raw_tlx('\b', ldata);
					echo_char_raw_tlx(' ', ldata);
					echo_char_raw_tlx('\b', ldata);
				}
			}
		}
		if (kill_type == ERASE)
			break;
	}
	if (ldata->read_head == ldata->canon_head && L_ECHO(tty)) {
		if (ldata->erasing) {
			echo_char_raw_tlx('/', ldata);
			ldata->erasing = 0;
		}
	}
		//finish_erasing(ldata);
}

int
n_tty_receive_char_special_tlx(struct tty_struct *tty, unsigned char c)
{
	struct n_tty_data *ldata = tty->disc_data;
	if (c == '\r') {
		if (I_IGNCR(tty))
			return 0;
		if (I_ICRNL(tty))
			c = '\n';
	} else if (c == '\n' && I_INLCR(tty))
		c = '\r';

	if (ldata->icanon) {
		if (c == ERASE_CHAR(tty) || c == KILL_CHAR(tty) ||
				(c == WERASE_CHAR(tty) && L_IEXTEN(tty))) {
			eraser_tlx(c, tty);
//			commit_echoes(tty);
			return 0;
		}
		if (c == LNEXT_CHAR(tty) && L_IEXTEN(tty)) {
			ldata->lnext = 1;
			if (L_ECHO(tty)) {
				if (ldata->erasing) {
					echo_char_raw_tlx('/', ldata);
					ldata->erasing = 0;
				}
				if (L_ECHOCTL(tty)) {
					echo_char_raw_tlx('^', ldata);
					echo_char_raw_tlx('\b', ldata);
//					commit_echoes(tty);
				}
			}
			return 1;
		}
		if (c == REPRINT_CHAR(tty) && L_ECHO(tty) && L_IEXTEN(tty)) {
			size_t tail = ldata->canon_head;

			if (ldata->erasing) {
				echo_char_raw_tlx('/', ldata);
				ldata->erasing = 0;
			}
			echo_char_tlx(c, tty);
			echo_char_raw_tlx('\n', ldata);
			while (tail != ldata->read_head) {
				echo_char_tlx(ldata->read_buf[tail & (N_TTY_BUF_SIZE - 1)], tty);
				tail++;
			}
//			commit_echoes(tty);
			return 0;
		}
		if (c == '\n') {
			if (L_ECHO(tty) || L_ECHONL(tty)) {
				echo_char_raw_tlx('\n', ldata);
//				commit_echoes(tty);
			}
			goto handle_newline;
		}
		if (c == EOF_CHAR(tty)) {
			c = __DISABLED_CHAR;
			goto handle_newline;
		}
		if ((c == EOL_CHAR(tty)) ||
				(c == EOL2_CHAR(tty) && L_IEXTEN(tty))) {
handle_newline:
			set_bit_tlx(ldata->read_head & (N_TTY_BUF_SIZE - 1), ldata->read_flags);
			put_tty_queue_tlx(c, ldata);
			ldata->canon_head = ldata->read_head;
	//		kill_fasync(&tty->fasync, SIGIO, POLL_IN);
			if (!list_empty(&(&tty->read_wait)->task_list)) {
			//	__wake_up_common(&tty->read_wait),TASK_INTERRUPTIBLE, 1, 0, NULL);
					wait_queue_head_t *q = &tty->read_wait;
					unsigned int mode = TASK_INTERRUPTIBLE;
					int nr_exclusive = 1;
					int wake_flags = 0;
					void *key = NULL;
					wait_queue_t *curr, *next;

					list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
								unsigned flags = curr->flags;

							if (curr->func(curr, mode, wake_flags, key) &&
																(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
												break;
				}
			}
			return 0;
		}
	}



	/* PARMRK doubling check */
	if (c == (unsigned char) '\377' && I_PARMRK(tty))
		put_tty_queue_tlx(c, ldata);

	put_tty_queue_tlx(c, ldata);
	return 0;
}






void commit_echoes_tlx(struct tty_struct *tty)
{
	struct n_tty_data *ldata = tty->disc_data;
	size_t nr, old, echoed;
	size_t head;

	head = ldata->echo_head;
	ldata->echo_mark = head;
	old = ldata->echo_commit - ldata->echo_tail;

	/* Process committed echoes if the accumulated # of bytes
	* is over the threshold (and try again each time another
	* block is accumulated) */
	nr = head - ldata->echo_tail;
	if (nr < ECHO_COMMIT_WATERMARK || (nr % ECHO_BLOCK > old % ECHO_BLOCK))
		return;

//	mutex_lock(&ldata->output_lock);
	ldata->echo_commit = head;
	echoed = __process_echoes_tlx(tty);
//	mutex_unlock(&ldata->output_lock);

	if (echoed && tty->ops->flush_chars)
		tty->ops->flush_chars(tty);
}



void process_echoes_tlx(struct tty_struct *tty)
{
	struct n_tty_data *ldata = tty->disc_data;
	size_t echoed;

	if (ldata->echo_mark == ldata->echo_tail)
		return;

//	mutex_lock(&ldata->output_lock);
	ldata->echo_commit = ldata->echo_mark;
	echoed = __process_echoes_tlx(tty);
//	mutex_unlock(&ldata->output_lock);

	if (echoed && tty->ops->flush_chars)
		tty->ops->flush_chars(tty);
}

void start_tty_tlx(struct tty_struct *tty);

void
n_tty_receive_char_inline_tlx(struct tty_struct *tty, unsigned char c)
{
	struct n_tty_data *ldata = tty->disc_data;

	if (tty->stopped && !tty->flow_stopped && I_IXON(tty) && I_IXANY(tty)) {
		start_tty_tlx(tty);
		process_echoes_tlx(tty);
	}
	if (L_ECHO(tty)) {
		if (ldata->erasing) {
	//		echo_char_raw_tlx('/', ldata);
			unsigned char c = '/';
				if (c == ECHO_OP_START) {
					add_echo_byte_tlx(ECHO_OP_START, ldata);
					add_echo_byte_tlx(ECHO_OP_START, ldata);
				} else {
					add_echo_byte_tlx(c, ldata);
				}
			ldata->erasing = 0;
		}
		/* Record the column of first canon char. */
		if (ldata->canon_head == ldata->read_head) {
			add_echo_byte_tlx(ECHO_OP_START, ldata);
			add_echo_byte_tlx(ECHO_OP_SET_CANON_COL, ldata);
		}
//			echo_set_canon_col(ldata);
//		echo_char_tlx(c, tty);
//		void echo_char_tlx(unsigned char c, struct tty_struct *tty)
//		{
			struct n_tty_data *ldata = tty->disc_data;

			if (c == ECHO_OP_START) {
				add_echo_byte_tlx(ECHO_OP_START, ldata);
				add_echo_byte_tlx(ECHO_OP_START, ldata);
			} else {
				if (L_ECHOCTL(tty) && ((__ismask(c)&(_C)) != 0) && c != '\t')
					add_echo_byte_tlx(ECHO_OP_START, ldata);
				add_echo_byte_tlx(c, ldata);
			}
//		}
		commit_echoes_tlx(tty);
	}
	/* PARMRK doubling check */
	if (c == (unsigned char) '\377' && I_PARMRK(tty))
		put_tty_queue_tlx(c, ldata);
	put_tty_queue_tlx(c, ldata);
}


void __receive_buf_tlx(struct tty_struct *tty, const unsigned char *cp,
				char *fp, int count)
{
	struct n_tty_data *ldata = tty->disc_data;
	bool preops = I_ISTRIP(tty) || (I_IUCLC(tty) && L_IEXTEN(tty));

		if (ldata->lnext) {
			char flag = TTY_NORMAL;

			if (fp)
				flag = *fp++;
//			n_tty_receive_char_lnext(tty, *cp++, flag);
				unsigned char c = cp++;
				struct n_tty_data *ldata = tty->disc_data;

				ldata->lnext = 0;
				if (likely(flag == TTY_NORMAL)) {
					if (I_ISTRIP(tty))
						c &= 0x7f;
						struct n_tty_data *ldata = tty->disc_data;
						if (c == (unsigned char) '\377' && I_PARMRK(tty))
							put_tty_queue_tlx(c, ldata);
						put_tty_queue_tlx(c, ldata);

				}
			count--;
		}

//		n_tty_receive_buf_standard(tty, cp, fp, count);
//			struct n_tty_data *ldata = tty->disc_data;
			char flag = TTY_NORMAL;

			while (count--) {
				if (fp)
					flag = *fp++;
				if (likely(flag == TTY_NORMAL)) {
					unsigned char c = *cp++;

					if (I_ISTRIP(tty))
						c &= 0x7f;
	//				if (I_IUCLC(tty) && L_IEXTEN(tty))
	//					c = tolower(c);
					if (L_EXTPROC(tty)) {
						put_tty_queue_tlx(c, ldata);
						continue;
					}
					if (!test_bit_tlx(c, ldata->char_map))
						n_tty_receive_char_inline_tlx(tty, c);
					else if (n_tty_receive_char_special_tlx(tty, c) && count) {
						if (fp)
							flag = *fp++;
	//					n_tty_receive_char_lnext(tty, *cp++, flag);
						unsigned char c = *cp++;
						struct n_tty_data *ldata = tty->disc_data;

						ldata->lnext = 0;
						if (likely(flag == TTY_NORMAL)) {
							if (I_ISTRIP(tty))
								c &= 0x7f;
								struct n_tty_data *ldata = tty->disc_data;
								if (c == (unsigned char) '\377' && I_PARMRK(tty))
									put_tty_queue_tlx(c, ldata);
								put_tty_queue_tlx(c, ldata);

						}
						count--;
					}
				}
			}

//		flush_echoes(tty);
//		struct n_tty_data *ldata = tty->disc_data;
		ldata->echo_commit = ldata->echo_head;
		__process_echoes_tlx(tty);

		if (tty->ops->flush_chars)
			tty->ops->flush_chars(tty);
//	}

	if ((!ldata->icanon && ((ldata->read_head - ldata->read_tail) >= ldata->minimum_to_wake)) ||
		L_EXTPROC(tty)) {
//		kill_fasync(&tty->fasync, SIGIO, POLL_IN);
		if (!list_empty(&(&tty->read_wait)->task_list)) {
		//	__wake_up_common(&tty->read_wait),TASK_INTERRUPTIBLE, 1, 0, NULL);
				wait_queue_head_t *q = &tty->read_wait;
				unsigned int mode = TASK_INTERRUPTIBLE;
				int nr_exclusive = 1;
				int wake_flags = 0;
				void *key = NULL;
				wait_queue_t *curr, *next;

				list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
							unsigned flags = curr->flags;

						if (curr->func(curr, mode, wake_flags, key) &&
															(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
											break;
			}
		}
//			wake_up_interruptible(&tty->read_wait);
	}
}



int canon_copy_from_read_buf_tlx(struct tty_struct *tty,
						unsigned char __user **b,
						size_t *nr)
{
	struct n_tty_data *ldata = tty->disc_data;
	size_t n, size, more, c;
	size_t eol;
	size_t tail;
	int ret, found = 0;
	bool eof_push = 0;

	/* N.B. avoid overrun if nr == 0 */
	n = min(*nr, ldata->read_head - ldata->read_tail);
	if (!n)
		return 0;

	tail = ldata->read_tail & (N_TTY_BUF_SIZE - 1);
	size = min_t(size_t, tail + n, N_TTY_BUF_SIZE);
	eol = find_next_bit_tlx(ldata->read_flags, size, tail);
	more = n - (size - tail);
	if (eol == N_TTY_BUF_SIZE && more) {
		/* scan wrapped without finding set bit */
		eol = find_next_bit_tlx(ldata->read_flags, more, 0);
		if (eol != more)
			found = 1;
	} else if (eol != size)
		found = 1;

	size = N_TTY_BUF_SIZE - tail;
	n = eol - tail;
	if (n > 4096)
		n += 4096;
	n += found;
	c = n;

	if (found && !ldata->push && ldata->read_buf[eol & (N_TTY_BUF_SIZE - 1)] == __DISABLED_CHAR) {
		n--;
		eof_push = !n && ldata->read_tail != ldata->line_start;
	}
	if (n > size) {
		ret = copy_to_user(*b, &ldata->read_buf[tail & (N_TTY_BUF_SIZE - 1)], size);
		if (ret)
			return -EFAULT;
		ret = copy_to_user(*b + size, ldata->read_buf, n - size);
	} else
		ret = copy_to_user(*b, &ldata->read_buf[tail & (N_TTY_BUF_SIZE - 1)], n);
	*b += n;
	*nr -= n;
	if (found)
		__clear_bit_tlx(eol, ldata->read_flags);
//	smp_mb__after_atomic();
	ldata->read_tail += c;
	if (found) {
		if (!ldata->push)
			ldata->line_start = ldata->read_tail;
		else
			ldata->push = 0;
	}
	return eof_push ? -EAGAIN : 0;
}




int do_output_char_tlx(unsigned char c, struct tty_struct *tty, int space)
{
	struct n_tty_data *ldata = tty->disc_data;
	int	spaces;

	if (!space)
		return -1;

	switch (c) {
	case '\n':
		if (O_ONLRET(tty))
			ldata->column = 0;
		if (O_ONLCR(tty)) {
			if (space < 2)
				return -1;
			ldata->canon_column = ldata->column = 0;
			tty->ops->write(tty, "\r\n", 2);
			return 2;
		}
		ldata->canon_column = ldata->column;
		break;
	case '\r':
		if (O_ONOCR(tty) && ldata->column == 0)
			return 0;
		if (O_OCRNL(tty)) {
			c = '\n';
			if (O_ONLRET(tty))
				ldata->canon_column = ldata->column = 0;
			break;
		}
		ldata->canon_column = ldata->column = 0;
		break;
	case '\t':
		spaces = 8 - (ldata->column & 7);
		if (O_TABDLY(tty) == XTABS) {
			if (space < spaces)
				return -1;
			ldata->column += spaces;
			tty->ops->write(tty, "        ", spaces);
			return spaces;
		}
		ldata->column += spaces;
		break;
	case '\b':
		if (ldata->column > 0)
			ldata->column--;
		break;
	default:
		if (!((__ismask(c)&(_C)) != 0)) {
			if (!(I_IUTF8(tty) && ((c & 0xc0) == 0x80)))
				ldata->column++;
		}
		break;
	}

	tty_put_char_tlx(tty, c);
	return 1;
}



size_t __process_echoes_tlx(struct tty_struct *tty)
{
	struct n_tty_data *ldata = tty->disc_data;
	int	space, old_space;
	size_t tail;
	unsigned char c;


	old_space = space = 2048;
//	tty_write_room(tty);
	if (tty->ops->write_room)
 		old_space = space = tty->ops->write_room(tty);
	tail = ldata->echo_tail;
	while (ldata->echo_commit != tail) {
		c = ldata->echo_buf[tail & (N_TTY_BUF_SIZE - 1)];
		if (c == ECHO_OP_START) {
			unsigned char op;
			int no_space_left = 0;
			op = ldata->echo_buf[tail + 1 & (N_TTY_BUF_SIZE - 1)];;
			switch (op) {
				unsigned int num_chars, num_bs;
			case ECHO_OP_ERASE_TAB:
				num_chars = ldata->echo_buf[tail + 2 & (N_TTY_BUF_SIZE - 1)];;
				if (!(num_chars & 0x80))
					num_chars += ldata->canon_column;
				num_bs = 8 - (num_chars & 7);
				if (num_bs > space) {
					no_space_left = 1;
					break;
				}
				space -= num_bs;
				while (num_bs--) {
					tty_put_char_tlx(tty, '\b');
					if (ldata->column > 0)
						ldata->column--;
				}
				tail += 3;
				break;
			case ECHO_OP_SET_CANON_COL:
				ldata->canon_column = ldata->column;
				tail += 2;
				break;
			case ECHO_OP_MOVE_BACK_COL:
				if (ldata->column > 0)
					ldata->column--;
				tail += 2;
				break;
			case ECHO_OP_START:
				/* This is an escaped echo op start code */
				if (!space) {
					no_space_left = 1;
					break;
				}
				tty_put_char_tlx(tty, ECHO_OP_START);
				ldata->column++;
				space--;
				tail += 2;
				break;
			default:
				if (space < 2) {
					no_space_left = 1;
					break;
				}
				tty_put_char_tlx(tty, '^');
				tty_put_char_tlx(tty, op ^ 0100);
				ldata->column += 2;
				space -= 2;
				tail += 2;
			}
			if (no_space_left)
				break;
		} else {
			if (O_OPOST(tty)) {
				int retval = do_output_char_tlx(c, tty, space);
				if (retval < 0)
					break;
				space -= retval;
			} else {
				if (!space)
					break;
				tty_put_char_tlx(tty, c);
				space -= 1;
			}
			tail += 1;
		}
	}
	ldata->echo_tail = tail;
	return old_space - space;
}





ssize_t n_tty_write_tlx(struct tty_struct *tty, struct file *file,
				const unsigned char *buf, size_t nr)
{
	const unsigned char *b = buf;
	DECLARE_WAITQUEUE(wait, current);
	int c;
	ssize_t retval = 0;
	down_read_tlx(&tty->termios_rwsem);
//	process_echoes(tty);
			struct n_tty_data *ldata = tty->disc_data;
			ldata->echo_commit = ldata->echo_head;
			__process_echoes_tlx(tty);

	add_wait_queue_tlx(&tty->write_wait, &wait);
	while (1) {
			struct n_tty_data *ldata = tty->disc_data;
			while (nr > 0) {
				mutex_lock_tlx(&ldata->output_lock);
				c = tty->ops->write(tty, b, nr);
				mutex_unlock_tlx(&ldata->output_lock);
				if (c < 0) {
					retval = c;
					goto break_out;
				}
				if (!c)
					break;
				b += c;
				nr -= c;
			}
//		}
		if (!nr)
			break;
		if (file->f_flags & O_NONBLOCK) {
			retval = -EAGAIN;
			break;
		}
		up_read_tlx(&tty->termios_rwsem);
	//	schedule();
		__schedule_tlx();
		down_read_tlx(&tty->termios_rwsem);
	}
break_out:
	__set_current_state(TASK_RUNNING);
	remove_wait_queue_tlx(&tty->write_wait, &wait);
	if (b - buf != nr && tty->fasync)
		set_bit_tlx(TTY_DO_WRITE_WAKEUP, &tty->flags);
	up_read_tlx(&tty->termios_rwsem);
	return (b - buf) ? b - buf : retval;
}

int
n_tty_receive_buf_common_tlx(struct tty_struct *tty, const unsigned char *cp,
			char *fp, int count, int flow)
{
	struct n_tty_data *ldata = tty->disc_data;
	int room, n, rcvd = 0;

	down_read_tlx(&tty->termios_rwsem);

	while (1) {
//		room = receive_room(tty);
		struct n_tty_data *ldata = tty->disc_data;
		int left;

		if (I_PARMRK(tty)) {
			left = N_TTY_BUF_SIZE - (ldata->read_head - ldata->read_tail) * 3 - 1;
		} else
			left = N_TTY_BUF_SIZE - (ldata->read_head - ldata->read_tail)- 1;
		if (left <= 0)
			left = ldata->icanon && ldata->canon_head == ldata->read_tail;

		room = left;

		n = min(count, room);
		if (!n) {
			if (flow && !room)
				ldata->no_room = 1;
			break;
		}
		__receive_buf_tlx(tty, cp, fp, n);
		cp += n;
		if (fp)
			fp += n;
		count -= n;
		rcvd += n;
	}

	tty->receive_room = room;
//	n_tty_check_throttle(tty);
	up_read_tlx(&tty->termios_rwsem);

	return rcvd;
}

int n_tty_receive_buf2_tlx(struct tty_struct *tty, const unsigned char *cp,
						char *fp, int count)
{
	return n_tty_receive_buf_common_tlx(tty, cp, fp, count, 1);
}


int input_available_p_tlx(struct tty_struct *tty, int poll)
{
	struct n_tty_data *ldata = tty->disc_data;
	int amt = poll && !TIME_CHAR(tty) && MIN_CHAR(tty) ? MIN_CHAR(tty) : 1;

	if (ldata->icanon && !L_EXTPROC(tty))
		return ldata->canon_head != ldata->read_tail;
	else
		return ldata->read_head - ldata->read_tail >= amt;
}

ssize_t n_tty_read_tlx(struct tty_struct *tty, struct file *file,
			unsigned char __user *buf, size_t nr)
{
	struct n_tty_data *ldata = tty->disc_data;
	unsigned char __user *b = buf;
	DECLARE_WAITQUEUE(wait, current);
	int c;
	int minimum, time;
	ssize_t retval = 0;
	long timeout;
	unsigned long flags;
	int packet;

	c = 0;
	if (c < 0)
		return c;

	down_read_tlx(&tty->termios_rwsem);
	minimum = time = 0;
	timeout = MAX_SCHEDULE_TIMEOUT;
	if (!ldata->icanon) {
		minimum = MIN_CHAR(tty);
		if (minimum) {
			time = (HZ / 10) * TIME_CHAR(tty);
			if (time)
				ldata->minimum_to_wake = 1;
			else if (!waitqueue_active_tlx(&tty->read_wait) ||
				(ldata->minimum_to_wake > minimum))
				ldata->minimum_to_wake = minimum;
		} else {
			timeout = (HZ / 10) * TIME_CHAR(tty);
			ldata->minimum_to_wake = minimum = 1;
		}
	}

	packet = tty->packet;

	add_wait_queue_tlx(&tty->read_wait, &wait);
	while (nr) {
		/* First test for status change. */
		if (packet && tty->link->ctrl_status) {
			unsigned char cs;
			if (b != buf)
				break;
			spin_lock_irqsave(&tty->link->ctrl_lock, flags);
			cs = tty->link->ctrl_status;
			tty->link->ctrl_status = 0;
			spin_unlock_irqrestore_tlx(&tty->link->ctrl_lock, flags);
			if ( put_user(cs, b++)) {
				retval = -EFAULT;
				b--;
				break;
			}
			nr--;
			break;
		}
		/* This statement must be first before checking for input
			so that any interrupt will set the state back to
			TASK_RUNNING. */
		set_current_state(TASK_INTERRUPTIBLE);

		if (((minimum - (b - buf)) < ldata->minimum_to_wake) &&
				((minimum - (b - buf)) >= 1))
			ldata->minimum_to_wake = (minimum - (b - buf));

		if (!input_available_p_tlx(tty, 0)) {
			if (test_bit_tlx(TTY_OTHER_CLOSED, &tty->flags)) {
				up_read_tlx(&tty->termios_rwsem);
//				tty_flush_to_ldisc(tty);
				down_read_tlx(&tty->termios_rwsem);
				if (!input_available_p_tlx(tty, 0)) {
					retval = -EIO;
					break;
				}
			} else {
				if (!timeout)
					break;
				if (file->f_flags & O_NONBLOCK) {
					retval = -EAGAIN;
					break;
				}
				if (signal_pending_tlx(current)) {
					retval = -ERESTARTSYS;
					break;
				}
//				n_tty_set_room(tty);
				up_read_tlx(&tty->termios_rwsem);

				timeout = schedule_timeout_tlx(timeout);

				down_read_tlx(&tty->termios_rwsem);
				continue;
			}
		}
		__set_current_state(TASK_RUNNING);

		/* Deal with packet mode. */
		if (packet && b == buf) {
			if (put_user(TIOCPKT_DATA, b++)) {
				retval = -EFAULT;
				b--;
				break;
			}
			nr--;
		}

		if (ldata->icanon && !L_EXTPROC(tty)) {
			retval = canon_copy_from_read_buf_tlx(tty, &b, &nr);
			if (retval == -EAGAIN) {
				retval = 0;
				continue;
			} else if (retval)
				break;
		}


		if (b - buf >= minimum)
			break;
		if (time)
			timeout = time;
	}
//	n_tty_set_room(tty);
	up_read_tlx(&tty->termios_rwsem);

	remove_wait_queue_tlx(&tty->read_wait, &wait);
	if (!waitqueue_active_tlx(&tty->read_wait))
		ldata->minimum_to_wake = minimum;

	mutex_unlock_tlx(&ldata->atomic_read_lock);

	__set_current_state(TASK_RUNNING);
	if (b - buf)
		retval = b - buf;

	return retval;
}

#define TTY_LDISC_MAGIC 0x5403

struct tty_ldisc_ops tty_ldisc_N_TTY_tlx = {
         .magic           = TTY_LDISC_MAGIC,
         .name            = "n_tty",
         .open            = n_tty_open_tlx,
         .close           = NULL,
         .flush_buffer    = NULL,
         .chars_in_buffer = NULL,
         .read            = n_tty_read_tlx,
         .write           = n_tty_write_tlx,
         .ioctl           = NULL,
         .set_termios     = NULL,
         .poll            = NULL,
         .receive_buf     = NULL,
         .write_wakeup    = NULL,
         .fasync          = NULL,
         .receive_buf2    = n_tty_receive_buf2_tlx,
};

#define N_TTY           0

typedef void (percpu_ref_func_t)(struct percpu_ref *);

struct percpu_ref {
         atomic_t                count;
         unsigned __percpu       *pcpu_count;
         percpu_ref_func_t       *release;
         percpu_ref_func_t       *confirm_kill;
         struct rcu_head         rcu;
 };

struct cgroup_subsys_state {
  /* PI: the cgroup that this css is attached to */
  struct cgroup *cgroup;

  /* PI: the cgroup subsystem that this css is attached to */
  struct cgroup_subsys *ss;

  /* reference count - access via css_[try]get() and css_put() */
  struct percpu_ref refcnt;

  /* PI: the parent css */
  struct cgroup_subsys_state *parent;

  /* siblings list anchored at the parent's ->children */
  struct list_head sibling;
  struct list_head children;

  /*
   * PI: Subsys-unique ID.  0 is unused and root is always 1.  The
   * matching css can be looked up using css_from_id().
   */
  int id;

  unsigned int flags;

  /*
   * Monotonically increasing unique serial number which defines a
   * uniform order among all csses.  It's guaranteed that all
   * ->children lists are in the ascending order of ->serial_nr and
   * used to allow interrupting and resuming iterations.
   */
  u64 serial_nr;

  /* percpu_ref killing and RCU release */
  struct rcu_head rcu_head;
  struct work_struct destroy_work;
};


static inline void delayacct_tsk_free(struct task_struct *tsk)
{
}

static inline bool mpol_equal(struct mempolicy *a, struct mempolicy *b)
{
         if (a == b)
                 return true;
         return false;
}

#define vma_policy(vma) NULL

struct shared_policy {};

struct anon_vma_chain {
         struct vm_area_struct *vma;
         struct anon_vma *anon_vma;
         struct list_head same_vma;   /* locked by mmap_sem & page_table_lock */
         struct rb_node rb;                      /* locked by anon_vma->rwsem */
         unsigned long rb_subtree_last;
};

struct anon_vma {
          struct anon_vma *root;          /* Root of this anon_vma tree */
          struct rw_semaphore rwsem;      /* W: modification, R: walking the list */
          atomic_t refcount;
          struct rb_root rb_root; /* Interval tree of private "related" vmas */
};






#define nr_free_pages() global_page_state_tlx(NR_FREE_PAGES)

unsigned long vm_total_pages_tlx;

struct reclaim_state {
         unsigned long reclaimed_slab;
};

struct task_struct *kthreadd_task_tlx;

struct fd {
         struct file *file;
         unsigned int flags;
};


static inline struct fd __to_fd(unsigned long v)
 {
          return (struct fd){(struct file *)(v & ~3),v & 3};
 }



#define FDPUT_FPUT       1
struct delayed_work delayed_fput_work_tlx;// = __DELAYED_WORK_INITIALIZER(n, delayed_fput, 0);
struct percpu_counter nr_files_tlx __cacheline_aligned_in_smp;

enum stat_item {
  ALLOC_FASTPATH,		/* Allocation from cpu slab */
  ALLOC_SLOWPATH,		/* Allocation by getting a new cpu slab */
  FREE_FASTPATH,		/* Free to cpu slab */
  FREE_SLOWPATH,		/* Freeing not to cpu slab */
  FREE_FROZEN,		/* Freeing to frozen slab */
  FREE_ADD_PARTIAL,	/* Freeing moves slab to partial list */
  FREE_REMOVE_PARTIAL,	/* Freeing removes last object */
  ALLOC_FROM_PARTIAL,	/* Cpu slab acquired from node partial list */
  ALLOC_SLAB,		/* Cpu slab acquired from page allocator */
  ALLOC_REFILL,		/* Refill cpu slab from slab freelist */
  ALLOC_NODE_MISMATCH,	/* Switching cpu slab */
  FREE_SLAB,		/* Slab freed to the page allocator */
  CPUSLAB_FLUSH,		/* Abandoning of the cpu slab */
  DEACTIVATE_FULL,	/* Cpu slab was full when deactivated */
  DEACTIVATE_EMPTY,	/* Cpu slab was empty when deactivated */
  DEACTIVATE_TO_HEAD,	/* Cpu slab was moved to the head of partials */
  DEACTIVATE_TO_TAIL,	/* Cpu slab was moved to the tail of partials */
  DEACTIVATE_REMOTE_FREES,/* Slab contained remotely freed objects */
  DEACTIVATE_BYPASS,	/* Implicit deactivation */
  ORDER_FALLBACK,		/* Number of times fallback was necessary */
  CMPXCHG_DOUBLE_CPU_FAIL,/* Failure of this_cpu_cmpxchg_double */
  CMPXCHG_DOUBLE_FAIL,	/* Number of times that cmpxchg double did not match */
  CPU_PARTIAL_ALLOC,	/* Used cpu partial on alloc */
  CPU_PARTIAL_FREE,	/* Refill cpu partial on free */
  CPU_PARTIAL_NODE,	/* Refill cpu partial from node partial */
  CPU_PARTIAL_DRAIN,	/* Drain cpu partial to node partial */
  NR_SLUB_STAT_ITEMS };

struct kmem_cache_cpu {
  void **freelist;	/* Pointer to next available object */
  unsigned long tid;	/* Globally unique transaction id */
  struct page *page;	/* The slab from which we are allocating */
  struct page *partial;	/* Partially allocated frozen slabs */
#ifdef CONFIG_SLUB_STATS
  unsigned stat[NR_SLUB_STAT_ITEMS];
#endif
};


struct kmem_cache_order_objects {
  unsigned long x;
};

struct kmem_cache {
  struct kmem_cache_cpu __percpu *cpu_slab;
  /* Used for retriving partial slabs etc */
  unsigned long flags;
  unsigned long min_partial;
  int size;
  int object_size;	/* The size of an object without meta data */
  int offset;		/* Free pointer offset. */
  int cpu_partial;	/* Number of per cpu partial objects to keep around */
  struct kmem_cache_order_objects oo;

  /* Allocation and freeing of slabs */
  struct kmem_cache_order_objects max;
  struct kmem_cache_order_objects min;
  gfp_t allocflags;	/* gfp flags to use on each alloc */
  int refcount;		/* Refcount for slab cache destroy */
  void (*ctor)(void *);
  int inuse;		/* Offset to metadata */
  int align;		/* Alignment */
  int reserved;		/* Reserved bytes at the end of slabs */
  const char *name;	/* Name (only for display!) */
  struct list_head list;	/* List of slab caches */
#ifdef CONFIG_SYSFS
  struct kobject kobj;	/* For sysfs */
#endif
#ifdef CONFIG_MEMCG_KMEM
  struct memcg_cache_params *memcg_params;
  int max_attr_size; /* for propagation, maximum size of a stored attr */
#ifdef CONFIG_SYSFS
  struct kset *memcg_kset;
#endif
#endif

#ifdef CONFIG_NUMA
  /*
   * Defragmentation by allocating from a remote node.
   */
  int remote_node_defrag_ratio;
#endif
  struct kmem_cache_node_tlx *node[MAX_NUMNODES];
};

struct kmem_cache_node_tlx {
  spinlock_t list_lock;

#ifdef CONFIG_SLAB
  struct list_head slabs_partial;	/* partial list first, better asm code */
  struct list_head slabs_full;
  struct list_head slabs_free;
  unsigned long free_objects;
  unsigned int free_limit;
  unsigned int colour_next;	/* Per-node cache coloring */
  struct array_cache *shared;	/* shared per node */
  struct array_cache **alien;	/* on other nodes */
  unsigned long next_reap;	/* updated without locking */
  int free_touched;		/* updated without locking */
#endif

#ifdef CONFIG_SLUB
  unsigned long nr_partial;
  struct list_head partial;
#ifdef CONFIG_SLUB_DEBUG
  atomic_long_t nr_slabs;
  atomic_long_t total_objects;
  struct list_head full;
#endif
#endif

};

#define need_reserve_slab_rcu						\
  (sizeof(((struct page *)NULL)->lru) < sizeof(struct rcu_head))

static void rcu_free_slab(struct rcu_head *h) {};

static inline void remove_full(struct kmem_cache *s, struct kmem_cache_node_tlx *n,
          struct page *page) {}

static inline void
__add_partial(struct kmem_cache_node_tlx *n, struct page *page, int tail)
{
  n->nr_partial++;
  if (tail == DEACTIVATE_TO_TAIL)
    list_add_tail(&page->lru, &n->partial);
  else
    list_add(&page->lru, &n->partial);
}

static inline void add_partial(struct kmem_cache_node_tlx *n,
        struct page *page, int tail)
{
  lockdep_assert_held(&n->list_lock);
  __add_partial(n, page, tail);
}

static void put_cpu_partial(struct kmem_cache *s, struct page *page, int drain)
{
}

static inline void
__remove_partial(struct kmem_cache_node_tlx *n, struct page *page)
{
  list_del(&page->lru);
  n->nr_partial--;
}

void remove_partial_tlx(struct kmem_cache_node_tlx *n,
          struct page *page)
{
  lockdep_assert_held(&n->list_lock);
  __remove_partial(n, page);
}

bool kmem_cache_has_cpu_partial_tlx(struct kmem_cache *s)
{

         return true;
}

bool cmpxchg_double_slab_tlx(struct kmem_cache *s, struct page *page,
    void *freelist_old, unsigned long counters_old,
    void *freelist_new, unsigned long counters_new,
    const char *n)
{
    unsigned long flags;

    local_irq_save(flags);
    if (page->freelist == freelist_old &&
          page->counters == counters_old) {
      page->freelist = freelist_new;
      struct page tmp;
      tmp.counters = counters_new;
      page->frozen  = tmp.frozen;
      page->inuse   = tmp.inuse;
      page->objects = tmp.objects;
      local_irq_restore(flags);
      return 1;
    }
    local_irq_restore(flags);
  barrier();
  return 0;
}

#define SLAB_DESTROY_BY_RCU     0x00080000UL    /* Defer freeing slabs to RCU */
#define SLAB_RECLAIM_ACCOUNT    0x00020000UL            /* Objects are reclaimable */



void kmem_cache_free_tlx(struct kmem_cache *s, void *x)
{
//  s = cache_from_obj(s, x);
//	slab_free(s, virt_to_head_page_tlx(x), x, _RET_IP_);
//	struct kmem_cache *s,
        struct page *page = virt_to_head_page_tlx(x);
//				void *x,
        unsigned long addr = _RET_IP_;
//	{
    void **object = (void *)x;
    struct kmem_cache_cpu *c;
    unsigned long tid;
  redo:
  preempt_disable();
  c = this_cpu_ptr(s->cpu_slab);
  tid = c->tid;
  preempt_enable();
  if (likely(page == c->page)) {
//      set_freepointer(s, object, c->freelist);
//      struct kmem_cache *s, void *object, void *fp)
//282 {
  *(void **)(object + s->offset) = c->freelist;
    if (unlikely(!this_cpu_cmpxchg_double(
        s->cpu_slab->freelist, s->cpu_slab->tid,
        c->freelist, tid,
        object, tid +1))) {
//        note_cmpxchg_failure("slab_free", s, tid);
      goto redo;
    }
  } else {
      void *prior;
      void **object = (void *)x;
      int was_frozen;
      struct page new;
      unsigned long counters;
      struct kmem_cache_node_tlx *n = NULL;
      unsigned long uninitialized_var(flags);

      do {
        if (unlikely(n)) {
          spin_unlock_irqrestore_tlx(&n->list_lock, flags);
          n = NULL;
        }
        prior = page->freelist;
        counters = page->counters;
//          set_freepointer(s, object, prior);
        *(void **)(object + s->offset) = prior;
        new.counters = counters;
        was_frozen = new.frozen;
        new.inuse--;
        if ((!new.inuse || !prior) && !was_frozen) {
          if (kmem_cache_has_cpu_partial_tlx(s) && !prior) {
            new.frozen = 1;
          } else { /* Needs to be taken off a list */
                              n =  s->node[page_to_nid_tlx(page)];
            spin_lock_irqsave(&n->list_lock, flags);
          }
        }

      } while (!cmpxchg_double_slab_tlx(s, page,
        prior, counters,
        object, new.counters,
        "__slab_free"));

      if (likely(!n)) {
        if (new.frozen && !was_frozen) {
          put_cpu_partial(s, page, 1);
        }
      return;
      }

      if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
        goto slab_empty;
      if (!kmem_cache_has_cpu_partial_tlx(s) && unlikely(!prior)) {
        add_partial(n, page, DEACTIVATE_TO_TAIL);
      }
      spin_unlock_irqrestore_tlx(&n->list_lock, flags);
      return;
    slab_empty:
      if (prior) {
        remove_partial_tlx(n, page);
      } else {
        remove_full(s, n, page);
      }
      spin_unlock_irqrestore_tlx(&n->list_lock, flags);
        n =  s->node[page_to_nid_tlx(page)]; //(s, page_to_nid_tlx(page));
        atomic_long_dec_tlx(&n->nr_slabs);
        atomic_long_sub_tlx(page->objects, &n->total_objects);
        struct kmem_cache *s = page->objects;
          if (unlikely(s->flags & SLAB_DESTROY_BY_RCU)) {
            struct rcu_head *head;
            if (need_reserve_slab_rcu) {
              int order = compound_order_tlx(page);
              int offset = (PAGE_SIZE << order) - s->reserved;
              head = page_address(page) + offset;
            } else {
              head = (void *)&page->lru;
            }
            call_rcu(head, rcu_free_slab);
          } else {
              int order = compound_order_tlx(page);
              int pages = 1 << order;
              kmemcheck_free_shadow(page, compound_order_tlx(page));
              mod_zone_page_state_tlx(page_zone_tlx(page),
                (s->flags & SLAB_RECLAIM_ACCOUNT) ?
                NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
                -pages);
              __ClearPageSlabPfmemalloc_tlx(page);
              __ClearPageSlab(page);
              page_mapcount_reset_tlx(page);
              if (current->reclaim_state)
                current->reclaim_state->reclaimed_slab += pages;
              __free_pages_tlx(page, order);
//                memcg_uncharge_slab(s, order);
          }
  }

}




static void file_free_rcu(struct rcu_head *head)
{
  struct file *f = container_of(head, struct file, f_u.fu_rcuhead);

  put_cred_tlx(f->f_cred);
  kmem_cache_free_tlx(filp_cachep_tlx, f);
}

struct mnt_pcp {
  int mnt_count;
  int mnt_writers;
};

struct vfsmount {
         struct dentry *mnt_root;        /* root of the mounted tree */
         struct super_block *mnt_sb;     /* pointer to superblock */
         int mnt_flags;
 };

struct mount {
  struct hlist_node mnt_hash;
  struct mount *mnt_parent;
  struct dentry *mnt_mountpoint;
  struct vfsmount mnt;
  struct rcu_head mnt_rcu;
  struct mnt_pcp __percpu *mnt_pcp;
  struct list_head mnt_mounts;	/* list of children, anchored here */
  struct list_head mnt_child;	/* and going through their mnt_child */
  struct list_head mnt_instance;	/* mount instance on sb->s_mounts */
  const char *mnt_devname;	/* Name of device e.g. /dev/dsk/hda1 */
  struct list_head mnt_list;
  struct list_head mnt_expire;	/* link in fs-specific expiry list */
  struct list_head mnt_share;	/* circular list of shared mounts */
  struct list_head mnt_slave_list;/* list of slave mounts */
  struct list_head mnt_slave;	/* slave list entry */
  struct mount *mnt_master;	/* slave is on master->mnt_slave_list */
  struct mnt_namespace *mnt_ns;	/* containing namespace */
  struct mountpoint *mnt_mp;	/* where is it mounted */
  int mnt_id;			/* mount identifier */
  int mnt_group_id;		/* peer group identifier */
  int mnt_expiry_mark;		/* true if marked for expiry */
  int mnt_pinned;
  struct path mnt_ex_mountpoint;
};

static inline struct mount *real_mount(struct vfsmount *mnt)
{
        return container_of(mnt, struct mount, mnt);
}

struct dentry *dget_tlx(struct dentry *dentry)
{
				if (dentry)
									dentry->d_lockref.count++;
//                 lockref_get(&dentry->d_lockref);
				return dentry;
}




int lockref_put_or_lock_tlx(struct lockref *lockref)
{
//	  __raw_spin_lock(&lockref->lock);
	raw_spinlock_t *lock_ = (raw_spinlock_t *) &lockref->lock;
//	do_raw_spin_lock(&lockref->lock);
//	arch_spin_trylock(&lock_->raw_lock);

	arch_spinlock_t *lock = &lock_->raw_lock;
//	{
		unsigned int tmp;
		arch_spinlock_t lockval;

	asm volatile(
	"	prfm	pstl1strm, %2\n"
	"1:	ldaxr	%w0, %2\n"
	"	eor	%w1, %w0, %w0, ror #16\n"
	"	cbnz	%w1, 2f\n"
	"	add	%w0, %w0, %3\n"
	"	stxr	%w1, %w0, %2\n"
	"	cbnz	%w1, 1b\n"
	"2:"
		: "=&r" (lockval), "=&r" (tmp), "+Q" (*lock)
		: "I" (1 << TICKET_SHIFT)
		: "memory");

	if (lockref->count <= 1)
		return 0;
	lockref->count--;
//	spin_unlock(&lockref->lock);
	lock = &(&(&lockref->lock)->rlock)->raw_lock;
	asm volatile(
	"       stlrh   %w1, %0\n"
		: "=Q" (lock->owner)
		: "r" (lock->owner + 1)
		: "memory");
	return 1;
}

void dput_tlx(struct dentry *dentry)
{
		if (unlikely(!dentry))
			return;

		if (lockref_put_or_lock_tlx(&dentry->d_lockref))
			return;

//		if (dput_(dentry))
//						return;
		arch_spinlock_t *lock = &(&(&dentry->d_lock)->rlock)->raw_lock;
	asm volatile(
  "       stlrh   %w1, %0\n"
			: "=Q" (lock->owner)
			: "r" (lock->owner + 1)
			: "memory");

		dentry->d_lockref.count--;


}

#define MNT_DOOMED              0x1000000

void mntput_tlx(struct vfsmount *mnt)
{
	if (mnt) {
		struct mount *m = real_mount(mnt);
		/* avoid cacheline pingpong, hope gcc doesn't get "smart" */
		if (unlikely(m->mnt_expiry_mark))
			m->mnt_expiry_mark = 0;
//		mntput_no_expire(m);
		struct mount *mnt_ = m;
			this_cpu_add(mnt_->mnt_pcp->mnt_count, -1);
			if (likely(mnt_->mnt_ns)) { /* shouldn't be the last one */
				return;
			}
			if (per_cpu_ptr(mnt_->mnt_pcp, 0)->mnt_count) {
				return;
			}
			mnt_->mnt.mnt_flags |= MNT_DOOMED;
			list_del(&mnt_->mnt_instance);
			dput_tlx(mnt_->mnt.mnt_root);
	}
}

void path_get_tlx(const struct path * path) {
//	mntget(path->mnt);
	struct mount *mnt  = real_mount(path->mnt);
	if (path->mnt)
				this_cpu_add(mnt->mnt_pcp->mnt_count, 1);
  //               mnt_add_count(real_mount(mnt), 1);
	dget_tlx(path->dentry);
};


void kfree_tlx(const void *x__);


void free_vfsmnt_tlx(struct mount *mnt)
{
	kfree_tlx(mnt->mnt_devname);
#ifdef CONFIG_SMP
	free_percpu_tlx(mnt->mnt_pcp);
#endif
	kmem_cache_free_tlx(mnt_cache_tlx, mnt);
}

static void delayed_free_vfsmnt_tlx(struct rcu_head *head)
{
	free_vfsmnt_tlx(container_of(head, struct mount, mnt_rcu));
}


seqlock_t_tlx mount_lock_tlx;

unsigned int mnt_get_count_tlx(struct mount *mnt)
{
#ifdef CONFIG_SMP
	unsigned int count = 0;
	int cpu;

	for_each_possible_cpu(cpu) {
		count += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_count;
	}

	return count;
#else
	return mnt->mnt_count;
#endif
}


void mntput_no_expire_tlx(struct mount *mnt)
{
put_again:
//	rcu_read_lock();
//	mnt_add_count(mnt, -1);
		this_cpu_add(mnt->mnt_pcp->mnt_count, -1);
//	this_cpu_add(mnt->mnt_pcp->mnt_count, -1);
	if (likely(mnt->mnt_ns)) { /* shouldn't be the last one */
//		rcu_read_unlock();
		return;
	}
//	lock_mount_hash();
	write_seqlock_tlx(&mount_lock_tlx);

	if (mnt_get_count_tlx(mnt)) {
//		rcu_read_unlock();
//		unlock_mount_hash();
		write_sequnlock_tlx(&mount_lock_tlx);
		return;
	}
	mnt->mnt.mnt_flags |= MNT_DOOMED;
//	rcu_read_unlock();
	list_del(&mnt->mnt_instance);
//	unlock_mount_hash();
	 write_sequnlock_tlx(&mount_lock_tlx);
//	fsnotify_vfsmount_delete(&mnt->mnt);
	dput_tlx(mnt->mnt.mnt_root);
	call_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt_tlx);
}


void path_put_tlx(const struct path * path) {
				dput_tlx(path->dentry);
//        mntput_tlx(path->mnt);
								if (path->mnt) {
								struct mount *m = real_mount(path->mnt);
								/* avoid cacheline pingpong, hope gcc doesn't get "smart" */
								if (unlikely(m->mnt_expiry_mark))
												m->mnt_expiry_mark = 0;
								mntput_no_expire_tlx(m);
				}
};


static void __fput(struct file *file)
{
  struct dentry *dentry = file->f_path.dentry;
  struct vfsmount *mnt = file->f_path.mnt;
  struct inode *inode = file->f_inode;
  might_sleep();
//  eventpoll_release(file);
//  locks_remove_file(file);
  if (unlikely(file->f_flags & FASYNC)) {
    if (file->f_op->fasync)
      file->f_op->fasync(-1, file, 0);
  }
//  ima_file_free(file);
  if (file->f_op->release)
    file->f_op->release(inode, file);
  if (unlikely(S_ISCHR(inode->i_mode) && inode->i_cdev != NULL &&
         !(file->f_mode & FMODE_PATH))) {
//    cdev_put(inode->i_cdev);
      if (inode->i_cdev) {
                 struct module *owner = inode->i_cdev->owner;
                 kobject_put_tlx(&(inode->i_cdev)->kobj);
                 module_put_tlx(owner);
      }
  }
  fops_put(file->f_op);
  put_pid_tlx(file->f_owner.pid);
  if ((file->f_mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ)
    i_readcount_dec_tlx(inode);
  if (file->f_mode & FMODE_WRITER) {
    put_write_access_tlx(inode);
//    __mnt_drop_write(mnt);
    preempt_disable();
//    mnt_dec_writers(real_mount(mnt));
    struct mount *tmp = real_mount(mnt);
     this_cpu_dec(tmp->mnt_pcp->mnt_writers);
    preempt_enable();
  }
  file->f_path.dentry = NULL;
  file->f_path.mnt = NULL;
  file->f_inode = NULL;
//  file_free(file);
  percpu_counter_dec_tlx(&nr_files_tlx);
  call_rcu(&file->f_u.fu_rcuhead, file_free_rcu);
  dput_tlx(dentry);
//  mntput_tlx(mnt);
}

static void ____fput(struct callback_head *work)
{
  __fput(container_of(work, struct file, f_u.fu_rcuhead));
}

void fput(struct file *file)
{
  if (atomic_long_dec_and_test_tlx(&file->f_count)) {
    struct task_struct *task = current;
    if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {
        (&file->f_u.fu_rcuhead)->func = ____fput;
      struct callback_head *work = &file->f_u.fu_rcuhead;
         struct callback_head *head;
         do {
                 head = ACCESS_ONCE(task->task_works);
                 work->next = head;
         } while (cmpxchg(&task->task_works, head, work) != head);
    }
  }
}

static inline void fdput(struct fd fd)
{
         if (fd.flags & FDPUT_FPUT)
                 fput(fd.file);
 }


#define GOLDEN_RATIO_PRIME_32 0x9e370001UL
#define KMALLOC_SHIFT_HIGH      (PAGE_SHIFT + 1)
#define SLAB_CACHE_DMA          0x00004000UL    /* Use GFP_DMA memory */

enum bdi_stat_item {
	BDI_RECLAIMABLE,
	BDI_WRITEBACK,
	BDI_DIRTIED,
	BDI_WRITTEN,
	NR_BDI_STAT_ITEMS
};


enum slab_state_tlx {
       DOWN,                   /* No slab functionality yet */
       PARTIAL,                /* SLUB: kmem_cache_node_tlx available */
       PARTIAL_ARRAYCACHE,     /* SLAB: kmalloc size for arraycache available */
       PARTIAL_NODE,           /* SLAB: kmalloc size for node struct available */
       UP,                     /* Slab caches usable but not all extras yet */
       FULL                    /* Everything is working */
};

enum slab_state_tlx slab_state_tlx;




#define SLAB_STORE_USER         0x00010000UL    /* DEBUG: Store the last owner for bug hunting */
#define SLAB_POISON             0x00000800UL    /* DEBUG: Poison objects */

#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)






static inline void mapping_set_gfp_mask(struct address_space *m, gfp_t mask)
{
         m->flags = (m->flags & ~(__force unsigned long)__GFP_BITS_MASK) |
                                 (__force unsigned long)mask;
}

typedef int (congested_fn)(void *, int);


#define PAGE_CACHE_SHIFT        PAGE_SHIFT
#define PAGE_CACHE_SIZE         PAGE_SIZE

# define SLAB_NOTRACK           0x00000000UL
#define SLAB_DESTROY_BY_RCU     0x00080000UL    /* Defer freeing slabs to RCU */
struct kmem_cache *kmalloc_caches_tlx[KMALLOC_SHIFT_HIGH + 1];
#define KMALLOC_SHIFT_HIGH      (PAGE_SHIFT + 1)
#define KMALLOC_SHIFT_LOW       3
#define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)

static __always_inline u64 hash_64(u64 val, unsigned int bits)
{
	u64 hash = val;

	/*  Sigh, gcc can't optimise this alone like it does for 32 bits. */
	u64 n = hash;
	n <<= 18;
	hash -= n;
	n <<= 33;
	hash -= n;
	n <<= 3;
	hash += n;
	n <<= 3;
	hash -= n;
	n <<= 4;
	hash += n;
	n <<= 2;
	hash += n;

	/* High bits are more random, so use them. */
	return hash >> (64 - bits);
}

static inline u32 hash_32(u32 val, unsigned int bits)
{
	/* On some cpus multiply is faster, on others gcc will do shifts */
	u32 hash = val * GOLDEN_RATIO_PRIME_32;

	/* High bits are more random, so use them. */
	return hash >> (32 - bits);
}

#define hash_long(val, bits) hash_64(val, bits)
#define HASH_SIZE(name) (ARRAY_SIZE(name))




#define HASH_BITS(name) ilog2(HASH_SIZE(name))

#define hash_min(val, bits)                                                     \
         (sizeof(val) <= 4 ? hash_32(val, bits) : hash_long(val, bits))

#define hash_for_each_possible(name, obj, member, key)                  \
         hlist_for_each_entry(obj, &name[hash_min(key, HASH_BITS(name))], member)

static inline void hash_del(struct hlist_node *node)
{
         hlist_del_init(node);
}

#define hash_add(hashtable, node, key)                                          \
          hlist_add_head(node, &hashtable[hash_min(key, HASH_BITS(hashtable))])

struct bdi_writeback {
	struct backing_dev_info *bdi;	/* our parent bdi */
	unsigned int nr;

	unsigned long last_old_flush;	/* last old data flush */

	struct delayed_work dwork;	/* work item used for writeback */
	struct list_head b_dirty;	/* dirty inodes */
	struct list_head b_io;		/* parked for writeback */
	struct list_head b_more_io;	/* parked for more writeback */
	spinlock_t list_lock;		/* protects the b_* lists */
};

struct fprop_local_percpu {
          /* the local events counter */
         struct percpu_counter events;
         /* Period in which we last updated events */
         unsigned int period;
         raw_spinlock_t lock;    /* Protect period and numerator */
};

struct backing_dev_info {
	struct list_head bdi_list;
	unsigned long ra_pages;	/* max readahead in PAGE_CACHE_SIZE units */
	unsigned long state;	/* Always use atomic bitops on this */
	unsigned int capabilities; /* Device capabilities */
	congested_fn *congested_fn; /* Function pointer if device is md/dm */
	void *congested_data;	/* Pointer to aux data for congested func */

	char *name;

	struct percpu_counter bdi_stat[NR_BDI_STAT_ITEMS];

	unsigned long bw_time_stamp;	/* last time write bw is updated */
	unsigned long dirtied_stamp;
	unsigned long written_stamp;	/* pages written at bw_time_stamp */
	unsigned long write_bandwidth;	/* the estimated write bandwidth */
	unsigned long avg_write_bandwidth; /* further smoothed write bw */

	/*
	 * The base dirty throttle rate, re-calculated on every 200ms.
	 * All the bdi tasks' dirty rate will be curbed under it.
	 * @dirty_ratelimit tracks the estimated @balanced_dirty_ratelimit
	 * in small steps and is much more smooth/stable than the latter.
	 */
	unsigned long dirty_ratelimit;
	unsigned long balanced_dirty_ratelimit;

	struct fprop_local_percpu completions;
	int dirty_exceeded;

	unsigned int min_ratio;
	unsigned int max_ratio, max_prop_frac;

	struct bdi_writeback wb;  /* default writeback info for this bdi */
	spinlock_t wb_lock;	  /* protects work_list & wb.dwork scheduling */

	struct list_head work_list;

	struct device *dev;

	struct timer_list laptop_mode_wb_timer;

#ifdef CONFIG_DEBUG_FS
	struct dentry *debug_dir;
	struct dentry *debug_stats;
#endif
};

#define BDI_CAP_NO_ACCT_DIRTY   0x00000001
#define BDI_CAP_NO_WRITEBACK    0x00000002
#define BDI_CAP_NO_ACCT_WB      0x00000080

#define BDI_CAP_NO_ACCT_AND_WRITEBACK \
         (BDI_CAP_NO_WRITEBACK | BDI_CAP_NO_ACCT_DIRTY | BDI_CAP_NO_ACCT_WB)

#define BDI_CAP_SWAP_BACKED     0x00000100

struct backing_dev_info shmem_backing_dev_info_tlx  = {
	.ra_pages	= 0,	/* No readahead */
	.capabilities	= BDI_CAP_NO_ACCT_AND_WRITEBACK | BDI_CAP_SWAP_BACKED,
};

struct backing_dev_info noop_backing_dev_info_tlx = {
				.name           = "noop",
				.capabilities   = BDI_CAP_NO_ACCT_AND_WRITEBACK,
};

struct backing_dev_info default_backing_dev_info_tlx = {};

#define SLAB_HWCACHE_ALIGN      0x00002000UL    /* Align objs on cache lines */
#define SLAB_RECLAIM_ACCOUNT    0x00020000UL            /* Objects are reclaimable */
#define SLAB_MEM_SPREAD         0x00100000UL    /* Spread some memory over cpuset */
#define SLAB_PANIC              0x00040000UL    /* Panic if kmem_cache_create() fails */

#define TTYAUX_MAJOR            5
#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)
#define PAGE_CACHE_MASK         PAGE_MASK


static inline void *kmap(struct page *page)
{
	might_sleep();
	return page_address(page);
}



#define ZERO_SIZE_PTR ((void *)16)
#define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \
                                 (unsigned long)ZERO_SIZE_PTR)
#define KMALLOC_MAX_SIZE        (1UL << KMALLOC_SHIFT_MAX)
#define KMALLOC_MAX_CACHE_SIZE  (1UL << KMALLOC_SHIFT_HIGH)
s8 size_index_tlx[24] = {
	3,	/* 8 */
	4,	/* 16 */
	5,	/* 24 */
	5,	/* 32 */
	6,	/* 40 */
	6,	/* 48 */
	6,	/* 56 */
	6,	/* 64 */
	1,	/* 72 */
	1,	/* 80 */
	1,	/* 88 */
	1,	/* 96 */
	7,	/* 104 */
	7,	/* 112 */
	7,	/* 120 */
	7,	/* 128 */
	2,	/* 136 */
	2,	/* 144 */
	2,	/* 152 */
	2,	/* 160 */
	2,	/* 168 */
	2,	/* 176 */
	2,	/* 184 */
	2	/* 192 */
};

static void *__kmalloc_tlx(size_t size, gfp_t flags)
{
  struct kmem_cache *s;
  void *ret;

  if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {
      unsigned int order = get_order(size);
        struct page *page;
        flags |= __GFP_COMP;
        page =  __alloc_pages_nodemask_tlx(flags, order, node_zonelist_tlx(numa_node_id_tlx(), flags), NULL);
        return page ? page_address(page) : NULL;
  }
    int index;
    if (size <= 192) {
      if (!size) {
        s = ZERO_SIZE_PTR;
        goto have_s;
      }
      index = size_index_tlx[ (size - 1) / 8];
    } else
      index = fls_tlx(size - 1);
    s = kmalloc_caches_tlx[index];
have_s:
  if (unlikely(ZERO_OR_NULL_PTR(s)))
    return s;
  ret = slab_alloc_tlx(s, flags, _RET_IP_);
  return ret;
}

void *kmem_cache_zalloc_tlx(struct kmem_cache *k, gfp_t flags)
{
//         return kmem_cache_alloc(k, flags | __GFP_ZERO);
       return   slab_alloc_tlx(k, flags | __GFP_ZERO, _RET_IP_);
}



struct kmem_cache *filelock_cache_tlx;


struct file_lock *locks_alloc_lock_tlx(void)
{
				struct file_lock *fl = kmem_cache_zalloc_tlx(filelock_cache_tlx, GFP_KERNEL);

				if (fl) {
						INIT_HLIST_NODE(&fl->fl_link);
         		INIT_LIST_HEAD(&fl->fl_block);
         		init_waitqueue_head(&fl->fl_wait);
				}
//								locks_init_lock_heads(fl);

				return fl;
}

void *kmalloc_tlx(size_t size, gfp_t flags)
{
         return __kmalloc_tlx(size, flags);
}

void *kzalloc_tlx(size_t size, gfp_t flags)
{
         return kmalloc_tlx(size, flags | __GFP_ZERO);
}

struct kthread_create_info
{
  /* Information passed to kthread() from kthreadd. */
  int (*threadfn)(void *data);
  void *data;
  int node;

  /* Result passed back to kthread_create() from kthreadd. */
  struct task_struct *result;
  struct completion *done;

  struct list_head list;
};

void kfree_tlx(const void *x__)
{
  struct page *page;
  void *object__ = (void *)x__;
  if (unlikely(ZERO_OR_NULL_PTR(x__)))
    return;
  page = virt_to_head_page_tlx(x__);
  if (unlikely(!PageSlab(page))) {
//		kfree_hook(x__);
    __free_pages_tlx(page, compound_order_tlx(page));
    return;
  }
  struct kmem_cache *s = page->slab_cache;
  void *x = object__;
  unsigned long addr = _RET_IP_;
    void **object = (void *)x;
    struct kmem_cache_cpu *c;
    unsigned long tid;

//    slab_free_hook(s, x);

  redo:
    preempt_disable();
    c = this_cpu_ptr(s->cpu_slab);
    tid = c->tid;
    preempt_enable();
    if (likely(page == c->page)) {
//      set_freepointer(s, object, c->freelist);
//      struct kmem_cache *s, void *object, void *fp)
//282 {
     *(void **)(object + s->offset) = c->freelist;
      if (unlikely(!this_cpu_cmpxchg_double(
          s->cpu_slab->freelist, s->cpu_slab->tid,
          c->freelist, tid,
          object, tid +1))) {
//        note_cmpxchg_failure("slab_free", s, tid);
        goto redo;
      }
    } else {
        void *prior;
        void **object = (void *)x;
        int was_frozen;
        struct page new;
        unsigned long counters;
        struct kmem_cache_node_tlx *n = NULL;
        unsigned long uninitialized_var(flags);

        do {
          if (unlikely(n)) {
            spin_unlock_irqrestore_tlx(&n->list_lock, flags);
            n = NULL;
          }
          prior = page->freelist;
          counters = page->counters;
//          set_freepointer(s, object, prior);
          *(void **)(object + s->offset) = prior;
          new.counters = counters;
          was_frozen = new.frozen;
          new.inuse--;
          if ((!new.inuse || !prior) && !was_frozen) {
            if (kmem_cache_has_cpu_partial_tlx(s) && !prior) {
              new.frozen = 1;
            } else { /* Needs to be taken off a list */
                                n =  s->node[page_to_nid_tlx(page)];
              spin_lock_irqsave(&n->list_lock, flags);
            }
          }

        } while (!cmpxchg_double_slab_tlx(s, page,
          prior, counters,
          object, new.counters,
          "__slab_free"));

        if (likely(!n)) {
          if (new.frozen && !was_frozen) {
            put_cpu_partial(s, page, 1);
          }
        return;
        }

        if (unlikely(!new.inuse && n->nr_partial >= s->min_partial))
          goto slab_empty;
        if (!kmem_cache_has_cpu_partial_tlx(s) && unlikely(!prior)) {
          add_partial(n, page, DEACTIVATE_TO_TAIL);
        }
        spin_unlock_irqrestore_tlx(&n->list_lock, flags);
        return;
      slab_empty:
        if (prior) {
          remove_partial_tlx(n, page);
        } else {
          remove_full(s, n, page);
        }
        spin_unlock_irqrestore_tlx(&n->list_lock, flags);
          n =  s->node[page_to_nid_tlx(page)]; //(s, page_to_nid_tlx(page));
          atomic_long_dec_tlx(&n->nr_slabs);
          atomic_long_sub_tlx(page->objects, &n->total_objects);
          struct kmem_cache *s = page->objects;
            if (unlikely(s->flags & SLAB_DESTROY_BY_RCU)) {
              struct rcu_head *head;
              if (need_reserve_slab_rcu) {
                int order = compound_order_tlx(page);
                int offset = (PAGE_SIZE << order) - s->reserved;
                head = page_address(page) + offset;
              } else {
                head = (void *)&page->lru;
              }
              call_rcu(head, rcu_free_slab);
            } else {
                int order = compound_order_tlx(page);
                int pages = 1 << order;
                kmemcheck_free_shadow(page, compound_order_tlx(page));
                mod_zone_page_state_tlx(page_zone_tlx(page),
                  (s->flags & SLAB_RECLAIM_ACCOUNT) ?
                  NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
                  -pages);
                __ClearPageSlabPfmemalloc_tlx(page);
                __ClearPageSlab(page);
                page_mapcount_reset_tlx(page);
                if (current->reclaim_state)
                  current->reclaim_state->reclaimed_slab += pages;
                __free_pages_tlx(page, order);
//                memcg_uncharge_slab(s, order);
            }
    }
}

enum format_type {
	FORMAT_TYPE_NONE, /* Just a string part */
	FORMAT_TYPE_WIDTH,
	FORMAT_TYPE_PRECISION,
	FORMAT_TYPE_CHAR,
	FORMAT_TYPE_STR,
	FORMAT_TYPE_PTR,
	FORMAT_TYPE_PERCENT_CHAR,
	FORMAT_TYPE_INVALID,
	FORMAT_TYPE_LONG_LONG,
	FORMAT_TYPE_ULONG,
	FORMAT_TYPE_LONG,
	FORMAT_TYPE_UBYTE,
	FORMAT_TYPE_BYTE,
	FORMAT_TYPE_USHORT,
	FORMAT_TYPE_SHORT,
	FORMAT_TYPE_UINT,
	FORMAT_TYPE_INT,
	FORMAT_TYPE_SIZE_T,
	FORMAT_TYPE_PTRDIFF
};


struct printf_spec {
         u8      type;           /* format_type enum */
         u8      flags;          /* flags to number() */
         u8      base;           /* number base, 8, 10 or 16 only */
         u8      qualifier;      /* number qualifier, one of 'hHlLtzZ' */
         s16     field_width;    /* width of output field */
         s16     precision;      /* # of digits/chars */
};

int format_decode_tlx(const char *fmt, struct printf_spec *spec)
{
	const char *start = fmt;
	for (; *fmt ; ++fmt) {
		if (*fmt == '%')
			break;
	}
	if (fmt != start || !*fmt)
		return fmt - start;



	return ++fmt - start;
}

int vsnprintf_tlx(char *buf, size_t size, const char *fmt, va_list args)
{
	unsigned long long num;
	char *str, *end;
	struct printf_spec spec = {0};
	str = buf;
	end = buf + size;

	/* Make sure end is always >= buf */
	if (end < buf) {
		end = ((void *)-1);
		size = end - buf;
	}

	while (*fmt) {
		const char *old_fmt = fmt;
		int read = format_decode_tlx(fmt, &spec);

		fmt += read;

		switch (spec.type) {
		case FORMAT_TYPE_NONE: {
			int copy = read;
			if (str < end) {
				if (copy > end - str)
					copy = end - str;
				memcpy_tlx(str, old_fmt, copy);
			}
			str += read;
			break;
		}

		default:
			switch (spec.type) {
			default:
				num = va_arg(args, unsigned int);
			}

		}
	}

	if (size > 0) {
		if (str < end)
			*str = '\0';
		else
			end[-1] = '\0';
	}

	/* the trailing null byte doesn't count towards the total */
	return str-buf;

}



int kobject_set_name_vargs_tlx(struct kobject *kobj, const char *fmt,
					va_list vargs)
{
	const char *old_name = kobj->name;
	char *s;

	if (kobj->name && !fmt)
		return 0;

//	kobj->name = kvasprintf(GFP_KERNEL, fmt, vargs);
	va_list ap = vargs;
				unsigned int len;
				char *p;
				va_list aq;

				va_copy(aq, ap);
				len = vsnprintf_tlx(NULL, 0, fmt, aq);
				va_end(aq);

				p = __kmalloc_tlx(len+1, GFP_KERNEL);
				vsnprintf_tlx(p, len+1, fmt, ap);

				kobj->name = p;

	if (!kobj->name) {
		kobj->name = old_name;
		return -ENOMEM;
	}



	kfree_tlx(old_name);
	return 0;
}

struct kobj_type {
				void (*release)(struct kobject *kobj);
				const struct sysfs_ops *sysfs_ops;
				struct attribute **default_attrs;
				const struct kobj_ns_type_operations *(*child_ns_type)(struct kobject *kobj);
				const void *(*namespace)(struct kobject *kobj);
};


struct kobj_type kset_ktype_tlx = {
				.sysfs_ops      = NULL,
				.release = NULL,
};

struct kobj_type dynamic_kobj_ktype_tlx = {};

struct kernfs_elem_dir {
				unsigned long           subdirs;
				/* children rbtree starts here and goes through kn->rb */
				struct rb_root          children;

				/*
					* The kernfs hierarchy this directory belongs to.  This fits
					* better directly in kernfs_node but is here to save space.
					*/
				struct kernfs_root      *root;
};

struct kernfs_elem_symlink {
				struct kernfs_node      *target_kn;
};

struct kernfs_elem_attr {
				const struct kernfs_ops *ops;
				struct kernfs_open_node *open;
				loff_t                  size;
				struct kernfs_node      *notify_next;   /* for kernfs_notify() */
};

struct kernfs_node {
	atomic_t		count;
	atomic_t		active;
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map	dep_map;
#endif
	/*
	* Use kernfs_get_parent() and kernfs_name/path() instead of
	* accessing the following two fields directly.  If the node is
	* never moved to a different parent, it is safe to access the
	* parent directly.
	*/
	struct kernfs_node	*parent;
	const char		*name;

	struct rb_node		rb;

	const void		*ns;	/* namespace tag */
	unsigned int		hash;	/* ns + name hash */
	union {
		struct kernfs_elem_dir		dir;
		struct kernfs_elem_symlink	symlink;
		struct kernfs_elem_attr		attr;
	};

	void			*priv;

	unsigned short		flags;
	umode_t			mode;
	unsigned int		ino;
	struct kernfs_iattrs	*iattr;
};

struct kernfs_ops {
	/*
	 * Read is handled by either seq_file or raw_read().
	 *
	 * If seq_show() is present, seq_file path is active.  Other seq
	 * operations are optional and if not implemented, the behavior is
	 * equivalent to single_open().  @sf->private points to the
	 * associated kernfs_open_file.
	 *
	 * read() is bounced through kernel buffer and a read larger than
	 * PAGE_SIZE results in partial operation of PAGE_SIZE.
	 */
	int (*seq_show)(struct seq_file *sf, void *v);

	void *(*seq_start)(struct seq_file *sf, loff_t *ppos);
	void *(*seq_next)(struct seq_file *sf, void *v, loff_t *ppos);
	void (*seq_stop)(struct seq_file *sf, void *v);

	ssize_t (*read)(struct kernfs_open_file *of, char *buf, size_t bytes,
			loff_t off);

	/*
	 * write() is bounced through kernel buffer.  If atomic_write_len
	 * is not set, a write larger than PAGE_SIZE results in partial
	 * operations of PAGE_SIZE chunks.  If atomic_write_len is set,
	 * writes upto the specified size are executed atomically but
	 * larger ones are rejected with -E2BIG.
	 */
	size_t atomic_write_len;
	ssize_t (*write)(struct kernfs_open_file *of, char *buf, size_t bytes,
			 loff_t off);

	int (*mmap)(struct kernfs_open_file *of, struct vm_area_struct *vma);

#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lock_class_key	lockdep_key;
#endif
};

struct sysfs_ops {
         ssize_t (*show)(struct kobject *, struct attribute *, char *);
         ssize_t (*store)(struct kobject *, struct attribute *, const char *, size_t);
 };


enum kernfs_node_flag {
	KERNFS_ACTIVATED	= 0x0010,
	KERNFS_NS		= 0x0020,
	KERNFS_HAS_SEQ_SHOW	= 0x0040,
	KERNFS_HAS_MMAP		= 0x0080,
	KERNFS_LOCKDEP		= 0x0100,
	KERNFS_STATIC_NAME	= 0x0200,
	KERNFS_SUICIDAL		= 0x0400,
	KERNFS_SUICIDED		= 0x0800,
};
#define KN_DEACTIVATED_BIAS		(INT_MIN + 1)
int kernfs_add_one_tlx(struct kernfs_node *kn)
{

	return 0;
}

struct kmem_cache *kernfs_node_cache_tlx; //0
struct kernfs_node_tlx *sysfs_root_kn_tlx; //0


int sysfs_create_dir_ns_tlx(struct kobject *kobj, const void *ns)
{
	struct kernfs_node *parent, *kn;
	if (kobj->parent)
		parent = kobj->parent->sd;
	else
		parent = sysfs_root_kn_tlx;
//	kn = kernfs_create_dir_ns(parent, kobject_name(kobj),
//				  S_IRWXU | S_IRUGO | S_IXUGO, kobj, ns);

						const char *name = kobj->name;
						umode_t mode = S_IRWXU | S_IRUGO | S_IXUGO;
//		struct kernfs_node *kn;
		int rc;

		/* allocate */
	//	kn = kernfs_new_node(parent, name, mode | S_IFDIR, KERNFS_DIR);
			mode = mode | S_IFDIR;
	//         kn = __kernfs_new_node(kernfs_root(parent), name, mode, KERNFS_DIR);
				struct kernfs_node *kn__ = parent;
				if (kn__->parent)
								kn__ = kn__->parent;
				struct kernfs_root *root = kn__->dir.root;
				char *dup_name = NULL;
	//			struct kernfs_node *kn;
				int ret;

				kn = kmem_cache_zalloc_tlx(kernfs_node_cache_tlx, GFP_KERNEL);
				atomic_set(&kn->count, 1);
				atomic_set(&kn->active, KN_DEACTIVATED_BIAS);
				RB_CLEAR_NODE(&kn->rb);

				kn->name = name;
				kn->mode = mode;
				kn->flags = KERNFS_DIR;


					if (kn) {
								if (parent)
											atomic_inc_tlx(&parent->count);
								kn->parent = parent;
					}

		kn->dir.root = parent->dir.root;
		kn->ns = ns;
		kn->priv = kobj;
		rc = kernfs_add_one_tlx(kn);
//		return kn;

	kobj->sd = kn;
	return 0;
}

struct kernfs_ops sysfs_file_kfops_rw_tlx = {};
struct kernfs_ops sysfs_bin_kfops_ro_tlx = {};
struct kernfs_ops sysfs_bin_kfops_wo_tlx = {};
struct kernfs_ops sysfs_bin_kfops_rw_tlx = {};
struct kernfs_ops sysfs_file_kfops_wo_tlx = {};
struct kernfs_ops sysfs_file_kfops_ro_tlx = {};
struct kernfs_ops sysfs_file_kfops_empty_tlx = {};
struct kernfs_ops sysfs_bin_kfops_mmap_tlx = {};

int sysfs_add_file_mode_tlx(struct kernfs_node *parent,
				const struct attribute *attr, bool is_bin,
				umode_t mode, const void *ns)
{
	struct lock_class_key *key = NULL;
	const struct kernfs_ops *ops;
	struct kernfs_node *kn;
	loff_t size;

	if (!is_bin) {
		struct kobject *kobj = parent->priv;
		const struct sysfs_ops *sysfs_ops = kobj->ktype->sysfs_ops;
		if (sysfs_ops->show && sysfs_ops->store)
			ops = &sysfs_file_kfops_rw_tlx;
		else if (sysfs_ops->show)
			ops = &sysfs_file_kfops_ro_tlx;
		else if (sysfs_ops->store)
			ops = &sysfs_file_kfops_wo_tlx;
		else
			ops = &sysfs_file_kfops_empty_tlx;

		size = PAGE_SIZE;
	} else {
		struct bin_attribute *battr = (void *)attr;

		if (battr->mmap)
			ops = &sysfs_bin_kfops_mmap_tlx;
		else if (battr->read && battr->write)
			ops = &sysfs_bin_kfops_rw_tlx;
		else if (battr->read)
			ops = &sysfs_bin_kfops_ro_tlx;
		else if (battr->write)
			ops = &sysfs_bin_kfops_wo_tlx;
		else
			ops = &sysfs_file_kfops_empty_tlx;

		size = battr->size;
	}
//	kn = __kernfs_create_file(parent, attr->name, mode, size, ops,
//				  (void *)attr, ns, true, key);
//	 struct kernfs_node *parent,
							const char *name = attr->name;
	//						umode_t mode, loff_t size,
	//						const struct kernfs_ops *ops,
							void *priv = (void *)attr;
//							const void *ns = true
							bool name_is_static = true;
//			struct kernfs_node *kn;
			unsigned flags;
			int rc;

			flags = KERNFS_FILE;
			if (name_is_static)
				flags |= KERNFS_STATIC_NAME;
//			kn = kernfs_new_node(parent, name, (mode & S_IALLUGO) | S_IFREG, flags);
//			struct kernfs_node *parent,
//553                                     const char *name, umode_t
		mode = (mode & S_IALLUGO) | S_IFREG;
//		 kn = __kernfs_new_node(kernfs_root(parent), name, mode, flags);
//			struct kernfs_root *root = kernfs_root(parent);
				struct kernfs_node *kn__ = parent;
				if (kn__->parent)
                 kn__ = kn__->parent;
         struct kernfs_root *root = kn__->dir.root;

				char *dup_name = NULL;
//				struct kernfs_node *kn;
				int ret;

				kn = kmem_cache_zalloc_tlx(kernfs_node_cache_tlx, GFP_KERNEL);
				atomic_set(&kn->count, 1);
				atomic_set(&kn->active, KN_DEACTIVATED_BIAS);
				RB_CLEAR_NODE(&kn->rb);

				kn->name = name;
				kn->mode = mode;
				kn->flags = flags;

		if (kn) {
//							kernfs_get(parent);
							if (parent)
										atomic_inc_tlx(&parent->count);
							kn->parent = parent;
			}
			kn->attr.ops = ops;
			kn->attr.size = size;
			kn->ns = ns;
			kn->priv = priv;
			if (ops->seq_show)
				kn->flags |= KERNFS_HAS_SEQ_SHOW;
			if (ops->mmap)
				kn->flags |= KERNFS_HAS_MMAP;

			rc = kernfs_add_one_tlx(kn);

	return 0;
}


int kobject_add_internal_tlx(struct kobject *kobj)
{
	int error = 0;
	struct kobject *parent;
	if (kobj)
				atomic_inc_tlx(&(&kobj->kref)->refcount);
	parent = kobj->parent;

	/* join kset if set, use it as parent if we do not already have one */
//	error = create_dir(kobj);

		const struct kobj_ns_type_operations *ops;
//		int error;
		error = sysfs_create_dir_ns_tlx(kobj, NULL);
	//	error = populate_dir(kobj);
		struct kobj_type *t =  kobj->ktype;
		struct attribute *attr;
	//	int error = 0;
		int i;

		if (t && t->default_attrs) {
			for (i = 0; (attr = t->default_attrs[i]) != NULL; i++) {
	//			error = sysfs_create_file(kobj, attr);
					error = sysfs_add_file_mode_tlx(kobj->sd, attr, false, attr->mode, NULL);
	//				sysfs_create_file_ns(kobj, attr, NULL);
				if (error)
					break;
			}
		}
	//	sysfs_get(kobj->sd);
		if (kobj->sd)
					atomic_inc_tlx(&(kobj->sd)->count);


	kobj->state_in_sysfs = 1;

	return 0;
}

int kset_register_tlx(struct kset *k)
{
	int err;
//	kset_init(k);
//	kobject_init_internal(&k->kobj);
	struct kobject *kobj = &k->kobj;
		atomic_set(&(&kobj->kref)->refcount, 1);
		INIT_LIST_HEAD(&kobj->entry);
		kobj->state_in_sysfs = 0;
		kobj->state_add_uevent_sent = 0;
		kobj->state_remove_uevent_sent = 0;
		kobj->state_initialized = 1;
	INIT_LIST_HEAD(&k->list);
	spin_lock_init(&k->list_lock);
	err = kobject_add_internal_tlx(&k->kobj);
//	kobject_uevent(&k->kobj, KOBJ_ADD);
	return 0;
}

int kobject_add_tlx(struct kobject *kobj, struct kobject *parent,
		const char *fmt, ...)
{
	va_list args;
	int retval;
	va_start(args, fmt);
//	retval = kobject_add_varg(kobj, parent, fmt, args);
	kobject_set_name_vargs_tlx(kobj, fmt, args);
	kobj->parent = parent;
	retval =  kobject_add_internal_tlx(kobj);
	va_end(args);
	return retval;
}


int kobject_init_and_add_tlx(struct kobject *kobj, struct kobj_type *ktype,
													struct kobject *parent, const char *fmt, ...)
{
				va_list args;
				int retval;

//         kobject_init_tlx(kobj, ktype);
//				 kobject_init_internal(kobj);
//				 kref_init(&kobj->kref);
					atomic_set(&(&kobj->kref)->refcount, 1);
					INIT_LIST_HEAD(&kobj->entry);
					kobj->state_in_sysfs = 0;
					kobj->state_add_uevent_sent = 0;
					kobj->state_remove_uevent_sent = 0;
					kobj->state_initialized = 1;

				kobj->ktype = ktype;
				va_start(args, fmt);
//         retval = kobject_add_varg(kobj, parent, fmt, args);
//					struct kobject *kobj, struct kobject *parent,
//										const char *fmt, va_list vargs)
//					{
//						int retval;

						kobject_set_name_vargs_tlx(kobj, fmt, args);
						kobj->parent = parent;
						retval =  kobject_add_internal_tlx(kobj);

				va_end(args);

				return retval;
}


enum KTHREAD_BITS {
  KTHREAD_IS_PER_CPU = 0,
  KTHREAD_SHOULD_STOP,
  KTHREAD_SHOULD_PARK,
  KTHREAD_IS_PARKED,
};

struct kthread {
  unsigned long flags;
  unsigned int cpu;
  void *data;
  struct completion parked;
  struct completion exited;
};

LIST_HEAD(kthread_create_list_tlx);
spinlock_t kthread_create_lock_tlx;
static void __kthread_parkme(struct kthread *self)
{
  __set_current_state(TASK_PARKED);
  while (test_bit_tlx(KTHREAD_SHOULD_PARK, &self->flags)) {
    if (!test_and_set_bit_tlx(KTHREAD_IS_PARKED, &self->flags))
      complete_tlx(&self->parked);
    __schedule_tlx();
    __set_current_state(TASK_PARKED);
  }
  __clear_bit_tlx(KTHREAD_IS_PARKED, &self->flags);
  __set_current_state(TASK_RUNNING);
}

static int kthread(void *_create)
{
  /* Copy data: it's on kthread's stack */
  struct kthread_create_info *create = _create;
  int (*threadfn)(void *data) = create->threadfn;
  void *data = create->data;
  struct completion *done;
  struct kthread self;
  int ret;

  self.flags = 0;
  self.data = data;
  init_completion_tlx(&self.exited);
  init_completion_tlx(&self.parked);
  current->vfork_done = &self.exited;

  /* If user was SIGKILLed, I release the structure. */
  done = xchg(&create->done, NULL);
  if (!done) {
    kfree_tlx(create);
    do_exit_tlx(-EINTR);
  }
  /* OK, tell user we're spawned, wait for stop or wakeup */
  __set_current_state(TASK_UNINTERRUPTIBLE);
  create->result = current;
  complete_tlx(done);
  __schedule_tlx();

  ret = -EINTR;

  if (!test_bit_tlx(KTHREAD_SHOULD_STOP, &self.flags)) {
    __kthread_parkme(&self);
    ret = threadfn(data);
  }
  /* we can't just return, we must preserve "self" on stack */
  do_exit_tlx(ret);
}

static int kthreadd(void *unused)
{
  struct task_struct *tsk = current;

  /* Setup a clean context for our children to inherit. */
  set_task_comm_tlx(tsk, "kthreadd");
  ignore_signals_tlx(tsk);
  set_cpus_allowed_ptr_tlx(tsk, cpu_all_mask);
//  set_mems_allowed(node_states_tlx[N_MEMORY]);
  current->flags |= PF_NOFREEZE;

  for (;;) {
    set_current_state(TASK_INTERRUPTIBLE);
    if (list_empty(&kthread_create_list_tlx))
      __schedule_tlx();
    __set_current_state(TASK_RUNNING);

    spin_lock_tlx(&kthread_create_lock_tlx);
    while (!list_empty(&kthread_create_list_tlx)) {
      struct kthread_create_info *create;

      create = list_entry(kthread_create_list_tlx.next,
              struct kthread_create_info, list);
      list_del_init(&create->list);
      spin_unlock_tlx(&kthread_create_lock_tlx);

//      create_kthread(create);
//      struct kthread_create_info *create)
//      {
        int pid;
        pid = kernel_thread_tlx(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
        if (pid < 0) {
          struct completion *done = xchg(&create->done, NULL);
          if (!done) {
            kfree_tlx(create);
            return;
          }
          create->result = ERR_PTR_tlx(pid);
          complete_tlx(done);
        }

      spin_lock_tlx(&kthread_create_lock_tlx);
    }
    spin_unlock_tlx(&kthread_create_lock_tlx);
  }

  return 0;
}

int wake_up_process_tlx(struct task_struct *tsk);

struct task_struct *kthread_create_on_node_tlx(int (*threadfn)(void *data),
             void *data, int node,
             const char namefmt[],
             ...)
{
  DECLARE_COMPLETION_ONSTACK(done);
  struct task_struct *task;
  struct kthread_create_info *create = kmalloc_tlx(sizeof(*create),
                 GFP_KERNEL);
  create->threadfn = threadfn;
  create->data = data;
  create->node = node;
  create->done = &done;
  spin_lock_tlx(&kthread_create_lock_tlx);
  list_add_tail(&create->list, &kthread_create_list_tlx);
  spin_unlock_tlx(&kthread_create_lock_tlx);
  wake_up_process_tlx(kthreadd_task_tlx);
  if (unlikely(wait_for_completion_killable_tlx(&done))) {
    if (xchg(&create->done, NULL))
      return ERR_PTR_tlx(-EINTR);
    wait_for_completion_tlx(&done);
  }
  task = create->result;
  kfree_tlx(create);
  return task;
}



struct simple_xattrs {
          struct list_head head;
          spinlock_t lock;
 };

struct shmem_inode_info {
  spinlock_t		lock;
  unsigned long		flags;
  unsigned long		alloced;	/* data pages alloced to file */
  union {
    unsigned long	swapped;	/* subtotal assigned to swap */
    char		*symlink;	/* unswappable short symlink */
  };
  struct shared_policy	policy;		/* NUMA memory alloc policy */
  struct list_head	swaplist;	/* chain of maybes on swap */
  struct simple_xattrs	xattrs;		/* list of xattrs */
  struct inode		vfs_inode;
};

struct shmem_sb_info {
  unsigned long max_blocks;   /* How many blocks are allowed */
  struct percpu_counter used_blocks;  /* How many are allocated */
  unsigned long max_inodes;   /* How many inodes are allowed */
  unsigned long free_inodes;  /* How many are left for allocation */
  spinlock_t stat_lock;	    /* Serialize shmem_sb_info changes */
  kuid_t uid;		    /* Mount uid for root directory */
  kgid_t gid;		    /* Mount gid for root directory */
  umode_t mode;		    /* Mount mode for root directory */
  struct mempolicy *mpol;     /* default memory policy for mappings */
};

#define COMMAND_LINE_SIZE       2048

extern char _text[], _stext[], _etext[];
extern char _data[], _sdata[], _edata[];
extern char __per_cpu_load[], __per_cpu_start[], __per_cpu_end[];
extern char _end[];

struct vfsmount *sock_mnt_tlx ;

static int kernel_init(void *);
bool early_boot_irqs_disabled ;
enum system_states system_state ;
EXPORT_SYMBOL(system_state);
/*
 * Boot command-line arguments
 */
#define MAX_INIT_ARGS CONFIG_INIT_ENV_ARG_LIMIT
#define MAX_INIT_ENVS CONFIG_INIT_ENV_ARG_LIMIT
/* Untouched command line saved by arch-specific code. */
char __initdata boot_command_line[COMMAND_LINE_SIZE];
/* Untouched saved command line (eg. for /proc) */
char *saved_command_line;
/* Command line for parameter parsing */
static char *static_command_line;
/* Command line for per-initcall parameter parsing */
static char *initcall_command_line;

static char *execute_command;
static char *ramdisk_execute_command;
/*
 * Used to generate warnings if static_key manipulation functions are used
 * before jump_label_init is called.
 */
bool static_key_initialized  = false;
EXPORT_SYMBOL_GPL(static_key_initialized);

/*
 * If set, this is an indication to the drivers that reset the underlying
 * device before going ahead with the initialization otherwise driver might
 * rely on the BIOS and skip the reset operation.
 *
 * This is useful if kernel is booting in an unreliable environment.
 * For ex. kdump situaiton where previous kernel has crashed, BIOS has been
 * skipped and devices will be in unknown state.
 */
unsigned int reset_devices;
EXPORT_SYMBOL(reset_devices);
static const char * argv_init[MAX_INIT_ARGS+2] = { "init", NULL, };
const char * envp_init[MAX_INIT_ENVS+2] = { "HOME=/", "TERM=linux", NULL, };
static const char *panic_later, *panic_param;
const struct obs_kernel_param __setup_start[], __setup_end[];
/*
 * This should be approx 2 Bo*oMips to start (note initial shift), and will
 * still work even if initially too large, it will just take slightly longer
 */
//unsigned long loops_per_jiffy = (1<<12);
EXPORT_SYMBOL(loops_per_jiffy);
static __initdata DECLARE_COMPLETION(kthreadd_done);
void __init parse_early_options(char *cmdline)
{
}




int memblock_tlx_memory_in_slab_tlx;// = 0;
int memblock_tlx_reserved_in_slab_tlx;//  = 0;


#define INIT_MEMBLOCK_REGIONS   128



struct memblock_tlx_region {
       phys_addr_t base;
       phys_addr_t size;
       unsigned long flags;
 #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
         int nid;
 #endif
};

#define for_each_mem_range_rev(i, type_a, type_b, nid,			\
             p_start, p_end, p_nid)			\
  for (i = (u64)ULLONG_MAX,					\
         __next_mem_range_rev_tlx(&i, nid, type_a, type_b,	\
           p_start, p_end, p_nid);	\
       i != (u64)ULLONG_MAX;					\
       __next_mem_range_rev_tlx(&i, nid, type_a, type_b,		\
          p_start, p_end, p_nid))


struct memblock_tlx_type {
         unsigned long cnt;      /* number of regions */
         unsigned long max;      /* size of the allocated array */
         phys_addr_t total_size; /* size of all regions */
         struct memblock_tlx_region *regions;
};

struct memblock_tlx {
         bool bottom_up;  /* is bottom up direction? */
         phys_addr_t current_limit;
         struct memblock_tlx_type memory;
         struct memblock_tlx_type reserved;
#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP
         struct memblock_tlx_type physmem;
#endif
};


#define for_each_free_mem_range_reverse(i, nid, p_start, p_end, p_nid)  \
         for_each_mem_range_rev(i, &memblock_tlx.memory, &memblock_tlx.reserved, \
                                nid, p_start, p_end, p_nid)

#define for_each_memblock_tlx(memblock_tlx_type, region)                                        \
         for (region = memblock_tlx.memblock_tlx_type.regions;                           \
              region < (memblock_tlx.memblock_tlx_type.regions + memblock_tlx.memblock_tlx_type.cnt);    \
              region++)

#define MEMBLOCK_ALLOC_ACCESSIBLE     0
#define MEMBLOCK_ALLOC_ANYWHERE (~(phys_addr_t)0)

struct memblock_tlx_region memblock_tlx_memory_init_regions_tlx[INIT_MEMBLOCK_REGIONS];
struct memblock_tlx_region memblock_tlx_reserved_init_regions_tlx[INIT_MEMBLOCK_REGIONS];

struct memblock_tlx memblock_tlx = {
	.memory.regions		= memblock_tlx_memory_init_regions_tlx,
	.memory.cnt		= 1,	/* empty dummy entry */
	.memory.max		= INIT_MEMBLOCK_REGIONS,

	.reserved.regions	= memblock_tlx_reserved_init_regions_tlx,
	.reserved.cnt		= 1,	/* empty dummy entry */
	.reserved.max		= INIT_MEMBLOCK_REGIONS,
	.bottom_up		= false,
	.current_limit		= MEMBLOCK_ALLOC_ANYWHERE,
};


void  __next_mem_range_rev_tlx(u64 *idx, int nid,
            struct memblock_tlx_type *type_a,
            struct memblock_tlx_type *type_b,
            phys_addr_t *out_start,
            phys_addr_t *out_end, int *out_nid)
{
  int idx_a = *idx & 0xffffffff;
  int idx_b = *idx >> 32;

  nid = NUMA_NO_NODE;
  if (*idx == (u64)ULLONG_MAX) {
    idx_a = type_a->cnt - 1;
    idx_b = type_b->cnt;
  }
  for (; idx_a >= 0; idx_a--) {
    struct memblock_tlx_region *m = &type_a->regions[idx_a];
    phys_addr_t m_start = m->base;
    phys_addr_t m_end = m->base + m->size;
    int m_nid = 0;
    if (!type_b) {
      if (out_start)
        *out_start = m_start;
      if (out_end)
        *out_end = m_end;
      if (out_nid)
        *out_nid = m_nid;
      idx_a++;
      *idx = (u32)idx_a | (u64)idx_b << 32;
      return;
    }
    for (; idx_b >= 0; idx_b--) {
      struct memblock_tlx_region *r;
      phys_addr_t r_start;
      phys_addr_t r_end;
      r = &type_b->regions[idx_b];
      r_start = idx_b ? r[-1].base + r[-1].size : 0;
      r_end = idx_b < type_b->cnt ?
        r->base : ULLONG_MAX;
      if (r_end <= m_start)
        break;
      if (m_end > r_start) {
        if (out_start)
          *out_start = max(m_start, r_start);
        if (out_end)
          *out_end = min(m_end, r_end);
        if (out_nid)
          *out_nid = m_nid;
        if (m_start >= r_start)
          idx_a--;
        else
          idx_b--;
        *idx = (u32)idx_a | (u64)idx_b << 32;
        return;
      }
    }
  }
  *idx = ULLONG_MAX;
}


void __next_mem_range_tlx(u64 *idx, int nid,
						struct memblock_tlx_type *type_a,
						struct memblock_tlx_type *type_b,
						phys_addr_t *out_start,
						phys_addr_t *out_end, int *out_nid)
{
int idx_a = *idx & 0xffffffff;
int idx_b = *idx >> 32;
for (; idx_a < type_a->cnt; idx_a++) {
	struct memblock_tlx_region *m = &type_a->regions[idx_a];
	phys_addr_t m_start = m->base;
	phys_addr_t m_end = m->base + m->size;
	int	    m_nid = NUMA_NO_NODE;
	if (nid != NUMA_NO_NODE && nid != m_nid)
		continue;
	if (!type_b) {
		if (out_start)
			*out_start = m_start;
		if (out_end)
			*out_end = m_end;
		if (out_nid)
			*out_nid = m_nid;
		idx_a++;
		*idx = (u32)idx_a | (u64)idx_b << 32;
		return;
	}
	/* scan areas before each reservation */
	for (; idx_b < type_b->cnt + 1; idx_b++) {
		struct memblock_tlx_region *r;
		phys_addr_t r_start;
		phys_addr_t r_end;
		r = &type_b->regions[idx_b];
		r_start = idx_b ? r[-1].base + r[-1].size : 0;
		r_end = idx_b < type_b->cnt ?
			r->base : ULLONG_MAX;
		/*
			* if idx_b advanced past idx_a,
			* break out to advance idx_a
			*/
		if (r_start >= m_end)
			break;
		/* if the two regions intersect, we're done */
		if (m_start < r_end) {
			if (out_start)
				*out_start =
					max(m_start, r_start);
			if (out_end)
				*out_end = min(m_end, r_end);
			if (out_nid)
				*out_nid = m_nid;
			/*
				* The region which ends first is
				* advanced for the next iteration.
				*/
			if (m_end <= r_end)
				idx_a++;
			else
				idx_b++;
			*idx = (u32)idx_a | (u64)idx_b << 32;
			return;
		}
	}
}

/* signal end of iteration */
*idx = ULLONG_MAX;
}

static inline void set_page_count(struct page *page, int v)
{
	atomic_set(&page->_count, v);
}


enum {
	MIGRATE_ISOLATE,	/* can't allocate from here */
	MIGRATE_TYPES_TLX
};

int page_is_buddy_tlx(struct page *page, struct page *buddy,
							unsigned int order)
{
	if (page_is_guard_tlx(buddy) && page_private(buddy) == order) {
		if (page_zone_id_tlx(page) != page_zone_id_tlx(buddy))
			return 0;
		return 1;
	}

	if (PageBuddy_tlx(buddy) && page_private(buddy) == order) {
		if (page_zone_id_tlx(page) != page_zone_id_tlx(buddy))
			return 0;
		return 1;
	}
	return 0;
}

void __free_pages_tlx(struct page *page, unsigned int order)
{
	if (put_page_testzero_tlx(page)) {

					unsigned long flags;
					int migratetype;
					unsigned long pfn = page_to_pfn(page);
						int i;
						int bad = 0;
						if (PageAnon_tlx(page))
							page->mapping = NULL;
						if (!PageHighMem(page)) {
							debug_check_no_locks_freed_tlx(page_address(page),
											PAGE_SIZE << order);
							debug_check_no_obj_freed_tlx(page_address(page),
											PAGE_SIZE << order);
						}
					kernel_map_pages_tlx(page, 1 << order, 0);
					migratetype = get_pfnblock_migratetype_tlx(page, pfn);
					local_irq_save(flags);
					__count_vm_events_tlx(PGFREE, 1 << order);
					set_freepage_migratetype_tlx(page, migratetype);
					struct zone *zone = page_zone_tlx(page);
					zone->pages_scanned = 0;
						unsigned long page_idx;
						unsigned long combined_idx;
						unsigned long uninitialized_var(buddy_idx);
						struct page *buddy;
						page_idx = pfn & ((1 << MAX_ORDER) - 1);
						while (order < MAX_ORDER-1) {
							buddy_idx = page_idx ^ (1 << order);
							buddy = page + (buddy_idx - page_idx);
							if (!page_is_buddy_tlx(page, buddy, order))
								break;
							if (page_is_guard_tlx(buddy)) {
								set_page_private(page, 0);
								__mod_zone_freepage_state_tlx(zone, 1 << order,
												migratetype);
							} else {
								list_del(&buddy->lru);
								zone->free_area[order].nr_free--;
									__ClearPageBuddy_tlx(buddy);
         					set_page_private(buddy, 0);
							}
							combined_idx = buddy_idx & page_idx;
							page = page + (combined_idx - page_idx);
							page_idx = combined_idx;
							order++;
						}
							set_page_private(page, order);
         			__SetPageBuddy_tlx(page);
						if ((order < MAX_ORDER-2) && pfn_valid_within(page_to_pfn(buddy))) {
							struct page *higher_page, *higher_buddy;
							combined_idx = buddy_idx & page_idx;
							higher_page = page + (combined_idx - page_idx);
							buddy_idx = combined_idx ^ (1 << (order + 1));//__find_buddy_index(combined_idx, order + 1);
							higher_buddy = higher_page + (buddy_idx - combined_idx);
							if (page_is_buddy_tlx(higher_page, higher_buddy, order + 1)) {
								list_add_tail(&page->lru,
									&zone->free_area[order].free_list[migratetype]);
								goto out;
							}
						}
						list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
					out:
						zone->free_area[order].nr_free++;

         	if (unlikely(!(migratetype == MIGRATE_ISOLATE)))
                 __mod_zone_freepage_state_tlx(zone, 1 << order, migratetype);
					local_irq_restore(flags);
		}
}

void __init __free_pages_bootmem_tlx(struct page *page, unsigned int order)
{
	unsigned int nr_pages = 1 << order;
	struct page *p = page;
	unsigned int loop;

	prefetchw(p);
	for (loop = 0; loop < (nr_pages - 1); loop++, p++) {
		prefetchw(p + 1);
		__ClearPageReserved(p);
		set_page_count(p, 0);
	}
	__ClearPageReserved(p);
	set_page_count(p, 0);

	page_zone_tlx(page)->managed_pages += nr_pages;
	set_page_count(page, 1);
	__free_pages_tlx(page, order);
}

static void __init __free_pages_memory_tlx(unsigned long start, unsigned long end)
{
	int order;

	while (start < end) {
		order = min(MAX_ORDER - 1UL, __ffs_tlx(start));

		while (start + (1UL << order) > end)
			order--;

		__free_pages_bootmem_tlx(pfn_to_page(start), order);

		start += (1UL << order);
	}
}

static void sighand_ctor_tlx(void *data)
{
         struct sighand_struct *sighand = data;
         spin_lock_init(&sighand->siglock);
         init_waitqueue_head(&sighand->signalfd_wqh);
}

static void anon_vma_ctor_tlx(void *data)
{
         struct anon_vma *anon_vma = data;
         init_rwsem(&anon_vma->rwsem);
         atomic_set(&anon_vma->refcount, 0);
         anon_vma->rb_root = RB_ROOT;
}


struct kmem_cache * proc_inode_cachep_tlx_tlx;
struct kmem_cache *signal_cachep_tlx;
struct kmem_cache *sighand_cachep_tlx;
struct kmem_cache *files_cachep_tlx;
struct kmem_cache *fs_cachep_tlx;
struct kmem_cache *vm_area_cachep_tlx;

int max_threads_tlx;
struct kmem_cache *task_struct_cachep_tlx;
struct kmem_cache *cred_jar_tlx_tlx;
struct  kmem_cache *anon_vma_cachep_tlx;
struct  kmem_cache *anon_vma_chain_cachep_tlx;
int sched_clock_running_tlx;
struct tty_ldisc_ops *tty_ldiscs_tlx[NR_LDISCS];

struct list_head rotation_list_tlx;

struct of_device_id_tlx
{
				char    name[32];
				char    type[32];
				char    compatible[128];
				const void *data;
};


#define INDEX(N) ((base->timer_jiffies_tlx >> (TVR_BITS + (N) * TVN_BITS)) & TVN_MASK)
#define TVN_MASK (TVN_SIZE - 1)
#define TVR_MASK (TVR_SIZE - 1)
#define MAX_TVAL ((unsigned long)((1ULL << (TVR_BITS + 4*TVN_BITS)) - 1))



DEFINE_PER_CPU(struct tvec_base *, tvec_bases_tlx);


int cascade_tlx(struct tvec_base *base, struct tvec *tv, int index)
{
	/* cascade all the timers from tv up one level */
	struct timer_list *timer, *tmp;
	struct list_head tv_list;

	list_replace_init(tv->vec + index, &tv_list);
	list_for_each_entry_safe(timer, tmp, &tv_list, entry) {
				unsigned long expires = timer->expires;
				unsigned long idx = expires - base->timer_jiffies_tlx;
				struct list_head *vec;
				if (idx < TVR_SIZE) {
					int i = expires & TVR_MASK;
					vec = base->tv1.vec + i;
				} else if (idx < 1 << (TVR_BITS + TVN_BITS)) {
					int i = (expires >> TVR_BITS) & TVN_MASK;
					vec = base->tv2.vec + i;
				} else if (idx < 1 << (TVR_BITS + 2 * TVN_BITS)) {
					int i = (expires >> (TVR_BITS + TVN_BITS)) & TVN_MASK;
					vec = base->tv3.vec + i;
				} else if (idx < 1 << (TVR_BITS + 3 * TVN_BITS)) {
					int i = (expires >> (TVR_BITS + 2 * TVN_BITS)) & TVN_MASK;
					vec = base->tv4.vec + i;
				} else if ((signed long) idx < 0) {
					vec = base->tv1.vec + (base->timer_jiffies_tlx & TVR_MASK);
				} else {
					int i;
					if (idx > MAX_TVAL) {
						idx = MAX_TVAL;
						expires = idx + base->timer_jiffies_tlx;
					}
					i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
					vec = base->tv5.vec + i;
				}
				list_add_tail(&timer->entry, vec);
	}

	return index;
}

void run_timer_softirq_tlx(struct softirq_action *h)
{
	struct tvec_base *base = __this_cpu_read(tvec_bases_tlx);
	if (time_after_eq(jiffies_tlx, base->timer_jiffies_tlx)) {
				struct timer_list *timer;

				spin_lock_irq_tlx(&base->lock);
				if (!base->all_timers) {
											base->timer_jiffies_tlx = jiffies_tlx;
											spin_unlock_irq_tlx(&base->lock);
											return;
					}
				while (time_after_eq(jiffies_tlx, base->timer_jiffies_tlx)) {
					struct list_head work_list;
					struct list_head *head = &work_list;
					int index = base->timer_jiffies_tlx & TVR_MASK;
					if (!index &&
						(!cascade_tlx(base, &base->tv2, INDEX(0))) &&
							(!cascade_tlx(base, &base->tv3, INDEX(1))) &&
								!cascade_tlx(base, &base->tv4, INDEX(2)))
						cascade_tlx(base, &base->tv5, INDEX(3));
					++base->timer_jiffies_tlx;
					list_replace_init(base->tv1.vec + index, head);
					while (!list_empty(head)) {
						void (*fn)(unsigned long);
						unsigned long data;
						bool irqsafe;
						timer = list_first_entry(head, struct timer_list,entry);
						fn = timer->function;
						data = timer->data;
						irqsafe = ((unsigned int)(unsigned long)(timer->base) & TIMER_IRQSAFE);
						base->running_timer = timer;
							struct list_head *entry = &timer->entry;
							__list_del(entry->prev, entry->next);
											entry->next = NULL;
							entry->prev = LIST_POISON2;
							if (!((unsigned int)(unsigned long)(timer->base) & TIMER_DEFERRABLE))
											base->active_timers--;
							base->all_timers--;
						if (irqsafe) {
							spin_unlock_tlx(&base->lock);
							fn(data);
							spin_lock_tlx(&base->lock);
						} else {
							spin_unlock_irq_tlx(&base->lock);
							fn(data);
							spin_lock_irq_tlx(&base->lock);
						}
					}
				}
				base->running_timer = NULL;
				spin_unlock_irq_tlx(&base->lock);

	}
}



struct timekeeper {
	/* Current clocksource used for timekeeping. */
	struct clocksource_tlx	*clock;
	/* NTP adjusted clock multiplier */
	u32			mult;
	/* The shift value of the current clocksource. */
	u32			shift;
	/* Number of clock cycles in one NTP interval. */
	cycle_t			cycle_interval;
	/* Last cycle value (also stored in clock->cycle_last) */
	cycle_t			cycle_last;
	/* Number of clock shifted nano seconds in one NTP interval. */
	u64			xtime_interval;
	/* shifted nano seconds left over when rounding cycle_interval */
	s64			xtime_remainder;
	/* Raw nano seconds accumulated per NTP interval. */
	u32			raw_interval;

	/* Current CLOCK_REALTIME time in seconds */
	u64			xtime_sec;
	/* Clock shifted nano seconds */
	u64			xtime_nsec;

	/* Difference between accumulated time and NTP time in ntp
	* shifted nano seconds. */
	s64			ntp_error;
	/* Shift conversion between clock shifted nano seconds and
	* ntp shifted nano seconds. */
	u32			ntp_error_shift;

	/*
	* wall_to_monotonic is what we need to add to xtime (or xtime corrected
	* for sub jiffie times) to get to monotonic time.  Monotonic is pegged
	* at zero at system boot time, so wall_to_monotonic will be negative,
	* however, we will ALWAYS keep the tv_nsec part positive so we can use
	* the usual normalization.
	*
	* wall_to_monotonic is moved after resume from suspend for the
	* monotonic time not to jump. We need to add total_sleep_time to
	* wall_to_monotonic to get the real boot based time offset.
	*
	* - wall_to_monotonic is no longer the boot time, getboottime must be
	* used instead.
	*/
	struct timespec		wall_to_monotonic;
	/* Offset clock monotonic -> clock realtime */
	ktime_t			offs_real;
	/* time spent in suspend */
	struct timespec		total_sleep_time;
	/* Offset clock monotonic -> clock boottime */
	ktime_t			offs_boot;
	/* The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock. */
	struct timespec		raw_time;
	/* The current UTC to TAI offset in seconds */
	s32			tai_offset;
	/* Offset clock monotonic -> clock tai */
	ktime_t			offs_tai;

};

#define NUM_RCU_NODES 1
#define RCU_NUM_LVLS 1
#define MAX_RCU_LVLS 4
#define RCU_NEXT_SIZE 4

struct rcu_data {
	unsigned long	completed;	/* Track rsp->completed gp number */
	unsigned long	gpnum;		/* Highest gp number that this CPU */
	bool		passed_quiesce;	/* User-mode/idle loop etc. */
	bool		qs_pending;	/* Core waits for quiesc state. */
	bool		beenonline;	/* CPU online at least once. */
	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
	struct rcu_head *nxtlist;
	struct rcu_head **nxttail[RCU_NEXT_SIZE];
	unsigned long	nxtcompleted[RCU_NEXT_SIZE];
	long		qlen_lazy;	/* # of lazy queued callbacks */
	long		qlen;		/* # of queued callbacks, incl lazy */
	long		qlen_last_fqs_check;
	unsigned long	n_cbs_invoked;	/* count of RCU cbs invoked. */
	unsigned long	n_nocbs_invoked; /* count of no-CBs RCU cbs invoked. */
	unsigned long   n_cbs_orphaned; /* RCU cbs orphaned by dying CPU */
	unsigned long   n_cbs_adopted;  /* RCU cbs adopted from dying CPU */
	unsigned long	n_force_qs_snap;
	long		blimit;		/* Upper limit on a processed batch */
	struct rcu_dynticks_tlx *dynticks;	/* Shared per-CPU dynticks state. */
	int dynticks_snap;		/* Per-GP tracking for dynticks. */
	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */
	unsigned long offline_fqs;	/* Kicked due to being offline. */
	unsigned long cond_resched_completed;
	unsigned long n_rcu_pending;	/* rcu_pending() calls since boot. */
	unsigned long n_rp_qs_pending;
	unsigned long n_rp_report_qs;
	unsigned long n_rp_cb_ready;
	unsigned long n_rp_cpu_needs_gp;
	unsigned long n_rp_gp_completed;
	unsigned long n_rp_gp_started;
	unsigned long n_rp_nocb_defer_wakeup;
	unsigned long n_rp_need_nothing;
	struct rcu_head barrier_head;
	int cpu;
	struct rcu_state *rsp;
};


struct timekeeper timekeeper_tlx;
struct timekeeper shadow_timekeeper_tlx;


//DECLARE_PER_CPU(struct rcu_data, rcu_sched_data_tlx);
struct rcu_state rcu_tlx_bh_state;
//DECLARE_PER_CPU(struct rcu_data, rcu_bh_data_tlx);

struct rcu_data rcu_sched_data_tlx;
struct rcu_data rcu_bh_data_tlx;


struct timespec current_kernel_time_tlx(void)
{
		struct timekeeper *tk = &timekeeper_tlx;
		struct timespec ts;
    ts.tv_sec = tk->xtime_sec;
    ts.tv_nsec = (long)(tk->xtime_nsec >> tk->shift);
    return ts;
}

void rcu_process_callbacks_tlx(struct softirq_action *unused)
{
}

struct hlist_head *pid_hash_tlx;
unsigned int pidhash_shift_tlx = 4;






struct kobject *base_probe_tlx(dev_t dev, int *part, void *data)
{
	return NULL;
}


static DEFINE_MUTEX(chrdevs_lock_tlx);
typedef struct kobject *kobj_probe_t(dev_t, int *, void *);

struct kobj_map {
	struct probe {
		struct probe *next;
		dev_t dev;
		unsigned long range;
		struct module *owner;
		kobj_probe_t *get;
		int (*lock)(dev_t, void *);
		void *data;
	} *probes[255];
	struct mutex *lock;
};


struct kobject *kobj_lookup_tlx(struct kobj_map *domain, dev_t dev, int *index)
{
	struct kobject *kobj;
	struct probe *p;
	unsigned long best = ~0UL;

retry:
	mutex_lock_tlx(domain->lock);
	for (p = domain->probes[MAJOR(dev) % 255]; p; p = p->next) {
		struct kobject *(*probe)(dev_t, int *, void *);
		struct module *owner;
		void *data;

		if (p->dev > dev || p->dev + p->range - 1 < dev)
			continue;
		if (p->range - 1 >= best)
			break;
//		if (!try_module_get(p->owner))
//			continue;
		owner = p->owner;
		data = p->data;
		probe = p->get;
		best = p->range - 1;
		*index = dev - p->dev;
		if (p->lock && p->lock(dev, data) < 0) {
//			module_put(owner);
			continue;
		}
		mutex_unlock_tlx(domain->lock);
		kobj = probe(dev, index, data);
		/* Currently ->owner protects _only_ ->probe() itself. */
//		module_put(owner);
		if (kobj)
			return kobj;
		goto retry;
	}
	mutex_unlock_tlx(domain->lock);
	return NULL;
}

struct kmem_cache *inode_cachep_tlx ;
struct hlist_head *inode_hashtable_tlx ;

void init_once_tlx_tlx(void *foo)
{
	struct inode *inode = (struct inode *) foo;
	memset_tlx(inode, 0, sizeof(*inode));
	INIT_HLIST_NODE(&inode->i_hash);
	INIT_LIST_HEAD(&inode->i_devices);
	INIT_LIST_HEAD(&inode->i_wb_list);
	INIT_LIST_HEAD(&inode->i_lru);
	struct address_space *mapping = &inode->i_data;
				memset_tlx(mapping, 0, sizeof(*mapping));
				INIT_RADIX_TREE(&mapping->page_tree, GFP_ATOMIC);
				spin_lock_init(&mapping->tree_lock);
				mutex_init(&mapping->i_mmap_mutex);
				INIT_LIST_HEAD(&mapping->private_list);
				spin_lock_init(&mapping->private_lock);
				mapping->i_mmap = RB_ROOT;
				INIT_LIST_HEAD(&mapping->i_mmap_nonlinear);
}

unsigned long ihash_entries_tlx;
unsigned int i_hash_mask_tlx ;
unsigned int i_hash_shift_tlx ;
struct kmem_cache *dentry_cache_tlx ;
__initdata unsigned long dhash_entries_tlx;
unsigned long __meminitdata nr_kernel_pages_tlx;




struct kmem_cache *kmem_cache_tlx;

unsigned long calculate_alignment_tlx(unsigned long flags,
		unsigned long align, unsigned long size)
{
	if (flags & SLAB_HWCACHE_ALIGN) {
		unsigned long ralign = cache_line_size_tlx();
		while (size <= ralign / 2)
			ralign /= 2;
		align = max(align, ralign);
	}

	if (align < ARCH_SLAB_MINALIGN)
		align = ARCH_SLAB_MINALIGN;

	return ALIGN(align, sizeof(void *));
}


LIST_HEAD(slab_caches_tlx);

int __kmem_cache_create_tlx(struct kmem_cache *s, unsigned long flags);

struct kmem_cache *__init create_kmalloc_cache_tlx(const char *name, size_t size,
				unsigned long flags)
{
	struct kmem_cache *s = kmem_cache_zalloc_tlx(kmem_cache_tlx, GFP_NOWAIT);
		int err;
		s->name = name;
		s->size = s->object_size = size;
		s->align = calculate_alignment_tlx(flags, ARCH_KMALLOC_MINALIGN, size);
		err = __kmem_cache_create_tlx(s, flags);
		s->refcount = -1;	/* Exempt from merging for now */
	list_add(&s->list, &slab_caches_tlx);
	s->refcount = 1;
	return s;
}

struct kmem_cache *kmem_cache_node_tlx;


struct pcpu_chunk {
	struct list_head	list;		/* linked to pcpu_slot_tlx lists */
	int			free_size;	/* free bytes in the chunk */
	int			contig_hint;	/* max contiguous size hint */
	void			*base_addr;	/* base address of this chunk */
	int			map_used;	/* # of map entries used before the sentry */
	int			map_alloc;	/* # of map entries allocated */
	int			*map;		/* allocation map */
	void			*data;		/* chunk data */
	int			first_free;	/* no free below this */
	bool			immutable;	/* no [de]population allowed */
	unsigned long		populated[];	/* populated bitmap */
};

struct pcpu_chunk *pcpu_first_chunk_tlx;
struct pcpu_chunk *pcpu_reserved_chunk_tlx;
long			time_adjust_tlx;
int			time_state_tlx;
long			time_maxerror_tlx;
long			time_esterror_tlx;
s64			ntp_tick_adj_tlx;
s64			time_freq_tlx;
u64			tick_length_tlx;
u64			tick_length_tlx_base;
s64			time_offset_tlx;
int			time_status_tlx;
phys_addr_t __fdt_pointer_tlx __initdata;


static inline void flush_tlb_all(void)
{
         dsb(ishst);
         asm("tlbi       vmalle1is");
         dsb(ish);
         isb();
}

pgd_t *pgd_alloc_tlx(struct mm_struct *mm)
{
	 struct page *page;

	if (PGD_SIZE == PAGE_SIZE) {
//		return (pgd_t *) __get_free_pages(GFP_KERNEL | __GFP_ZERO, 0);
		page = alloc_pages(GFP_KERNEL | __GFP_ZERO, 0);
		return (unsigned long) page_address(page);
	}
	else
		return kzalloc_tlx(PGD_SIZE, GFP_KERNEL);
}

#define ASID_FIRST_VERSION      (1 << MAX_ASID_BITS)
#define MAX_ASID_BITS   16
#define asid_bits(reg) \
          (((read_cpuid(ID_AA64MMFR0_EL1) & 0xf0) >> 2) + 8)

raw_spinlock_t cpu_asid_lock_tlx; //= __RAW_SPIN_LOCK_UNLOCKED(cpu_asid_lock_tlx);

static inline void cpu_set_reserved_ttbr0(void)
{
	unsigned long ttbr = page_to_phys(empty_zero_page_tlx);

	asm(
	"	msr	ttbr0_el1, %0			// set TTBR0\n"
	"	isb"
	:
	: "r" (ttbr));
}

#define ASID_FIRST_VERSION      (1 << MAX_ASID_BITS)
unsigned int cpu_last_asid_tlx = ASID_FIRST_VERSION;

void __new_context_tlx(struct mm_struct *mm)
{
	unsigned int asid;
	unsigned int bits = asid_bits();

	raw_spin_lock(&cpu_asid_lock_tlx);
	if (!unlikely((mm->context.id ^ cpu_last_asid_tlx) >> MAX_ASID_BITS)) {
		cpumask_set_cpu_tlx(smp_processor_id(), mm_cpumask_tlx(mm));
		raw_spin_unlock(&cpu_asid_lock_tlx);
		goto out;
	}
	asid = ++cpu_last_asid_tlx;
	if (unlikely((asid & ((1 << bits) - 1)) == 0)) {
		cpu_last_asid_tlx += (1 << MAX_ASID_BITS) - (1 << bits);
		if (cpu_last_asid_tlx == 0)
			cpu_last_asid_tlx = ASID_FIRST_VERSION;
		asid = cpu_last_asid_tlx + smp_processor_id();
		cpu_set_reserved_ttbr0();
		flush_tlb_all();
		if (icache_policy_tlx() == ICACHE_POLICY_AIVIVT){
			asm("ic ialluis");
			dsb(ish);
		}
		smp_wmb();
				preempt_disable();
						unsigned int asid;
						unsigned int cpu = smp_processor_id();
						struct mm_struct *mm = current->active_mm;
						smp_rmb();
						asid = cpu_last_asid_tlx + cpu;
							cpu_set_reserved_ttbr0();
							flush_tlb_all();
							if (icache_policy_tlx() == ICACHE_POLICY_AIVIVT) {
								asm("ic ialluis");
								dsb(ish);
							}
						if (likely((mm->context.id ^ cpu_last_asid_tlx) >> MAX_ASID_BITS)) {
							mm->context.id = asid;
							cpumask_clear_tlx(mm_cpumask_tlx(mm));
						}
						cpumask_set_cpu_tlx(smp_processor_id(), mm_cpumask_tlx(mm));
						cpu_switch_mm(mm->pgd, mm);
				preempt_enable();
		cpu_last_asid_tlx += NR_CPUS - 1;
	}
		if (likely((mm->context.id ^ cpu_last_asid_tlx) >> MAX_ASID_BITS)) {
			mm->context.id = asid;
			cpumask_clear_tlx(mm_cpumask_tlx(mm));
		}
		cpumask_set_cpu_tlx(smp_processor_id(), mm_cpumask_tlx(mm));

	raw_spin_unlock(&cpu_asid_lock_tlx);
out:
	return;
}


#define MAX_ASID_BITS   16


#define __phys_to_virt_tlx(x)       ((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
#define __virt_to_phys_tlx(x)       (((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET))






int memblock_tlx_double_array_tlx(struct memblock_tlx_type *type,
            phys_addr_t new_area_start,
            phys_addr_t new_area_size);


int memblock_tlx_remove_range_tlx(struct memblock_tlx_type *type,
            phys_addr_t base, phys_addr_t size)
{
  int _start_rgn, _end_rgn;
  int i, ret;
    int *start_rgn = &_start_rgn;
    int *end_rgn = &_end_rgn;
    size = min(size, (phys_addr_t)ULLONG_MAX - base);
    phys_addr_t end = base + size;
    *start_rgn = *end_rgn = 0;
    while (type->cnt + 2 > type->max)
        memblock_tlx_double_array_tlx(type, base, size);
    for (i = 0; i < type->cnt; i++) {
      struct memblock_tlx_region *rgn = &type->regions[i];
      phys_addr_t rbase = rgn->base;
      phys_addr_t rend = rbase + rgn->size;
      if (rbase >= end)
        break;
      if (rend <= base)
        continue;
      if (rbase < base) {
        rgn->base = base;
        rgn->size -= base - rbase;
        type->total_size -= base - rbase;
          int idx = i;
          phys_addr_t base = rbase;
          phys_addr_t size = base - rbase;
          int nid = 0; //memblock_tlx_get_region_node(rgn);
          unsigned long flags = rgn->flags;
          struct memblock_tlx_region *rgn = &type->regions[idx];
          memmove_tlx(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));
          rgn->base = base;
          rgn->size = size;
          rgn->flags = flags;
          type->cnt++;
          type->total_size += size;
      } else if (rend > end) {
        rgn->base = end;
        rgn->size -= end - rbase;
        type->total_size -= end - rbase;
          int idx = i--;
          phys_addr_t base = rbase;
          phys_addr_t size = end - rbase;
          int nid = 0; //memblock_tlx_get_region_node(rgn);
          unsigned long flags = rgn->flags;
          struct memblock_tlx_region *rgn = &type->regions[idx];
          memmove_tlx(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));
          rgn->base = base;
          rgn->size = size;
          rgn->flags = flags;
          type->cnt++;
          type->total_size += size;
      } else {
        if (!*end_rgn)
          *start_rgn = i;
        *end_rgn = i + 1;
      }
    }
  for (i = _end_rgn - 1; i >= _start_rgn; i--) {
        unsigned long r = i;
        type->total_size -= type->regions[r].size;
        memmove_tlx(&type->regions[r], &type->regions[r + 1],
          (type->cnt - (r + 1)) * sizeof(type->regions[r]));
        type->cnt--;
        if (type->cnt == 0) {
          WARN_ON(type->total_size != 0);
          type->cnt = 1;
          type->regions[0].base = 0;
          type->regions[0].size = 0;
          type->regions[0].flags = 0;
        }
  }
  return 0;
}


int memblock_tlx_double_array_tlx(struct memblock_tlx_type *type,
						phys_addr_t new_area_start,
						phys_addr_t new_area_size)
{
	struct memblock_tlx_region *new_array, *old_array;
	phys_addr_t old_alloc_size, new_alloc_size;
	phys_addr_t old_size, new_size, addr;
	int use_slab = slab_state_tlx >= UP;
		int *in_slab;
	old_size = type->max * sizeof(struct memblock_tlx_region);
	new_size = old_size << 1;
	old_alloc_size = PAGE_ALIGN(old_size);
	new_alloc_size = PAGE_ALIGN(new_size);
	if (type == &memblock_tlx.memory)
		in_slab = &memblock_tlx_memory_in_slab_tlx;
	else
		in_slab = &memblock_tlx_reserved_in_slab_tlx;
	if (use_slab) {
		new_array = kmalloc_tlx(new_size, GFP_KERNEL);
		addr = new_array ? __pa(new_array) : 0;
	} else {
		if (type != &memblock_tlx.reserved)
			new_area_start = new_area_size = 0;
				phys_addr_t size = new_area_start + new_area_size;
				phys_addr_t align = memblock_tlx.current_limit;
				phys_addr_t start = new_alloc_size;
				phys_addr_t end = PAGE_SIZE;
				int ret;
				phys_addr_t kernel_end;
				if (end == MEMBLOCK_ALLOC_ACCESSIBLE)
					end = memblock_tlx.current_limit;
				start = max_t(phys_addr_t, start, PAGE_SIZE);
				end = max(start, end);
				kernel_end = __pa_symbol(_end);
					phys_addr_t this_start, this_end, cand;
					u64 i;
					for_each_free_mem_range_reverse(i, NUMA_NO_NODE, &this_start, &this_end, NULL) {
						this_start = clamp(this_start, start, end);
						this_end = clamp(this_end, start, end);

						if (this_end < size)
							continue;

						cand = round_down(this_end - size, align);
						if (cand >= this_start) {
							addr = cand;
							break;
						}
					}
		if (!addr && new_area_size) {
			phys_addr_t size = 0;
			phys_addr_t align = min(new_area_start, memblock_tlx.current_limit);
			phys_addr_t start = new_alloc_size;
			phys_addr_t end = PAGE_SIZE;
			int ret;
			phys_addr_t kernel_end;
			if (end == MEMBLOCK_ALLOC_ACCESSIBLE)
				end = memblock_tlx.current_limit;
			start = max_t(phys_addr_t, start, PAGE_SIZE);
			end = max(start, end);
			kernel_end = __pa_symbol(_end);
				phys_addr_t this_start, this_end, cand;
				u64 i;
				for_each_free_mem_range_reverse(i, NUMA_NO_NODE, &this_start, &this_end, NULL) {
					this_start = clamp(this_start, start, end);
					this_end = clamp(this_end, start, end);

					if (this_end < size)
						continue;

					cand = round_down(this_end - size, align);
					if (cand >= this_start) {
						addr = cand;
						break;
					}
				}
			}
		new_array = addr ? __va(addr) : NULL;
	}
	memcpy_tlx(new_array, type->regions, old_size);
	memset_tlx(new_array + type->max, 0, old_size);
	old_array = type->regions;
	type->regions = new_array;
	type->max <<= 1;
	if (*in_slab)
		kfree_tlx(old_array);
	else if (old_array != memblock_tlx_memory_init_regions_tlx &&
		old_array != memblock_tlx_reserved_init_regions_tlx)
		memblock_tlx_remove_range_tlx(&memblock_tlx.reserved, __pa(old_array), old_alloc_size);
	*in_slab = use_slab;

	return 0;
}


int memblock_reserve_region_tlx(phys_addr_t base,
							phys_addr_t size,
							int nid,
							unsigned long flags)
{
struct memblock_tlx_type *_rgn = &memblock_tlx.reserved;
	struct memblock_tlx_type *type = _rgn;
	bool insert = false;
	phys_addr_t obase = base;
	size = min(size, (phys_addr_t)ULLONG_MAX - base);
	phys_addr_t end = base + size;
	int i, nr_new;
	if (type->regions[0].size == 0) {
		type->regions[0].base = base;
		type->regions[0].size = size;
		type->regions[0].flags = flags;
		type->total_size = size;
		return 0;
	}
repeat:
	base = obase;
	nr_new = 0;
	for (i = 0; i < type->cnt; i++) {
		struct memblock_tlx_region *rgn = &type->regions[i];
		phys_addr_t rbase = rgn->base;
		phys_addr_t rend = rbase + rgn->size;
		if (rbase >= end)
			break;
		if (rend <= base)
			continue;
		if (rbase > base) {
			nr_new++;
			if (insert) {
					int idx = i++;
						phys_addr_t _size = rbase - base;
					struct memblock_tlx_region *rgn = &type->regions[idx];
					memmove_tlx(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));
					rgn->base = base;
					rgn->size = _size;
					rgn->flags = flags;
					type->cnt++;
					type->total_size += _size;
			}
		}
		base = min(rend, end);
	}
	if (base < end) {
		nr_new++;
		if (insert)
				{
					int idx = i;
						phys_addr_t _size = end - base;
					struct memblock_tlx_region *rgn = &type->regions[idx];
					memmove_tlx(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn));
					rgn->base = base;
					rgn->size = _size;
					rgn->flags = flags;
					type->cnt++;
					type->total_size += _size;

				}
	}
	if (!insert) {
		while (type->cnt + nr_new > type->max)
				memblock_tlx_double_array_tlx(type, obase, size);
		insert = true;
		goto repeat;
	} else {
		int i = 0;
		while (i < type->cnt - 1) {
			struct memblock_tlx_region *this = &type->regions[i];
			struct memblock_tlx_region *next = &type->regions[i + 1];
			if (this->base + this->size != next->base ||
					this->flags != next->flags) {
				i++;
				continue;
			}
			this->size += next->size;
			memmove_tlx(next, next + 1, (type->cnt - (i + 2)) * sizeof(*next));
			type->cnt--;
		}
		return 0;
	}
}


void * __init memblock_tlx_virt_alloc_internal_tlx(
				phys_addr_t size, phys_addr_t align,
				phys_addr_t min_addr, phys_addr_t max_addr,
				int nid)
{
	phys_addr_t alloc;
	void *ptr;

	if (!align)
		align = SMP_CACHE_BYTES;

	if (max_addr > memblock_tlx.current_limit)
		max_addr = memblock_tlx.current_limit;
		phys_addr_t start;
		phys_addr_t end;
again:
	start = min_addr;
	end = max_addr;
		int ret;
		phys_addr_t kernel_end;
		if (end == MEMBLOCK_ALLOC_ACCESSIBLE)
			end = memblock_tlx.current_limit;
		start = max_t(phys_addr_t, start, PAGE_SIZE);
		end = max(start, end);
		kernel_end = __pa_symbol(_end);
			phys_addr_t this_start, this_end, cand;
			u64 i;
			for_each_free_mem_range_reverse(i, nid, &this_start, &this_end, NULL) {
				this_start = clamp(this_start, start, end);
				this_end = clamp(this_end, start, end);

				if (this_end < size)
					continue;

				cand = round_down(this_end - size, align);
				if (cand >= this_start) {
					alloc = cand;
					goto done;
				}
			}
	if (min_addr) {
		min_addr = 0;
		goto again;
	} else {
		goto error;
	}

done:
	memblock_reserve_region_tlx(alloc, size, MAX_NUMNODES, 0);
	ptr = phys_to_virt_tlx(alloc);
	memset_tlx(ptr, 0, size);
	return ptr;
error:
	return NULL;
}


void __init memory_present_tlx(int nid, unsigned long start, unsigned long end)
{
	unsigned long pfn;

	start &= PAGE_SECTION_MASK;
//	mminit_validate_memmodel_limits(&start, &end);
	for (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {
		unsigned long section = pfn_to_section_nr(pfn);
		struct mem_section_tlx *ms;

//		sparse_index_init(section, nid);

			unsigned long root = SECTION_NR_TO_ROOT(section);
//      struct mem_section_tlx *section;
//      section = sparse_index_alloc(nid);
			struct mem_section_tlx *section_ = NULL;
			unsigned long array_size = SECTIONS_PER_ROOT *
							sizeof(struct mem_section_tlx);


				section_ = memblock_tlx_virt_alloc_internal_tlx(array_size, 0, BOOTMEM_LOW_LIMIT,
																						BOOTMEM_ALLOC_ACCESSIBLE, nid);
//				memblock_tlx_virt_alloc_node(array_size, nid);

			mem_section_tlx[root] = section_;

//		set_section_nid(section, nid);

		ms = __nr_to_section_tlx(section);
		if (!ms->section_mem_map)
			ms->section_mem_map = (nid << SECTION_NID_SHIFT) |
							SECTION_MARKED_PRESENT;
	}
}


void __init bootmem_init_tlx(void)
{
	unsigned long min, max;
   int idx = memblock_tlx.memory.cnt - 1;
	min = PFN_UP(memblock_tlx.memory.regions[0].base);
	max = PFN_DOWN((memblock_tlx.memory.regions[idx].base + memblock_tlx.memory.regions[idx].size));
	struct memblock_tlx_region *reg;

	for_each_memblock_tlx(memory, reg)
		memory_present_tlx(0, PFN_UP(reg->base),
					PFN_DOWN(reg->base + reg->size));

	unsigned long pnum;
	struct page *map;
	unsigned long *usemap;
	unsigned long **usemap_map;
	int size, size0;
	size = sizeof(unsigned long *) * NR_MEM_SECTIONS;
	usemap_map =
				memblock_tlx_virt_alloc_internal_tlx(strlen_tlx(boot_command_line) + 1, 0,BOOTMEM_LOW_LIMIT,
																							BOOTMEM_ALLOC_ACCESSIBLE,
																							NUMA_NO_NODE);
	void *data = (void *)usemap_map;
	unsigned long map_count;
	int nodeid_begin = 0;
	unsigned long pnum_begin = 0;

	for (pnum = 0; pnum < NR_MEM_SECTIONS; pnum++) {
		struct mem_section_tlx *ms;

		if (!(__nr_to_section_tlx(pnum) && (__nr_to_section_tlx(pnum)->section_mem_map & SECTION_MARKED_PRESENT)))
			continue;
		ms = __nr_to_section_tlx(pnum);
		nodeid_begin = ms->section_mem_map >> SECTION_NID_SHIFT;
		pnum_begin = pnum;
		break;
	}
	map_count = 1;
	for (pnum = pnum_begin + 1; pnum < NR_MEM_SECTIONS; pnum++) {
		struct mem_section_tlx *ms;
		int nodeid;

		if (!(__nr_to_section_tlx(pnum) && (__nr_to_section_tlx(pnum)->section_mem_map & SECTION_MARKED_PRESENT)))
			continue;
		ms = __nr_to_section_tlx(pnum);
		nodeid = ms->section_mem_map >> SECTION_NID_SHIFT;
		if (nodeid == nodeid_begin) {
			map_count++;
			continue;
		}
			void *usemap;
			unsigned long pnum0;
			unsigned long **usemap_map = (unsigned long **)data;
			size0  = roundup(SECTION_BLOCKFLAGS_BITS, 8) / 8;
			size0 = roundup(size0, sizeof(unsigned long));
			usemap =
				memblock_tlx_virt_alloc_internal_tlx(size * map_count, 0, BOOTMEM_LOW_LIMIT,
                                                     BOOTMEM_ALLOC_ACCESSIBLE,
                                                     NODE_DATA(nodeid_begin)->node_id);
			for (pnum0 = pnum_begin; pnum0 < pnum; pnum0++) {
				if (!(__nr_to_section_tlx(pnum0) && (__nr_to_section_tlx(pnum0)->section_mem_map & SECTION_MARKED_PRESENT)))
					continue;
				usemap_map[pnum0] = usemap;
				usemap += size0;
			}

		/* new start, update count etc*/
		nodeid_begin = nodeid;
		pnum_begin = pnum;
		map_count = 1;
	}
	/* ok, last chunk */
	unsigned long pnum0;
	usemap_map = (unsigned long **)data;
	usemap =
				memblock_tlx_virt_alloc_internal_tlx(size * map_count, 0, BOOTMEM_LOW_LIMIT,
																										BOOTMEM_ALLOC_ACCESSIBLE,
																										NODE_DATA(nodeid_begin)->node_id);
	for (pnum0 = pnum_begin; pnum0 < NR_MEM_SECTIONS; pnum0++) {
		if (!(__nr_to_section_tlx(pnum0) && (__nr_to_section_tlx(pnum0)->section_mem_map & SECTION_MARKED_PRESENT)))
			continue;
		usemap_map[pnum0] = usemap;
		usemap += size0;
	}

for (pnum = 0; pnum < NR_MEM_SECTIONS; pnum++) {
	if (!(__nr_to_section_tlx(pnum) && (__nr_to_section_tlx(pnum)->section_mem_map & SECTION_MARKED_PRESENT)))
		continue;

	usemap = usemap_map[pnum];
	if (!usemap)
		continue;
		struct mem_section_tlx *ms;
		ms = __nr_to_section_tlx(pnum);
		int nid = (ms->section_mem_map >> SECTION_NID_SHIFT);
		map = alloc_remap_tlx(nid, sizeof(struct page) * PAGES_PER_SECTION);
		if (!map) {
			size = PAGE_ALIGN(sizeof(struct page) * PAGES_PER_SECTION);
			phys_addr_t align = PAGE_SIZE;
			phys_addr_t min_addr = __virt_to_phys_tlx((unsigned long)(MAX_DMA_ADDRESS));
			phys_addr_t max_addr = BOOTMEM_ALLOC_ACCESSIBLE;
			map = memblock_tlx_virt_alloc_internal_tlx(size, align,
																					min_addr, max_addr, nid);
		}

	unsigned long *pageblock_bitmap = usemap;
		ms->section_mem_map &= ~SECTION_MAP_MASK;
		ms->section_mem_map |= (unsigned long)(map - ((pnum) << PFN_SECTION_SHIFT)) |
								SECTION_HAS_MEM_MAP;
		ms->pageblock_flags = usemap;
	}
	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
	unsigned long max_dma = min;

	memset_tlx(zone_size, 0, sizeof(zone_size));

	/* 4GB maximum for 32-bit only capable devices */
	if (IS_ENABLED(CONFIG_ZONE_DMA)) {
		#define GENMASK_ULL(h, l)       (((U64_C(1) << ((h) - (l) + 1)) - 1) << (l))
		phys_addr_t offset = memblock_tlx.memory.regions[0].base & GENMASK_ULL(63, 32);
		int idx = memblock_tlx.memory.cnt - 1;
		max_dma = PFN_DOWN(min(offset + (1ULL << 32), (memblock_tlx.memory.regions[idx].base + memblock_tlx.memory.regions[idx].size)));
		zone_size[ZONE_DMA] = max_dma - min;
	}
	zone_size[ZONE_NORMAL] = max - max_dma;

	memcpy_tlx(zhole_size, zone_size, sizeof(zhole_size));

	for_each_memblock_tlx(memory, reg) {
		unsigned long start = PFN_UP(reg->base);
		unsigned long end = PFN_DOWN(reg->base + reg->size);

		if (start >= max)
			continue;

		if (IS_ENABLED(CONFIG_ZONE_DMA) && start < max_dma) {
			unsigned long dma_end = min(end, max_dma);
			zhole_size[ZONE_DMA] -= dma_end - start;
		}

		if (end > max_dma) {
			unsigned long normal_end = min(end, max);
			unsigned long normal_start = max(start, max_dma);
			zhole_size[ZONE_NORMAL] -= normal_end - normal_start;
		}
	}

	free_area_init_node_tlx(0, zone_size, min, zhole_size);

	high_memory_tlx = __va((max << PAGE_SHIFT) - 1) + 1;
	max_pfn_tlx = max_low_pfn_tlx = max;
}


DECLARE_BITMAP(cpu_possible_bits_tlx, CONFIG_NR_CPUS) ;
DECLARE_BITMAP(cpu_online_bits_tlx, CONFIG_NR_CPUS) ;
DECLARE_BITMAP(cpu_present_bits_tlx, CONFIG_NR_CPUS) ;
DECLARE_BITMAP(cpu_active_bits_tlx, CONFIG_NR_CPUS) ;

void *initial_boot_params_tlx;

__initdata unsigned long mhash_entries_tlx;


struct kernfs_root_tlx *sysfs_root_tlx;


struct file_system_type rootfs_fs_type_tlx = {
         .name           = "rootfs",
         .mount          = NULL,
       .kill_sb        = NULL,
};

struct kmem_cache *shmem_inode_cachep_tlx;


static void shmem_init_inode_tlx(void *foo)
{
         struct shmem_inode_info *info = foo;
				 struct inode *inode = &info->vfs_inode;
					memset_tlx(inode, 0, sizeof(*inode));
					INIT_HLIST_NODE(&inode->i_hash);
					INIT_LIST_HEAD(&inode->i_devices);
					INIT_LIST_HEAD(&inode->i_wb_list);
					INIT_LIST_HEAD(&inode->i_lru);
				//	address_space_init_once_tlx(&inode->i_data);
					struct address_space *mapping = &inode->i_data;
								memset_tlx(mapping, 0, sizeof(*mapping));
								INIT_RADIX_TREE(&mapping->page_tree, GFP_ATOMIC);
								spin_lock_init(&mapping->tree_lock);
								mutex_init(&mapping->i_mmap_mutex);
								INIT_LIST_HEAD(&mapping->private_list);
								spin_lock_init(&mapping->private_lock);
								mapping->i_mmap = RB_ROOT;
								INIT_LIST_HEAD(&mapping->i_mmap_nonlinear);
}

struct hlist_head *mountpoint_hashtable_tlx ;
__initdata unsigned long mphash_entries_tlx;

bool is_tmpfs_tlx;

#define MNT_NOSUID      0x01
#define MNT_WRITE_HOLD  0x200
#define MNT_READONLY    0x40    /* does the user want this to be r/o? */
#define MNT_MARKED              0x4000000
#define MNT_LOCK_READONLY       0x400000
#define MNT_LOCKED              0x800000
#define MNT_SHARED      0x1000  /* if the vfsmount is a shared mount */
#define MNT_SHARED_MASK (MNT_UNBINDABLE)
#define MNT_UNBINDABLE  0x2000  /* if the vfsmount is a unbindable mount */
#define MNT_SHRINKABLE  0x100
#define MNT_SYNC_UMOUNT         0x2000000
#define MNT_INTERNAL    0x4000
#define MNT_DOOMED              0x1000000


#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \
                              MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)

//int mnt_want_write(struct vfsmount *m);



struct fs_struct {
         int users;
          spinlock_t lock;
          seqcount_t seq;
          int umask;
          int in_exec;
          struct path root, pwd;
 };

 # define SEQCOUNT_DEP_MAP_INIT(lockname)
 #define SEQCNT_ZERO(lockname) { .sequence = 0, SEQCOUNT_DEP_MAP_INIT(lockname)}

struct fs_struct init_fs_tlx
  = {
           .users          = 1,
           .lock           = __SPIN_LOCK_UNLOCKED(init_fs.lock),
           .seq            = SEQCNT_ZERO(init_fs.seq),
           .umask          = 0022,
  };

struct file_system_type *file_systems_tlx;

struct percpu_counter vm_committed_as_tlx ____cacheline_aligned_in_smp;
unsigned int pcpu_low_unit_cpu_tlx ;
unsigned int pcpu_high_unit_cpu_tlx ;
int pcpu_nr_units_tlx ;
int pcpu_nr_groups_tlx ;
unsigned long *pcpu_group_offsets_tlx ;
size_t *pcpu_group_sizes_tlx ;
const int *pcpu_unit_map_tlx ;		/* cpu -> unit */
int pcpu_unit_pages_tlx ;
int pcpu_unit_size_tlx ;
int pcpu_atom_size_tlx ;
size_t pcpu_chunk_struct_size_tlx ;
int pcpu_nr_slots_tlx ;
int pcpu_reserved_chunk_tlx_limit_tlx;
struct list_head *pcpu_slot_tlx ; /* chunk list slots */
#define PCPU_SLOT_BASE_SHIFT		5


struct list_head clk_provider_list_tlx = LIST_HEAD_INIT(clk_provider_list_tlx);

typedef void (*of_clk_init_cb_t)(struct device_node *);

struct clock_provider {
				of_clk_init_cb_t clk_init_cb;
				struct device_node *np;
				struct list_head node;
};

typedef u32 phandle;
struct property {
				char    *name;
				int     length;
				void    *value;
				struct property *next;
				unsigned long _flags;
				unsigned int unique_id;
				struct bin_attribute attr;
};

struct device_node {
	const char *name;
	const char *type;
	phandle phandle;
	const char *full_name;

	struct	property *properties;
	struct	property *deadprops;	/* removed properties */
	struct	device_node *parent;
	struct	device_node *child;
	struct	device_node *sibling;
	struct	device_node *next;	/* next device of same type */
	struct	device_node *allnext;	/* next in list of all nodes */
	struct	kobject kobj;
	unsigned long _flags;
	void	*data;
#if defined(CONFIG_SPARC)
	const char *path_component_name;
	unsigned int unique_id;
	struct of_irq_controller *irq_trans;
#endif
};
struct device_node *of_allnodes_tlx;




static struct resource iomem_resource; /* = {
 37         .name   = "PCI mem",
 38         .start  = 0,
 39         .end    = -1,
 40         .flags  = IORESOURCE_MEM,
 41 };
 */




static inline resource_size_t resource_size(const struct resource *res)
 {
         return res->end - res->start + 1;
 }

struct device_driver;

struct bus_type {
	const char		*name;
	const char		*dev_name;
	struct device		*dev_root;
	struct device_attribute	*dev_attrs;	/* use dev_groups instead */
	const struct attribute_group **bus_groups;
	const struct attribute_group **dev_groups;
	const struct attribute_group **drv_groups;

	int (*match)(struct device *dev, struct device_driver *drv);
	int (*uevent)(struct device *dev, struct kobj_uevent_env *env);
	int (*probe)(struct device *dev);
	int (*remove)(struct device *dev);
	void (*shutdown)(struct device *dev);

	int (*online)(struct device *dev);
	int (*offline)(struct device *dev);

	int (*suspend)(struct device *dev, pm_message_t state);
	int (*resume)(struct device *dev);

	const struct dev_pm_ops *pm;

	struct iommu_ops *iommu_ops;

	struct subsys_private *p;
	struct lock_class_key lock_key;
};



struct device {
	struct device		*parent;

	struct device_private	*p;

	struct kobject kobj;
	const char		*init_name; /* initial name of the device */
	const struct device_type *type;

	struct mutex		mutex;	/* mutex to synchronize calls to
					 * its driver.
					 */

	struct bus_type	*bus;		/* type of bus device is on */
	struct device_driver *driver;	/* which driver has allocated this
					   device */
	void		*platform_data;	/* Platform specific data, device
					   core doesn't touch it */
	void		*driver_data;	/* Driver data, set and get with
					   dev_set/get_drvdata */
	struct dev_pm_info	power;
	struct dev_pm_domain	*pm_domain;

#ifdef CONFIG_PINCTRL
	struct dev_pin_info	*pins;
#endif

#ifdef CONFIG_NUMA
	int		numa_node;	/* NUMA node this device is close to */
#endif
	u64		*dma_mask;	/* dma mask (if dma'able device) */
	u64		coherent_dma_mask;/* Like dma_mask, but for
					     alloc_coherent mappings as
					     not all hardware supports
					     64 bit addresses for consistent
					     allocations such descriptors. */
	unsigned long	dma_pfn_offset;

	struct device_dma_parameters *dma_parms;

	struct list_head	dma_pools;	/* dma pools (if dma'ble) */

	struct dma_coherent_mem	*dma_mem; /* internal for coherent mem
					     override */
#ifdef CONFIG_DMA_CMA
	struct cma *cma_area;		/* contiguous memory area for dma
					   allocations */
#endif
	/* arch specific additions */
	struct dev_archdata	archdata;

	struct device_node	*of_node; /* associated device tree node */
	struct acpi_dev_node	acpi_node; /* associated ACPI device node */

	dev_t			devt;	/* dev_t, creates the sysfs "dev" */
	u32			id;	/* device instance */

	spinlock_t		devres_lock;
	struct list_head	devres_head;

	struct klist_node	knode_class;
	struct class		*class;
	const struct attribute_group **groups;	/* optional groups */

	void	(*release)(struct device *dev);
	struct iommu_group	*iommu_group;

	bool			offline_disabled:1;
	bool			offline:1;
};

struct device_attribute {
       struct attribute        attr;
       ssize_t (*show)(struct device *dev, struct device_attribute *attr,
                         char *buf);
       ssize_t (*store)(struct device *dev, struct device_attribute *attr,
                          const char *buf, size_t count);
};

int device_create_file_tlx(struct device *dev,
		       const struct device_attribute *attr)
{
	int error = 0;

	if (dev) {
		error = sysfs_add_file_mode_tlx((&dev->kobj)->sd, attr, false, (&attr->attr)->mode, NULL);
		//sysfs_create_file(&dev->kobj, &attr->attr);
	}

	return error;
}

int dev_set_name_tlx(struct device *dev, const char *fmt, ...)
{
	va_list vargs;
	int err;

	va_start(vargs, fmt);
	err = kobject_set_name_vargs_tlx(&dev->kobj, fmt, vargs);
	va_end(vargs);
	return err;
}

struct resource * __request_resource_tlx(struct resource *root, struct resource *new)
{
	resource_size_t start = new->start;
	resource_size_t end = new->end;
	struct resource *tmp, **p;

	if (end < start)
		return root;
	if (start < root->start)
		return root;
	if (end > root->end)
		return root;
	p = &root->child;
	for (;;) {
		tmp = *p;
		if (!tmp || tmp->start > end) {
			new->sibling = tmp;
			*p = new;
			new->parent = root;
			return NULL;
		}
		p = &tmp->sibling;
		if (tmp->end < start)
			continue;
		return tmp;
	}
}




struct device_driver {
	const char		*name;
	struct bus_type		*bus;

	struct module		*owner;
	const char		*mod_name;	/* used for built-in modules */

	bool suppress_bind_attrs;	/* disables bind/unbind via sysfs */

	const struct of_device_id	*of_match_table;
	const struct acpi_device_id	*acpi_match_table;

	int (*probe) (struct device *dev);
	int (*remove) (struct device *dev);
	void (*shutdown) (struct device *dev);
	int (*suspend) (struct device *dev, pm_message_t state);
	int (*resume) (struct device *dev);
	const struct attribute_group **groups;

	const struct dev_pm_ops *pm;

	struct driver_private *p;
};




static inline const char *dev_name(const struct device *dev)
{
         if (dev->init_name)
                 return dev->init_name;

         return kobject_name_tlx(&dev->kobj);
}




void klist_add_tail_tlx(struct klist_node *n, struct klist *k)
 {
				 INIT_LIST_HEAD(&n->n_node);
  			 atomic_set(&(&n->n_ref)->refcount, 1);
				 n->n_klist = k;
         if (k->get)
                 k->get(n);
         list_add_tail(&n->n_node, &k->k_list);
 }

void klist_init_tlx(struct klist *k, void (*get)(struct klist_node *),
                 void (*put)(struct klist_node *))
 {
         INIT_LIST_HEAD(&k->k_list);
         spin_lock_init(&k->k_lock);
         k->get = get;
         k->put = put;
 }



struct driver_private {
	struct kobject kobj;
	struct klist klist_devices;
	struct klist_node knode_bus;
	struct module_kobject *mkobj;
	struct device_driver *driver;
};




#define IORESOURCE_MEM          0x00000200
 static inline void set_dev_node(struct device *dev, int node)
 {
//         dev->numa_node = node;
 }

char *of_prop_next_string_tlx(struct property *prop, const char *cur)
{
	const void *curv = cur;

	if (!prop)
		return NULL;

	if (!cur)
		return prop->value;

	curv += strlen_tlx(cur) + 1;
	if (curv >= prop->value + prop->length)
		return NULL;

	return curv;
}

struct device_node *of_find_matching_node_and_match_tlx(struct device_node *from,
					const struct of_device_id_tlx *matches,
					const struct of_device_id_tlx **match)
{
	struct device_node *np;
	const struct of_device_id_tlx *m;
	unsigned long flags;

	if (match)
		*match = NULL;
	np = from ? from->allnext : of_allnodes_tlx;
	for (; np; np = np->allnext) {
		const struct of_device_id_tlx *matches0 = matches;
		const struct device_node *node = np;
			const struct of_device_id_tlx *best_match = NULL;
			int score = 0, best_score = 0;
			if (!matches0) {
					m = NULL;
					goto out;
			}
			for (; matches0->name[0] || matches0->type[0] || matches0->compatible[0]; matches0++) {
				const struct device_node *device = node;
				const char *compat = matches0->compatible;
				const char *type = matches0->type;
				const char *name = matches0->name;
					struct property *prop;
					const char *cp;
					int index = 0;
					if (compat && compat[0]) {
							struct property *pp;
							for (pp = device->properties; pp; pp = pp->next) {
								if (strcmp_tlx(pp->name, "compatible") == 0) {
									break;
								}
							}
						prop = pp;
						for (cp = of_prop_next_string_tlx(prop, NULL); cp;
								cp = of_prop_next_string_tlx(prop, cp), index++) {
							if (strcasecmp_tlx(cp, compat) == 0) {
								score = INT_MAX/2 - (index << 2);
								break;
							}
						}
						if (!score)
							goto out2;
					}
					if (type && type[0]) {
						if (!device->type ||  strcasecmp_tlx(type, device->type))
							goto out2;
						score += 2;
					}
					if (name && name[0]) {
						if (!device->name ||  strcasecmp_tlx(name, device->name))
							return 0;
						score++;
					}
out2:
				if (score > best_score) {
					best_match = matches0;
					best_score = score;
				}
			}

		m = best_match;
out:
		if (m &&  kobject_get_tlx(&np->kobj)) {
			if (match)
				*match = m;
			break;
		}
	}
	return np;
}


struct of_device_id_tlx __clksrc_of_table_tlx[];
typedef void (*of_init_fn_1)(struct device_node *);

#define FDT_TAGSIZE     sizeof(uint32_t)
#define FDT_BEGIN_NODE  0x1
#define FDT_ERR_BADOFFSET       4
#define FDT_TAGSIZE     sizeof(uint32_t)
#define FDT_PROP        0x3
#define FDT_END_NODE    0x2
#define FDT_MAGIC       0xd00dfeed      /* 4: version, 4: total size */
#define FDT_ERR_TRUNCATED       8
#define FDT_END         0x9
#define FDT_ERR_BADSTRUCTURE    11
#define FDT_NOP         0x4
#define FDT_ERR_NOTFOUND        1

struct fdt_header {
					uint32_t magic;                  /* magic word FDT_MAGIC */
					uint32_t totalsize;              /* total size of DT block */
					uint32_t off_dt_struct;          /* offset to structure */
					uint32_t off_dt_strings;         /* offset to strings */
					uint32_t off_mem_rsvmap;         /* offset to memory reserve map */
					uint32_t version;                /* format version */
					uint32_t last_comp_version;      /* last compatible version */

					/* version 2 fields below */
					uint32_t boot_cpuid_phys;        /* Which physical CPU id we're
																							booting on */
					/* version 3 fields below */
					uint32_t size_dt_strings;        /* size of the strings block */

					/* version 17 fields below */
					uint32_t size_dt_struct;         /* size of the structure block */
};
struct fdt_node_header {
				uint32_t tag;
				char name[0];
};

struct kobj_type of_node_ktype_tlx = {};
// = {
//         .release = of_node_release,
//};

const void *of_get_property_tlx(const struct device_node *np, const char *name,
					int *lenp)
{
	struct property *pp;
	for (pp = np->properties; pp; pp = pp->next) {
				if (strcmp_tlx(pp->name, name) == 0) {
							if (lenp)
										*lenp = pp->length;
											break;
							}
				}
	return pp ? pp->value : NULL;
}

#define FDT_ERR_INTERNAL        13
#define EXTRACT_BYTE(n) ((unsigned long long)((uint8_t *)&x)[n])

static inline uint32_t fdt32_to_cpu(uint32_t x)
{
				return (EXTRACT_BYTE(0) << 24) | (EXTRACT_BYTE(1) << 16) | (EXTRACT_BYTE(2) << 8) | EXTRACT_BYTE(3);
}

struct fdt_property {
				uint32_t tag;
				uint32_t len;
				uint32_t nameoff;
				char data[0];
};


#define FDT_ALIGN(x, a)         (((x) + (a) - 1) & ~((a) - 1))
#define FDT_TAGALIGN(x)         (FDT_ALIGN((x), FDT_TAGSIZE))

const void *fdt_offset_ptr_tlx(const void *fdt, int offset, unsigned int len)
{
				const char *p;
				p = (const char *)fdt + fdt32_to_cpu(((const struct fdt_header *)(fdt))->off_dt_struct) + offset;
				if (p + len < p)
									return NULL;
				return p;
}

uint32_t fdt_next_tag_tlx(const void *fdt, int startoffset, int *nextoffset)
{
	const uint32_t *tagp, *lenp;
	uint32_t tag;
	int offset = startoffset;
	const char *p;

	*nextoffset = -FDT_ERR_TRUNCATED;
	tagp = fdt_offset_ptr_tlx(fdt, offset, FDT_TAGSIZE);
	if (!tagp)
		return FDT_END; /* premature end */
	tag = fdt32_to_cpu(*tagp);
	offset += FDT_TAGSIZE;

	*nextoffset = -FDT_ERR_BADSTRUCTURE;
	switch (tag) {
	case FDT_BEGIN_NODE:
		/* skip name */
		do {
			p = fdt_offset_ptr_tlx(fdt, offset++, 1);
		} while (p && (*p != '\0'));
		if (!p)
			return FDT_END; /* premature end */
		break;

	case FDT_PROP:
		lenp = fdt_offset_ptr_tlx(fdt, offset, sizeof(*lenp));
		if (!lenp)
			return FDT_END; /* premature end */
		/* skip-name offset, length and value */
		offset += sizeof(struct fdt_property) - FDT_TAGSIZE
			+ fdt32_to_cpu(*lenp);
		break;

	case FDT_END:
	case FDT_END_NODE:
	case FDT_NOP:
		break;

	default:
		return FDT_END;
	}

	if (!fdt_offset_ptr_tlx(fdt, startoffset, offset - startoffset))
		return FDT_END; /* premature end */

	*nextoffset = FDT_TAGALIGN(offset);
	return tag;
}


int fdt_first_property_offset_tlx_tlx(const void *fdt, int nodeoffset)
{
	int offset;

	if ((nodeoffset < 0) || (nodeoffset % FDT_TAGSIZE)
			|| (fdt_next_tag_tlx(fdt, nodeoffset, &nodeoffset) != FDT_BEGIN_NODE))
		return -FDT_ERR_BADOFFSET;
		offset = nodeoffset;
		uint32_t tag;
		int nextoffset;
		do {
			tag = fdt_next_tag_tlx(fdt, offset, &nextoffset);
			switch (tag) {
			case FDT_END:
				if (nextoffset >= 0)
					return -FDT_ERR_BADSTRUCTURE;
				else
					return nextoffset;
			case FDT_PROP:
				return offset;
			}
			offset = nextoffset;
		} while (tag == FDT_NOP);
		return -FDT_ERR_NOTFOUND;

}

int fdt_next_property_offset_tlx_tlx(const void *fdt, int offset)
{
	if ((offset < 0) || (offset % FDT_TAGSIZE)
			|| (fdt_next_tag_tlx(fdt, offset, &offset) != FDT_PROP))
		return -FDT_ERR_BADOFFSET;

		uint32_t tag;
		int nextoffset;
		do {
			tag = fdt_next_tag_tlx(fdt, offset, &nextoffset);
			switch (tag) {
			case FDT_END:
				if (nextoffset >= 0)
					return -FDT_ERR_BADSTRUCTURE;
				else
					return nextoffset;
			case FDT_PROP:
				return offset;
			}
			offset = nextoffset;
		} while (tag == FDT_NOP);
		return -FDT_ERR_NOTFOUND;
}


int fdt_next_node_tlx_tlx(const void *fdt, int offset, int *depth)
{
	int nextoffset = 0;
	uint32_t tag;
	if (offset >= 0) {
		int *nextoffset0 = &offset;
		int offset0 = offset;
		const char *p;
		offset0 += FDT_TAGSIZE;
	do {
				p = fdt_offset_ptr_tlx(fdt, offset0++, 1);
	} while (p && (*p != '\0'));
	*nextoffset0 = FDT_TAGALIGN(offset0);
	nextoffset = offset;
};

do {
		offset = nextoffset;
		int startoffset =  offset;
		int *nextoffset0 = &nextoffset;
			const uint32_t *tagp, *lenp;
			int offset0 = startoffset;
			const char *p;

			*nextoffset0 = -FDT_ERR_TRUNCATED;
			tagp = fdt_offset_ptr_tlx(fdt, offset0, FDT_TAGSIZE);
			if (!tagp)
				return FDT_END; /* premature end */
			tag = fdt32_to_cpu(*tagp);
			offset0 += FDT_TAGSIZE;

			*nextoffset0 = -FDT_ERR_BADSTRUCTURE;
			switch (tag) {
			case FDT_BEGIN_NODE:
				/* skip name */
				do {
					p = fdt_offset_ptr_tlx(fdt, offset0++, 1);
				} while (p && (*p != '\0'));
				if (!p)
					tag = FDT_END; /* premature end */
				break;

			case FDT_PROP:
				lenp = fdt_offset_ptr_tlx(fdt, offset0, sizeof(*lenp));
				if (!lenp) {
					tag =  FDT_END;
					break;
				}; /* premature end */
				/* skip-name offset, length and value */
				offset0 += sizeof(struct fdt_property) - FDT_TAGSIZE
					+ fdt32_to_cpu(*lenp);
				break;

			case FDT_END:
			case FDT_END_NODE:
			case FDT_NOP:
				break;

			default:
				tag = FDT_END;
			}

			if (!fdt_offset_ptr_tlx(fdt, startoffset, offset0 - startoffset))
				tag =  FDT_END; /* premature end */

			*nextoffset0 = FDT_TAGALIGN(offset0);

		switch (tag) {
		case FDT_PROP:
		case FDT_NOP:
			break;

		case FDT_BEGIN_NODE:
			if (depth)
				(*depth)++;
			break;

		case FDT_END_NODE:
			if (depth && ((--(*depth)) < 0))
				return nextoffset;
			break;

		case FDT_END:
			if ((nextoffset >= 0)
					|| ((nextoffset == -FDT_ERR_TRUNCATED) && !depth))
				return -FDT_ERR_NOTFOUND;
			else
				return nextoffset;
		}
	} while (tag != FDT_BEGIN_NODE);

	return offset;
}

void * unflatten_dt_node_tlx(void *blob,
				void *mem,
				int *poffset,
				struct device_node *dad,
				struct device_node ***allnextpp,
				unsigned long fpsize)
{
	const __be32 *p;
	struct device_node *np;
	struct property *pp, **prev_pp = NULL;
	const char *pathp = NULL;
	unsigned int l, allocl;
	static int depth = 0;
	int old_depth;
	int offset;
	int has_name = 0;
	int new_format = 0;
	const void *fdt = blob;
	int nodeoffset = *poffset;
	int *len = &l;
	const struct fdt_node_header *nh = (const char *)fdt + fdt32_to_cpu(((const struct fdt_header *)(fdt))->off_dt_struct) + nodeoffset;
	if (fdt32_to_cpu(((const struct fdt_header *)(fdt))->magic) != FDT_MAGIC)
				goto fail;
	*len = strlen_tlx(nh->name);
	pathp = nh->name;
	allocl = l++;
	if ((*pathp) != '/') {
		new_format = 1;
		if (fpsize == 0) {
			fpsize = 1;
			allocl = 2;
			l = 1;
			pathp = "";
		} else {
			fpsize += l;
			allocl = fpsize;
		}
	}
	void **mem0 = &mem;
	*mem0 = PTR_ALIGN(*mem0, __alignof__(struct device_node));
	np = *mem0;
	*mem0 += sizeof(struct device_node) + allocl;
	if (allnextpp) {
		char *fn;
		kobject_init_tlx(&np->kobj, &of_node_ktype_tlx);
		np->full_name = fn = ((char *)np) + sizeof(*np);
		if (new_format) {
			if (dad && dad->parent) {
				strcpy_tlx(fn, dad->full_name);
				fn += strlen_tlx(fn);
			}
			*(fn++) = '/';
		}
		memcpy_tlx(fn, pathp, l);

		prev_pp = &np->properties;
		**allnextpp = np;
		*allnextpp = &np->allnext;
		if (dad != NULL) {
			np->parent = dad;
			if (dad->next == NULL)
				dad->child = np;
			else
				dad->next->sibling = np;
			dad->next = np;
		}
	}
	for (offset = fdt_first_property_offset_tlx_tlx(blob, *poffset);
			(offset >= 0);
			(offset = fdt_next_property_offset_tlx_tlx(blob, offset))) {
		const char *pname;
		u32 sz;

		const struct fdt_property *prop;
			prop = (const char *)blob +(fdt32_to_cpu(((const struct fdt_header *)(blob))->off_dt_struct)) + offset;
      *(&sz) = fdt32_to_cpu(prop->len);
      *(&pname) = (const char *)fdt + (fdt32_to_cpu(((const struct fdt_header *)(blob))->off_dt_strings)) +  fdt32_to_cpu(prop->nameoff);
 		p = prop->data;
		if (strcmp_tlx(pname, "name") == 0)
			has_name = 1;
		void **mem0 = &mem;
		*mem0 = PTR_ALIGN(*mem0, __alignof__(struct property));
		pp = *mem0;
		*mem0 += sizeof(struct property) + sz;
		if (allnextpp) {
			if ((strcmp_tlx(pname, "phandle") == 0) ||
					(strcmp_tlx(pname, "linux,phandle") == 0)) {
				if (np->phandle == 0)
					np->phandle = be32_to_cpup(p);
			}
			if (strcmp_tlx(pname, "ibm,phandle") == 0)
				np->phandle = be32_to_cpup(p);
			pp->name = (char *)pname;
			pp->length = sz;
			pp->value = (__be32 *)p;
			*prev_pp = pp;
			prev_pp = &pp->next;
		}
	}
	if (!has_name) {
		const char *p1 = pathp, *ps = pathp, *pa = NULL;
		int sz;
		while (*p1) {
			if ((*p1) == '@')
				pa = p1;
			if ((*p1) == '/')
				ps = p1 + 1;
			p1++;
		}
		if (pa < ps)
			pa = p1;
		sz = (pa - ps) + 1;
		void **mem0 = &mem;
		*mem0 = PTR_ALIGN(*mem0, __alignof__(struct property));
		pp = *mem0;
		*mem0 += sizeof(struct property) + sz;
	}
	if (allnextpp) {
		*prev_pp = NULL;
      for (pp = np->properties; pp; pp = pp->next)
                 if (strcmp_tlx(pp->name, "name")) break;
			np->name = pp;
			for (pp = np->properties; pp; pp = pp->next)
								if (strcmp_tlx(pp->name, "device_type")) break;
		  np->type = pp;
		if (!np->name)
			np->name = "<NULL>";
		if (!np->type)
			np->type = "<NULL>";
	}
	old_depth = depth;
	*poffset = fdt_next_node_tlx_tlx(blob, *poffset, &depth);
	if (depth < 0)
		depth = 0;
	while (*poffset > 0 && depth > old_depth)
		mem = unflatten_dt_node_tlx(blob, mem, poffset, np, allnextpp,
					fpsize);
fail:
	return mem;
}

struct softirq_action softirq_vec_tlx[NR_SOFTIRQS] __cacheline_aligned_in_smp;

void set_bit_tlx(int nr, volatile unsigned long *addr)
{
         unsigned long mask = BIT_MASK(nr);
         unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);
         unsigned long flags;
          *p  |= mask;
}

/*
#  define RCU_NUM_LVLS        1
#  define NUM_RCU_LVL_0       1
#  define NUM_RCU_LVL_1       (NR_CPUS)
#  define NUM_RCU_LVL_2       0
#  define NUM_RCU_LVL_3       0
#  define NUM_RCU_LVL_4       0

int rcu_num_lvls_tlx_tlx;


static int num_rcu_lvl_tlx_tlx[]
				NUM_RCU_LVL_0,
				NUM_RCU_LVL_1,
				NUM_RCU_LVL_2,
				NUM_RCU_LVL_3,
				NUM_RCU_LVL_4,
};
*/



#define MAX_RCU_LVLS 4
#define RCU_FANOUT_1          (CONFIG_RCU_FANOUT_LEAF)
#define RCU_FANOUT_2          (RCU_FANOUT_1 * CONFIG_RCU_FANOUT)
#define RCU_FANOUT_3          (RCU_FANOUT_2 * CONFIG_RCU_FANOUT)
#define RCU_FANOUT_4          (RCU_FANOUT_3 * CONFIG_RCU_FANOUT)

#if NR_CPUS <= RCU_FANOUT_1
#  define RCU_NUM_LVLS        1
#  define NUM_RCU_LVL_0       1
#  define NUM_RCU_LVL_1       (NR_CPUS)
#  define NUM_RCU_LVL_2       0
#  define NUM_RCU_LVL_3       0
#  define NUM_RCU_LVL_4       0
#elif NR_CPUS <= RCU_FANOUT_2
#  define RCU_NUM_LVLS        2
#  define NUM_RCU_LVL_0       1
#  define NUM_RCU_LVL_1       DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
#  define NUM_RCU_LVL_2       (NR_CPUS)
#  define NUM_RCU_LVL_3       0
#  define NUM_RCU_LVL_4       0
#elif NR_CPUS <= RCU_FANOUT_3
#  define RCU_NUM_LVLS        3
#  define NUM_RCU_LVL_0       1
#  define NUM_RCU_LVL_1       DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
#  define NUM_RCU_LVL_2       DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
#  define NUM_RCU_LVL_3       (NR_CPUS)
#  define NUM_RCU_LVL_4       0
#elif NR_CPUS <= RCU_FANOUT_4
#  define RCU_NUM_LVLS        4
#  define NUM_RCU_LVL_0       1
#  define NUM_RCU_LVL_1       DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_3)
#  define NUM_RCU_LVL_2       DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
#  define NUM_RCU_LVL_3       DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
#  define NUM_RCU_LVL_4       (NR_CPUS)
#else
# error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
#endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */

int rcu_num_lvls_tlx = RCU_NUM_LVLS;


int num_rcu_lvl_tlx[] = {  /* Number of rcu_nodes at specified level. */
         NUM_RCU_LVL_0,
         NUM_RCU_LVL_1,
         NUM_RCU_LVL_2,
         NUM_RCU_LVL_3,
         NUM_RCU_LVL_4,
};

struct lock_class_key rcu_node_class_tlx[RCU_NUM_LVLS];
struct lock_class_key rcu_fqs_class_tlx[RCU_NUM_LVLS];



struct rcu_node {
	raw_spinlock_t lock;	/* Root rcu_node's lock protects some */
	unsigned long gpnum;	/* Current grace period for this node. */
	unsigned long completed; /* Last GP completed for this node. */
	unsigned long qsmask;	/* CPUs or groups that need to switch in */
	unsigned long expmask;	/* Groups that have ->blkd_tasks */
	unsigned long qsmaskinit;
	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
	int	grplo;		/* lowest-numbered CPU or group here. */
	int	grphi;		/* highest-numbered CPU or group here. */
	u8	grpnum;		/* CPU/group number for next level up. */
	u8	level;		/* root is at level 0. */
	struct rcu_node *parent;
	struct list_head blkd_tasks;
	struct list_head *gp_tasks;
	struct list_head *exp_tasks;
	int need_future_gp[2];
	raw_spinlock_t fqslock ____cacheline_internodealigned_in_smp;
} ____cacheline_internodealigned_in_smp;

struct rcu_state {
	struct rcu_node node[NUM_RCU_NODES];	/* Hierarchy. */
	struct rcu_node *level[RCU_NUM_LVLS];	/* Hierarchy levels. */
	u32 levelcnt[MAX_RCU_LVLS + 1];		/* # nodes in each level. */
	u8 levelspread[RCU_NUM_LVLS];		/* kids/node in each level. */
	u8 flavor_mask;				/* bit in flavor mask. */
	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
	void (*call)(struct rcu_head *head,	/* call_rcu() flavor. */
				void (*func)(struct rcu_head *head));
	u8	fqs_state ____cacheline_internodealigned_in_smp;
	u8	boost;				/* Subject to priority boost. */
	unsigned long gpnum;			/* Current gp number. */
	unsigned long completed;		/* # of last completed gp. */
	struct task_struct *gp_kthread;		/* Task for grace periods. */
	wait_queue_head_t gp_wq;		/* Where GP task waits. */
	short gp_flags;				/* Commands for GP task. */
	short gp_state;				/* GP kthread sleep state. */
	raw_spinlock_t orphan_lock ____cacheline_internodealigned_in_smp;
	struct rcu_head *orphan_nxtlist;	/* Orphaned callbacks that */
	struct rcu_head **orphan_nxttail;	/* Tail of above. */
	struct rcu_head *orphan_donelist;	/* Orphaned callbacks that */
	struct rcu_head **orphan_donetail;	/* Tail of above. */
	long qlen_lazy;				/* Number of lazy callbacks. */
	long qlen;				/* Total number of callbacks. */
	struct mutex onoff_mutex;		/* Coordinate hotplug & GPs. */
	struct mutex barrier_mutex;		/* Guards barrier fields. */
	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */
	struct completion barrier_completion;	/* Wake at barrier end. */
	unsigned long n_barrier_done;		/* ++ at start and end of */
	atomic_long_t expedited_start;		/* Starting ticket. */
	atomic_long_t expedited_done;		/* Done ticket. */
	atomic_long_t expedited_wrap;		/* # near-wrap incidents. */
	atomic_long_t expedited_tryfail;	/* # acquisition failures. */
	atomic_long_t expedited_workdone1;	/* # done by others #1. */
	atomic_long_t expedited_workdone2;	/* # done by others #2. */
	atomic_long_t expedited_normal;		/* # fallbacks_tlx to normal. */
	atomic_long_t expedited_stoppedcpus;	/* # successful stop_cpus. */
	atomic_long_t expedited_done_tries;	/* # tries to update _done. */
	atomic_long_t expedited_done_lost;	/* # times beaten to _done. */
	atomic_long_t expedited_done_exit;	/* # times exited _done loop. */
	unsigned long jiffies_tlx_force_qs;		/* Time at which to invoke */
	unsigned long n_force_qs;		/* Number of calls to */
	unsigned long n_force_qs_lh;		/* ~Number of calls leaving */
	unsigned long n_force_qs_ngp;		/* Number of calls leaving */
	unsigned long gp_start;			/* Time at which GP started, */
	unsigned long jiffies_tlx_stall;		/* Time at which to check */
	unsigned long jiffies_tlx_resched;		/* Time at which to resched */
	unsigned long gp_max;			/* Maximum GP duration in */
	const char *name;			/* Name of structure. */
	char abbr;				/* Abbreviated name. */
	struct list_head flavors;		/* List of RCU flavors. */
};



struct rcu_dynticks_tlx {
	long long dynticks_nesting; /* Track irq/process nesting level. */
	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
	atomic_t dynticks;	    /* Even value for idle, else odd. */
};

DEFINE_PER_CPU(struct rcu_dynticks_tlx, rcu_dynticks_tlx);
LIST_HEAD(rcu_struct_flavors_tlx);


void __init rcu_init_one_tlx(struct rcu_state *rsp,
		struct rcu_data __percpu *rda)
{
	static char *buf[] = { "rcu_node_0",
						"rcu_node_1",
						"rcu_node_2",
						"rcu_node_3" };  /* Match MAX_RCU_LVLS */
	static char *fqs[] = { "rcu_node_fqs_0",
						"rcu_node_fqs_1",
						"rcu_node_fqs_2",
						"rcu_node_fqs_3" };  /* Match MAX_RCU_LVLS */
	static u8 fl_mask = 0x1;
	int cpustride = 1;
	int i;
	int j;
	struct rcu_node *rnp;
	for (i = 0; i < rcu_num_lvls_tlx; i++)
		rsp->levelcnt[i] = num_rcu_lvl_tlx[i];
	for (i = 1; i < rcu_num_lvls_tlx; i++)
		rsp->level[i] = rsp->level[i - 1] + rsp->levelcnt[i - 1];
	rsp->flavor_mask = fl_mask;
	fl_mask <<= 1;
	for (i = rcu_num_lvls_tlx - 1; i >= 0; i--) {
		cpustride *= rsp->levelspread[i];
		rnp = rsp->level[i];
		for (j = 0; j < rsp->levelcnt[i]; j++, rnp++) {
			raw_spin_lock_init(&rnp->lock);
			lockdep_set_class_and_name(&rnp->lock,
							&rcu_node_class_tlx[i], buf[i]);
			raw_spin_lock_init(&rnp->fqslock);
			lockdep_set_class_and_name(&rnp->fqslock,
							&rcu_fqs_class_tlx[i], fqs[i]);
			rnp->gpnum = rsp->gpnum;
			rnp->completed = rsp->completed;
			rnp->qsmask = 0;
			rnp->qsmaskinit = 0;
			rnp->grplo = j * cpustride;
			rnp->grphi = (j + 1) * cpustride - 1;
			if (rnp->grphi >= nr_cpu_ids_tlx_tlx)
				rnp->grphi = nr_cpu_ids_tlx_tlx - 1;
			if (i == 0) {
				rnp->grpnum = 0;
				rnp->grpmask = 0;
				rnp->parent = NULL;
			} else {
				rnp->grpnum = j % rsp->levelspread[i - 1];
				rnp->grpmask = 1UL << rnp->grpnum;
				rnp->parent = rsp->level[i - 1] +
								j / rsp->levelspread[i - 1];
			}
			rnp->level = i;
			INIT_LIST_HEAD(&rnp->blkd_tasks);
		}
	}

	rsp->rda = rda;
	init_waitqueue_head(&rsp->gp_wq);
	rnp = rsp->level[rcu_num_lvls_tlx - 1];
	for_each_possible_cpu(i) {
		while (i > rnp->grphi)
			rnp++;
		per_cpu_ptr(rsp->rda, i)->mynode = rnp;
		int cpu = i;
			unsigned long flags;
			struct rcu_data *rdp0 = per_cpu_ptr(rsp->rda, cpu);
			struct rcu_node *rnp0 = &rsp->node[0];
			rdp0->grpmask = 1UL << (cpu - rdp0->mynode->grplo);
			int i0;
			rdp0->nxtlist = NULL;
				for (i0 = 0; i0 < RCU_NEXT_SIZE; i0++)
								rdp0->nxttail[i0] = &rdp0->nxtlist;
			rdp0->qlen_lazy = 0;
			ACCESS_ONCE(rdp0->qlen) = 0;
			rdp0->dynticks = &per_cpu(rcu_dynticks_tlx, cpu);
			rdp0->cpu = cpu;
			rdp0->rsp = rsp;
	}
	list_add(&rsp->flavors, &rcu_struct_flavors_tlx);
}


struct open_flags {
         int open_flag;
         umode_t mode;
         int acc_mode;
         int intent;
         int lookup_flags;
 };




#define LOOKUP_OPEN             0x0100
#define LOOKUP_FOLLOW           0x0001
#define LOOKUP_RCU              0x0040
#define LOOKUP_JUMPED           0x1000
#define LOOKUP_DIRECTORY        0x0002
#define LOOKUP_CREATE           0x0200
#define LOOKUP_EXCL             0x0400
#define LOOKUP_ROOT             0x2000
#define LOOKUP_PARENT           0x0010
#define MAY_APPEND              0x00000008
#define MAY_WRITE               0x00000002

#define O_APPEND        00002000
#define O_NOFOLLOW	0100000	/* don't follow links */
#define __O_TMPFILE     020000000
#define O_TMPFILE (__O_TMPFILE | O_DIRECTORY)
#define O_TMPFILE_MASK (__O_TMPFILE | O_DIRECTORY | O_CREAT)
#define O_DSYNC         00010000        /* used to be O_SYNC, see below */
#define __O_SYNC        04000000
#define O_CLOEXEC       02000000        /* set close_on_exec */
enum { MAX_NESTED_LINKS = 8 };
#define ACC_MODE(x) ("\004\002\006\006"[(x)&O_ACCMODE])


int build_open_flags_tlx(int flags, umode_t mode, struct open_flags *op)
{
	int lookup_flags = 0;
	int acc_mode;

	if (flags & (O_CREAT | __O_TMPFILE))
		op->mode = (mode & S_IALLUGO) | S_IFREG;
	else
		op->mode = 0;

	/* Must never be set by userspace */
	flags &= ~FMODE_NONOTIFY & ~O_CLOEXEC;

	/*
	* O_SYNC is implemented as __O_SYNC|O_DSYNC.  As many places only
	* check for O_DSYNC if the need any syncing at all we enforce it's
	* always set instead of having to deal with possibly weird behaviour
	* for malicious applications setting only __O_SYNC.
	*/
	if (flags & __O_SYNC)
		flags |= O_DSYNC;

	if (flags & __O_TMPFILE) {
		if ((flags & O_TMPFILE_MASK) != O_TMPFILE)
			return -EINVAL;
		acc_mode = MAY_OPEN | ACC_MODE(flags);
		if (!(acc_mode & MAY_WRITE))
			return -EINVAL;
	} else if (flags & O_PATH) {
		/*
		* If we have O_PATH in the open flag. Then we
		* cannot have anything other than the below set of flags
		*/
		flags &= O_DIRECTORY | O_NOFOLLOW | O_PATH;
		acc_mode = 0;
	} else {
		acc_mode = MAY_OPEN | ACC_MODE(flags);
	}

	op->open_flag = flags;

	/* O_TRUNC implies we need access checks for write permissions */
	if (flags & O_TRUNC)
		acc_mode |= MAY_WRITE;

	/* Allow the LSM permission hook to distinguish append
		access from general write access. */
	if (flags & O_APPEND)
		acc_mode |= MAY_APPEND;

	op->acc_mode = acc_mode;

	op->intent = flags & O_PATH ? 0 : LOOKUP_OPEN;

	if (flags & O_CREAT) {
		op->intent |= LOOKUP_CREATE;
		if (flags & O_EXCL)
			op->intent |= LOOKUP_EXCL;
	}

	if (flags & O_DIRECTORY)
		lookup_flags |= LOOKUP_DIRECTORY;
	if (!(flags & O_NOFOLLOW))
		lookup_flags |= LOOKUP_FOLLOW;
	op->lookup_flags = lookup_flags;
	return 0;
}

#define NR_OPEN_DEFAULT BITS_PER_LONG
struct fdtable {
	unsigned int max_fds;
	struct file __rcu **fd;      /* current fd array */
	unsigned long *close_on_exec;
	unsigned long *open_fds;
	struct rcu_head rcu;
};

struct files_struct {
	/*
	* read mostly part
	*/
	atomic_t count;
	struct fdtable __rcu *fdt;
	struct fdtable fdtab;
	/*
	* written part on a separate cache line in SMP
	*/
	spinlock_t file_lock ____cacheline_aligned_in_smp;
	int next_fd;
	unsigned long close_on_exec_init[1];
	unsigned long open_fds_init[1];
	struct file __rcu * fd_array[NR_OPEN_DEFAULT];
};


struct files_struct init_files_tlx
= {
         .count          = ATOMIC_INIT(1),
         .fdt            = &init_files_tlx.fdtab,
       .fdtab          = {
                 .max_fds        = NR_OPEN_DEFAULT,
                 .fd             = &init_files_tlx.fd_array[0],
                 .close_on_exec  = init_files_tlx.close_on_exec_init,
               .open_fds       = init_files_tlx.open_fds_init,
         },
         .file_lock      = __SPIN_LOCK_UNLOCKED(init_files_tlx.file_lock),
};




unsigned long find_next_zero_bit_tlx(const unsigned long *addr, unsigned long size,
				unsigned long offset)
{
	const unsigned long *p = addr + BITOP_WORD(offset);
	unsigned long result = offset & ~(BITS_PER_LONG-1);
	unsigned long tmp;

	if (offset >= size)
		return size;
	size -= result;
	offset %= BITS_PER_LONG;
	if (offset) {
		tmp = *(p++);
		tmp |= ~0UL >> (BITS_PER_LONG - offset);
		if (size < BITS_PER_LONG)
			goto found_first;
		if (~tmp)
			goto found_middle;
		size -= BITS_PER_LONG;
		result += BITS_PER_LONG;
	}
	while (size & ~(BITS_PER_LONG-1)) {
		if (~(tmp = *(p++)))
			goto found_middle;
		result += BITS_PER_LONG;
		size -= BITS_PER_LONG;
	}
	if (!size)
		return result;
	tmp = *p;

found_first:
	tmp |= ~0UL << size;
	if (tmp == ~0UL)	/* Are any bits zero? */
		return result + size;	/* Nope. */
found_middle:
	return result + __ffs_tlx(~(tmp));
}

struct nameidata {
					struct path     path;
					struct qstr     last;
					struct path     root;
					struct inode    *inode; /* path.dentry.d_inode */
					unsigned int    flags;
					unsigned        seq, m_seq;
					int             last_type;
					unsigned        depth;
					char *saved_names[MAX_NESTED_LINKS + 1];
};

enum {LAST_NORM, LAST_ROOT, LAST_DOT, LAST_DOTDOT, LAST_BIND};

int complete_walk_tlx(struct nameidata *nd);
int follow_link_tlx(struct path *link, struct nameidata *nd, void **p);
int link_path_walk_tlx(const char *name, struct nameidata *nd);
int path_init_tlx(int dfd, const char *name, unsigned int flags,
				struct nameidata *nd, struct file **fp);
int walk_component_tlx(struct nameidata *nd, struct path *path,
		int follow);

int path_lookupat_tlx(int dfd, const char *name,
				unsigned int flags, struct nameidata *nd)
{
	struct file *base = NULL;
	struct path path;
	int err;
	err = path_init_tlx(dfd, name, flags | LOOKUP_PARENT, nd, &base);
	current->total_link_count = 0;
	err = link_path_walk_tlx(name, nd);

	if (!err && !(flags & LOOKUP_PARENT)) {
//		err = lookup_last(nd, &path);
	if (nd->last_type == LAST_NORM && nd->last.name[nd->last.len])
							nd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;
	nd->flags &= ~LOOKUP_PARENT;
	err  = walk_component_tlx(nd, &path, nd->flags & LOOKUP_FOLLOW);
		while (err > 0) {
			void *cookie;
			struct path link = path;
			nd->flags |= LOOKUP_PARENT;
			err = follow_link_tlx(&link, nd, &cookie);
			if (err)
				break;
//			err = lookup_last(nd, &path);
			if (nd->last_type == LAST_NORM && nd->last.name[nd->last.len])
									nd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;
			nd->flags &= ~LOOKUP_PARENT;
			err  = walk_component_tlx(nd, &path, nd->flags & LOOKUP_FOLLOW);
//			put_link(nd, &link, cookie);
		}
	}

	if (!err)
		err = complete_walk_tlx(nd);
	return err;
}


#define EMBEDDED_NAME_MAX       (PATH_MAX - sizeof(struct filename))


struct kstat {
       u64             ino;
       dev_t           dev;
       umode_t         mode;
       unsigned int    nlink;
       kuid_t          uid;
       kgid_t          gid;
       dev_t           rdev;
       loff_t          size;
       struct timespec  atime;
       struct timespec mtime;
       struct timespec ctime;
       unsigned long   blksize;
       unsigned long long      blocks;
};


#define LOOKUP_EMPTY            0x4000
#define AT_EMPTY_PATH           0x1000  /* Allow empty relative pathname */
#define AT_SYMLINK_FOLLOW       0x400   /* Follow symbolic links.  */
#define AT_NO_AUTOMOUNT         0x800   /* Suppress terminal automount traversal */
#define AT_SYMLINK_NOFOLLOW     0x100   /* Do not follow symbolic links.  */


int vfs_fstatat_tlx(int dfd, const char __user *filename, struct kstat *stat,
		int flag)
{
	struct path path;
	int error = -EINVAL;
	unsigned int lookup_flags = 0;
	struct nameidata nd;
	struct filename *tmp;
	if ((flag & ~(AT_SYMLINK_NOFOLLOW | AT_NO_AUTOMOUNT |
					AT_EMPTY_PATH)) != 0)
		goto out;

	if (!(flag & AT_SYMLINK_NOFOLLOW))
		lookup_flags |= LOOKUP_FOLLOW;
	if (flag & AT_EMPTY_PATH)
		lookup_flags |= LOOKUP_EMPTY;
//retry:
//	error = user_path_at_empty(dfd, filename, lookup_flags, &path, NULL);
//	int dfd, const char __user *name, unsigned flags,
//			struct path *path, int *empty)
//	{
//		struct nameidata nd;
	//	tmp = getname_flags(filename, lookup_flags, NULL);
//		const char __user *filename, int flags, int *empty)
//		{
			struct filename *result;
			int len;
			long max;
			char *kname;
			result = slab_alloc_tlx(names_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
//			kmem_cache_alloc(names_cachep_tlx, GFP_KERNEL);
				kname = (char *)result + sizeof(*result);
			result->name = kname;
			result->separate = false;
			max = EMBEDDED_NAME_MAX;
		//	len = __strncpy_from_user(kname, filename, max);
			char *dst = kname;
			char __user *src = filename;
			long count = max;
				char *tmp_;
					strncpy_tlx(dst, (const char __force *)src, count);
					for (tmp_ = dst; *tmp_ && count > 0; tmp_++, count--)
										;

			result->uptr = filename;
			result->aname = NULL;
			tmp = result;

		int err = PTR_ERR_tlx(tmp);
		if (!IS_ERR_tlx(tmp)) {
			err = path_lookupat_tlx(dfd, tmp->name, lookup_flags | LOOKUP_RCU, &nd);
//			filename_lookup(dfd, tmp, lookup_flags, &nd);
			__putname(tmp);
			if (!err)
				path = nd.path;
		}
	error = err;
	if (error)
		goto out;

//	error = vfs_getattr_nosec(&path, stat);
	struct inode *inode = path.dentry->d_inode;
//  generic_fillattr(inode, stat);
	stat->dev = inode->i_sb->s_dev;
	stat->ino = inode->i_ino;
	stat->mode = inode->i_mode;
	stat->nlink = inode->i_nlink;
	stat->uid = inode->i_uid;
	stat->gid = inode->i_gid;
	stat->rdev = inode->i_rdev;
//	stat->size = i_size_read(inode);
	stat->atime = inode->i_atime;
	stat->mtime = inode->i_mtime;
	stat->ctime = inode->i_ctime;
	stat->blksize = (1 << inode->i_blkbits);
	stat->blocks = inode->i_blocks;

	error = 0;
	dput_tlx((&path)->dentry);
out:
	return error;
}


struct kmem_cache *sock_inode_cachep_tlx ;
struct workqueue_struct *system_wq_tlx;

void __list_add_rcu_tlx(struct list_head *new,
                     struct list_head *prev, struct list_head *next) {};

static inline void list_add_rcu_tlx(struct list_head *new, struct list_head *head)
 {
         __list_add_rcu_tlx(new, head, head->next);
  }


#define pgd_offset_k_tlx(addr)      pgd_offset_tlx(&init_mm_tlx, addr)
#define pgd_index_tlx(addr)         (((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
#define pgd_offset_tlx(mm, addr)    ((mm)->pgd+pgd_index_tlx(addr))

#define pgd_addr_end_tlx(addr, end)                                         \
 ({      unsigned long __boundary = ((addr) + PGDIR_SIZE) & PGDIR_MASK;  \
         (__boundary - 1 < (end) - 1)? __boundary: (end);                \
})


struct vm_struct {
				struct vm_struct        *next;
				void                    *addr;
				unsigned long           size;
				unsigned long           flags;
				struct page             **pages;
				unsigned int            nr_pages;
				phys_addr_t             phys_addr;
				const void              *caller;
};


#define VM_IOREMAP              0x00000001      /* ioremap() and friends */
#define PGALLOC_GFP     (GFP_KERNEL | __GFP_NOTRACK | __GFP_REPEAT | __GFP_ZERO)


int __pte_alloc_kernel_tlx(pmd_t *pmd, unsigned long address)
{
	pte_t *new = (pte_t *) page_address(alloc_pages_node_tlx(0, GFP_KERNEL | __GFP_ZERO, 0));
	smp_wmb(); /* See comment in __pte_alloc */
	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
//		pmd_populate_kernel(&init_mm_tlx, pmd, new);
//		 __pmd_populate(pmd, __pa(new), PMD_TYPE_TABLE);
		set_pmd(pmd, __pmd(__pa(new) | PMD_TYPE_TABLE));
		new = NULL;
	}
	if (new)
			free_page((unsigned long)new);
	return 0;
}


#define pte_alloc_kernel(pmd, address)                  \
         ((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel_tlx(pmd, address))? \
                 NULL: pte_offset_kernel(pmd, address))

int ioremap_pte_range_tlx(pmd_t *pmd, unsigned long addr,
		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
{
	pte_t *pte;
	u64 pfn;

	pfn = phys_addr >> PAGE_SHIFT;
	pte = pte_alloc_kernel(pmd, addr);
	if (!pte)
		return -ENOMEM;
	do {
		set_pte_at(&init_mm_tlx, addr, pte, pfn_pte(pfn, prot));
		pfn++;
	} while (pte++, addr += PAGE_SIZE, addr != end);
	return 0;
}


int __pmd_alloc_tlx(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	pmd_t *new = (pmd_t *) page_address(alloc_pages((GFP_KERNEL | __GFP_REPEAT) | __GFP_ZERO, 0));
	smp_wmb(); /* See comment in __pte_alloc */
//	pud_populate(mm, pud, new);
	set_pud_tlx(pud, __pud(__pa(new) | PMD_TYPE_TABLE));
	return 0;
}

pmd_t *pmd_alloc_tlx(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
         return (unlikely(pud_none(*pud)) && __pmd_alloc_tlx(mm, pud, address))?
                 NULL: pmd_offset_tlx(pud, address);
}


int ioremap_pmd_range_tlx(pud_t *pud, unsigned long addr,
		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
{
	pmd_t *pmd;
	unsigned long next;

	phys_addr -= addr;
	pmd = pmd_alloc_tlx(&init_mm_tlx, pud, addr);
	if (!pmd)
		return -ENOMEM;
	do {
		next = pmd_addr_end(addr, end);
		if (ioremap_pte_range_tlx(pmd, addr, next, phys_addr + addr, prot))
			return -ENOMEM;
	} while (pmd++, addr = next, addr != end);
	return 0;
}

int ioremap_pud_range_tlx(pgd_t *pgd, unsigned long addr,
		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
{
	pud_t *pud;
	unsigned long next;

	phys_addr -= addr;
	pud = pud_offset_tlx(pgd, addr);
	do {
		next = pud_addr_end(addr, end);
		if (ioremap_pmd_range_tlx(pud, addr, next, phys_addr + addr, prot))
			return -ENOMEM;
	} while (pud++, addr = next, addr != end);
	return 0;
}

int ioremap_page_range_tlx(unsigned long addr,
					unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
{
	pgd_t *pgd;
	unsigned long start;
	unsigned long next;
	int err;
	start = addr;
	phys_addr -= addr;
	pgd = pgd_offset_k_tlx(addr);
	do {
		next = pgd_addr_end(addr, end);
		err = ioremap_pud_range_tlx(pgd, addr, next, phys_addr+addr, prot);
		if (err)
			break;
	} while (pgd++, addr = next, addr != end);

//	flush_cache_vmap(start, end);
	dsb(ish);
	return err;
}

#define VMALLOC_START           (UL(0xffffffffffffffff) << VA_BITS)
#define IOREMAP_MAX_ORDER       (7 + PAGE_SHIFT)        /* 128 pages */

struct vmap_area {
         unsigned long va_start;
         unsigned long va_end;
         unsigned long flags;
         struct rb_node rb_node;         /* address sorted rbtree */
         struct list_head list;          /* address sorted list */
         struct list_head purge_list;    /* "lazy purge" list */
         struct vm_struct *vm;
         struct rcu_head rcu_head;
};

#define VM_VM_AREA      0x04

struct list_head vmap_area_list_tlx_tlx;
struct rb_node *free_vmap_cache_tlx_tlx;
unsigned long cached_hole_size_tlx_tlx;
unsigned long cached_vstart_tlx_tlx;
unsigned long cached_align_tlx_tlx;
struct rb_root vmap_area_root_tlx = RB_ROOT;
static inline void dummy_rotate_tlx(struct rb_node *old, struct rb_node *new) {}

struct rb_node *rb_prev_tlx(const struct rb_node *node)
{
	struct rb_node *parent;

	if (node->rb_left) {
		node = node->rb_left;
		while (node->rb_right)
			node=node->rb_right;
		return (struct rb_node *)node;
	}
	while ((parent = rb_parent(node)) && node == parent->rb_left)
		node = parent;
	return parent;
}


static inline void rb_set_parent_color(struct rb_node *rb,
							struct rb_node *p, int color);

static inline struct rb_node *rb_red_parent(struct rb_node *red)
{
	return (struct rb_node *)red->__rb_parent_color;
}

#define	RB_RED		0
#define	RB_BLACK	1

#define __rb_parent(pc)    ((struct rb_node *)(pc & ~3))

#define __rb_color(pc)     ((pc) & 1)
#define __rb_is_black(pc)  __rb_color(pc)
#define __rb_is_red(pc)    (!__rb_color(pc))

#define rb_color(rb)       __rb_color((rb)->__rb_parent_color)
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)
#define __rb_parent(pc)    ((struct rb_node *)(pc & ~3))

void
__rb_rotate_set_parents(struct rb_node *old, struct rb_node *new,
			struct rb_root *root, int color);

void
__rb_insert_tlx(struct rb_node *node, struct rb_root *root,
	    void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
	struct rb_node *parent = rb_red_parent(node), *gparent, *tmp;

	while (true) {
		if (!parent) {
			rb_set_parent_color(node, NULL, RB_BLACK);
			break;
		} else if (rb_is_black(parent))
			break;

		gparent = rb_red_parent(parent);
		tmp = gparent->rb_right;
		if (parent != tmp) {	/* parent == gparent->rb_left */
			if (tmp && rb_is_red(tmp)) {
				rb_set_parent_color(tmp, gparent, RB_BLACK);
				rb_set_parent_color(parent, gparent, RB_BLACK);
				node = gparent;
				parent = rb_parent(node);
				rb_set_parent_color(node, parent, RB_RED);
				continue;
			}
			tmp = parent->rb_right;
			if (node == tmp) {
				parent->rb_right = tmp = node->rb_left;
				node->rb_left = parent;
				if (tmp)
					rb_set_parent_color(tmp, parent,
							    RB_BLACK);
				rb_set_parent_color(parent, node, RB_RED);
				augment_rotate(parent, node);
				parent = node;
				tmp = node->rb_right;
			}
			gparent->rb_left = tmp;  /* == parent->rb_right */
			parent->rb_right = gparent;
			if (tmp)
				rb_set_parent_color(tmp, gparent, RB_BLACK);
			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
			augment_rotate(gparent, parent);
			break;
		} else {
			tmp = gparent->rb_left;
			if (tmp && rb_is_red(tmp)) {
				/* Case 1 - color flips */
				rb_set_parent_color(tmp, gparent, RB_BLACK);
				rb_set_parent_color(parent, gparent, RB_BLACK);
				node = gparent;
				parent = rb_parent(node);
				rb_set_parent_color(node, parent, RB_RED);
				continue;
			}

			tmp = parent->rb_left;
			if (node == tmp) {
				/* Case 2 - right rotate at parent */
				parent->rb_left = tmp = node->rb_right;
				node->rb_right = parent;
				if (tmp)
					rb_set_parent_color(tmp, parent,
							    RB_BLACK);
				rb_set_parent_color(parent, node, RB_RED);
				augment_rotate(parent, node);
				parent = node;
				tmp = node->rb_left;
			}
			/* Case 3 - left rotate at gparent */
			gparent->rb_right = tmp;  /* == parent->rb_left */
			parent->rb_left = gparent;
			if (tmp)
				rb_set_parent_color(tmp, gparent, RB_BLACK);
			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
			augment_rotate(gparent, parent);
			break;
		}
	}
}

LIST_HEAD(vmap_area_list_tlx);
void rb_insert_color_tlx(struct rb_node *node, struct rb_root *root)
{
	__rb_insert_tlx(node, root, dummy_rotate_tlx);
}

#define list_next_rcu(list)     (*((struct list_head __rcu **)(&(list)->next)))
static inline void __list_add_rcu_tlx_tlx(struct list_head *new,
                 struct list_head *prev, struct list_head *next)
 {
         new->next = next;
         new->prev = prev;
         rcu_assign_pointer(list_next_rcu(prev), new);
         next->prev = new;
}

void __insert_vmap_area_tlx(struct vmap_area *va)
{
	struct rb_node **p = &vmap_area_root_tlx.rb_node;
	struct rb_node *parent = NULL;
	struct rb_node *tmp;

	while (*p) {
		struct vmap_area *tmp_va;

		parent = *p;
		tmp_va = rb_entry(parent, struct vmap_area, rb_node);
		if (va->va_start < tmp_va->va_end)
			p = &(*p)->rb_left;
		else if (va->va_end > tmp_va->va_start)
			p = &(*p)->rb_right;
	}

	rb_link_node_tlx(&va->rb_node, parent, p);
	rb_insert_color_tlx(&va->rb_node, &vmap_area_root_tlx);

	/* address-sort this list */
	tmp = rb_prev_tlx(&va->rb_node);
	if (tmp) {
		struct vmap_area *prev;
		prev = rb_entry(tmp, struct vmap_area, rb_node);
		 __list_add_rcu_tlx_tlx(&va->list, &prev->list, (&prev->list)->next);
	} else
		 __list_add_rcu_tlx_tlx(&va->list, &vmap_area_list_tlx,  (&vmap_area_list_tlx)->next);
}

DEFINE_SPINLOCK(vmap_area_lock_tlx);
unsigned long cached_hole_size_tlx;
unsigned long cached_vstart_tlx;
unsigned long cached_align_tlx;
struct rb_node *free_vmap_cache_tlx;

struct vmap_area *alloc_vmap_area_tlx(unsigned long size,
				unsigned long align,
				unsigned long vstart, unsigned long vend,
				int node, gfp_t gfp_mask)
{
	struct vmap_area *va;
	struct rb_node *n;
	unsigned long addr;
	int purged = 0;
	struct vmap_area *first;
	va = kmalloc_tlx(sizeof(struct vmap_area),
			gfp_mask & GFP_RECLAIM_MASK);
	retry:
	spin_lock_tlx(&vmap_area_lock_tlx);
	if (!free_vmap_cache_tlx ||
			size < cached_hole_size_tlx ||
			vstart < cached_vstart_tlx ||
			align < cached_align_tlx) {
	nocache:
		cached_hole_size_tlx = 0;
		free_vmap_cache_tlx = NULL;
	}
	/* record if we encounter less permissive parameters */
	cached_vstart_tlx = vstart;
	cached_align_tlx = align;

	/* find starting point for our search */
	if (free_vmap_cache_tlx) {
		first = rb_entry(free_vmap_cache_tlx, struct vmap_area, rb_node);
		addr = ALIGN(first->va_end, align);
		if (addr < vstart)
			goto nocache;
	} else {
		addr = ALIGN(vstart, align);
		n = vmap_area_root_tlx.rb_node;
		first = NULL;

		while (n) {
			struct vmap_area *tmp;
			tmp = rb_entry(n, struct vmap_area, rb_node);
			if (tmp->va_end >= addr) {
				first = tmp;
				if (tmp->va_start <= addr)
					break;
				n = n->rb_left;
			} else
				n = n->rb_right;
		}

		if (!first)
			goto found;
	}

	/* from the starting point, walk areas until a suitable hole is found */
	while (addr + size > first->va_start && addr + size <= vend) {
		if (addr + cached_hole_size_tlx < first->va_start)
			cached_hole_size_tlx = first->va_start - addr;
		addr = ALIGN(first->va_end, align);
		if (list_is_last(&first->list, &vmap_area_list_tlx))
			goto found;
		first = list_entry(first->list.next,
				struct vmap_area, list);
	}

	found:
	va->va_start = addr;
	va->va_end = addr + size;
	va->flags = 0;
	__insert_vmap_area_tlx(va);
	free_vmap_cache_tlx = &va->rb_node;
	spin_unlock_tlx(&vmap_area_lock_tlx);
	return va;

}

struct vm_struct *__get_vm_area_node_tlx(unsigned long size,
		unsigned long align, unsigned long flags, unsigned long start,
		unsigned long end, int node, gfp_t gfp_mask, const void *caller)
{
	struct vmap_area *va;
	struct vm_struct *area;
	if (flags & VM_IOREMAP)
		align = 1ul << clamp(fls_tlx(size), PAGE_SHIFT, IOREMAP_MAX_ORDER);
	size = PAGE_ALIGN(size);
	area = kzalloc_tlx(sizeof(*area), gfp_mask & GFP_RECLAIM_MASK);
	size += PAGE_SIZE;
	va = alloc_vmap_area_tlx(size, align, start, end, node, gfp_mask);
//	setup_vmalloc_vm(area, va, flags, caller);
				area->flags = flags;
				area->addr = (void *)va->va_start;
				area->size = va->va_end - va->va_start;
				area->caller = caller;
				va->vm = area;
				va->flags |= VM_VM_AREA;
	return area;
}


void *__ioremap_caller_tlx(phys_addr_t phys_addr, size_t size,
							pgprot_t prot, void *caller)
{
	unsigned long last_addr;
	unsigned long offset = phys_addr & ~PAGE_MASK;
	int err;
	unsigned long addr;
	struct vm_struct *area;
	phys_addr &= PAGE_MASK;
	size = PAGE_ALIGN(size + offset);
	last_addr = phys_addr + size - 1;
//	area = get_vm_area_caller(size, VM_IOREMAP, caller);
	area = __get_vm_area_node_tlx(size, 1, VM_IOREMAP, VMALLOC_START, VMALLOC_END,
                                   NUMA_NO_NODE, GFP_KERNEL, caller);
	addr = (unsigned long)area->addr;
	err = ioremap_page_range_tlx(addr, addr + size, phys_addr, prot);
	return (void __iomem *)(offset + addr);
}

void __iomem *__ioremap_tlx(phys_addr_t phys_addr, size_t size, pgprot_t prot)
{
				return __ioremap_caller_tlx(phys_addr, size, prot,
																__builtin_return_address(0));
}

phys_addr_t __init memblock_tlx_alloc_range_nid_tlx(phys_addr_t size,
					phys_addr_t align, phys_addr_t start,
					phys_addr_t end, int nid)
{
	phys_addr_t found;

	if (!align)
		align = SMP_CACHE_BYTES;
		phys_addr_t kernel_end;
		if (end == MEMBLOCK_ALLOC_ACCESSIBLE)
			end = memblock_tlx.current_limit;
		start = max_t(phys_addr_t, start, PAGE_SIZE);
		end = max(start, end);
		kernel_end = __pa_symbol(_end);
			phys_addr_t this_start, this_end, cand;
			u64 i;
			for_each_free_mem_range_reverse(i, nid, &this_start, &this_end, NULL) {
				this_start = clamp(this_start, start, end);
				this_end = clamp(this_end, start, end);

				if (this_end < size)
					continue;

				cand = round_down(this_end - size, align);
				if (cand >= this_start) {
					found =  cand;
					break;
				}
			}

	if (found && !memblock_reserve_region_tlx(found, size, MAX_NUMNODES, 0)) {
		return found;
	}
	return 0;
}



void __init *early_alloc_tlx(unsigned long sz)
{
	void *ptr = __va(memblock_tlx_alloc_range_nid_tlx(sz, sz, 0, MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE));
	memset_tlx(ptr, 0, sz);
	return ptr;
}

void __init alloc_init_pte_tlx(pmd_t *pmd, unsigned long addr,
					unsigned long end, unsigned long pfn,
					pgprot_t prot)
{
	pte_t *pte;

	if (pmd_none(*pmd)) {
		pte = early_alloc_tlx(PTRS_PER_PTE * sizeof(pte_t));
			set_pmd_tlx(pmd, __pmd(__pa(pte) | PMD_TYPE_TABLE));
	}
	pte = pte_offset_kernel(pmd, addr);
	do {
		set_pte_tlx(pte, pfn_pte(pfn, prot));
		pfn++;
	} while (pte++, addr += PAGE_SIZE, addr != end);
}

void __init alloc_init_pmd_tlx(pud_t *pud, unsigned long addr,
					unsigned long end, phys_addr_t phys,
					int map_io)
{
	pmd_t *pmd;
	unsigned long next;
	pmdval_t prot_sect;
	pgprot_t prot_pte;

	if (map_io) {
		prot_sect = PROT_SECT_DEVICE_nGnRE;
		prot_pte = __pgprot(PROT_DEVICE_nGnRE);
	} else {
		prot_sect = PROT_SECT_NORMAL_EXEC;
		prot_pte = PAGE_KERNEL_EXEC;
	}
	if (pud_none(*pud) || pud_bad(*pud)) {
		pmd = early_alloc_tlx(PTRS_PER_PMD * sizeof(pmd_t));
		set_pud_tlx(pud, __pud(__pa(pmd) | PMD_TYPE_TABLE));
	}

	pmd = pmd_offset_tlx(pud, addr);
	do {
		next = pmd_addr_end(addr, end);
		if (((addr | next | phys) & ~SECTION_MASK) == 0) {
			pmd_t old_pmd =*pmd;
			set_pmd_tlx(pmd, __pmd(phys | prot_sect));
			if (!pmd_none(old_pmd))
				flush_tlb_all();
		} else {
			alloc_init_pte_tlx(pmd, addr, next, __phys_to_pfn(phys),
							prot_pte);
		}
		phys += next - addr;
	} while (pmd++, addr = next, addr != end);
}

void __init alloc_init_pud_tlx(pgd_t *pgd, unsigned long addr,
					unsigned long end, unsigned long phys,
					int map_io)
{
	pud_t *pud = pud_offset_tlx(pgd, addr);
	unsigned long next;

	do {
		next = pud_addr_end(addr, end);
		if (!map_io && (PAGE_SHIFT == 12) &&
				((addr | next | phys) & ~PUD_MASK) == 0) {
			pud_t old_pud = *pud;
			set_pud_tlx(pud, __pud(phys | PROT_SECT_NORMAL_EXEC));
			if (!pud_none(old_pud)) {
				phys_addr_t table = __pa(pmd_offset_tlx(&old_pud, 0));
				memblock_tlx_remove_range_tlx(&memblock_tlx.reserved, table, PAGE_SIZE);
				flush_tlb_all();
			}
		} else {
			alloc_init_pmd_tlx(pud, addr, next, phys, map_io);
		}
		phys += next - addr;
	} while (pud++, addr = next, addr != end);
}

#define MAX_PHANDLE_ARGS 16
struct of_phandle_args {
          struct device_node *np;
          int args_count;
          uint32_t args[MAX_PHANDLE_ARGS];
};
struct of_clk_provider {
         struct list_head link;

        struct device_node *node;
        struct clk *(*get)(struct of_phandle_args *clkspec, void *data);
        void *data;
};


struct list_head of_clk_providers_tlx = LIST_HEAD_INIT(of_clk_providers_tlx);
int parent_ready_tlx(struct device_node *np)
{
	int i = 0;

	while (true) {
				struct of_phandle_args clkspec;
				struct clk *clk;
				struct of_clk_provider *provider;
				const char *list_name = "clocks";
				const char *cells_name = "#clock-cells";
				int cell_count = 0;
				int index = i;
				struct of_phandle_args *out_args = &clkspec;
				const __be32 *list, *list_end;
				int rc = 0, size, cur_index = 0;
				uint32_t count = 0;
				struct device_node *node = NULL;
				phandle phandle;
				list = of_get_property_tlx(np, list_name, &size);
				if (!list)
					goto out;
				list_end = list + size / sizeof(*list);
				while (list < list_end) {
					rc = -EINVAL;
					count = 0;
					phandle = be32_to_cpup(list++);
					if (phandle) {
						if (cells_name || cur_index == index) {
							for (node = of_allnodes_tlx; node; node = node->allnext)
											if (node->phandle == phandle)
															break;
							if (node)
                  kobject_get_tlx(&node->kobj);
							if (!node) {
								goto err;
							}
						}

						if (cells_name) {
								const struct device_node *np = node;
								const char *propname = cells_name;
								u32 *out_values =&count;
								size_t sz = 1;
									const __be32 *val;
									u32 len = sz * sizeof(*out_values);
									struct property *prop;
										struct property *pp;
         						for (pp = np->properties; pp; pp = pp->next) {
                 		if (strcmp_tlx(pp->name, propname) == 0) {
                         break;
                 			}
         						}
 										prop = pp;
										if (!prop)
											goto err;
										if (!prop->value)
											goto err;
										if (len > prop->length)
											goto err;
										val =  prop->value;
								have_prop:
									while (sz--)
										*out_values++ = be32_to_cpup(val++);
						} else {
							count = cell_count;
						}

						if (list + count > list_end) {
							goto err;
						}
					}

					rc = -ENOENT;
					if (cur_index == index) {
						if (!phandle)
							goto err;

						if (out_args) {
							int i;
							if (WARN_ON(count > MAX_PHANDLE_ARGS))
								count = MAX_PHANDLE_ARGS;
							out_args->np = node;
							out_args->args_count = count;
							for (i = 0; i < count; i++)
								out_args->args[i] = be32_to_cpup(list++);
						}

						goto out;
					}

					node = NULL;
					list += count;
					cur_index++;
				}
				rc = index < 0 ? cur_index : -ENOENT;
			err:
				if (node)
out:
			clk = ERR_PTR_tlx(-EPROBE_DEFER);
			list_for_each_entry(provider, &of_clk_providers_tlx, link) {
								if (provider->node == (&clkspec)->np)
												clk = provider->get(&clkspec, provider->data);
								if (!((clk) >= (unsigned long)-MAX_ERRNO))
												break;
			}
		if (!((clk) >= (unsigned long)-MAX_ERRNO)) {
			i++;
			continue;
		}
		return 1;
	}
}

int slub_min_objects_tlx;
int slub_min_order_tlx;
int slub_max_order_tlx = PAGE_ALLOC_COSTLY_ORDER;
#define MAX_OBJS_PER_PAGE       32767

int slab_order_tlx(int size, int min_objects,
				int max_order, int fract_leftover, int reserved)
{
	int order;
	int rem;
	int min_order = slub_min_order_tlx;
	if ((((PAGE_SIZE << min_order) - reserved) / size) > MAX_OBJS_PER_PAGE)
		return get_order(size * MAX_OBJS_PER_PAGE) - 1;
	for (order = max(min_order,
				fls_tlx(min_objects * size - 1) - PAGE_SHIFT);
			order <= max_order; order++) {
		unsigned long slab_size = PAGE_SIZE << order;
		if (slab_size < min_objects * size + reserved)
			continue;
		rem = (slab_size - reserved) % size;
		if (rem <= slab_size / fract_leftover)
			break;
	}
	return order;
}


#define OO_SHIFT        16
#define OO_MASK         ((1 << OO_SHIFT) - 1)

static inline int oo_objects_tlx(struct kmem_cache_order_objects x)
{
         return x.x & OO_MASK;
}

static inline struct kmem_cache_order_objects oo_make_tlx(int order,
                 unsigned long size, int reserved)
{
         struct kmem_cache_order_objects x = {
               (order << OO_SHIFT) + ((PAGE_SIZE << order) - reserved) / size
       };

       return x;
}

#define MIN_PARTIAL 5
#define MAX_PARTIAL 10
struct kobj_type slab_ktype_tlx;
struct kset *slab_kset_tlx;

int __kmem_cache_create_tlx(struct kmem_cache *s, unsigned long flags)
{
		int err = 0;
		unsigned long object_size = s->size;
			const char *name = s->name;
			void (*ctor)(void *) = s->ctor;
		s->flags = flags;
		s->reserved = 0;
			unsigned long size = s->object_size;
			int order;
			size = ALIGN(size, sizeof(void *));
			s->inuse = size;
			size = ALIGN(size, s->align);
			s->size = size;
						int min_objects;
						int fraction;
						int max_objects;
						min_objects = slub_min_objects_tlx;
						if (!min_objects)
							min_objects = 4 * (fls_tlx(nr_cpu_ids_tlx_tlx) + 1);
						max_objects = ((PAGE_SIZE << slub_max_order_tlx) - s->reserved) / size; // (slub_max_order_tlx, size, s->reserved);
						min_objects = min(min_objects, max_objects);

						while (min_objects > 1) {
							fraction = 16;
							while (fraction >= 4) {
								order = slab_order_tlx(size, min_objects,
										slub_max_order_tlx, fraction, s->reserved);
								if (order <= slub_max_order_tlx)
									goto have_oder;
								fraction /= 2;
							}
							min_objects--;
						}
						order = slab_order_tlx(size, 1, slub_max_order_tlx, 1, s->reserved);
						if (order <= slub_max_order_tlx)
							goto have_oder;
						order = slab_order_tlx(size, 1, MAX_ORDER, 1, s->reserved);
	have_oder:
			s->allocflags = 0;
			if (order)
				s->allocflags |= __GFP_COMP;

			if (s->flags & SLAB_CACHE_DMA)
				s->allocflags |= GFP_DMA;

			if (s->flags & SLAB_RECLAIM_ACCOUNT)
				s->allocflags |= __GFP_RECLAIMABLE;

			s->oo = oo_make_tlx(order, size, s->reserved);
			s->min = oo_make_tlx(get_order(size), size, s->reserved);

			if (oo_objects_tlx(s->oo) > oo_objects_tlx(s->max))
				s->max = s->oo;

		unsigned long min = ilog2(s->size) / 2;
		if (min < MIN_PARTIAL)
									min = MIN_PARTIAL;
		else if (min > MAX_PARTIAL)
								min = MAX_PARTIAL;
		s->min_partial = min;


		if (s->size >= PAGE_SIZE)
			s->cpu_partial = 2;
		else if (s->size >= 1024)
			s->cpu_partial = 6;
		else if (s->size >= 256)
			s->cpu_partial = 13;
		else
			s->cpu_partial = 30;
			s->cpu_slab = pcpu_alloc_tlx(sizeof(struct kmem_cache_cpu),
																				2 * sizeof(void *), false);
			int cpu;
			for_each_possible_cpu(cpu)
				per_cpu_ptr(s->cpu_slab, cpu)->tid = cpu;
		if (slab_state_tlx <= UP)
			return 0;
			name = s->name;
		s->kobj.kset = slab_kset_tlx;
		err = kobject_init_and_add_tlx(&s->kobj, &slab_ktype_tlx, NULL, "%s", name);
		return err;
}


bool __cmpxchg_double_slab_tlx(struct kmem_cache *s, struct page *page,
		void *freelist_old, unsigned long counters_old,
		void *freelist_new, unsigned long counters_new,
		const char *n)
{
		if (page->freelist == freelist_old &&
					page->counters == counters_old) {
				page->freelist = freelist_new;
				struct page tmp;
				tmp.counters = counters_new;
				page->frozen  = tmp.frozen;
				page->inuse   = tmp.inuse;
				page->objects = tmp.objects;
			return 1;
		}
	return 0;
}

#define OO_SHIFT        16

static inline int oo_order_tlx(struct kmem_cache_order_objects x)
{
	return x.x >> OO_SHIFT;
}

#define for_each_object(__p, __s, __addr, __objects) \
	for (__p = (__addr); __p < (__addr) + (__objects) * (__s)->size;\
			__p += (__s)->size)


struct page *new_slab_tlx(struct kmem_cache *s, gfp_t flags, int node)
{
	struct page *page;
	void *start;
	void *last;
	void *p;
	int order;
	struct kmem_cache_node_tlx *n;
		flags &= (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK);
		struct kmem_cache_order_objects oo = s->oo;
		gfp_t alloc_gfp;
		flags &= gfp_allowed_mask_tlx;
		if (flags & __GFP_WAIT)
			local_irq_enable();
		flags |= s->allocflags;
		alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;
		order = oo_order_tlx(oo);
		page = alloc_pages(alloc_gfp | __GFP_NOTRACK, order);
		if (flags & __GFP_WAIT)
			local_irq_disable();
		page->objects = oo_objects_tlx(oo);
		mod_zone_page_state_tlx(page_zone_tlx(page),
			(s->flags & SLAB_RECLAIM_ACCOUNT) ?
			NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
			1 << oo_order_tlx(oo));
	order = 0 ;
	if (!PageHead(page))
								goto have_order;
	order = (unsigned long)page[1].lru.prev;
have_order:
	n = s->node[page_to_nid_tlx(page)];
			if (likely(n)) {
							atomic_long_inc_tlx(&n->nr_slabs);
							atomic_long_add_tlx(page->objects, &n->total_objects);         }
	page->slab_cache = s;
	__SetPageSlab(page);
	if (page->pfmemalloc)
		SetPageSlabPfmemalloc_tlx(page);
	start = page_address(page);
	if (unlikely(s->flags & SLAB_POISON))
		memset_tlx(start, POISON_INUSE, PAGE_SIZE << order);
	last = start;
	for_each_object(p, s, start, page->objects) {
		if (unlikely(s->ctor))
								s->ctor(last);
		*(void **)(last + s->offset) = p;
		last = p;
	}
		if (unlikely(s->ctor))
								s->ctor(last);
	*(void **)(last + s->offset) = NULL;
	page->freelist = start;
	page->inuse = page->objects;
	page->frozen = 1;
out:
	return page;
}

static void rcu_free_slab_tlx(struct rcu_head *h)
{
	struct page *page;

	if ((sizeof(((struct page *)NULL)->lru) < sizeof(struct rcu_head)))
		page = virt_to_head_page_tlx(h);
	else
		page = container_of((struct list_head *)h, struct page, lru);
		int order = 0;
		if (PageHead(page))
						order =  (unsigned long)page[1].lru.prev;
		int pages = 1 << order;
		mod_zone_page_state_tlx(page_zone_tlx(page),
			(page->slab_cache->flags & SLAB_RECLAIM_ACCOUNT) ?
			NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
			-pages);
		__ClearPageSlabPfmemalloc_tlx(page);
		__ClearPageSlab(page);
		atomic_set(&(page)->_mapcount, -1);
		if (current->reclaim_state)
			current->reclaim_state->reclaimed_slab += pages;
		__free_pages_tlx(page, order);
}

struct rcu_ctrlblk {
         struct rcu_head *rcucblist;     /* List of pending callbacks (CBs). */
         struct rcu_head **donetail;     /* ->next pointer of last "done" CB. */
         struct rcu_head **curtail;      /* ->next pointer of last CB. */
};

struct rcu_state rcu_preempt_state;

void __call_rcu_tlx(struct rcu_head *head,
                        void (*func)(struct rcu_head *rcu),
                        struct rcu_ctrlblk *rcp)
{
         unsigned long flags;
         head->func = func;
         head->next = NULL;

         local_irq_save(flags);
         *rcp->curtail = head;
         rcp->curtail = &head->next;
         local_irq_restore(flags);
}

#define RCU_GP_IDLE             0       /* No grace period in progress. */

#define RCU_STATE_INITIALIZER(sname, sabbr, cr) \
static char sname##_varname[] = #sname; \
static const char *tp_##sname##_varname = sname##_varname; \
struct rcu_state sname##_state = { \
	.level = { &sname##_state.node[0] }, \
	.call = cr, \
	.fqs_state = RCU_GP_IDLE, \
	.gpnum = 0UL - 300UL, \
	.completed = 0UL - 300UL, \
	.orphan_lock = __RAW_SPIN_LOCK_UNLOCKED(&sname##_state.orphan_lock), \
	.orphan_nxttail = &sname##_state.orphan_nxtlist, \
	.orphan_donetail = &sname##_state.orphan_donelist, \
	.barrier_mutex = __MUTEX_INITIALIZER(sname##_state.barrier_mutex), \
	.onoff_mutex = __MUTEX_INITIALIZER(sname##_state.onoff_mutex), \
	.name = sname##_varname, \
	.abbr = sabbr, \
}; \


void call_rcu_bh_tlx(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
{
	__call_rcu_tlx(head, func, &rcu_tlx_bh_state);
}

RCU_STATE_INITIALIZER(rcu_tlx_sched, 's', call_rcu_sched_tlx);
RCU_STATE_INITIALIZER(rcu_tlx_bh, 'b', call_rcu_bh_tlx);

void deactivate_slab_tlx(struct kmem_cache *s, struct page *page,
				void *freelist)
{
	enum slab_modes { M_NONE, M_PARTIAL, M_FULL, M_FREE };
	struct kmem_cache_node_tlx *n = s->node[page_to_nid_tlx(page)];
	int lock = 0;
	enum slab_modes l = M_NONE, m = M_NONE;
	void *nextfree;
	int tail = DEACTIVATE_TO_HEAD;
	struct page new;
	struct page old;

	if (page->freelist) {
		tail = DEACTIVATE_TO_TAIL;
	}
	while (freelist && (nextfree = *(void **)(freelist + s->offset))) {
		void *prior;
		unsigned long counters;
		do {
			prior = page->freelist;
			counters = page->counters;
			*(void **)(freelist + s->offset) = prior;
			new.counters = counters;
			new.inuse--;
		} while (!__cmpxchg_double_slab_tlx(s, page,
			prior, counters,
			freelist, new.counters,
			"drain percpu freelist"));
		freelist = nextfree;
	}
redo:
	old.freelist = page->freelist;
	old.counters = page->counters;
	new.counters = old.counters;
	if (freelist) {
		new.inuse--;
		*(void **)(freelist + s->offset) = old.freelist;
		new.freelist = freelist;
	} else
		new.freelist = old.freelist;
	new.frozen = 0;
	if (!new.inuse && n->nr_partial >= s->min_partial)
		m = M_FREE;
	else if (new.freelist) {
		m = M_PARTIAL;
		if (!lock) {
			spin_lock_tlx(&n->list_lock);
		}
	} else {
		m = M_FULL;
	}

	if (l != m) {
		if (l == M_PARTIAL) {
					lockdep_assert_held(&n->list_lock);
					list_del(&page->lru);
         	n->nr_partial--;
		}	else if (l == M_FULL)
				if (s->flags & SLAB_STORE_USER){
					lockdep_assert_held(&n->list_lock);
						list_del(&page->lru);
				}
		if (m == M_PARTIAL) {
				lockdep_assert_held(&n->list_lock);
				n->nr_partial++;
			if (tail == DEACTIVATE_TO_TAIL)
							list_add_tail(&page->lru, &n->partial);
			else
							list_add(&page->lru, &n->partial);
		} else if (m == M_FULL) {
				if (s->flags & SLAB_STORE_USER){
					lockdep_assert_held(&n->list_lock);
					list_add(&page->lru, &n->full);
				}
		}
	}
	l = m;
	if (!__cmpxchg_double_slab_tlx(s, page,
				old.freelist, old.counters,
				new.freelist, new.counters,
				"unfreezing slab"))
		goto redo;
	if (lock)
		spin_unlock_tlx(&n->list_lock);
	if (m == M_FREE) {
				struct kmem_cache_node_tlx *n = s->node[page_to_nid_tlx(page)];
				atomic_long_dec_tlx(&n->nr_slabs);
				atomic_long_sub_tlx(page->objects, &n->total_objects);
		if (unlikely(s->flags & SLAB_DESTROY_BY_RCU)) {
			struct rcu_head *head;
			if ((sizeof(((struct page *)NULL)->lru) < sizeof(struct rcu_head))) {
				int order;
				if (PageHead(page))
								order =  (unsigned long)page[1].lru.prev;
				int offset = (PAGE_SIZE << order) - s->reserved;
				head = page_address(page) + offset;
			} else {
				head = (void *)&page->lru;
			}
			__call_rcu_tlx(head, rcu_free_slab_tlx, &rcu_preempt_state);
		} else {
				int order = 0;
				if (PageHead(page))
								order =  (unsigned long)page[1].lru.prev;
				int pages = 1 << order;
				mod_zone_page_state_tlx(page_zone_tlx(page),
					(s->flags & SLAB_RECLAIM_ACCOUNT) ?
					NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
					-pages);
				__ClearPageSlabPfmemalloc_tlx(page);
				__ClearPageSlab(page);
				atomic_set(&(page)->_mapcount, -1);
				if (current->reclaim_state)
					current->reclaim_state->reclaimed_slab += pages;
				__free_pages_tlx(page, order);
		}
	}
}

void *slab_alloc_tlx(struct kmem_cache *s,
		gfp_t gfpflags, unsigned long addr)
{
	void **object;
	struct kmem_cache_cpu *c;
	struct page *page;
	unsigned long tid;
redo_:
	c =	per_cpu_ptr(s->cpu_slab, 0);
	tid = c->tid;
	object = c->freelist;
	page = c->page;
	if (unlikely(!object)) {
			int node = NUMA_NO_NODE;
			void *freelist;
			struct page *page;
			unsigned long flags;
			local_irq_save(flags);
			page = c->page;
			if (!page)
				goto new_slab;
redo:
			freelist = c->freelist;
			if (freelist)
				goto load_freelist;
				struct page new;
				unsigned long counters;
				do {
					freelist = page->freelist;
					counters = page->counters;
					new.counters = counters;
					new.inuse = page->objects;
					new.frozen = freelist != NULL;
				} while (!__cmpxchg_double_slab_tlx(s, page,
					freelist, counters,
					NULL, new.counters,
					"get_freelist"));
			freelist = freelist;
			if (!freelist) {
				c->page = NULL;
				goto new_slab;
			}
load_freelist:
			c->freelist = *(void **)(freelist + s->offset);
			c->tid = c->tid + 1;
			local_irq_restore(flags);
			object = freelist;
			goto ok;
new_slab:
			if (c->partial) {
				page = c->page = c->partial;
				c->partial = page->next;
				c->freelist = NULL;
				goto redo;
			}
				struct kmem_cache_cpu **pc = &c;
				c = *pc;
				page = new_slab_tlx(s, flags, node);
				if (page) {
					c = raw_cpu_ptr(s->cpu_slab);
					if (c->page) {
						deactivate_slab_tlx(s, c->page, c->freelist);
						c->tid = c->tid + 1;
						c->page = NULL;
						c->freelist = NULL;
					}
					freelist = page->freelist;
					page->freelist = NULL;
					c->page = page;
					*pc = c;
				} else
					freelist = NULL;

have_freelist:
			page = c->page;
			if (unlikely(PageSlabPfmemalloc_tlx(page))) {
									if (gfp_pfmemalloc_allowed_tlx(gfpflags)) goto load_freelist;
							} else goto load_freelist;
			deactivate_slab_tlx(s, page, *(void **)(freelist + s->offset));
			c->page = NULL;
			c->freelist = NULL;
			local_irq_restore(flags);
			object = freelist;
	} else {
		void *next_object = *(void **)(object + s->offset);
		if (unlikely(!this_cpu_cmpxchg_double(
				s->cpu_slab->freelist, s->cpu_slab->tid,
				object, tid,
				next_object, tid + 1))) {
			goto redo_;
		}
		__builtin_prefetch(next_object + s->offset);
	}
ok:
	if (unlikely(gfpflags & __GFP_ZERO) && object)
		memset_tlx(object, 0, s->object_size);
	return object;
}

struct kmem_cache *
kmem_cache_create_tlx(const char *name, size_t size, size_t align,
			unsigned long flags, void (*ctor)(void *))
{
	struct kmem_cache *s;
	char *cache_name;
	int err;
	flags &= 0xAF6D00;
		cache_name = kstrdup_tlx(name, GFP_KERNEL);
		err = -ENOMEM;
		s =  slab_alloc_tlx(kmem_cache_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_); //kmem_cache_alloc(kmem_cache, GFP_KERNEL | __GFP_ZERO);
		s->name = cache_name;
		s->object_size = size;
		s->size = size;
			if (flags & SLAB_HWCACHE_ALIGN) {
				unsigned long ralign = cache_line_size_tlx();
				while (size <= ralign / 2)
					ralign /= 2;
				align = max(align, ralign);
			}
			if (align < ARCH_SLAB_MINALIGN)
				align = ARCH_SLAB_MINALIGN;
			s->align = ALIGN(align, sizeof(void *));
		s->ctor = ctor;
		err = __kmem_cache_create_tlx(s, flags);
		s->refcount = 1;
		list_add(&s->list, &slab_caches_tlx);
	return s;
}

int mnt_id_start_tlx;
struct ida mnt_id_ida_tlx;



#define SOCKFS_MAGIC            0x534F434B

typedef enum {
	SS_FREE = 0,			/* not allocated		*/
	SS_UNCONNECTED,			/* unconnected to any socket	*/
	SS_CONNECTING,			/* in process of connecting	*/
	SS_CONNECTED,			/* connected to socket		*/
	SS_DISCONNECTING		/* in process of disconnecting	*/
} socket_state;

struct socket {
	socket_state		state;

	kmemcheck_bitfield_begin(type);
	short			type;
	kmemcheck_bitfield_end(type);

	unsigned long		flags;

	struct socket_wq __rcu	*wq;

	struct file		*file;
	struct sock		*sk;
	const struct proto_ops	*ops;
};

struct socket_wq {
         /* Note: wait MUST be first field of socket_wq */
         wait_queue_head_t       wait;
         struct fasync_struct    *fasync_list;
         struct rcu_head         rcu;
 };

struct socket_alloc {
	struct socket socket;
	struct inode vfs_inode;
};


struct inode *sock_alloc_inode_tlx(struct super_block *sb)
{
	struct socket_alloc *ei;
	struct socket_wq *wq;
//	ei = kmem_cache_alloc(sock_inode_cachep_tlx, GFP_KERNEL);
	ei = slab_alloc_tlx(sock_inode_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
	wq = kmalloc_tlx(sizeof(*wq), GFP_KERNEL);
	init_waitqueue_head(&wq->wait);
	wq->fasync_list = NULL;
//	RCU_INIT_POINTER(ei->socket.wq, wq);
	ei->socket.state = SS_UNCONNECTED;
	ei->socket.flags = 0;
	ei->socket.ops = NULL;
	ei->socket.sk = NULL;
	ei->socket.file = NULL;

	return &ei->vfs_inode;
}

const struct super_operations sockfs_ops_tlx = {
       .alloc_inode    = sock_alloc_inode_tlx,
       .destroy_inode  = NULL,
       .statfs         = NULL,
};

struct dentry_operations sockfs_dentry_operations_tlx;
struct super_operations simple_super_operations_tlx;
long nr_dentry_tlx;
spinlock_t inode_sb_list_lock_tlx;

struct dentry *__d_alloc_tlx(struct super_block *sb, const struct qstr *name)
{
	struct dentry *dentry;
	char *dname;

	dentry = slab_alloc_tlx(dentry_cache_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
	dentry->d_iname[DNAME_INLINE_LEN-1] = 0;
	if (name->len > DNAME_INLINE_LEN-1) {
		dname = kmalloc_tlx(name->len + 1, GFP_KERNEL);
	} else  {
		dname = dentry->d_iname;
	}
	dentry->d_name.len = name->len;
	dentry->d_name.hash = name->hash;
	memcpy_tlx(dname, name->name, name->len);
	dname[name->len] = 0;
	smp_wmb();
	dentry->d_name.name = dname;
	dentry->d_lockref.count = 1;
	dentry->d_flags = 0;
	spin_lock_init(&dentry->d_lock);
	seqcount_init(&dentry->d_seq);
	dentry->d_inode = NULL;
	dentry->d_parent = dentry;
	dentry->d_sb = sb;
	dentry->d_op = NULL;
	dentry->d_fsdata = NULL;
	INIT_HLIST_BL_NODE_tlx(&dentry->d_hash);
	INIT_LIST_HEAD(&dentry->d_lru);
	INIT_LIST_HEAD(&dentry->d_subdirs);
	INIT_HLIST_NODE(&dentry->d_alias);
	INIT_LIST_HEAD(&dentry->d_u.d_child);
	struct dentry_operations *op = dentry->d_sb->s_d_op;
		dentry->d_op = op;
		if (!op)
			goto no_op;
		if (op->d_hash)
			dentry->d_flags |= DCACHE_OP_HASH;
		if (op->d_compare)
			dentry->d_flags |= DCACHE_OP_COMPARE;
		if (op->d_revalidate)
			dentry->d_flags |= DCACHE_OP_REVALIDATE;
		if (op->d_weak_revalidate)
			dentry->d_flags |= DCACHE_OP_WEAK_REVALIDATE;
		if (op->d_delete)
			dentry->d_flags |= DCACHE_OP_DELETE;
		if (op->d_prune)
			dentry->d_flags |= DCACHE_OP_PRUNE;

no_op:
	this_cpu_inc(nr_dentry_tlx);

	return dentry;
}

struct dentry *d_alloc_tlx(struct dentry * parent, const struct qstr *name)
{
       struct dentry *dentry = __d_alloc_tlx(parent->d_sb, name);
       if (!dentry)
                 return NULL;
       dentry->d_parent = parent;
       list_add(&dentry->d_u.d_child, &parent->d_subdirs);

         return dentry;
}


void __d_instantiate_tlx(struct dentry *dentry, struct inode *inode)
{
	unsigned add_flags = DCACHE_FILE_TYPE;
	if (!inode) {
		add_flags =DCACHE_MISS_TYPE;
		goto have_flags;
	}
	if (S_ISDIR(inode->i_mode)) {
		add_flags = DCACHE_DIRECTORY_TYPE;
		if (unlikely(!(inode->i_opflags & IOP_LOOKUP))) {
			if (unlikely(!inode->i_op->lookup))
				add_flags = DCACHE_AUTODIR_TYPE;
			else
				inode->i_opflags |= IOP_LOOKUP;
		}
	} else if (unlikely(!(inode->i_opflags & IOP_NOFOLLOW))) {
		if (unlikely(inode->i_op->follow_link))
			add_flags = DCACHE_SYMLINK_TYPE;
		else
			inode->i_opflags |= IOP_NOFOLLOW;
	}
have_flags:
	spin_lock_tlx(&dentry->d_lock);
	dentry->d_flags = (dentry->d_flags & ~DCACHE_ENTRY_TYPE) | add_flags;
	if (inode)
		hlist_add_head(&dentry->d_alias, &inode->i_dentry);
	dentry->d_inode = inode;
	smp_wmb();
	(&dentry->d_seq)->sequence+=2;
	spin_unlock_tlx(&dentry->d_lock);
}


struct inode *new_inode_tlx(struct super_block *sb)
{
	struct inode *inode;
	inode = sb->s_op->alloc_inode(sb);
		static const struct inode_operations empty_iops;
		static const struct file_operations empty_fops;
		struct address_space *const mapping = &inode->i_data;
		inode->i_sb = sb;
		inode->i_blkbits = sb->s_blocksize_bits;
		inode->i_flags = 0;
		atomic_set(&inode->i_count, 1);
		inode->i_op = &empty_iops;
		inode->i_fop = &empty_fops;
		inode->__i_nlink = 1;
		inode->i_opflags = 0;
		i_uid_write_tlx(inode, 0);
		i_gid_write_tlx(inode, 0);
		atomic_set(&inode->i_writecount, 0);
		inode->i_size = 0;
		inode->i_blocks = 0;
		inode->i_bytes = 0;
		inode->i_generation = 0;
		inode->i_pipe = NULL;
		inode->i_bdev = NULL;
		inode->i_cdev = NULL;
		inode->i_rdev = 0;
		inode->dirtied_when = 0;
		spin_lock_init(&inode->i_lock);
		lockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);
		mutex_init(&inode->i_mutex);
		lockdep_set_class(&inode->i_mutex, &sb->s_type->i_mutex_key);
		atomic_set(&inode->i_dio_count, 0);
		mapping->a_ops = &empty_aops_tlx;
		mapping->host = inode;
		mapping->flags = 0;
		mapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);
		mapping->private_data = NULL;
		mapping->backing_dev_info = &default_backing_dev_info_tlx;
		mapping->writeback_index = 0;
		inode->i_mapping = mapping;
		inode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;
	if (inode) {
						inode->i_state = 0;
						INIT_LIST_HEAD(&inode->i_sb_list);
	}
	list_add(&inode->i_sb_list, &inode->i_sb->s_inodes);
	return inode;
}

struct dentry *mount_pseudo_tlx(struct file_system_type *fs_type, char *name,
	const struct super_operations *ops,
	const struct dentry_operations *dops, unsigned long magic)
{
	struct super_block *s;
	struct dentry *dentry;
	struct inode *root;
	struct qstr d_name = QSTR_INIT(name, strlen_tlx(name));

	s = sget_tlx(fs_type, NULL, set_anon_super_tlx, MS_NOUSER, NULL);
	s->s_maxbytes = MAX_LFS_FILESIZE;
	s->s_blocksize = PAGE_SIZE;
	s->s_blocksize_bits = PAGE_SHIFT;
	s->s_magic = magic;
	s->s_op = ops ? ops : &simple_super_operations_tlx;
	s->s_time_gran = 1;
	struct inode *inode;
	spin_lock_prefetch_tlx(&inode_sb_list_lock_tlx);
	inode =new_inode_tlx(s);

	root = inode;
	root->i_ino = 1;
	root->i_mode = S_IFDIR | S_IRUSR | S_IWUSR;
	root->i_atime = root->i_mtime = root->i_ctime = CURRENT_TIME;
	dentry = __d_alloc_tlx(s, &d_name);
	__d_instantiate_tlx(dentry, root);
	s->s_root = dentry;
	s->s_d_op = dops;
	s->s_flags |= MS_ACTIVE;
	return s->s_root;
}


struct vfsmount *
vfs_kern_mount_sk(struct file_system_type *type, int flags, const char *name, void *data)
{
	struct dentry *root;
	struct mount *mnt = kmem_cache_zalloc_tlx(mnt_cache_tlx, GFP_KERNEL);
	if (mnt) {
		int err;
		int res;
		ida_pre_get_tlx(&mnt_id_ida_tlx, GFP_KERNEL);
		res = ida_get_new_above_tlx(&mnt_id_ida_tlx, mnt_id_start_tlx, &mnt->mnt_id);
		if (!res)
								mnt_id_start_tlx = mnt->mnt_id + 1;
		if (name) {
					size_t len;
					char *buf;
					len = strlen_tlx(name) + 1;
					buf =  __kmalloc_tlx(len, GFP_KERNEL);
					if (buf)
								memcpy_tlx(buf, name, len);
					mnt->mnt_devname = buf;
		}
		mnt->mnt_pcp = alloc_percpu(struct mnt_pcp);
		this_cpu_add(mnt->mnt_pcp->mnt_count, 1);
		INIT_HLIST_NODE(&mnt->mnt_hash);
		INIT_LIST_HEAD(&mnt->mnt_child);
		INIT_LIST_HEAD(&mnt->mnt_mounts);
		INIT_LIST_HEAD(&mnt->mnt_list);
		INIT_LIST_HEAD(&mnt->mnt_expire);
		INIT_LIST_HEAD(&mnt->mnt_share);
		INIT_LIST_HEAD(&mnt->mnt_slave_list);
		INIT_LIST_HEAD(&mnt->mnt_slave);
	}

	if (flags & MS_KERNMOUNT)
		mnt->mnt.mnt_flags = MNT_INTERNAL;
	struct super_block *sb;
	char *secdata = NULL;
	root = mount_pseudo_tlx(type, "socket:", &sockfs_ops_tlx,
                 &sockfs_dentry_operations_tlx, SOCKFS_MAGIC);
	sb = root->d_sb;
	sb->s_flags |= MS_BORN;
	up_write_tlx(&sb->s_umount);
	free_page((unsigned long)secdata);
	mnt->mnt.mnt_root = root;
	mnt->mnt.mnt_sb = root->d_sb;
	mnt->mnt_mountpoint = mnt->mnt.mnt_root;
	mnt->mnt_parent = mnt;
	list_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);
	return &mnt->mnt;
}


struct file_operations pipefifo_fops_tlx;
#define LAST_INO_BATCH 1024
#define BOGO_DIRENT_SIZE 20
unsigned int last_ino_tlx;
#define TMPFS_MAGIC             0x01021994

struct inode *shmem_alloc_inode_tlx(struct super_block *sb)
{
	struct shmem_inode_info *info;
	info = slab_alloc_tlx(shmem_inode_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
//	info = kmem_cache_alloc(shmem_inode_cachep_tlx, GFP_KERNEL);
	return &info->vfs_inode;
}


struct super_operations shmem_ops_tlx = {
         .alloc_inode    = shmem_alloc_inode_tlx,
         .destroy_inode  = NULL,
 #ifdef CONFIG_TMPFS
         .statfs         = NULL,
         .remount_fs     = NULL,
         .show_options   = NULL,
 #endif
         .evict_inode    = NULL,
         .drop_inode     = NULL,
         .put_super      = NULL,
 };;

struct backing_dev_info shmem_backing_dev_info_tlx;
struct inode_operations shmem_special_inode_operations_tlx;
struct address_space_operations shmem_aops_tlx_tlx;
int shmem_setattr_tlx(struct dentry *dentry, struct iattr *attr)
{
	return 0;
}


struct inode_operations shmem_inode_operations_tlx = {
	.setattr	= shmem_setattr_tlx,
};


struct inode_operations shmem_dir_inode_operations_tlx;


static inline struct shmem_inode_info *SHMEM_I(struct inode *inode)
{
         return container_of(inode, struct shmem_inode_info, vfs_inode);
}

static inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)
 {
         return sb->s_fs_info;
 }







# define RELOC_HIDE_TLX(ptr, off)                                   \
	({ unsigned long __ptr;                                       \
			__ptr = (unsigned long) (ptr);                             \
		(typeof(ptr)) (__ptr + (off)); })

enum sgp_type {
         SGP_READ,       /* don't exceed i_size, don't allocate page */
         SGP_CACHE,      /* don't exceed i_size, may allocate page */
         SGP_DIRTY,      /* like SGP_CACHE, but set new page dirty */
         SGP_WRITE,      /* may exceed i_size, may allocate !Uptodate page */
         SGP_FALLOC,     /* like SGP_WRITE, but make existing page Uptodate */
 };


typedef int (kiocb_cancel_fn)(struct kiocb *);

 struct kiocb {
 	struct file		*ki_filp;
 	struct kioctx		*ki_ctx;	/* NULL for sync ops */
 	kiocb_cancel_fn		*ki_cancel;
 	void			*private;

 	union {
 		void __user		*user;
 		struct task_struct	*tsk;
 	} ki_obj;

 	__u64			ki_user_data;	/* user's data for completion */
 	loff_t			ki_pos;
 	size_t			ki_nbytes;	/* copy of iocb->aio_nbytes */

 	struct list_head	ki_list;	/* the aio core uses this
 						* for cancellation */

 	/*
 	* If the aio_resfd field of the userspace iocb is not zero,
 	* this is the underlying eventfd context to deliver events to.
 	*/
 	struct eventfd_ctx	*ki_eventfd;
 };

 struct bio_vec {
          struct page     *bv_page;
          unsigned int    bv_len;
          unsigned int    bv_offset;
 };

 #define kunmap_atomic(addr)                                     \
 do {                                                            \
        preempt_enable();                                \
 } while (0)

 void *kmap_atomic_tlx(struct page *page)
  {
          barrier();
          return page_address(page);
  }




static __always_inline int *preempt_count_ptr(void)
{
        return &current_thread_info_tlx_tlx()->preempt_count;
}

#define O_NOATIME       01000000

static inline void file_accessed(struct file *file)
{
        if (!(file->f_flags & O_NOATIME))
                touch_atime_tlx(&file->f_path);
}

static inline loff_t i_size_read(const struct inode *inode)
{
        return inode->i_size;
}

static inline size_t iov_iter_count(struct iov_iter *i)
 {
         return i->count;
 }

 static inline int mapping_writably_mapped(struct address_space *mapping)
 {
         return mapping->i_mmap_writable != 0;
 }

 void flush_dcache_page_tlx_tlx(struct page *page)
 {
          if (test_bit_tlx(PG_dcache_clean, &page->flags))
                  __clear_bit_tlx(PG_dcache_clean, &page->flags);
 }

#define ZERO_PAGE(vaddr)        (empty_zero_page_tlx)

static inline struct inode *file_inode_tlx(struct file *f)
 {
         return f->f_inode;
 }

 enum rq_flag_bits {
          /* common flags */
          __REQ_WRITE,            /* not set, read. set, write */
          __REQ_FAILFAST_DEV,     /* no driver retries of device errors */
          __REQ_FAILFAST_TRANSPORT, /* no driver retries of transport errors */
          __REQ_FAILFAST_DRIVER,  /* no driver retries of driver errors */

          __REQ_SYNC,             /* request is sync (sync write or read) */
          __REQ_META,             /* metadata io request */
          __REQ_PRIO,             /* boost priority in cfq */
          __REQ_DISCARD,          /* request to discard sectors */
          __REQ_SECURE,           /* secure discard (used with __REQ_DISCARD) */
          __REQ_WRITE_SAME,       /* write same block many times */

          __REQ_NOIDLE,           /* don't anticipate more IO after this one */
          __REQ_FUA,              /* forced unit access */
          __REQ_FLUSH,            /* request for cache flush */

          /* bio only flags */
          __REQ_RAHEAD,           /* read ahead, can fail anytime */
          __REQ_THROTTLED,        /* This bio has already been subjected to
                                   * throttling rules. Don't do it again. */

          /* request only flags */
        __REQ_SORTED,           /* elevator knows about this request */
          __REQ_SOFTBARRIER,      /* may not be passed by ioscheduler */
          __REQ_NOMERGE,          /* don't touch this for merging */
          __REQ_STARTED,          /* drive already may have started this one */
          __REQ_DONTPREP,         /* don't call prep for this one */
          __REQ_QUEUED,           /* uses queueing */
        __REQ_ELVPRIV,          /* elevator private data attached */
          __REQ_FAILED,           /* set if the request failed */
          __REQ_QUIET,            /* don't worry about errors */
          __REQ_PREEMPT,          /* set for "ide_preempt" requests */
          __REQ_ALLOCED,          /* request came from our alloc pool */
          __REQ_COPY_USER,        /* contains copies of user pages */
          __REQ_FLUSH_SEQ,        /* request for flush sequence */
          __REQ_IO_STAT,          /* account I/O stat */
          __REQ_MIXED_MERGE,      /* merge of different types, fail separately */
          __REQ_KERNEL,           /* direct IO to kernel pages */
          __REQ_PM,               /* runtime pm request */
          __REQ_END,              /* last of chain of requests */
          __REQ_HASHED,           /* on IO scheduler merge hash */
        __REQ_MQ_INFLIGHT,      /* track inflight for MQ */
          __REQ_NR_BITS,          /* stops here */
  };

 #define REQ_WRITE               (1ULL << __REQ_WRITE)
 #define RW_MASK                 REQ_WRITE
 #define WRITE                   RW_MASK
 #define READ                    0
 #define SEEK_DATA       3       /* seek to the next data */
 #define SEEK_HOLE       4       /* seek to the next hole */

 static inline int PageUptodate(struct page *page)
 {
 	int ret = test_bit_tlx(PG_uptodate, &(page)->flags);

 	/*
 	* Must ensure that the data we read out of the page is loaded
 	* _after_ we've loaded page->flags to check for PageUptodate.
 	* We can skip the barrier if the page is not uptodate, because
 	* we wouldn't be reading anything from it.
 	*
 	* See SetPageUptodate() for the other side of the story.
 	*/
 	if (ret)
 		smp_rmb();

 	return ret;
 }

 static inline int radix_tree_exceptional_entry(void *arg)
 {
         /* Not unlikely because radix_tree_exception often tested first */
         return (unsigned long)arg & RADIX_TREE_EXCEPTIONAL_ENTRY;
 }

 #define PAGEVEC_SIZE    14

 struct pagevec {
 				unsigned long nr;
 				unsigned long cold;
 				struct page *pages[PAGEVEC_SIZE];
 };

 static inline void pagevec_init(struct pagevec *pvec, int cold)
 {
          pvec->nr = 0;
          pvec->cold = cold;
 }


int shmem_getpage_gfp_tlx(struct inode *inode, pgoff_t index,
 struct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type);

 #define VM_FAULT_LOCKED 0x0200  /* ->fault locked the returned page */
 #define VM_FAULT_NOPAGE 0x0100  /* ->fault installed the pte, not return page */
 #define VM_FAULT_RETRY  0x0400  /* ->fault blocked, must retry */

 struct vm_fault {
 			unsigned int flags;             /* FAULT_FLAG_xxx flags */
 			pgoff_t pgoff;                  /* Logical page offset based on vma */
 			void __user *virtual_address;   /* Faulting virtual address */

 			struct page *page;              /* ->fault handlers should return a
 																					* page here, unless VM_FAULT_NOPAGE
 																					* is set (which is also implied by
 																					* VM_FAULT_ERROR).
 																					*/
 				/* for ->map_pages() only */
 				pgoff_t max_pgoff;              /* map pages for offset from pgoff till
 																					* max_pgoff inclusive */
 				pte_t *pte;                     /* pte entry associated with ->pgoff */
 };


 struct shmem_falloc {
          wait_queue_head_t *waitq; /* faults into hole wait for punch to end */
          pgoff_t start;          /* start of range currently being fallocated */
          pgoff_t next;           /* the next page offset to be fallocated */
          pgoff_t nr_falloced;    /* how many new pages have been fallocated */
          pgoff_t nr_unswapped;   /* how often writepage refused to swap out */
  };

	#define FOLL_WRITE      0x01    /* check pte is writable */
	#define FOLL_TOUCH      0x02    /* mark page accessed */
	#define FOLL_GET        0x04    /* do get_page on page */
	#define FOLL_FORCE      0x10    /* get_user_pages read/write w/o permission */
	#define FAULT_FLAG_WRITE        0x01    /* Fault was a write access */
	#define FAULT_FLAG_NONLINEAR    0x02    /* Fault was via a nonlinear mapping */
	#define FAULT_FLAG_MKWRITE      0x04    /* Fault was mkwrite of existing pte */
	#define FAULT_FLAG_ALLOW_RETRY  0x08    /* Retry fault if blocking */
	#define FAULT_FLAG_RETRY_NOWAIT 0x10    /* Don't drop mmap_sem and wait when retrying */
	#define FAULT_FLAG_KILLABLE     0x20    /* The fault task is in SIGKILL killable region */
	#define FAULT_FLAG_TRIED        0x40    /* second try */
	#define FAULT_FLAG_USER         0x80    /* The fault originated in userspace */
	#define FOLL_NOWAIT     0x20    /* if a disk transfer is needed, start the IO
	                                  * and return without waiting upon it */
	#define FOLL_NUMA       0x200   /* force NUMA hinting page fault */



	static inline long atomic_long_dec_return(atomic_long_t *l)
	{
	        atomic64_t *v = (atomic64_t *)l;

	        return (long)atomic64_dec_return(v);
	}

 static int shmem_fault_tlx(struct vm_area_struct *vma, struct vm_fault *vmf)
 {
 	struct inode *inode = vma->vm_file->f_inode;
 	int error;
 	int ret = VM_FAULT_LOCKED;
 	if (unlikely(inode->i_private)) {
 		struct shmem_falloc *shmem_falloc;
 //		spin_lock_tlx(&inode->i_lock);
 		shmem_falloc = inode->i_private;
 		if (shmem_falloc &&
 				shmem_falloc->waitq &&
 				vmf->pgoff >= shmem_falloc->start &&
 				vmf->pgoff < shmem_falloc->next) {
 			wait_queue_head_t *shmem_falloc_waitq;
 //			DEFINE_WAIT(shmem_fault_wait);
 			wait_queue_t shmem_fault_wait = {
 								.private        = current,
 //								.func           = autoremove_wake_function,
 								.task_list      = LIST_HEAD_INIT((shmem_fault_wait).task_list),
 				};

 			ret = VM_FAULT_NOPAGE;
 			if ((vmf->flags & FAULT_FLAG_ALLOW_RETRY) &&
 				!(vmf->flags & FAULT_FLAG_RETRY_NOWAIT)) {
 				/* It's polite to up mmap_sem if we can */
 //				up_read_tlx(&vma->vm_mm->mmap_sem);
 				struct rw_semaphore *sem = &vma->vm_mm->mmap_sem;
        	long tmp;
          tmp = atomic_long_dec_return((atomic_long_t *)&sem->count);
 //          if (unlikely(tmp < -1 && (tmp & RWSEM_ACTIVE_MASK) == 0))
 //                 if (!list_empty(&sem->wait_list))
 //                 sem = __rwsem_do_wake(sem, RWSEM_WAKE_ANY);
 				ret = VM_FAULT_RETRY;
 			}

 			shmem_falloc_waitq = shmem_falloc->waitq;
 			wait_queue_head_t *q = shmem_falloc_waitq;
 			wait_queue_t *wait = &shmem_fault_wait;
 //			prepare_to_wait(shmem_falloc_waitq, &shmem_fault_wait,
 //					TASK_UNINTERRUPTIBLE);
 			wait->flags &= ~WQ_FLAG_EXCLUSIVE;
 			if (list_empty(&wait->task_list))
 						list_add(&wait->task_list, &q->task_list);
 //							__add_wait_queue(q, wait);
 				set_current_state(TASK_UNINTERRUPTIBLE);

 //			spin_unlock_tlx(&inode->i_lock);
 			__schedule_tlx();
 //			schedule();
 //			spin_lock_tlx(&inode->i_lock);
 //			finish_wait(shmem_falloc_waitq, &shmem_fault_wait);
 			__set_current_state(TASK_RUNNING);
 			if (!list_empty_careful(&wait->task_list)) {
 								list_del_init(&wait->task_list);
 				}
 //			spin_unlock_tlx(&inode->i_lock);
 			return ret;
 		}
 //		spin_unlock_tlx(&inode->i_lock);
 	}

 //	error = shmem_getpage(inode, vmf->pgoff, &vmf->page, SGP_CACHE, &ret);

 	error = shmem_getpage_gfp_tlx(inode, vmf->pgoff, &vmf->page, SGP_WRITE,
 			(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK , NULL);
 	return ret;
 }

 struct vm_operations_struct shmem_vm_ops_tlx = {
 	.fault		= shmem_fault_tlx,
 };

 int shmem_mmap_tlx(struct file *file, struct vm_area_struct *vma)
 {
 	file_accessed(file);
 	vma->vm_ops = &shmem_vm_ops_tlx;
 	return 0;
 }

 struct radix_tree_iter {
         unsigned long   index;
         unsigned long   next_index;
         unsigned long   tags;
  };

void ** radix_tree_iter_init_tlx(struct radix_tree_iter *iter, unsigned long start)
 {
         iter->index = 0;
         iter->next_index = start;
         return NULL;
 }

 static inline int radix_tree_exception(void *arg)
 {
         return unlikely((unsigned long)arg &
                 (RADIX_TREE_INDIRECT_PTR | RADIX_TREE_EXCEPTIONAL_ENTRY));
 }

static inline void *radix_tree_deref_slot(void **pslot)
{
         return rcu_dereference(*pslot);
}
#define RADIX_TREE_ITER_TAGGED          0x0100  /* lookup tagged slots */
#define RADIX_TREE_ITER_CONTIG          0x0200  /* stop at first hole */
#define RADIX_TREE_ITER_TAG_MASK        0x00FF  /* tag index in lower byte */

static __always_inline unsigned long
radix_tree_find_next_bit(const unsigned long *addr,
			 unsigned long size, unsigned long offset)
{
	if (!__builtin_constant_p(size))
		return find_next_bit_tlx(addr, size, offset);

	if (offset < size) {
		unsigned long tmp;

		addr += offset / BITS_PER_LONG;
		tmp = *addr >> (offset % BITS_PER_LONG);
		if (tmp)
			return __ffs_tlx(tmp) + offset;
		offset = (offset + BITS_PER_LONG) & ~(BITS_PER_LONG - 1);
		while (offset < size) {
			tmp = *++addr;
			if (tmp)
				return __ffs_tlx(tmp) + offset;
			offset += BITS_PER_LONG;
		}
	}
	return size;
}

void **radix_tree_next_chunk_tlx(struct radix_tree_root *root,
			     struct radix_tree_iter *iter, unsigned flags)
{
	unsigned shift, tag = flags & RADIX_TREE_ITER_TAG_MASK;
	struct radix_tree_node *rnode, *node;
	unsigned long index, offset, height;
	index = iter->next_index;
	rnode = rcu_dereference_raw(root->rnode);
	if (radix_tree_is_indirect_ptr(rnode)) {
		rnode = indirect_to_ptr(rnode);
	} else if (rnode && !index) {
		/* Single-slot tree */
		iter->index = 0;
		iter->next_index = 1;
		iter->tags = 1;
		return (void **)&root->rnode;
	} else
		return NULL;

restart:
	height = rnode->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
	offset = index >> shift;
	node = rnode;
	while (1) {
		if ((flags & RADIX_TREE_ITER_TAGGED) ?
				!test_bit_tlx(offset, node->tags[tag]) :
				!node->slots[offset]) {
			/* Hole detected */
			if (flags & RADIX_TREE_ITER_CONTIG)
				return NULL;

			if (flags & RADIX_TREE_ITER_TAGGED)
				offset = radix_tree_find_next_bit(
						node->tags[tag],
						RADIX_TREE_MAP_SIZE,
						offset + 1);
			else
				while (++offset	< RADIX_TREE_MAP_SIZE) {
					if (node->slots[offset])
						break;
				}
			index &= ~((RADIX_TREE_MAP_SIZE << shift) - 1);
			index += offset << shift;
			/* Overflow after ~0UL */
			if (!index)
				return NULL;
			if (offset == RADIX_TREE_MAP_SIZE)
				goto restart;
		}

		/* This is leaf-node */
		if (!shift)
			break;

		node = rcu_dereference_raw(node->slots[offset]);
		if (node == NULL)
			goto restart;
		shift -= RADIX_TREE_MAP_SHIFT;
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
	}

	/* Update the iterator state */
	iter->index = index;
	iter->next_index = (index | RADIX_TREE_MAP_MASK) + 1;

	/* Construct iter->tags bit-mask from node->tags[tag] array */
	if (flags & RADIX_TREE_ITER_TAGGED) {
		unsigned tag_long, tag_bit;

		tag_long = offset / BITS_PER_LONG;
		tag_bit  = offset % BITS_PER_LONG;
		iter->tags = node->tags[tag][tag_long] >> tag_bit;
		/* This never happens if RADIX_TREE_TAG_LONGS == 1 */
		if (tag_long < RADIX_TREE_TAG_LONGS - 1) {
			/* Pick tags from next element */
			if (tag_bit)
				iter->tags |= node->tags[tag][tag_long + 1] <<
						(BITS_PER_LONG - tag_bit);
			/* Clip chunk size, here only BITS_PER_LONG tags */
			iter->next_index = index + BITS_PER_LONG;
		}
	}

	return node->slots + offset;
}


static __always_inline void **
radix_tree_next_slot_tlx(void **slot, struct radix_tree_iter *iter, unsigned flags)
{
	if (flags & RADIX_TREE_ITER_TAGGED) {
		iter->tags >>= 1;
		if (likely(iter->tags & 1ul)) {
			iter->index++;
			return slot + 1;
		}
		if (!(flags & RADIX_TREE_ITER_CONTIG) && likely(iter->tags)) {
			unsigned offset = __ffs_tlx(iter->tags);

			iter->tags >>= offset;
			iter->index += offset + 1;
			return slot + offset + 1;
		}
	} else {
		unsigned size = (iter->next_index - iter->index) - 1;

		while (size--) {
			slot++;
			iter->index++;
			if (likely(*slot))
				return slot;
			if (flags & RADIX_TREE_ITER_CONTIG) {
				/* forbid switching to the next chunk */
				iter->next_index = 0;
				break;
			}
		}
	}
	return NULL;
}


 #define radix_tree_for_each_slot(slot, root, iter, start)               \
         for (slot = radix_tree_iter_init_tlx(iter, start) ;                 \
            	slot || (slot = radix_tree_next_chunk_tlx(root, iter, 0)) ;    \
               slot = radix_tree_next_slot_tlx(slot, iter, 0))

 unsigned find_get_entries_tlx(struct address_space *mapping,
 			  pgoff_t start, unsigned int nr_entries,
 			  struct page **entries, pgoff_t *indices)
 {
 	void **slot;
 	unsigned int ret = 0;
 	struct radix_tree_iter iter;

 	if (!nr_entries)
 		return 0;

 restart:
 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
 		struct page *page;
 repeat:
 		page = radix_tree_deref_slot(slot);
 		if (unlikely(!page))
 			continue;
 		if (radix_tree_exception(page)) {
 			goto export;
 		}
 export:
 		indices[ret] = iter.index;
 		entries[ret] = page;
 		if (++ret == nr_entries)
 			break;
 	}
 	return ret;
 }

pgoff_t shmem_seek_hole_data_tlx(struct address_space *mapping,
				    pgoff_t index, pgoff_t end, int whence)
{
	struct page *page;
	struct pagevec pvec;
	pgoff_t indices[PAGEVEC_SIZE];
	bool done = false;
	int i;

	pagevec_init(&pvec, 0);
	pvec.nr = 1;		/* start small: we may be there already */
	while (!done) {
		pvec.nr = find_get_entries_tlx(mapping, index,
					pvec.nr, pvec.pages, indices);
		if (!pvec.nr) {
			if (whence == SEEK_DATA)
				index = end;
			break;
		}
		for (i = 0; i < pvec.nr; i++, index++) {
			if (index < indices[i]) {
				if (whence == SEEK_HOLE) {
					done = true;
					break;
				}
				index = indices[i];
			}
			page = pvec.pages[i];
			if (page && !radix_tree_exceptional_entry(page)) {
				if (!PageUptodate(page))
					page = NULL;
			}
			if (index >= end ||
			    (page && whence == SEEK_DATA) ||
			    (!page && whence == SEEK_HOLE)) {
				done = true;
				break;
			}
		}


		pvec.nr = PAGEVEC_SIZE;
		cond_resched();
	}
	return index;
}


loff_t shmem_file_llseek_tlx(struct file *file, loff_t offset, int whence)
{
	struct address_space *mapping = file->f_mapping;
	struct inode *inode = mapping->host;
	pgoff_t start, end;
	loff_t new_offset;

	if (whence != SEEK_DATA && whence != SEEK_HOLE)
		return 0;
	mutex_lock_tlx(&inode->i_mutex);
	/* We're holding i_mutex so we can access i_size directly */

	if (offset < 0)
		offset = -EINVAL;
	else if (offset >= inode->i_size)
		offset = -ENXIO;
	else {
		start = offset >> PAGE_CACHE_SHIFT;
		end = (inode->i_size + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
		new_offset = shmem_seek_hole_data_tlx(mapping, start, end, whence);
		new_offset <<= PAGE_CACHE_SHIFT;
		if (new_offset > offset) {
			if (new_offset < inode->i_size)
				offset = new_offset;
			else if (whence == SEEK_DATA)
				offset = -ENXIO;
			else
				offset = inode->i_size;
		}
	}

	if (offset >= 0)
		offset = 0;
	mutex_unlock_tlx(&inode->i_mutex);
	return offset;
}


static inline void init_sync_kiocb(struct kiocb *kiocb, struct file *filp)
{
         *kiocb = (struct kiocb) {
                         .ki_ctx = NULL,
                         .ki_filp = filp,
                         .ki_obj.tsk = current,
                 };
}

void iov_iter_init_tlx_tlx(struct iov_iter *i, int direction,
      const struct iovec *iov, unsigned long nr_segs,
      size_t count)
{
  /* It will get better.  Eventually... */
  if (segment_eq(get_fs(), KERNEL_DS))
    direction |= ITER_KVEC;
  i->type = direction;
  i->iov = iov;
  i->nr_segs = nr_segs;
  i->iov_offset = 0;
  i->count = count;
}


ssize_t new_sync_read_tlx_tlx(struct file *filp, char __user *buf, size_t len, loff_t *ppos)
{
  struct iovec iov = { .iov_base = buf, .iov_len = len };
  struct kiocb kiocb;
  struct iov_iter iter;
  ssize_t ret;

  init_sync_kiocb(&kiocb, filp);
  kiocb.ki_pos = *ppos;
  kiocb.ki_nbytes = len;
  iov_iter_init_tlx_tlx(&iter, READ, &iov, 1, len);

  ret = filp->f_op->read_iter(&kiocb, &iter);
//	if (-EIOCBQUEUED == ret)
//		ret = wait_on_sync_kiocb(&kiocb);
  *ppos = kiocb.ki_pos;
  return ret;
}






ssize_t new_sync_write_tlx_tlx(struct file *filp, const char __user *buf, size_t len, loff_t *ppos)
{
  struct iovec iov = { .iov_base = (void __user *)buf, .iov_len = len };
  struct kiocb kiocb;
  struct iov_iter iter;
  ssize_t ret;

  init_sync_kiocb(&kiocb, filp);
  kiocb.ki_pos = *ppos;
  kiocb.ki_nbytes = len;
  iov_iter_init_tlx_tlx(&iter, WRITE, &iov, 1, len);

  ret = filp->f_op->write_iter(&kiocb, &iter);
//	if (-EIOCBQUEUED == ret)
//		ret = wait_on_sync_kiocb(&kiocb);
  *ppos = kiocb.ki_pos;
  return ret;
}

static void memcpy_tlx_to_page_tlx_tlx(struct page *page, size_t offset, char *from, size_t len)
{
         char *to = kmap_atomic_tlx(page);
         memcpy_tlx(to + offset, from, len);
         kunmap_atomic(to);
}

size_t copy_page_to_iter_bvec_tlx_tlx(struct page *page, size_t offset, size_t bytes,
       struct iov_iter *i)
{
  size_t skip, copy, wanted;
  const struct bio_vec *bvec;
  void *kaddr, *from;

  if (unlikely(bytes > i->count))
    bytes = i->count;

  if (unlikely(!bytes))
    return 0;

  wanted = bytes;
  bvec = i->bvec;
  skip = i->iov_offset;
  copy = min_t(size_t, bytes, bvec->bv_len - skip);

  *preempt_count_ptr() += 1;
  kaddr = kmap_atomic_tlx(page);
  from = kaddr + offset;
  memcpy_tlx_to_page_tlx_tlx(bvec->bv_page, skip + bvec->bv_offset, from, copy);
  skip += copy;
  from += copy;
  bytes -= copy;
  while (bytes) {
    bvec++;
    copy = min(bytes, (size_t)bvec->bv_len);
    memcpy_tlx_to_page_tlx_tlx(bvec->bv_page, bvec->bv_offset, from, copy);
    skip = copy;
    from += copy;
    bytes -= copy;
  }
//	kunmap_atomic(kaddr);

    barrier();
  //         preempt_count_dec();
  *preempt_count_ptr() -= 1;
  if (skip == bvec->bv_len) {
    bvec++;
    skip = 0;
  }
  i->count -= wanted - bytes;
  i->nr_segs -= bvec - i->bvec;
  i->bvec = bvec;
  i->iov_offset = skip;
  return wanted - bytes;
}



static inline int fault_in_pages_writeable_tlx_tlx(char __user *uaddr, int size)
{
        int ret;

      if (unlikely(size == 0))
                return 0;

        /*
         * Writing zeroes into userspace here is OK, because we know that if
         * the zero gets there, we'll be overwriting it.
         */
        ret = __put_user(0, uaddr);
        if (ret == 0) {
                char __user *end = uaddr + size - 1;

                /*
                 * If the page was already mapped, this will get a cache miss
                 * for sure, so try to avoid doing it.
                 */
                if (((unsigned long)uaddr & PAGE_MASK) !=
                                ((unsigned long)end & PAGE_MASK))
                        ret = __put_user(0, end);
        }
      return ret;
}


size_t copy_page_to_iter_iovec_tlx_tlx(struct page *page, size_t offset, size_t bytes,
      struct iov_iter *i)
{
 size_t skip, copy, left, wanted;
 const struct iovec *iov;
 char __user *buf;
 void *kaddr, *from;

 if (unlikely(bytes > i->count))
   bytes = i->count;

 if (unlikely(!bytes))
   return 0;

 wanted = bytes;
 iov = i->iov;
 skip = i->iov_offset;
 buf = iov->iov_base + skip;
 copy = min(bytes, iov->iov_len - skip);

 if (!fault_in_pages_writeable_tlx_tlx(buf, copy)) {
   *preempt_count_ptr() += 1;
   kaddr = kmap_atomic_tlx(page);
   from = kaddr + offset;

   /* first chunk, usually the only one */
   left = __copy_to_user_tlx(buf, from, copy);
   copy -= left;
   skip += copy;
   from += copy;
   bytes -= copy;

   while (unlikely(!left && bytes)) {
     iov++;
     buf = iov->iov_base;
     copy = min(bytes, iov->iov_len);
     left = __copy_to_user_tlx(buf, from, copy);
     copy -= left;
     skip = copy;
     from += copy;
     bytes -= copy;
   }
   if (likely(!bytes)) {
//			kunmap_atomic(kaddr);
     barrier();
     *preempt_count_ptr() -= 1;
     goto done;
   }
   offset = from - kaddr;
   buf += copy;
//		kunmap_atomic(kaddr);
   barrier();
   *preempt_count_ptr() -= 1;
   copy = min(bytes, iov->iov_len - skip);
 }
 /* Too bad - revert to non-atomic kmap */
 kaddr = kmap_atomic_tlx(page);
 from = kaddr + offset;
 left = __copy_to_user_tlx(buf, from, copy);
 copy -= left;
 skip += copy;
 from += copy;
 bytes -= copy;
 while (unlikely(!left && bytes)) {
   iov++;
   buf = iov->iov_base;
   copy = min(bytes, iov->iov_len);
   left = __copy_to_user_tlx(buf, from, copy);
   copy -= left;
   skip = copy;
   from += copy;
   bytes -= copy;
 }
//	kunmap(page);
   barrier();
   *preempt_count_ptr() -= 1;
done:
 if (skip == iov->iov_len) {
   iov++;
   skip = 0;
 }
 i->count -= wanted - bytes;
 i->nr_segs -= iov - i->iov;
 i->iov = iov;
 i->iov_offset = skip;
 return wanted - bytes;
}

size_t copy_page_to_iter_tlx_tlx(struct page *page, size_t offset, size_t bytes,
       struct iov_iter *i)
{
  if (i->type & ITER_BVEC)
    return copy_page_to_iter_bvec_tlx_tlx(page, offset, bytes, i);
  else
    return copy_page_to_iter_iovec_tlx_tlx(page, offset, bytes, i);
}

ssize_t shmem_file_read_iter_tlx(struct kiocb *iocb, struct iov_iter *to)
{
	struct file *file = iocb->ki_filp;
	struct inode *inode = file_inode_tlx(file);
	struct address_space *mapping = inode->i_mapping;
	pgoff_t index;
	unsigned long offset;
	enum sgp_type sgp = SGP_READ;
	int error = 0;
	ssize_t retval = 0;
	loff_t *ppos = &iocb->ki_pos;

	/*
	 * Might this read be for a stacking filesystem?  Then when reading
	 * holes of a sparse file, we actually need to allocate those pages,
	 * and even mark them dirty, so it cannot exceed the max_blocks limit.
	 */
	if (segment_eq(get_fs(), KERNEL_DS))
		sgp = SGP_DIRTY;

	index = *ppos >> PAGE_CACHE_SHIFT;
	offset = *ppos & ~PAGE_CACHE_MASK;

	for (;;) {
		struct page *page = NULL;
		pgoff_t end_index;
		unsigned long nr, ret;
		loff_t i_size = i_size_read(inode);

		end_index = i_size >> PAGE_CACHE_SHIFT;
		if (index > end_index)
			break;
		if (index == end_index) {
			nr = i_size & ~PAGE_CACHE_MASK;
			if (nr <= offset)
				break;
		}

//		error = shmem_getpage(inode, index, &page, sgp, NULL);
		error =  shmem_getpage_gfp_tlx(inode, index, &page, sgp,
												(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK, NULL);
		if (error) {
			if (error == -EINVAL)
				error = 0;
			break;
		}

		/*
		 * We must evaluate after, since reads (unlike writes)
		 * are called without i_mutex protection against truncate
		 */
		nr = PAGE_CACHE_SIZE;
		i_size = i_size_read(inode);
		end_index = i_size >> PAGE_CACHE_SHIFT;
		if (index == end_index) {
			nr = i_size & ~PAGE_CACHE_MASK;
			if (nr <= offset) {
				if (page)
//					page_cache_release(page);
				break;
			}
		}
		nr -= offset;

		if (page) {
			/*
			 * If users can be writing to this page using arbitrary
			 * virtual addresses, take care about potential aliasing
			 * before reading the page on the kernel side.
			 */
			if (mapping_writably_mapped(mapping))
				flush_dcache_page_tlx_tlx(page);
		} else {
			page = ZERO_PAGE(0);
//			page_cache_get(page);
		}

		/*
		 * Ok, we have the page, and it's up-to-date, so
		 * now we can copy it to user space...
		 */
		ret = copy_page_to_iter_tlx_tlx(page, offset, nr, to);
		retval += ret;
		offset += ret;
		index += offset >> PAGE_CACHE_SHIFT;
		offset &= ~PAGE_CACHE_MASK;

//		page_cache_release(page);
		if (!iov_iter_count(to))
			break;
		if (ret < nr) {
			error = -EFAULT;
			break;
		}
		cond_resched();
	}

	*ppos = ((loff_t) index << PAGE_CACHE_SHIFT) + offset;
	file_accessed(file);
	return retval ? retval : error;
}



void advance_iovec_tlx_tlx(struct iov_iter *i, size_t bytes)
{

  if (likely(i->nr_segs == 1)) {
    i->iov_offset += bytes;
    i->count -= bytes;
  } else {
    const struct iovec *iov = i->iov;
    size_t base = i->iov_offset;
    unsigned long nr_segs = i->nr_segs;

    /*
     * The !iov->iov_len check ensures we skip over unlikely
     * zero-length segments (without overruning the iovec).
     */
    while (bytes || unlikely(i->count && !iov->iov_len)) {
      int copy;

      copy = min(bytes, iov->iov_len - base);
      i->count -= copy;
      bytes -= copy;
      base += copy;
      if (iov->iov_len == base) {
        iov++;
        nr_segs--;
        base = 0;
      }
    }
    i->iov = iov;
    i->iov_offset = base;
    i->nr_segs = nr_segs;
  }
}


void advance_bvec_tlx_tlx(struct iov_iter *i, size_t bytes)
{

  if (likely(i->nr_segs == 1)) {
    i->iov_offset += bytes;
    i->count -= bytes;
  } else {
    const struct bio_vec *bvec = i->bvec;
    size_t base = i->iov_offset;
    unsigned long nr_segs = i->nr_segs;

    /*
     * The !iov->iov_len check ensures we skip over unlikely
     * zero-length segments (without overruning the iovec).
     */
    while (bytes || unlikely(i->count && !bvec->bv_len)) {
      int copy;

      copy = min(bytes, bvec->bv_len - base);
      i->count -= copy;
      bytes -= copy;
      base += copy;
      if (bvec->bv_len == base) {
        bvec++;
        nr_segs--;
        base = 0;
      }
    }
    i->bvec = bvec;
    i->iov_offset = base;
    i->nr_segs = nr_segs;
  }
}



size_t copy_from_user_atomic_iovec_tlx_tlx(struct page *page,
    struct iov_iter *i, unsigned long offset, size_t bytes)
{
  char *kaddr;
  size_t copied;
  *preempt_count_ptr() += 1;
  kaddr = kmap_atomic_tlx(page);
  if (likely(i->nr_segs == 1)) {
    int left;
    char __user *buf = i->iov->iov_base + i->iov_offset;
    left = __copy_from_user_tlx(kaddr + offset, buf, bytes);
    copied = bytes - left;
  } else {
  //	copied = __iovec_copy_from_user_inatomic(kaddr + offset,
//						i->iov, i->iov_offset, bytes);
    char *vaddr = kaddr + offset;
    const struct iovec *iov = i->iov;
    size_t base = i->iov_offset;
//		, size_t bytes)
//		{
      size_t copied = 0, left = 0;

      while (bytes) {
        char __user *buf = iov->iov_base + base;
        int copy = min(bytes, iov->iov_len - base);

        base = 0;
        left =  __copy_from_user_tlx(vaddr, buf, copy);
        copied += copy;
        bytes -= copy;
        vaddr += copy;
        iov++;

        if (unlikely(left))
          break;
      }
      copied = copied - left;

  }
//	BUILD_BUG_ON(__same_type((kaddr), struct page *));
// __kunmap_atomic(kaddr);
//	pagefault_enable();
  #ifndef CONFIG_PREEMPT
         barrier();
//         preempt_count_dec();
        *preempt_count_ptr() -= 1;
 #else
//         preempt_enable();
 #endif
  return copied;
}


size_t copy_from_user_bvec_tlx_tlx(struct page *page,
    struct iov_iter *i, unsigned long offset, size_t bytes)
{
  char *kaddr;
  size_t left;
  const struct bio_vec *bvec;
  size_t base = i->iov_offset;

//  kaddr = kmap_atomic(page);
  *preempt_count_ptr() += 1;
  kaddr = kmap_atomic_tlx(page);

  for (left = bytes, bvec = i->bvec; left; bvec++, base = 0) {
    size_t copy = min(left, bvec->bv_len - base);
    if (!bvec->bv_len)
      continue;
//    memcpy_tlx_from_page(kaddr + offset, bvec->bv_page,
//         bvec->bv_offset + base, copy);
  //    (char *to, struct page *page, size_t offset, size_t len)
      char *kaddr_;
      *preempt_count_ptr() += 1;
      kaddr_ = kmap_atomic_tlx(bvec->bv_page);
      memcpy_tlx(kaddr_ + offset, kaddr_ + bvec->bv_offset + base, copy);
      barrier();
    //         preempt_count_dec();
     *preempt_count_ptr() -= 1;
    offset += copy;
    left -= copy;
  }
//  kunmap_atomic(kaddr);
  barrier();
//         preempt_count_dec();
 *preempt_count_ptr() -= 1;
  return bytes;
}




int iov_iter_fault_in_readable_tlx_tlx(struct iov_iter *i, size_t bytes)
{
  if (!(i->type & ITER_BVEC)) {
    char __user *buf = i->iov->iov_base + i->iov_offset;
    bytes = min(bytes, i->iov->iov_len - i->iov_offset);
//		return fault_in_pages_readable(buf, bytes);
    const char __user *uaddr = buf;
    int size = bytes;
         volatile char c;
         int ret;
          if (unlikely(size == 0))
                 return 0;

         ret = __get_user(c, uaddr);
         if (ret == 0) {
                 const char __user *end = uaddr + size - 1;
                 if (((unsigned long)uaddr & PAGE_MASK) !=
                                 ((unsigned long)end & PAGE_MASK)) {
                         ret = __get_user(c, end);
                         (void)c;
                 }
         }
         return ret;
  }
  return 0;
}




#define AOP_FLAG_UNINTERRUPTIBLE        0x0001 /* will not do a short write */



ssize_t generic_perform_write_tlx_tlx(struct file *file,
        struct iov_iter *i, loff_t pos)
{
  struct address_space *mapping = file->f_mapping;
  const struct address_space_operations *a_ops = mapping->a_ops;
  long status = 0;
  ssize_t written = 0;
  unsigned int flags = 0;

  /*
   * Copies from kernel address space cannot fail (NFSD is a big user).
   */
  if (segment_eq(get_fs(), KERNEL_DS))
    flags |= AOP_FLAG_UNINTERRUPTIBLE;

  do {
    struct page *page;
    unsigned long offset;	/* Offset into pagecache page */
    unsigned long bytes;	/* Bytes to write to page */
    size_t copied;		/* Bytes copied from user */
    void *fsdata;

    offset = (pos & (PAGE_CACHE_SIZE - 1));
    bytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,
            iov_iter_count(i));

again:
    if (unlikely(iov_iter_fault_in_readable_tlx_tlx(i, bytes))) {
      status = -EFAULT;
      break;
    }
    status = a_ops->write_begin(file, mapping, pos, bytes, flags,
            &page, &fsdata);
    if (unlikely(status < 0))
      break;
//		if (mapping_writably_mapped(mapping))
      flush_dcache_page_tlx_tlx(page);
//		copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);
      if (i->type & ITER_BVEC)
                 copied =  copy_from_user_bvec_tlx_tlx(page, i, offset, bytes);
         else
                 copied =  copy_from_user_atomic_iovec_tlx_tlx(page, i, offset, bytes);
    flush_dcache_page_tlx_tlx(page);
    status = a_ops->write_end(file, mapping, pos, bytes, copied,
            page, fsdata);
    if (unlikely(status < 0))
      break;
    copied = status;

//		cond_resched();
      _cond_resched_tlx();
//		iov_iter_advance(i, copied);
         if (i->type & ITER_BVEC)
                 advance_bvec_tlx_tlx(i, copied);
         else
                 advance_iovec_tlx_tlx(i, copied);
    pos += copied;
    written += copied;
//		balance_dirty_pages_ratelimited(mapping);
    if (__fatal_signal_pending_tlx(current)) {
      status = -EINTR;
      break;
    }
  } while (i->count);

  return written ? written : status;
}

ssize_t generic_file_write_iter_tlx_tlx(struct kiocb *iocb, struct iov_iter *from){
//    struct file *file = iocb->ki_filp;
//    struct inode *inode = file->f_mapping->host;
      struct file *file = iocb->ki_filp;
      struct address_space * mapping = file->f_mapping;
      struct inode 	*inode = mapping->host;
      loff_t		pos = iocb->ki_pos;
      ssize_t		written = 0;
      ssize_t		err;
      ssize_t		status;
      size_t		count = from->count;

      /* We can write back this queue in page reclaim */
      current->backing_dev_info = mapping->backing_dev_info;
    //	err = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));
      if (count == 0)
        goto out;
    //	iov_iter_truncate(from, count);
        written = generic_perform_write_tlx_tlx(file, from, pos);
        if (likely(written >= 0))
          iocb->ki_pos = pos + written;
    //	}
    out:
      current->backing_dev_info = NULL;
      return written ? written : err;

};







static void mem_cgroup_uncharge_start(void) {};
static void mem_cgroup_uncharge_end(void) {};
static inline unsigned pagevec_count(struct pagevec *pvec)
{
         return pvec->nr;
}

static int shmem_free_swap(struct address_space *mapping,
			   pgoff_t index, void *radswap)
{

	return 0;
}
static inline int trylock_page(struct page *page)
 {
         return 0;
 }










void zero_user_segments_tlx_tlx(struct page *page,
         unsigned start1, unsigned end1,
         unsigned start2, unsigned end2)
{
         void *kaddr = kmap_atomic_tlx(page);



         if (end1 > start1)
                 memset_tlx(kaddr + start1, 0, end1 - start1);

         if (end2 > start2)
                 memset_tlx(kaddr + start2, 0, end2 - start2);

         kunmap_atomic(kaddr);
         flush_dcache_page_tlx_tlx(page);
 }


 static inline void zero_user_segment_tlx_tlx(struct page *page,
          unsigned start, unsigned end)
 {
          zero_user_segments_tlx_tlx(page, start, end, 0, 0);
 }






struct partial_page {
         unsigned int offset;
         unsigned int len;
         unsigned long private;
};


struct splice_pipe_desc {
         struct page **pages;            /* page map */
         struct partial_page *partial;   /* pages[] may not be contig */
         int nr_pages;                   /* number of populated pages in map */
         unsigned int nr_pages_max;      /* pages[] & partial[] arrays size */
         unsigned int flags;             /* splice flags */
         const struct pipe_buf_operations *ops;/* ops associated with output pipe */
         void (*spd_release)(struct splice_pipe_desc *, unsigned int);
 };

 #define PIPE_DEF_BUFFERS        16

ssize_t shmem_file_splice_read_tlx(struct file *in, loff_t *ppos,
				struct pipe_inode_info *pipe, size_t len,
				unsigned int flags)
{
	struct address_space *mapping = in->f_mapping;
	struct inode *inode = mapping->host;
	unsigned int loff, nr_pages, req_pages;
	struct page *pages[PIPE_DEF_BUFFERS];
	struct partial_page partial[PIPE_DEF_BUFFERS];
	struct page *page;
	pgoff_t index, end_index;
	loff_t isize, left;
	int error, page_nr;
	struct splice_pipe_desc spd = {
		.pages = pages,
		.partial = partial,
		.nr_pages_max = PIPE_DEF_BUFFERS,
		.flags = flags,
	};

	isize = i_size_read(inode);
	if (unlikely(*ppos >= isize))
		return 0;

	left = isize - *ppos;
	if (unlikely(left < len))
		len = left;



	index = *ppos >> PAGE_CACHE_SHIFT;
	loff = *ppos & ~PAGE_CACHE_MASK;
	req_pages = (len + loff + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
	nr_pages = min(req_pages, spd.nr_pages_max);

	spd.nr_pages = 0;
	index += spd.nr_pages;
	error = 0;

	while (spd.nr_pages < nr_pages) {
//		error = shmem_getpage(inode, index, &page, SGP_CACHE, NULL);
		error =  shmem_getpage_gfp_tlx(inode, index, &page, SGP_CACHE,
												(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK, NULL);
		if (error)
			break;
//		unlock_page_tlx(page);
		spd.pages[spd.nr_pages++] = page;
		index++;
	}

	index = *ppos >> PAGE_CACHE_SHIFT;
	nr_pages = spd.nr_pages;
	spd.nr_pages = 0;

	for (page_nr = 0; page_nr < nr_pages; page_nr++) {
		unsigned int this_len;

		if (!len)
			break;

		this_len = min_t(unsigned long, len, PAGE_CACHE_SIZE - loff);
		page = spd.pages[page_nr];

		if (!PageUptodate(page) || page->mapping != mapping) {
				error =  shmem_getpage_gfp_tlx(inode, index, &page,	SGP_CACHE,
																	(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK, NULL);
			if (error)
				break;
//			unlock_page_tlx(page);
//			page_cache_release(spd.pages[page_nr]);
			spd.pages[page_nr] = page;
		}

		isize = i_size_read(inode);
		end_index = (isize - 1) >> PAGE_CACHE_SHIFT;
		if (unlikely(!isize || index > end_index))
			break;

		if (end_index == index) {
			unsigned int plen;

			plen = ((isize - 1) & ~PAGE_CACHE_MASK) + 1;
			if (plen <= loff)
				break;

			this_len = min(this_len, plen - loff);
			len = this_len;
		}

		spd.partial[page_nr].offset = loff;
		spd.partial[page_nr].len = this_len;
		len -= this_len;
		loff = 0;
		spd.nr_pages++;
		index++;
	}

//	while (page_nr < nr_pages)
//		page_cache_release(spd.pages[page_nr++]);



	if (error > 0) {
		*ppos += error;
		file_accessed(in);
	}
	return error;
}

void shmem_undo_range_tlx(struct inode *inode, loff_t lstart, loff_t lend,
								 bool unfalloc)
{
	struct address_space *mapping = inode->i_mapping;
	struct shmem_inode_info *info = SHMEM_I(inode);
	pgoff_t start = (lstart + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
	pgoff_t end = (lend + 1) >> PAGE_CACHE_SHIFT;
	unsigned int partial_start = lstart & (PAGE_CACHE_SIZE - 1);
	unsigned int partial_end = (lend + 1) & (PAGE_CACHE_SIZE - 1);
	struct pagevec pvec;
	pgoff_t indices[PAGEVEC_SIZE];
	long nr_swaps_freed = 0;
	pgoff_t index;
	int i;

	if (lend == -1)
		end = -1;	/* unsigned, so actually very big */

	pagevec_init(&pvec, 0);
	index = start;
	while (index < end) {
		pvec.nr = find_get_entries_tlx(mapping, index,
			min(end - index, (pgoff_t)PAGEVEC_SIZE),
			pvec.pages, indices);
		if (!pvec.nr)
			break;
		mem_cgroup_uncharge_start();
		for (i = 0; i < pagevec_count(&pvec); i++) {
			struct page *page = pvec.pages[i];

			index = indices[i];
			if (index >= end)
				break;

			if (radix_tree_exceptional_entry(page)) {
				if (unfalloc)
					continue;
				nr_swaps_freed += !shmem_free_swap(mapping,
								index, page);
				continue;
			}

			if (!trylock_page(page))
				continue;
			if (!unfalloc || !PageUptodate(page)) {
				if (page->mapping == mapping) {
//					VM_BUG_ON_PAGE(PageWriteback(page), page););
				}
			}
//			unlock_page_tlx(page);
		}


		mem_cgroup_uncharge_end();
		cond_resched();
		index++;
	}

	if (partial_start) {
		struct page *page = NULL;
		shmem_getpage_gfp_tlx(inode, start - 1, &page, SGP_READ,
						(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK, NULL);
		if (page) {
			unsigned int top = PAGE_CACHE_SIZE;
			if (start > end) {
				top = partial_end;
				partial_end = 0;
			}
			zero_user_segment_tlx_tlx(page, partial_start, top);
//			set_page_dirty_tlx(page);
//			unlock_page_tlx(page);
//			page_cache_release(page);
		}
	}
	if (partial_end) {
		struct page *page = NULL;
//		shmem_getpage(inode, end, &page, SGP_READ, NULL);
		shmem_getpage_gfp_tlx(inode, end, &page, SGP_READ,
						(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK, NULL);
		if (page) {
			zero_user_segment_tlx_tlx(page, 0, partial_end);
//			set_page_dirty_tlx(page);
//			unlock_page_tlx(page);
//			page_cache_release(page);
		}
	}
	if (start >= end)
		return;

	index = start;
	while (index < end) {
		cond_resched();

		pvec.nr = find_get_entries_tlx(mapping, index,
				min(end - index, (pgoff_t)PAGEVEC_SIZE),
				pvec.pages, indices);
		if (!pvec.nr) {
			/* If all gone or hole-punch or unfalloc, we're done */
			if (index == start || end != -1)
				break;
			/* But if truncating, restart to make sure all gone */
			index = start;
			continue;
		}
		mem_cgroup_uncharge_start();
		for (i = 0; i < pagevec_count(&pvec); i++) {
			struct page *page = pvec.pages[i];

			index = indices[i];
			if (index >= end)
				break;

			if (radix_tree_exceptional_entry(page)) {
				if (unfalloc)
					continue;
				if (shmem_free_swap(mapping, index, page)) {
					/* Swap was replaced by page: retry */
					index--;
					break;
				}
				nr_swaps_freed++;
				continue;
			}

//			lock_page_tlx(page);
			if (!unfalloc || !PageUptodate(page)) {
				if (page->mapping == mapping) {
//					VM_BUG_ON_PAGE(PageWriteback(page), page);
				} else {
					/* Page was replaced by swap: retry */
//					unlock_page_tlx(page);
					index--;
					break;
				}
			}
//			unlock_page_tlx(page);
		}

		mem_cgroup_uncharge_end();
		index++;
	}

	spin_lock_tlx(&info->lock);
	info->swapped -= nr_swaps_freed;
//	shmem_recalc_inode(inode);
	spin_unlock_tlx(&info->lock);
}




#define ENOSPC          28      /* No space left on device */

static inline void i_size_write(struct inode *inode, loff_t i_size)
{
        inode->i_size = i_size;
}

static void __wake_up_tlx(wait_queue_head_t *q, unsigned int mode,
                         int nr_exclusive, void *key)
{
         unsigned long flags;

//         spin_lock_irqsave(&q->lock, flags);
//         __wake_up_common(q, mode, nr_exclusive, 0, key);
//          wait_queue_head_t *q = &x->wait;
  //        unsigned int mode = TASK_NORMAL;
//          int nr_exclusive = 1;
          int wake_flags = 0;
  //       void *key = NULL;
          wait_queue_t *curr, *next;

          list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
                unsigned flags = curr->flags;

              if (curr->func(curr, mode, wake_flags, key) &&
                                (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
                        break;
         }

//         spin_unlock_irqrestore(&q->lock, flags);
}


#define wake_up_all(x)                  __wake_up_tlx(x, TASK_NORMAL, 0, NULL)

void shmem_truncate_range_tlx(struct inode *inode, loff_t lstart, loff_t lend)
{
	shmem_undo_range_tlx(inode, lstart, lend, false);
	inode->i_ctime = inode->i_mtime = CURRENT_TIME;
}


#define FALLOC_FL_KEEP_SIZE     0x01 /* default is extend size */
#define FALLOC_FL_PUNCH_HOLE    0x02 /* de-allocates range */




long shmem_fallocate_tlx(struct file *file, int mode, loff_t offset,
							 loff_t len)
{
	struct inode *inode = file_inode_tlx(file);
	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
	struct shmem_falloc shmem_falloc;
	pgoff_t start, index, end;
	int error;
	mutex_lock_tlx(&inode->i_mutex);

	if (mode & FALLOC_FL_PUNCH_HOLE) {
		struct address_space *mapping = file->f_mapping;
		loff_t unmap_start = round_up(offset, PAGE_SIZE);
		loff_t unmap_end = round_down(offset + len, PAGE_SIZE) - 1;
//		DECLARE_WAIT_QUEUE_HEAD_ONSTACK(shmem_falloc_waitq);
		 wait_queue_head_t shmem_falloc_waitq =  {                           \
         .lock           = __SPIN_LOCK_UNLOCKED(shmem_falloc_waitq.lock),              \
         .task_list      = { &(shmem_falloc_waitq).task_list, &(shmem_falloc_waitq).task_list } };
	//	__WAIT_QUEUE_HEAD_INITIALIZER(name)
		shmem_falloc.waitq = &shmem_falloc_waitq;
		shmem_falloc.start = unmap_start >> PAGE_SHIFT;
		shmem_falloc.next = (unmap_end + 1) >> PAGE_SHIFT;
		spin_lock_tlx(&inode->i_lock);
		inode->i_private = &shmem_falloc;
		spin_unlock_tlx(&inode->i_lock);

		shmem_truncate_range_tlx(inode, offset, offset + len - 1);
		/* No need to unmap again: hole-punching leaves COWed pages */

		spin_lock_tlx(&inode->i_lock);
		inode->i_private = NULL;
		wake_up_all(&shmem_falloc_waitq);
		spin_unlock_tlx(&inode->i_lock);
		error = 0;
		goto out;
	}

	/* We need to check rlimit even when FALLOC_FL_KEEP_SIZE */
	error = 0;//inode_newsize_ok_tlx(inode, offset + len);
	start = offset >> PAGE_CACHE_SHIFT;
	end = (offset + len + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;
	/* Try to avoid a swapstorm if len is impossible to satisfy */
	if (sbinfo->max_blocks && end - start > sbinfo->max_blocks) {
		error = -ENOSPC;
		goto out;
	}

	shmem_falloc.waitq = NULL;
	shmem_falloc.start = start;
	shmem_falloc.next  = start;
	shmem_falloc.nr_falloced = 0;
	shmem_falloc.nr_unswapped = 0;
	spin_lock_tlx(&inode->i_lock);
	inode->i_private = &shmem_falloc;
	spin_unlock_tlx(&inode->i_lock);

	for (index = start; index < end; index++) {
		struct page *page;
		if (signal_pending_tlx(current))
			error = -EINTR;
		else if (shmem_falloc.nr_unswapped > shmem_falloc.nr_falloced)
			error = -ENOMEM;
		else
			error =  shmem_getpage_gfp_tlx(inode, index, &page, SGP_FALLOC,
													(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK, NULL);
		if (error) {
			/* Remove the !PageUptodate pages we added */
			shmem_undo_range_tlx(inode,
				(loff_t)start << PAGE_CACHE_SHIFT,
				(loff_t)index << PAGE_CACHE_SHIFT, true);
			goto undone;
		}
		shmem_falloc.next++;
		if (!PageUptodate(page))
			shmem_falloc.nr_falloced++;

//		set_page_dirty_tlx(page);
//		unlock_page(page);
		__clear_bit_tlx(PG_locked, &page->flags);
//		put_page_tlx(page);
//		page_cache_release(page);
		cond_resched();
	}

	if (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size)
		i_size_write(inode, offset + len);
	inode->i_ctime = CURRENT_TIME;
undone:
	spin_lock_tlx(&inode->i_lock);
	inode->i_private = NULL;
	spin_unlock_tlx(&inode->i_lock);
out:
	mutex_unlock_tlx(&inode->i_mutex);
	return error;
}


struct file_operations shmem_file_operations_tlx = {
	.mmap		= shmem_mmap_tlx,
#ifdef CONFIG_TMPFS
	.llseek		= shmem_file_llseek_tlx,
	.read		= new_sync_read_tlx_tlx,
	.write		= new_sync_write_tlx_tlx,
	.read_iter	= shmem_file_read_iter_tlx,
	.write_iter	= generic_file_write_iter_tlx_tlx,
	.splice_read	= shmem_file_splice_read_tlx,
	.fallocate	= shmem_fallocate_tlx,
#endif
};


int
shmem_write_begin_tlx(struct file *file, struct address_space *mapping,
			loff_t pos, unsigned len, unsigned flags,
			struct page **pagep, void **fsdata);

int
shmem_write_end_tlx(struct file *file, struct address_space *mapping,
			loff_t pos, unsigned len, unsigned copied,
			struct page *page, void *fsdata);

struct address_space_operations shmem_aops_tlx = {
#ifdef CONFIG_TMPFS
	.write_begin	= shmem_write_begin_tlx,
	.write_end	= shmem_write_end_tlx,
#endif
};


struct inode *shmem_get_inode_tlx(struct super_block *sb, const struct inode *dir,
				     umode_t mode, dev_t dev, unsigned long flags)
{
		struct inode *inode;
		struct shmem_inode_info *info;
	//	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
		struct shmem_sb_info *sbinfo = sb->s_fs_info;
			if (sbinfo->max_inodes) {
	//			spin_lock(&sbinfo->stat_lock);
				sbinfo->free_inodes--;
	//			spin_unlock(&sbinfo->stat_lock);
			}
		inode = new_inode_tlx(sb);
		if (inode) {
//			unsigned int *p = &(*({SHIFT_PERCPU_PTR(&last_ino_tlx, __my_cpu_offset_tlx); }));
			unsigned int *p = &(*({({                              \
         		RELOC_HIDE_TLX((typeof(*(&last_ino_tlx)) __kernel __force *)(&last_ino_tlx), (__my_cpu_offset)); \
 					});
			}));
//			&get_cpu_var(last_ino_tlx);
			unsigned int res = *p;
			if (unlikely((res & (LAST_INO_BATCH-1)) == 0)) {
				static atomic_t shared_last_ino;
				int next = atomic_add_return_tlx(LAST_INO_BATCH, &shared_last_ino);
				res = next - LAST_INO_BATCH;
			}
			*p = ++res;
	//		put_cpu_var(last_ino_tlx);
			inode->i_ino = res;
			inode->i_uid = ((typeof(*current->cred) __force __kernel *)(current->cred))->fsuid;
	//		rcu_dereference_protected(current->cred, 1)->fsuid; // (fsuid);
			if (dir && dir->i_mode & S_ISGID) {
							inode->i_gid = dir->i_gid;
									if (S_ISDIR(mode))
													mode |= S_ISGID;
				} else
			inode->i_gid =  ((typeof(*current->cred) __force __kernel *)(current->cred)) ->fsgid;
	//		rcu_dereference_protected(current->cred, 1)->fsgid; //(fsgid);
			inode->i_mode = mode;
			inode->i_blocks = 0;
			inode->i_mapping->backing_dev_info = &shmem_backing_dev_info_tlx;
			inode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;
			struct timekeeper *tk = &timekeeper_tlx;
			inode->i_generation = tk->xtime_sec;
			info = container_of(inode, struct shmem_inode_info, vfs_inode);
			memset_tlx(info, 0, (char *)inode - (char *)info);
//			spin_lock_init(&info->lock);
			info->flags = flags & VM_NORESERVE;
			INIT_LIST_HEAD(&info->swaplist);
			struct simple_xattrs *xattrs = &info->xattrs;
			INIT_LIST_HEAD(&xattrs->head);
//			spin_lock_init(&xattrs->lock);
			switch (mode & S_IFMT) {
			default:
				inode->i_op = &shmem_special_inode_operations_tlx;
				dev_t rdev = dev;
					inode->i_mode = mode;
					if (S_ISCHR(mode)) {
						inode->i_fop = &def_chr_fops_tlx;
						inode->i_rdev = rdev;
					} else if (S_ISBLK(mode)) {
						inode->i_fop = &def_blk_fops_tlx;
						inode->i_rdev = rdev;
					} else if (S_ISFIFO(mode))
						inode->i_fop = &pipefifo_fops_tlx;
					else if (S_ISSOCK(mode))
						inode->i_fop = &bad_sock_fops_tlx;
				break;
				case S_IFREG:
					inode->i_mapping->a_ops = &shmem_aops_tlx;
					inode->i_op = &shmem_inode_operations_tlx;
					inode->i_fop = &shmem_file_operations_tlx;
					break;
				case S_IFDIR:
					if (unlikely(inode->i_nlink == 0)) {
									atomic_long_dec_tlx(&inode->i_sb->s_remove_count);
						}
						inode->__i_nlink++;
					inode->i_size = 2 * BOGO_DIRENT_SIZE;
					inode->i_op = &shmem_dir_inode_operations_tlx;
					inode->i_fop = &simple_dir_operations_tlx;
					break;
				case S_IFLNK:
					break;
			}
		}
		return inode;
}



int
shmem_mknod_tlx(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev)
{
	struct inode *inode;
	int error = -ENOSPC;

	inode = shmem_get_inode_tlx(dir->i_sb, dir, mode, dev, VM_NORESERVE);
//	inode = ramfs_get_inode(sb, dir, mode, dev);
	if (inode) {
		error = 0;
		dir->i_size += BOGO_DIRENT_SIZE;
		dir->i_ctime = dir->i_mtime = CURRENT_TIME;
		__d_instantiate_tlx(dentry, inode);
		dget_tlx(dentry); /* Extra count - pin the dentry in core */
	}
	return error;
}


int shmem_create_tlx(struct inode *dir, struct dentry *dentry, umode_t mode,
		bool excl)
{
	return shmem_mknod_tlx(dir, dentry, mode | S_IFREG, 0);
}



int shmem_mkdir_tlx(struct inode *dir, struct dentry *dentry, umode_t mode)
{
	shmem_mknod_tlx(dir, dentry, mode | S_IFDIR, 0);
//	inc_nlink(dir);
	return 0;
}

struct hlist_bl_head *d_hash_tlx(const struct dentry *parent,
					unsigned int hash);
#define DCACHE_RCUACCESS                0x00000080 /* Entry has ever been RCU-visible */
#define SHORT_SYMLINK_LEN 128



struct dentry *simple_lookup_tlx(struct inode *dir, struct dentry *dentry, unsigned int flags)
{
//	d_add(dentry, NULL);
	__d_instantiate_tlx(dentry, NULL);
//		d_rehash(dentry);
//	__d_rehash(dentry, d_hash_tlx(dentry->d_parent, dentry->d_name.hash));
//	struct dentry * entry,
	struct hlist_bl_head *b =d_hash_tlx(dentry->d_parent, dentry->d_name.hash);
//	{
	//	BUG_ON(!d_unhashed(entry));
	//	hlist_bl_lock(b);
		dentry->d_flags |= DCACHE_RCUACCESS;
//		hlist_bl_add_head_rcu(&dentry->d_hash, b);
						struct hlist_bl_node *n = &dentry->d_hash;
					struct hlist_bl_head *h = b;
				struct hlist_bl_node *first;
				first = (struct hlist_bl_node *)
									((unsigned long)h->first & ~LIST_BL_LOCKMASK);
				n->next = first;
				if (first)
								first->pprev = &n->next;
				n->pprev = &h->first;
//         hlist_bl_set_first_rcu(h, n);
				rcu_assign_pointer(h->first,
								(struct hlist_bl_node *)((unsigned long)n | LIST_BL_LOCKMASK));
	return NULL;
}

static void *shmem_follow_short_symlink_tlx(struct dentry *dentry, struct nameidata *nd)
{
//	nd_set_link(nd, SHMEM_I(dentry->d_inode)->symlink);
	nd->saved_names[nd->depth] = SHMEM_I(dentry->d_inode)->symlink;
	return NULL;
}

struct inode_operations shmem_short_symlink_operations_tlx = {
	.follow_link	= shmem_follow_short_symlink_tlx,
};

int shmem_symlink_tlx(struct inode *dir, struct dentry *dentry, const char *symname)
{
//	int error;
	int len;
	struct inode *inode;
	struct page *page;
	char *kaddr;
	struct shmem_inode_info *info;

	len = strlen_tlx(symname) + 1;
	inode = shmem_get_inode_tlx(dir->i_sb, dir, S_IFLNK|S_IRWXUGO, 0, VM_NORESERVE);
	info = SHMEM_I(inode);
	inode->i_size = len-1;
	if (len <= SHORT_SYMLINK_LEN) {
//		info->symlink = kmemdup(symname, len, GFP_KERNEL);
		const void *src = symname;
//		size_t len,
		gfp_t gfp = GFP_KERNEL;
    void *p;
    p = kmalloc_tlx(len, gfp);
    if (p)
                  memcpy_tlx(p, src, len);
		info->symlink = p;
		inode->i_op = &shmem_short_symlink_operations_tlx;
	}
	dir->i_size += BOGO_DIRENT_SIZE;
	dir->i_ctime = dir->i_mtime = CURRENT_TIME;
	__d_instantiate_tlx(dentry, inode);
	dget_tlx(dentry);
	return 0;
}


struct inode_operations shmem_dir_inode_operations_tlx = {
#ifdef CONFIG_TMPFS
	.create		= shmem_create_tlx,
	.lookup		= simple_lookup_tlx,
	.symlink	= shmem_symlink_tlx,
	.mkdir		= shmem_mkdir_tlx,
	.mknod		= shmem_mknod_tlx,
#endif
};








int shmem_fill_super1(struct super_block *sb, void *data, int silent)
{
//	shmem_fill_super_tlx(sb, data, silent);
//	printk_tlx(KERN_ERR "===============================tmpfs: 1 \n");
	struct inode *inode;
	struct shmem_sb_info *sbinfo;
	int err = -ENOMEM;
	sbinfo = kzalloc_tlx(max((int)sizeof(struct shmem_sb_info),
				L1_CACHE_BYTES), GFP_KERNEL);
	sbinfo->mode = S_IRWXUGO | S_ISVTX;
	sbinfo->uid = current_fsuid();
	sbinfo->gid = current_fsgid();
	sb->s_fs_info = sbinfo;
	spin_lock_init(&sbinfo->stat_lock);
	percpu_counter_init(&sbinfo->used_blocks, 0);
	sbinfo->free_inodes = sbinfo->max_inodes;

	sb->s_maxbytes = MAX_LFS_FILESIZE;
	sb->s_blocksize = PAGE_CACHE_SIZE;
	sb->s_blocksize_bits = PAGE_CACHE_SHIFT;
	sb->s_magic = TMPFS_MAGIC;
	sb->s_op = &shmem_ops_tlx;
	sb->s_time_gran = 1;
	const struct inode *dir = NULL;
	umode_t mode = S_IFDIR | sbinfo->mode;
	dev_t dev = 0;
	unsigned long flags = VM_NORESERVE;
	struct shmem_inode_info *info;
	sbinfo = sb->s_fs_info;
	if (sbinfo->max_inodes) {
				__raw_spin_lock_tlx(&sbinfo->stat_lock);
				sbinfo->free_inodes--;
				__raw_spin_unlock_tlx(&sbinfo->stat_lock);
	}
		inode = new_inode_tlx(sb);
		if (inode) {
			unsigned int *p = &get_cpu_var(last_ino_tlx);
			unsigned int res = *p;
			if (unlikely((res & (LAST_INO_BATCH-1)) == 0)) {
				static atomic_t shared_last_ino;
				int next = atomic_add_return_tlx(LAST_INO_BATCH, &shared_last_ino);
				res = next - LAST_INO_BATCH;
			}
			*p = ++res;
			put_cpu_var(last_ino_tlx);
			inode->i_ino = res;
			inode->i_uid = current_cred_xxx(fsuid);
			if (dir && dir->i_mode & S_ISGID) {
							inode->i_gid = dir->i_gid;
									if (S_ISDIR(mode))
													mode |= S_ISGID;
				} else
			inode->i_gid = current_cred_xxx(fsgid);
			inode->i_mode = mode;
			inode->i_blocks = 0;
			inode->i_mapping->backing_dev_info = &shmem_backing_dev_info_tlx;
			inode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;
			struct timekeeper *tk = &timekeeper_tlx;
			inode->i_generation = tk->xtime_sec;
			info = container_of(inode, struct shmem_inode_info, vfs_inode);
			memset_tlx(info, 0, (char *)inode - (char *)info);
			spin_lock_init(&info->lock);
			info->flags = flags & VM_NORESERVE;
			INIT_LIST_HEAD(&info->swaplist);
			struct simple_xattrs *xattrs = &info->xattrs;
			INIT_LIST_HEAD(&xattrs->head);
			spin_lock_init(&xattrs->lock);
			switch (mode & S_IFMT) {
			default:
				inode->i_op = &shmem_special_inode_operations_tlx;
				dev_t rdev = dev;
					inode->i_mode = mode;
					if (S_ISCHR(mode)) {
						inode->i_fop = &def_chr_fops_tlx;
						inode->i_rdev = rdev;
					} else if (S_ISBLK(mode)) {
						inode->i_fop = &def_blk_fops_tlx;
						inode->i_rdev = rdev;
					} else if (S_ISFIFO(mode))
						inode->i_fop = &pipefifo_fops_tlx;
					else if (S_ISSOCK(mode))
						inode->i_fop = &bad_sock_fops_tlx;
				break;
			case S_IFREG:
				inode->i_mapping->a_ops = &shmem_aops_tlx;
				inode->i_op = &shmem_inode_operations_tlx;
				inode->i_fop = &shmem_file_operations_tlx;
				break;
			case S_IFDIR:
				if (unlikely(inode->i_nlink == 0)) {
								atomic_long_dec_tlx(&inode->i_sb->s_remove_count);
					}
					inode->__i_nlink++;
				inode->i_size = 2 * BOGO_DIRENT_SIZE;
				inode->i_op = &shmem_dir_inode_operations_tlx;
				inode->i_fop = &simple_dir_operations_tlx;
				break;
			case S_IFLNK:
				break;
			}
		}
	inode->i_uid = sbinfo->uid;
	inode->i_gid = sbinfo->gid;
	static const struct qstr name = QSTR_INIT("/", 1);

	sb->s_root = __d_alloc_tlx(inode->i_sb, &name);
	__d_instantiate_tlx(sb->s_root, inode);
	return 0;
}




struct vfsmount *
vfs_kern_mount_rd(struct file_system_type *type, int flags, const char *name, void *data)
{
	struct dentry *root;
	struct mount *mnt = kmem_cache_zalloc_tlx(mnt_cache_tlx, GFP_KERNEL);
	if (mnt) {
		int err;
		int res;
		ida_pre_get_tlx(&mnt_id_ida_tlx, GFP_KERNEL);
		res = ida_get_new_above_tlx(&mnt_id_ida_tlx, mnt_id_start_tlx, &mnt->mnt_id);
		if (!res)
								mnt_id_start_tlx = mnt->mnt_id + 1;
		if (name) {
					size_t len;
					char *buf;
					len = strlen_tlx(name) + 1;
					buf =  __kmalloc_tlx(len, GFP_KERNEL);
					if (buf)
								memcpy_tlx(buf, name, len);
					mnt->mnt_devname = buf;
		}
		mnt->mnt_pcp = alloc_percpu(struct mnt_pcp);
		this_cpu_add(mnt->mnt_pcp->mnt_count, 1);
		INIT_HLIST_NODE(&mnt->mnt_hash);
		INIT_LIST_HEAD(&mnt->mnt_child);
		INIT_LIST_HEAD(&mnt->mnt_mounts);
		INIT_LIST_HEAD(&mnt->mnt_list);
		INIT_LIST_HEAD(&mnt->mnt_expire);
		INIT_LIST_HEAD(&mnt->mnt_share);
		INIT_LIST_HEAD(&mnt->mnt_slave_list);
		INIT_LIST_HEAD(&mnt->mnt_slave);
	}

	if (flags & MS_KERNMOUNT)
		mnt->mnt.mnt_flags = MNT_INTERNAL;
	struct super_block *sb;
	char *secdata = NULL;
	struct super_block *s = sget_tlx(type, NULL, set_anon_super_tlx, flags, NULL);
	shmem_fill_super1(s, data, flags & MS_SILENT ? 1 : 0);
	s->s_flags |= MS_ACTIVE;
	root = s->s_root;
	sb = root->d_sb;
	sb->s_flags |= MS_BORN;
	up_write_tlx(&sb->s_umount);
	free_page((unsigned long)secdata);
	mnt->mnt.mnt_root = root;
	mnt->mnt.mnt_sb = root->d_sb;
	mnt->mnt_mountpoint = mnt->mnt.mnt_root;
	mnt->mnt_parent = mnt;
	list_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);
	return &mnt->mnt;
}




struct cfs_rq {
	struct load_weight load;
	unsigned int nr_running, h_nr_running;

	u64 exec_clock;
	u64 min_vruntime;
#ifndef CONFIG_64BIT
	u64 min_vruntime_copy;
#endif

	struct rb_root tasks_timeline;
	struct rb_node *rb_leftmost;
	struct sched_entity *curr, *next, *last, *skip;

#ifdef	CONFIG_SCHED_DEBUG
	unsigned int nr_spread_over;
#endif

#ifdef CONFIG_SMP
	unsigned long runnable_load_avg, blocked_load_avg;
	atomic64_t decay_counter;
	u64 last_decay;
	atomic_long_t removed_load;

#ifdef CONFIG_FAIR_GROUP_SCHED
	u32 tg_runnable_contrib;
	unsigned long tg_load_contrib;
	unsigned long h_load;
	u64 last_h_load_update;
	struct sched_entity *h_load_next;
#endif /* CONFIG_FAIR_GROUP_SCHED */
#endif /* CONFIG_SMP */

#ifdef CONFIG_FAIR_GROUP_SCHED
	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */
	int on_list;
	struct list_head leaf_cfs_rq_list;
	struct task_group *tg;	/* group that "owns" this runqueue */

#ifdef CONFIG_CFS_BANDWIDTH
	int runtime_enabled;
	u64 runtime_expires;
	s64 runtime_remaining;

	u64 throttled_clock, throttled_clock_task;
	u64 throttled_clock_task_time;
	int throttled, throttle_count;
	struct list_head throttled_list;
#endif /* CONFIG_CFS_BANDWIDTH */
#endif /* CONFIG_FAIR_GROUP_SCHED */
};

struct rt_prio_array {
         DECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* +1 bit for delimiter */
         struct list_head queue[MAX_RT_PRIO];
};

struct rt_rq {
	struct rt_prio_array active;
	unsigned int rt_nr_running;
#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
	struct {
		int curr; /* highest queued rt task prio */
#ifdef CONFIG_SMP
		int next; /* next highest */
#endif
	} highest_prio;
#endif
#ifdef CONFIG_SMP
	unsigned long rt_nr_migratory;
	unsigned long rt_nr_total;
	int overloaded;
	struct plist_head_tlx pushable_tasks;
#endif
	int rt_queued;

	int rt_throttled;
	u64 rt_time;
	u64 rt_runtime;
	/* Nests inside the rq lock: */
	raw_spinlock_t rt_runtime_lock;

#ifdef CONFIG_RT_GROUP_SCHED
	unsigned long rt_nr_boosted;

	struct rq *rq;
	struct task_group *tg;
#endif
};

/* Deadline class' related fields in a runqueue */
struct dl_rq {
	/* runqueue is an rbtree, ordered by deadline */
	struct rb_root rb_root;
	struct rb_node *rb_leftmost;

	unsigned long dl_nr_running;

#ifdef CONFIG_SMP
	struct {
		u64 curr;
		u64 next;
	} earliest_dl;

	unsigned long dl_nr_migratory;
	int overloaded;
	struct rb_root pushable_dl_tasks_root;
	struct rb_node *pushable_dl_tasks_leftmost;
#else
	struct dl_bw dl_bw;
#endif
};

typedef int (*cpu_stop_fn_t)(void *arg);

struct cpu_stop_work {
				struct work_struct      work;
				cpu_stop_fn_t           fn;
				void                    *arg;
};

struct rq {
	/* runqueue lock: */
	raw_spinlock_t lock;
	unsigned int nr_running;
#ifdef CONFIG_NUMA_BALANCING
	unsigned int nr_numa_running;
	unsigned int nr_preferred_running;
#endif
	#define CPU_LOAD_IDX_MAX 5
	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
	unsigned long last_load_update_tick;
#ifdef CONFIG_NO_HZ_COMMON
	u64 nohz_stamp;
	unsigned long nohz_flags;
#endif
#ifdef CONFIG_NO_HZ_FULL
	unsigned long last_sched_tick;
#endif
	int skip_clock_update;
	struct load_weight load;
	unsigned long nr_load_updates;
	u64 nr_switches;
	struct cfs_rq cfs;
	struct rt_rq rt;
	struct dl_rq dl;

#ifdef CONFIG_FAIR_GROUP_SCHED
	struct list_head leaf_cfs_rq_list;
	struct sched_avg avg;
#endif /* CONFIG_FAIR_GROUP_SCHED */
	unsigned long nr_uninterruptible;
	struct task_struct *curr, *idle, *stop;
	unsigned long next_balance;
	struct mm_struct *prev_mm;
	u64 clock;
	u64 clock_task;
	atomic_t nr_iowait;
#ifdef CONFIG_SMP
	struct root_domain *rd;
	struct sched_domain *sd;
	unsigned long cpu_capacity;
	unsigned char idle_balance;
	int post_schedule;
	int active_balance;
	int push_cpu;
	struct cpu_stop_work active_balance_work;
	int cpu;
	int online;
	struct list_head cfs_tasks;
	u64 rt_avg;
	u64 age_stamp;
	u64 idle_stamp;
	u64 avg_idle;
	u64 max_idle_balance_cost;
#endif
#ifdef CONFIG_IRQ_TIME_ACCOUNTING
	u64 prev_irq_time;
#endif
#ifdef CONFIG_PARAVIRT
	u64 prev_steal_time;
#endif
#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
	u64 prev_steal_time_rq;
#endif
	unsigned long calc_load_update;
	long calc_load_active;
#ifdef CONFIG_SCHED_HRTICK
#ifdef CONFIG_SMP
	int hrtick_csd_pending;
	struct call_single_data hrtick_csd;
#endif
	struct hrtimer hrtick_timer;
#endif
#ifdef CONFIG_SCHEDSTATS
	struct sched_info rq_sched_info;
	unsigned long long rq_cpu_time;
	unsigned int yld_count;
	unsigned int sched_count;
	unsigned int sched_goidle;
	unsigned int ttwu_count;
	unsigned int ttwu_local;
#endif
#ifdef CONFIG_SMP
	struct llist_head_tlx wake_list;
#endif
};


struct sched_class {
	const struct sched_class *next;
	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*yield_task) (struct rq *rq);
	bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
	struct task_struct * (*pick_next_task) (struct rq *rq,
						struct task_struct *prev);
	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
#ifdef CONFIG_SMP
	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
	void (*post_schedule) (struct rq *this_rq);
	void (*task_waking) (struct task_struct *task);
	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
	void (*set_cpus_allowed)(struct task_struct *p,
				const struct cpumask *newmask);
	void (*rq_online)(struct rq *rq);
	void (*rq_offline)(struct rq *rq);
#endif
	void (*set_curr_task) (struct rq *rq);
	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
	void (*task_fork) (struct task_struct *p);
	void (*task_dead) (struct task_struct *p);
	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
	void (*switched_to) (struct rq *this_rq, struct task_struct *task);
	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
					int oldprio);
	unsigned int (*get_rr_interval) (struct rq *rq,
					struct task_struct *task);
#ifdef CONFIG_FAIR_GROUP_SCHED
	void (*task_move_group) (struct task_struct *p, int on_rq);
#endif
};



static void check_preempt_curr_idle_tlx(struct rq *rq, struct task_struct *p, int flags)
{
//         resched_task(rq->idle);
				set_bit_tlx(TIF_NEED_RESCHED, (unsigned long *)&((struct thread_info_tlx *)(rq->idle)->stack)->flags);
}

struct task_struct *
pick_next_task_idle_tlx(struct rq *rq, struct task_struct *prev)
{
	prev->sched_class->put_prev_task(rq, prev);
//	put_prev_task(rq, prev);

//	schedstat_inc(rq, sched_goidle);
	return rq->idle;
}

void put_prev_task_idle_tlx(struct rq *rq, struct task_struct *prev)
{
//	idle_exit_fair(rq);
//	update_rq_runnable_avg(rq, 0);
//	rq_last_tick_reset(rq);
}

int
select_task_rq_idle_tlx(struct task_struct *p, int cpu, int sd_flag, int flags)
{
	return task_thread_info(p)->cpu;
}


struct sched_class idle_sched_class_tlx = {
			/* .next is NULL */
			/* no enqueue/yield_task for idle tasks */

			/* dequeue is not valid, we print a debug message there: */
			.dequeue_task		= NULL,

			.check_preempt_curr	= check_preempt_curr_idle_tlx,

			.pick_next_task		= pick_next_task_idle_tlx,
			.put_prev_task		= put_prev_task_idle_tlx,

		#ifdef CONFIG_SMP
			.select_task_rq		= select_task_rq_idle_tlx,
		#endif

			.set_curr_task          = NULL,
			.task_tick		= NULL,

			.get_rr_interval	= NULL,

			.prio_changed		= NULL,
			.switched_to		= NULL,
};

struct rq runqueues_tlx;

//DEFINE_PER_CPU_SECTION(struct rq, runqueues_tlx_tlx, PER_CPU_SHARED_ALIGNED_SECTION) \
//				____cacheline_aligned_in_smp;

#define DEQUEUE_SLEEP           1
#define ENQUEUE_WAKEUP          1
#define ENQUEUE_WAKING          4
#define for_each_sched_entity(se) \
								for (; se; se = NULL)

static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se)
{
         return se->cfs_rq;
}


struct rb_node *rb_next_tlx(const struct rb_node *node)
{
	struct rb_node *parent;
	if (node->rb_right) {
		node = node->rb_right;
		while (node->rb_left)
			node=node->rb_left;
		return (struct rb_node *)node;
	}
	while ((parent = rb_parent(node)) && node == parent->rb_right)
		node = parent;
	return parent;
}






#define SCHED_IDLE              5
static unsigned int sched_nr_latency = 8;
static inline struct cfs_rq *task_cfs_rq(struct task_struct *p)
{
       return p->se.cfs_rq;
}

void check_preempt_wakeup_tlx(struct rq *rq, struct task_struct *p, int wake_flags)
{
	struct task_struct *curr = rq->curr;
	struct sched_entity *se = &curr->se, *pse = &p->se;
	struct cfs_rq *cfs_rq = task_cfs_rq(curr);
	int scale = cfs_rq->nr_running >= sched_nr_latency;
	int next_buddy_marked = 0;
	if (unlikely(curr->policy == SCHED_IDLE) &&
			likely(p->policy != SCHED_IDLE))
		goto preempt;
	return;
preempt:
//	resched_task(curr);
		set_bit_tlx(TIF_NEED_RESCHED, (unsigned long *)&((struct thread_info_tlx *)(curr)->stack)->flags);
}

static inline int entity_before(struct sched_entity *a,
																struct sched_entity *b)
{
			return (s64)(a->vruntime - b->vruntime) < 0;
}


void __enqueue_entity_tlx(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
	struct rb_node **link = &cfs_rq->tasks_timeline.rb_node;
	struct rb_node *parent = NULL;
	struct sched_entity *entry;
	int leftmost = 1;

	/*
	* Find the right place in the rbtree:
	*/
	while (*link) {
		parent = *link;
		entry = rb_entry(parent, struct sched_entity, run_node);
		/*
		* We dont care about collisions. Nodes with
		* the same key stay together.
		*/
		if (entity_before(se, entry)) {
			link = &parent->rb_left;
		} else {
			link = &parent->rb_right;
			leftmost = 0;
		}
	}

	/*
	* Maintain a cache of leftmost tree entries (it is frequently
	* used):
	*/
	if (leftmost)
		cfs_rq->rb_leftmost = &se->run_node;

	rb_link_node_tlx(&se->run_node, parent, link);
	rb_insert_color_tlx(&se->run_node, &cfs_rq->tasks_timeline);
}


void rb_erase_tlx(struct rb_node *node, struct rb_root *root);

void __dequeue_entity_tlx(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
	if (cfs_rq->rb_leftmost == &se->run_node) {
		struct rb_node *next_node;

		next_node = rb_next_tlx(&se->run_node);
		cfs_rq->rb_leftmost = next_node;
	}

	rb_erase_tlx(&se->run_node, &cfs_rq->tasks_timeline);
}


static struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)
 {
         struct rb_node *left = cfs_rq->rb_leftmost;

         if (!left)
                 return NULL;

         return rb_entry(left, struct sched_entity, run_node);
 }

 static struct sched_entity *__pick_next_entity(struct sched_entity *se)
 {
         struct rb_node *next = rb_next_tlx(&se->run_node);

         if (!next)
                 return NULL;

         return rb_entry(next, struct sched_entity, run_node);
 }

struct sched_entity *
pick_next_entity_tlx(struct cfs_rq *cfs_rq, struct sched_entity *curr)
{
	struct sched_entity *left = __pick_first_entity(cfs_rq);
	struct sched_entity *se;
	if (!left || (curr && entity_before(curr, left)))
		left = curr;
	se = left; /* ideally we run the leftmost entity */
	if (cfs_rq->skip == se) {
		struct sched_entity *second;
		if (se == curr) {
			second = __pick_first_entity(cfs_rq);
		} else {
			second = __pick_next_entity(se);
			if (!second || (curr && entity_before(curr, second)))
				second = curr;
		}
	}
//	clear_buddies(cfs_rq, se);

	return se;
}
#define LOAD_AVG_PERIOD 32
#define LOAD_AVG_MAX 47742 /* maximum possible load avg */
#define LOAD_AVG_MAX_N 345 /* number of full periods to produce LOAD_MAX_AVG */

/* Precomputed fixed inverse multiplies for multiplication by y^n*/
static const u32 runnable_avg_yN_inv[] = {
         0xffffffff, 0xfa83b2da, 0xf5257d14, 0xefe4b99a, 0xeac0c6e6, 0xe5b906e6,
         0xe0ccdeeb, 0xdbfbb796, 0xd744fcc9, 0xd2a81d91, 0xce248c14, 0xc9b9bd85,
         0xc5672a10, 0xc12c4cc9, 0xbd08a39e, 0xb8fbaf46, 0xb504f333, 0xb123f581,
         0xad583ee9, 0xa9a15ab4, 0xa5fed6a9, 0xa2704302, 0x9ef5325f, 0x9b8d39b9,
         0x9837f050, 0x94f4efa8, 0x91c3d373, 0x8ea4398a, 0x8b95c1e3, 0x88980e80,
         0x85aac367, 0x82cd8698,
 };


static const u32 runnable_avg_yN_sum[] = {
         0, 1002, 1982, 2941, 3880, 4798, 5697, 6576, 7437, 8279, 9103,
        9909,10698,11470,12226,12966,13690,14398,15091,15769,16433,17082,
         17718,18340,18949,19545,20128,20698,21256,21802,22336,22859,23371,
};

static __always_inline u64 decay_load(u64 val, u64 n)
{
	unsigned int local_n;

	if (!n)
		return val;
	else if (unlikely(n > LOAD_AVG_PERIOD * 63))
		return 0;

	/* after bounds checking we can collapse to 32-bit */
	local_n = n;

	/*
	* As y^PERIOD = 1/2, we can combine
	*    y^n = 1/2^(n/PERIOD) * k^(n%PERIOD)
	* With a look-up table which covers k^n (n<PERIOD)
	*
	* To achieve constant time decay_load.
	*/
	if (unlikely(local_n >= LOAD_AVG_PERIOD)) {
		val >>= local_n / LOAD_AVG_PERIOD;
		local_n %= LOAD_AVG_PERIOD;
	}

	val *= runnable_avg_yN_inv[local_n];
	/* We don't use SRR here since we always want to round down. */
	return val >> 32;
}

static __always_inline int __update_entity_runnable_avg_tlx(u64 now,
							struct sched_avg *sa,
							int runnable)
{
	u64 delta, periods;
	u32 runnable_contrib;
	int delta_w, decayed = 0;

	delta = now - sa->last_runnable_update;
	/*
	* This should only happen when time goes backwards, which it
	* unfortunately does during sched clock init when we swap over to TSC.
	*/
	if ((s64)delta < 0) {
		sa->last_runnable_update = now;
		return 0;
	}

	/*
	* Use 1024ns as the unit of measurement since it's a reasonable
	* approximation of 1us and fast to compute.
	*/
	delta >>= 10;
	if (!delta)
		return 0;
	sa->last_runnable_update = now;

	/* delta_w is the amount already accumulated against our next period */
	delta_w = sa->runnable_avg_period % 1024;
	if (delta + delta_w >= 1024) {
		/* period roll-over */
		decayed = 1;

		/*
		* Now that we know we're crossing a period boundary, figure
		* out how much from delta we need to complete the current
		* period and accrue it.
		*/
		delta_w = 1024 - delta_w;
		if (runnable)
			sa->runnable_avg_sum += delta_w;
		sa->runnable_avg_period += delta_w;

		delta -= delta_w;

		/* Figure out how many additional periods this update spans */
		periods = delta / 1024;
		delta %= 1024;

		sa->runnable_avg_sum = decay_load(sa->runnable_avg_sum,
							periods + 1);
		sa->runnable_avg_period = decay_load(sa->runnable_avg_period,
								periods + 1);

		/* Efficiently calculate \sum (1..n_period) 1024*y^i */
//		runnable_contrib = __compute_runnable_contrib(periods);
		u32 contrib = 0;

			if (likely(periods <= LOAD_AVG_PERIOD))
							return runnable_avg_yN_sum[periods];
			else if (unlikely(periods >= LOAD_AVG_MAX_N))
								return LOAD_AVG_MAX;
			goto have_runnable_contrib;
				/* Compute \Sum k^n combining precomputed values for k^i, \Sum k^j */
				do {
								contrib /= 2; /* y^LOAD_AVG_PERIOD = 1/2 */
								contrib += runnable_avg_yN_sum[LOAD_AVG_PERIOD];

								periods -= LOAD_AVG_PERIOD;
				} while (periods > LOAD_AVG_PERIOD);

				contrib = decay_load(contrib, periods);
				runnable_contrib = contrib + runnable_avg_yN_sum[periods];
have_runnable_contrib:
		if (runnable)
			sa->runnable_avg_sum += runnable_contrib;
		sa->runnable_avg_period += runnable_contrib;
	}

	/* Remainder of delta accrued against u_0` */
	if (runnable)
		sa->runnable_avg_sum += delta;
	sa->runnable_avg_period += delta;

	return decayed;
}

#define entity_is_task(se)      (!se->my_q)
# define scale_load(w)          (w)
# define scale_load_down(w)     (w)
#define SCHED_LOAD_RESOLUTION  10
#define SCHED_LOAD_SHIFT        (10 + SCHED_LOAD_RESOLUTION)
#define SCHED_LOAD_SCALE        (1L << SCHED_LOAD_SHIFT)
#define NICE_0_SHIFT            SCHED_LOAD_SHIFT
#define abs(x) ({                                               \
                 long ret;                                       \
                 if (sizeof(x) == sizeof(long)) {                \
                         long __x = (x);                         \
                         ret = (__x < 0) ? -__x : __x;           \
                 } else {                                        \
                         int __x = (x);                          \
                         ret = (__x < 0) ? -__x : __x;           \
                 }                                               \
                 ret;                                            \
         })

static inline void atomic_add(int i, atomic_t *v)
{
				unsigned long tmp;
				int result;

				asm volatile("// atomic_add\n"
"1:     ldxr    %w0, %2\n"
"       add     %w0, %w0, %w3\n"
"       stxr    %w1, %w0, %2\n"
"       cbnz    %w1, 1b"
				: "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
				: "Ir" (i));
}

struct cfs_bandwidth {
#ifdef CONFIG_CFS_BANDWIDTH
	raw_spinlock_t lock;
	ktime_t period;
	u64 quota, runtime;
	s64 hierarchal_quota;
	u64 runtime_expires;
	int idle, timer_active;
	struct hrtimer period_timer, slack_timer;
	struct list_head throttled_cfs_rq;
	int nr_periods, nr_throttled;
	u64 throttled_time;
#endif
};


struct task_group {
	struct cgroup_subsys_state css;

#ifdef CONFIG_FAIR_GROUP_SCHED
	/* schedulable entities of this group on each cpu */
	struct sched_entity **se;
	/* runqueue "owned" by this group on each cpu */
	struct cfs_rq **cfs_rq;
	unsigned long shares;

#ifdef	CONFIG_SMP
	atomic_long_t load_avg;
	atomic_t runnable_avg;
#endif
#endif

#ifdef CONFIG_RT_GROUP_SCHED
	struct sched_rt_entity **rt_se;
	struct rt_rq **rt_rq;

	struct rt_bandwidth rt_bandwidth;
#endif

	struct rcu_head rcu;
	struct list_head list;

	struct task_group *parent;
	struct list_head siblings;
	struct list_head children;

#ifdef CONFIG_SCHED_AUTOGROUP
	struct autogroup *autogroup;
#endif

	struct cfs_bandwidth cfs_bandwidth;
};


static inline void update_entity_load_avg_tlx(struct sched_entity *se,
						int update_cfs_rq)
{
	struct cfs_rq *cfs_rq = cfs_rq_of(se);
	long contrib_delta;
	u64 now;

	/*
	* For a group entity we need to use their owned cfs_rq_clock_task() in
	* case they are the parent of a throttled hierarchy.
	*/
	if (entity_is_task(se))
		now = ((struct rq *)((cfs_rq))->rq)->clock_task;
	else
		now = ((struct rq *)((se)->my_q))->clock_task;

	if (!__update_entity_runnable_avg_tlx(now, &se->avg, se->on_rq))
		return;
	long old_contrib = se->avg.load_avg_contrib;
	if (entity_is_task(se)) {
//   	__update_task_entity_contrib(se);
			u32 contrib;
		contrib = se->avg.runnable_avg_sum * scale_load_down(se->load.weight);
		contrib /= (se->avg.runnable_avg_period + 1);
		se->avg.load_avg_contrib = scale_load(contrib);
	} else {
			struct task_group *tg = cfs_rq->tg;
			long contrib;
			struct sched_avg *sa = (struct sched_avg *)se;
			contrib = div_u64_tlx((u64)sa->runnable_avg_sum << NICE_0_SHIFT,
													sa->runnable_avg_period + 1);
			contrib -= cfs_rq->tg_runnable_contrib;

			if (abs(contrib) > cfs_rq->tg_runnable_contrib / 64) {
							atomic_add(contrib, &tg->runnable_avg);
								cfs_rq->tg_runnable_contrib += contrib;
				}

//         __update_tg_runnable_avg(&se->avg, group_cfs_rq(se));
//         __update_group_entity_contrib(se);
	}
	contrib_delta = se->avg.load_avg_contrib - old_contrib;
	if (!update_cfs_rq)
		return;
	if (se->on_rq)
		cfs_rq->runnable_load_avg += contrib_delta;

}

void dequeue_entity_load_avg_tlx(struct cfs_rq *cfs_rq,
							struct sched_entity *se,
							int sleep)
{
	update_entity_load_avg_tlx(se, 1);
	/* we force update consideration on load-balancer moves */
//	update_cfs_rq_blocked_load(cfs_rq, !sleep);

	cfs_rq->runnable_load_avg -= se->avg.load_avg_contrib;
	if (sleep) {
		cfs_rq->blocked_load_avg += se->avg.load_avg_contrib;
		se->avg.decay_count = atomic64_read(&cfs_rq->decay_counter);
	} /* migrations, e.g. sleep=0 leave decay_count == 0 */
}

void dequeue_task_fair_tlx(struct rq *rq, struct task_struct *p, int flags)
{
	struct cfs_rq *cfs_rq;
	struct sched_entity *se = &p->se;
	int task_sleep = flags & DEQUEUE_SLEEP;

	for_each_sched_entity(se) {
		cfs_rq = cfs_rq_of(se);
//		dequeue_entity(cfs_rq, se, flags);
				dequeue_entity_load_avg_tlx(cfs_rq, se, flags & DEQUEUE_SLEEP);
		//	clear_buddies(cfs_rq, se);
			if (se != cfs_rq->curr)
				__dequeue_entity_tlx(cfs_rq, se);
			se->on_rq = 0;
//			account_entity_dequeue(cfs_rq, se);
			cfs_rq->nr_running--;
		//	if (!(flags & DEQUEUE_SLEEP))
		//		se->vruntime -= cfs_rq->min_vruntime;

		cfs_rq->h_nr_running--;
		if (cfs_rq->load.weight) {
			se = se->parent;
			break;
		}
		flags |= DEQUEUE_SLEEP;
	}

	for_each_sched_entity(se) {
		cfs_rq = cfs_rq_of(se);
		cfs_rq->h_nr_running--;
//		update_cfs_shares(cfs_rq);
		update_entity_load_avg_tlx(se, 1);
	}

	if (!se) {
//		sub_nr_running(rq, 1);
		rq->nr_running -= 1;
	}
}
struct task_struct *
pick_next_task_fair_tlx(struct rq *rq, struct task_struct *prev)
{
	struct cfs_rq *cfs_rq = &rq->cfs;
	struct sched_entity *se;
	struct task_struct *p;
	int new_tasks;

again:
	if (!cfs_rq->nr_running)
		goto idle;

//	put_prev_task(rq, prev);
	prev->sched_class->put_prev_task(rq, prev);
	do {
		se = pick_next_entity_tlx(cfs_rq, NULL);
//		set_next_entity(cfs_rq, se);
			if (se->on_rq) {
//           update_stats_wait_end(cfs_rq, se);
					__dequeue_entity_tlx(cfs_rq, se);
			}
//     update_stats_curr_start(cfs_rq, se);
		cfs_rq->curr = se;
		cfs_rq = se->my_q;
	} while (cfs_rq);

	p =  container_of(se, struct task_struct, se);
	return p;

idle:
	return NULL;
}


void
enqueue_task_fair_tlx(struct rq *rq, struct task_struct *p, int flags)
{
	struct cfs_rq *cfs_rq;
	struct sched_entity *se = &p->se;

	for_each_sched_entity(se) {
		if (se->on_rq)
			break;
		cfs_rq = cfs_rq_of(se);
		if (!(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_WAKING))
							se->vruntime += cfs_rq->min_vruntime;
//			account_entity_enqueue(cfs_rq, se);
				cfs_rq->nr_running++;
		if (se != cfs_rq->curr)
							__enqueue_entity_tlx(cfs_rq, se);
		se->on_rq = 1;
		cfs_rq->h_nr_running++;
		flags = ENQUEUE_WAKEUP;
	}

	if (!se) {
//		add_nr_running(rq, 1);
			rq->nr_running += 1;
	}
}

void put_prev_entity_tlx(struct cfs_rq *cfs_rq, struct sched_entity *prev)
{
	if (prev->on_rq) {
		__enqueue_entity_tlx(cfs_rq, prev);
	}
	cfs_rq->curr = NULL;
}

void put_prev_task_fair_tlx(struct rq *rq, struct task_struct *prev)
{
	struct sched_entity *se = &prev->se;
	struct cfs_rq *cfs_rq;

	for_each_sched_entity(se) {
		cfs_rq = cfs_rq_of(se);
		put_prev_entity_tlx(cfs_rq, se);
	}
}

int
select_task_rq_fair_tlx(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags)
{
	return 0;
}


struct sched_class fair_sched_class_tlx = {
	.next			= NULL,
	.enqueue_task		= enqueue_task_fair_tlx,
	.dequeue_task		= dequeue_task_fair_tlx,
	.yield_task		= NULL,
	.yield_to_task		= NULL,

	.check_preempt_curr	= check_preempt_wakeup_tlx,

	.pick_next_task		= pick_next_task_fair_tlx,
	.put_prev_task		= put_prev_task_fair_tlx,

#ifdef CONFIG_SMP
	.select_task_rq		= select_task_rq_fair_tlx,
	.migrate_task_rq	= NULL,

	.rq_online		= NULL,
	.rq_offline		= NULL,

	.task_waking		= NULL,
#endif

	.set_curr_task          = NULL,
	.task_tick		= NULL,
	.task_fork		= NULL,

	.prio_changed		= NULL,
	.switched_from		= NULL,
	.switched_to		= NULL,

	.get_rr_interval	= NULL,

#ifdef CONFIG_FAIR_GROUP_SCHED
	.task_move_group	= NULL,
#endif
};;



#define DEFAULT_PRIO            (MAX_RT_PRIO + NICE_WIDTH / 2)


#define SCHED_IDLE              5
#define WEIGHT_IDLEPRIO                3
# define SCHED_LOAD_RESOLUTION  10
#define WMULT_IDLEPRIO         1431655765

static void enqueue_task_tlx(struct rq *rq, struct task_struct *p, int flags)
{
         p->sched_class->enqueue_task(rq, p, flags);
}

static void dequeue_task_tlx(struct rq *rq, struct task_struct *p, int flags)
{
//       update_rq_clock(rq);
//       sched_info_dequeued(rq, p);
        p->sched_class->dequeue_task(rq, p, flags);
}



static const int prio_to_weight_tlx[40] = {
/* -20 */     88761,     71755,     56483,     46273,     36291,
/* -15 */     29154,     23254,     18705,     14949,     11916,
/* -10 */      9548,      7620,      6100,      4904,      3906,
/*  -5 */      3121,      2501,      1991,      1586,      1277,
/*   0 */      1024,       820,       655,       526,       423,
/*   5 */       335,       272,       215,       172,       137,
/*  10 */       110,        87,        70,        56,        45,
/*  15 */        36,        29,        23,        18,        15,
};

/*
* Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.
*
* In cases where the weight does not change often, we can use the
* precalculated inverse to speed up arithmetics by turning divisions
* into multiplications:
*/
static const u32 prio_to_wmult_tlx[40] = {
/* -20 */     48388,     59856,     76040,     92818,    118348,
/* -15 */    147320,    184698,    229616,    287308,    360437,
/* -10 */    449829,    563644,    704093,    875809,   1099582,
/*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
/*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
/*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
/*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
/*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
};



int wake_up_process_tlx(struct task_struct *tsk)
{
			struct task_struct *p = tsk;
			unsigned int state = TASK_NORMAL;
			unsigned long flags =0;
			int cpu, success = 0;
			smp_mb__before_spinlock();
			raw_spin_lock_irqsave(&p->pi_lock, flags);
			if (!(p->state & state))
				goto out;
			success = 1; /* we're going to change ->state */
			cpu = task_cpu_tlx(p);
					struct rq *rq = cpu_rq(cpu);
					raw_spin_lock(&rq->lock);
						int en_flags  = ENQUEUE_WAKEUP | ENQUEUE_WAKING;
						p->sched_class->enqueue_task(rq, p, flags);
						p->on_rq = 1;
						rq->curr->sched_class->check_preempt_curr(rq, p, 0);
					p->state = TASK_RUNNING;
					raw_spin_unlock(&rq->lock);
		out:
			raw_spin_unlock_irqrestore(&p->pi_lock, flags);
			return success;
}


void set_user_nice_tlx(struct task_struct *p, long nice)
{
	int old_prio, delta, on_rq;
	unsigned long flags;
	struct rq *rq;

	if (((p)->static_prio- DEFAULT_PRIO) == nice || nice < MIN_NICE || nice > MAX_NICE)
		return;
//	rq = task_rq(p);
	int _cpu = task_cpu_tlx(p);
	rq = (&per_cpu(runqueues_tlx, (_cpu)));

	on_rq = p->on_rq;
	if (on_rq)
		dequeue_task_tlx(rq, p, 0);
	p->static_prio = (nice + DEFAULT_PRIO);
//	set_load_weight(p);
		int prio = p->static_prio - MAX_RT_PRIO;
		struct load_weight *load = &p->se.load;
		if (p->policy == SCHED_IDLE) {
			load->weight = (WEIGHT_IDLEPRIO) << SCHED_LOAD_RESOLUTION ;
			load->inv_weight = WMULT_IDLEPRIO;
			goto have_weight;
		}
		load->weight = (prio_to_weight_tlx[prio]) << SCHED_LOAD_RESOLUTION;
		load->inv_weight = prio_to_wmult_tlx[prio];
have_weight:
	old_prio = p->prio;
//	p->prio = effective_prio(p);
	p->normal_prio =  p->static_prio;
	p->prio = p->normal_prio;
	delta = p->prio - old_prio;
	if (on_rq) {
		enqueue_task_tlx(rq, p, 0);
		if (delta < 0 || (delta > 0 &&  p->on_cpu)) {

//			if (test_tsk_need_resched(p))
//               return;
////       set_tsk_need_resched(p);
//					set_tsk_thread_flag(tsk,TIF_NEED_RESCHED);
//					 set_ti_thread_flag(task_thread_info(tsk), TIF_NEED_RESCHED);
					set_bit_tlx(TIF_NEED_RESCHED, (unsigned long *)&((struct thread_info_tlx *)(p)->stack)->flags);
		}
//			resched_task(rq->curr);
	}
}

int default_wake_function_tlx(wait_queue_t *curr, unsigned mode, int wake_flags,
													void *key)
{
//         return try_to_wake_up(curr->private, mode, wake_flags);

				struct task_struct *p = curr->private;
				unsigned int state = mode;
				unsigned long flags;
				int cpu, success = 0;
				smp_mb__before_spinlock();
				raw_spin_lock_irqsave(&p->pi_lock, flags);
				if (!(p->state & state))
					goto out;
				success = 1; /* we're going to change ->state */
				cpu = task_cpu_tlx(p);
						struct rq *rq = cpu_rq(cpu);
						raw_spin_lock(&rq->lock);
							int en_flags  = ENQUEUE_WAKEUP | ENQUEUE_WAKING;
							p->sched_class->enqueue_task(rq, p, flags);
							p->on_rq = 1;
							rq->curr->sched_class->check_preempt_curr(rq, p, wake_flags);
						p->state = TASK_RUNNING;
						raw_spin_unlock(&rq->lock);
			out:
				raw_spin_unlock_irqrestore(&p->pi_lock, flags);
				return success;
}


struct worker {
	union {
		struct list_head	entry;	/* L: while idle */
		struct hlist_node	hentry;	/* L: while busy */
	};
	struct work_struct	*current_work;	/* L: work being processed */
	work_func_t		current_func;	/* L: current_work's fn */
	struct pool_workqueue	*current_pwq; /* L: current_work's pwq */
	bool			desc_valid;	/* ->desc is valid */
	struct list_head	scheduled;	/* L: scheduled works */
	struct task_struct	*task;		/* I: worker task */
	struct worker_pool	*pool;		/* I: the associated pool */
	struct list_head	node;		/* A: anchored at pool->workers */
	unsigned long		last_active;	/* L: last active timestamp */
	unsigned int		flags;		/* X: flags */
	int			id;		/* I: worker id */
	char			desc[WORKER_DESC_LEN];
	struct workqueue_struct	*rescue_wq;	/* I: the workqueue to rescue */
};


enum event_type_t {
	EVENT_FLEXIBLE = 0x1,
	EVENT_PINNED = 0x2,
	EVENT_ALL = EVENT_FLEXIBLE | EVENT_PINNED,
};

struct autogroup {
	struct kref		kref;
	struct task_group	*tg;
	struct rw_semaphore	lock;
	unsigned long		id;
	int			nice;
};


struct list_head pmus_tlx;
atomic_t perf_branch_stack_events_tlx;
atomic_t perf_cgroup_events_tlx;
static void autogroup_destroy(struct kref *kref)
{

};

struct task_struct *__switch_to(struct task_struct *prev,
        struct task_struct *next)
{
  struct task_struct *last;
//  fpsimd_thread_switch(next);

  unsigned long tpidr, tpidrro;
  if (!is_compat_task_tlx()) {
    asm("mrs %0, tpidr_el0" : "=r" (tpidr));
    current->thread.tp_value = tpidr;
  }
  if (is_compat_thread_tlx(task_thread_info(next))) {
    tpidr = 0;
    tpidrro = next->thread.tp_value;
  } else {
    tpidr = next->thread.tp_value;
    tpidrro = 0;
  }

  asm(
  "	msr	tpidr_el0, %0\n"
  "	msr	tpidrro_el0, %1"
  : : "r" (tpidr), "r" (tpidrro));

  asm(
         "       msr     contextidr_el1, %0\n"
         "       isb"
         :
          : "r" (task_pid_nr_tlx(next)));
  dsb(ish);
  last = cpu_switch_to_tlx(prev, next);
  return last;
}

#define test_and_clear_thread_flag(flag) \
         test_and_clear_ti_thread_flag_tlx(current_thread_info_tlx_tlx(), flag)

void
context_switch_tlx(struct rq *rq_, struct task_struct *prev,
				struct task_struct *next)
{
	struct mm_struct *_mm, *oldmm;
	_mm = next->mm;
	oldmm = prev->active_mm;
	if (!_mm) {
		next->active_mm = oldmm;
		atomic_inc_tlx(&oldmm->mm_count);
	} else {
			struct mm_struct *prev = oldmm;
			struct mm_struct *next_ = _mm;
			struct task_struct *tsk = next;
			unsigned int cpu = smp_processor_id();
			if (!cpumask_test_and_set_cpu_tlx(cpu, mm_cpumask_tlx(next_)) || prev != next_) {
						cpu_set_reserved_ttbr0();
					if (!((next_->context.id ^ cpu_last_asid_tlx) >> MAX_ASID_BITS))
									cpu_switch_mm(next_->pgd, next_);
						else if (irqs_disabled())
								set_ti_thread_flag_tlx(task_thread_info(tsk), TIF_SWITCH_MM);
						else {
									__new_context_tlx(next_);
								cpu_switch_mm(next_->pgd, next_);
							}
						}
	}
	if (!prev->mm) {
		prev->active_mm = NULL;
		rq_->prev_mm = oldmm;
	}
	(prev) = __switch_to((prev), (next));
	barrier();
	struct rq *rq = (&__get_cpu_var(runqueues_tlx));
		struct mm_struct *mm = rq->prev_mm;
		long prev_state;
		rq->prev_mm = NULL;
		prev_state = prev->state;
		#define for_each_task_context_nr(ctxn)					\
			for ((ctxn) = 0; (ctxn) < perf_nr_task_contexts; (ctxn)++)

		if (static_key_false_tlx(&perf_sched_events_tlx.key)) {
						struct perf_event_context *ctx;
						int ctxn;
						for_each_task_context_nr(ctxn) {
							ctx = current->perf_event_ctxp[ctxn];
							if (likely(!ctx))
								continue;
									struct perf_cpu_context *cpuctx;
									cpuctx =  this_cpu_ptr(ctx->pmu->pmu_cpu_context);
									if (cpuctx->task_ctx == ctx)
										continue;
									if (ctx->nr_events)
										cpuctx->task_ctx = ctx;
									struct perf_cpu_context *cpuctx_ = this_cpu_ptr(ctx->pmu->pmu_cpu_context);
									struct list_head *head = &__get_cpu_var(rotation_list_tlx);
									if (list_empty(&cpuctx_->rotation_list_tlx))
										list_add(&cpuctx_->rotation_list_tlx, head);

						}
						if (atomic_read(&__get_cpu_var(perf_branch_stack_events_tlx))) {
									struct task_struct *task = current;
									struct perf_cpu_context *cpuctx;
									struct pmu *pmu;
									unsigned long flags;
									if (prev == task)
										return;
									local_irq_save(flags);
									rcu_read_lock_tlx();
									list_for_each_entry_rcu(pmu, &pmus_tlx, entry) {
										cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
										if (cpuctx->ctx.nr_branch_stack > 0
												&& pmu->flush_branch_stack) {
												pmu->flush_branch_stack();
											}
									}
									rcu_read_unlock_tlx();
									local_irq_restore(flags);
						}
			}
		smp_wmb();
		prev->on_cpu = 0;
			spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
		raw_spin_unlock_irq(&rq->lock);
		if (test_and_clear_thread_flag(TIF_SWITCH_MM)) {
									struct mm_struct *mm = current->mm;
									unsigned long flags;
									__new_context_tlx(mm);
									local_irq_save(flags);
									cpu_switch_mm(mm->pgd, mm);
									local_irq_restore(flags);
			}
		if (mm)
			mmdrop_tlx(mm);
		if (unlikely(prev_state == TASK_DEAD)) {
			if (prev->sched_class->task_dead)
				prev->sched_class->task_dead(prev);
			if (atomic_dec_and_test(&prev->usage)) {
					delayacct_tsk_free(prev);
					if (atomic_dec_and_test(&(prev->signal)->sigcnt)) {
							kref_put_tlx(&(prev->signal->autogroup)->kref, autogroup_destroy);
							kmem_cache_free_tlx(signal_cachep_tlx, prev->signal);
						}
			}
		}

}



#define RETRY_TASK              ((void *)-1UL)

#define sched_class_highest (&fair_sched_class_tlx)
#define for_each_class(class) \
		for (class = sched_class_highest; class; class = class->next)

void __schedule_tlx(void)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq *rq;
	int cpu;

need_resched:
	preempt_disable();
	cpu = smp_processor_id();
	rq = (&per_cpu(runqueues_tlx, (cpu)));
	rcu_note_context_switch_tlx(cpu);
	prev = rq->curr;
	smp_mb__before_spinlock();
	raw_spin_lock_irq(&rq->lock);

	switch_count = &prev->nivcsw;
	if (prev->state && !(preempt_count_tlx() & PREEMPT_ACTIVE)) {
		if (unlikely(signal_pending_state_tlx(prev->state, prev))) {
			prev->state = TASK_RUNNING;
		} else {
			if ((prev->state & TASK_UNINTERRUPTIBLE) != 0 && \
																(prev->flags & PF_FROZEN) == 0)
								rq->nr_uninterruptible++;
				prev->sched_class->dequeue_task(rq, prev, DEQUEUE_SLEEP);

			prev->on_rq = 0;
			if (prev->flags & PF_WQ_WORKER) {
				struct task_struct *to_wakeup;
				struct task_struct *task = prev;
					to_wakeup = NULL;
					struct worker *worker = (container_of(task->vfork_done, struct kthread, exited))->data;
           struct worker *to_wakeup_ = NULL;
					struct worker_pool *pool;
					if (worker->flags & WORKER_NOT_RUNNING)
						goto have_to_wakeup;
					pool = worker->pool;
					if (atomic_dec_and_test(&pool->nr_running) &&
							!list_empty(&pool->worklist))
					if (unlikely(list_empty(&pool->idle_list)))
											goto have_to_wakeup;
					to_wakeup_ = list_first_entry(&pool->idle_list, struct worker, entry);
					to_wakeup = to_wakeup_->task;
have_to_wakeup:
				if (to_wakeup) {
						int _cpu = task_cpu_tlx(to_wakeup);
						struct rq *rq = (&per_cpu(runqueues_tlx, (_cpu))); //task_rq(to_wakeup);
						lockdep_assert_held(&rq->lock);
						if (!raw_spin_trylock(&to_wakeup->pi_lock)) {
							raw_spin_unlock(&rq->lock);
							raw_spin_lock(&to_wakeup->pi_lock);
							raw_spin_lock(&rq->lock);
						}
						if (!to_wakeup->on_rq) {
								if ((to_wakeup->state & TASK_UNINTERRUPTIBLE) != 0 && \
																									(to_wakeup->flags & PF_FROZEN) == 0)
									rq->nr_uninterruptible--;
									to_wakeup->sched_class->enqueue_task(rq, to_wakeup, ENQUEUE_WAKEUP);
									to_wakeup->on_rq = 1;
						}
							to_wakeup->state = TASK_RUNNING;
					out:
						raw_spin_unlock(&to_wakeup->pi_lock);
				}
			}
		}
		switch_count = &prev->nvcsw;
	}
		const struct sched_class *class = &fair_sched_class_tlx;
		struct task_struct *p;
		if (likely(prev->sched_class == class &&
				rq->nr_running == rq->cfs.h_nr_running)) {
			p = fair_sched_class_tlx.pick_next_task(rq, prev);
			if (unlikely(!p))
				p = idle_sched_class_tlx.pick_next_task(rq, prev);
				next = p;
				goto have_next;
		}
again:
		for_each_class(class) {
			p = class->pick_next_task(rq, prev);
			if (p) {
				if (unlikely(p == RETRY_TASK))
					goto again;
				next = p;
				goto have_next;
			}
		}

have_next:
	clear_tsk_thread_flag_tlx(prev,TIF_NEED_RESCHED);
	rq->skip_clock_update = 0;
	if (likely(prev != next)) {
		rq->nr_switches++;
		rq->curr = next;
		++*switch_count;
		context_switch_tlx(rq, prev, next); /* unlocks the rq */
		cpu = smp_processor_id();
		rq = (&per_cpu(runqueues_tlx, (cpu)));
	} else
		raw_spin_unlock_irq(&rq->lock);
	barrier();
	if (need_resched_tlx())
		goto need_resched;
}


void cpu_idle_loop_tlx(void)
{
	while (1) {
		while (!need_resched_tlx()) {
			local_irq_disable();
				if (need_resched_tlx()) {
					local_irq_enable();
					return;
				}
				dmb(ish);
				if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
						local_irq_enable();
				else {
							cpu_do_idle_tlx();
							local_irq_enable();
				}
					continue;
		}
		__schedule_tlx();
	}
}





struct autogroup autogroup_default_tlx;
LIST_HEAD(task_groups_tlx);

typedef int (*cpu_stop_fn_t)(void *arg);


#define CLONE_VFORK     0x00004000      /* set if the parent wants the child to wake it up on mm_release */
#define PTRACE_EVENT_VFORK      2
#define CSIGNAL         0x000000ff      /* signal mask to be sent at exit */
#define PTRACE_EVENT_CLONE      3
#define PTRACE_EVENT_FORK       1
#define STACK_END_MAGIC         0x57AC6E9D


unsigned long total_forks_tlx;
int nr_threads_tlx;
unsigned long process_counts_tlx;// = 0;


#define pid_hash_tlxfn(nr, ns)      \
				hash_long((unsigned long)nr + (unsigned long)ns, 4)

#define PIDNS_HASH_ADDING (1U << 31)

#define find_next_offset(map, off)                                      \
                 find_next_zero_bit_tlx((map)->page, BITS_PER_PAGE, off)


void set_last_pid_tlx(struct pid_namespace *pid_ns, int base, int pid)
{
	int prev;
	int last_write = base;
	do {
		prev = last_write;
		last_write = cmpxchg(&pid_ns->last_pid, prev, pid);
	} while ((prev != last_write) && ((unsigned)(last_write - base) < (unsigned)(pid - base)));
}

int pid_max_tlx = PID_MAX_DEFAULT;

#define RESERVED_PIDS           300

int alloc_pidmap_tlx(struct pid_namespace *pid_ns)
{
	int i, offset, max_scan, pid, last = pid_ns->last_pid;
	struct pidmap *map;

	pid = last + 1;
	if (pid >= pid_max_tlx)
		pid = RESERVED_PIDS;
	offset = pid & BITS_PER_PAGE_MASK;
	map = &pid_ns->pidmap[pid/BITS_PER_PAGE];
	max_scan = DIV_ROUND_UP(pid_max_tlx, BITS_PER_PAGE) - !offset;
	for (i = 0; i <= max_scan; ++i) {
		if (unlikely(!map->page)) {
			void *page = kzalloc_tlx(PAGE_SIZE, GFP_KERNEL);
			if (!map->page) {
				map->page = page;
				page = NULL;
			}
			kfree_tlx(page);
			if (unlikely(!map->page))
				break;
		}
		if (likely(atomic_read(&map->nr_free))) {
			for ( ; ; ) {
				if (!test_and_set_bit_tlx(offset, map->page)) {
					atomic_dec_tlx(&map->nr_free);
					set_last_pid_tlx(pid_ns, last, pid);
					return pid;
				}
				offset = find_next_offset(map, offset);
				if (offset >= BITS_PER_PAGE)
					break;
//				pid = mk_pid(pid_ns, map, offset);
				pid =  (map - pid_ns->pidmap)*BITS_PER_PAGE + offset;
				if (pid >= pid_max_tlx)
					break;
			}
		}
		if (map < &pid_ns->pidmap[(pid_max_tlx-1)/BITS_PER_PAGE]) {
			++map;
			offset = 0;
		} else {
			map = &pid_ns->pidmap[0];
			offset = RESERVED_PIDS;
			if (unlikely(last == offset))
				break;
		}
		pid = (map - pid_ns->pidmap)*BITS_PER_PAGE + offset;
	}
	return -1;
}

struct pid *alloc_pid_tlx(struct pid_namespace *ns)
{
	struct pid *pid;
	enum pid_type type;
	int i, nr;
	struct pid_namespace *tmp;
	struct upid *upid;

	pid = kmem_cache_zalloc_tlx(ns->pid_cachep, GFP_KERNEL);
	tmp = ns;
	pid->level = ns->level;
	for (i = ns->level; i >= 0; i--) {
		nr = alloc_pidmap_tlx(tmp);
		pid->numbers[i].nr = nr;
		pid->numbers[i].ns = tmp;
		tmp = tmp->parent;
	}
	atomic_set(&pid->count, 1);
	for (type = 0; type < PIDTYPE_MAX; ++type)
		INIT_HLIST_HEAD(&pid->tasks[type]);

	upid = pid->numbers + ns->level;
	if (!(ns->nr_hashed & PIDNS_HASH_ADDING))
		goto out;
	for ( ; upid >= pid->numbers; --upid) {
		hlist_add_head_rcu_tlx(&upid->pid_chain,
				&pid_hash_tlx[pid_hash_tlxfn(upid->nr, upid->ns)]);
		upid->ns->nr_hashed++;
	}
out:
	return pid;
}



struct pid init_struct_pid_tlx; //= INIT_STRUCT_PID;

struct fpsimd_state *fpsimd_last_state_tlx;
#ifndef this_cpu_write
 # ifndef this_cpu_write_1
 #  define this_cpu_write_1(pcp, val)    _this_cpu_generic_to_op((pcp), (val), =)
 # endif
 # ifndef this_cpu_write_2
 #  define this_cpu_write_2(pcp, val)    _this_cpu_generic_to_op((pcp), (val), =)
 # endif
 # ifndef this_cpu_write_4
 #  define this_cpu_write_4(pcp, val)    _this_cpu_generic_to_op((pcp), (val), =)
 # endif
 # ifndef this_cpu_write_8
 #  define this_cpu_write_8(pcp, val)    _this_cpu_generic_to_op((pcp), (val), =)
 # endif
 # define this_cpu_write(pcp, val)       __pcpu_size_call(this_cpu_write_, (pcp), (val))
#endif

extern void fpsimd_load_state_tlx(struct fpsimd_state *state);
# define this_cpu_write(pcp, val)       __pcpu_size_call(this_cpu_write_, (pcp), (val))


void fpsimd_restore_current_state_tlx(void)
{
//	preempt_disable();
	if (test_and_clear_thread_flag(TIF_FOREIGN_FPSTATE)) {
		struct fpsimd_state *st = &current->thread.fpsimd_state;

		fpsimd_load_state_tlx(st);
		this_cpu_write(fpsimd_last_state_tlx, st);
		st->cpu = smp_processor_id();
	}
//	preempt_enable();
}

#define TIF_SIGPENDING          0
#define TIF_NEED_RESCHED        1
#define TIF_NOTIFY_RESUME       2       /* callback before returning to user */
#define TIF_FOREIGN_FPSTATE     3       /* CPU's FP state is not current's */
#define TIF_SYSCALL_TRACE       8
#define TIF_SYSCALL_AUDIT       9
#define TIF_SYSCALL_TRACEPOINT  10
#define TIF_SECCOMP             11
#define TIF_MEMDIE              18      /* is terminating due to OOM killer */
#define TIF_FREEZE              19
#define TIF_RESTORE_SIGMASK     20
#define TIF_SINGLESTEP          21
#define TIF_32BIT               22      /* 32bit process */
#define TIF_SWITCH_MM           23      /* deferred switch_mm */

#define _TIF_SIGPENDING         (1 << TIF_SIGPENDING)
#define _TIF_NEED_RESCHED       (1 << TIF_NEED_RESCHED)
#define _TIF_NOTIFY_RESUME      (1 << TIF_NOTIFY_RESUME)
#define _TIF_FOREIGN_FPSTATE    (1 << TIF_FOREIGN_FPSTATE)
#define _TIF_SYSCALL_TRACE      (1 << TIF_SYSCALL_TRACE)
#define _TIF_SYSCALL_AUDIT      (1 << TIF_SYSCALL_AUDIT)
#define _TIF_SYSCALL_TRACEPOINT (1 << TIF_SYSCALL_TRACEPOINT)
#define _TIF_SECCOMP            (1 << TIF_SECCOMP)
#define _TIF_32BIT              (1 << TIF_32BIT)



void do_notify_resume_tlx(struct pt_regs *regs,
				 unsigned int thread_flags)
{

	if (thread_flags & _TIF_NOTIFY_RESUME) {
		test_and_clear_thread_flag(TIF_NOTIFY_RESUME);
	}

	if (thread_flags & _TIF_FOREIGN_FPSTATE)
		fpsimd_restore_current_state_tlx();

}

extern void ret_from_fork_tlx(void);


struct page *vm_normal_page_tlx(struct vm_area_struct *vma, unsigned long addr,
				pte_t pte);

unsigned long
copy_one_pte_tlx(struct mm_struct *dst_mm, struct mm_struct *src_mm,
		pte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *vma,
		unsigned long addr, int *rss)
{
	unsigned long vm_flags = vma->vm_flags;
	pte_t pte = *src_pte;
	struct page *page;

	/* pte contains position in swap or file, so copy. */
	if (unlikely(!pte_present(pte))) {
		if (!pte_file(pte)) {
			if (unlikely(list_empty(&dst_mm->mmlist))) {
//				spin_lock(&mmlist_lock);
				if (list_empty(&dst_mm->mmlist))
					list_add(&dst_mm->mmlist,
						&src_mm->mmlist);
	//			spin_unlock(&mmlist_lock);
			}
				rss[MM_SWAPENTS]++;
		}
		goto out_set_pte;
	}

	/*
	* If it's a COW mapping, write protect it both
	* in the parent and the child
	*/
	if ((vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE) {
//		ptep_set_wrprotect(src_mm, addr, src_pte);
		pte_t old_pte = *src_pte;
		set_pte_at(src_mm, addr, src_pte, pte_wrprotect(old_pte));
		pte = pte_wrprotect(pte);
	}

	/*
	* If it's a shared mapping, mark it clean in
	* the child
	*/
	if (vm_flags & VM_SHARED)
			pte_val(pte) &= ~PTE_DIRTY;
//		pte = pte_mkclean(pte);

	pte = pte_mkold(pte);

	page = vm_normal_page_tlx(vma, addr, pte);
	if (page) {
			rss[MM_FILEPAGES]++;
	}

out_set_pte:
	set_pte_at(dst_mm, addr, dst_pte, pte);
	return 0;
}


typedef struct {
				unsigned long val;
} swp_entry_t;

#define pte_none(pte)           (!pte_val(pte))
#define pte_offset_map_lock(mm, pmd, address, ptlp)     \
({                                                      \
				spinlock_t *__ptl = pte_lockptr_tlx(mm, pmd);       \
				pte_t *__pte = pte_offset_map(pmd, address);    \
				*(ptlp) = __ptl;                                \
				spin_lock_tlx(__ptl);                               \
				__pte;                                          \
})

int copy_pte_range_tlx(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
			unsigned long addr, unsigned long end)
{
	pte_t *orig_src_pte, *orig_dst_pte;
	pte_t *src_pte, *dst_pte;
	spinlock_t *src_ptl, *dst_ptl;
	int progress = 0;
	int rss[NR_MM_COUNTERS];
	swp_entry_t entry = (swp_entry_t){0};
	pgtable_t new;
again:
//	init_rss_vec(rss);
	new = alloc_pages(PGALLOC_GFP, 0);
//				pte_alloc_one(new);
	smp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */
	if (likely(pmd_none(*dst_pmd))) {	/* Has another populated it ? */
		atomic_long_inc_tlx(&dst_mm->nr_ptes);
//					pmd_populate(mm, pmd, new);
		set_pmd(dst_pmd,  page_to_phys(new) | PMD_TYPE_TABLE);
		new = NULL;
	}
	dst_pte = ((unlikely(pmd_none(*(dst_pmd))))? \
								NULL: pte_offset_map_lock(dst_mm, dst_pmd, addr, &dst_ptl));
//	pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
//	if (!dst_pte)
//		return -ENOMEM;
	src_pte = pte_offset_map(src_pmd, addr);
//	src_ptl = pte_lockptr(src_mm, src_pmd);
//	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
	orig_src_pte = src_pte;
	orig_dst_pte = dst_pte;
//	arch_enter_lazy_mmu_mode();

	do {
		/*
		* We are holding two locks at this point - either of them
		* could generate latencies in another task on another CPU.
		*/
		if (progress >= 32) {
			progress = 0;
		}
		if (pte_none(*src_pte)) {
			progress++;
			continue;
		}
		entry.val = copy_one_pte_tlx(dst_mm, src_mm, dst_pte, src_pte,
							vma, addr, rss);
		if (entry.val)
			break;
		progress += 8;
	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);

//	arch_leave_lazy_mmu_mode();
//	spin_unlock(src_ptl);
	pte_unmap(orig_src_pte);
//	add_mm_rss_vec(dst_mm, rss);
	pte_unmap_unlock(orig_dst_pte, dst_ptl);
	cond_resched();
	if (addr != end)
		goto again;
	return 0;
}


#define pmd_none(pmd)           (!pmd_val(pmd))
static inline pud_t * pud_offset(pgd_t * pgd, unsigned long address)
{
				return (pud_t *)pgd;
}

int pmd_none_or_clear_bad_tlx(pmd_t *pmd)
{
				if (pmd_none(*pmd))
								return 1;
				return 0;
}

int copy_pmd_range_tlx(struct mm_struct *dst_mm, struct mm_struct *src_mm,
		pud_t *dst_pud, pud_t *src_pud, struct vm_area_struct *vma,
		unsigned long addr, unsigned long end)
{
	pmd_t *src_pmd, *dst_pmd;
	unsigned long next;

	dst_pmd = pmd_alloc_tlx(dst_mm, dst_pud, addr);
	src_pmd = pmd_offset_tlx(src_pud, addr);
	do {
		next = pmd_addr_end(addr, end);
		if (pmd_none_or_clear_bad_tlx(src_pmd))
			continue;
		if (copy_pte_range_tlx(dst_mm, src_mm, dst_pmd, src_pmd,
						vma, addr, next))
			return -ENOMEM;
	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
	return 0;
}


int copy_pud_range_tlx(struct mm_struct *dst_mm, struct mm_struct *src_mm,
		pgd_t *dst_pgd, pgd_t *src_pgd, struct vm_area_struct *vma,
		unsigned long addr, unsigned long end)
{
	pud_t *src_pud, *dst_pud;
	unsigned long next;

	dst_pud = pud_offset_tlx(dst_pgd, addr);
	src_pud = pud_offset_tlx(src_pgd, addr);
	do {
		next = pud_addr_end(addr, end);
		if (pud_none_or_clear_bad_tlx(src_pud))
			continue;
		if (copy_pmd_range_tlx(dst_mm, src_mm, dst_pud, src_pud,
						vma, addr, next))
			return -ENOMEM;
	} while (dst_pud++, src_pud++, addr = next, addr != end);
	return 0;
}

int dup_mmap_tlx(struct mm_struct *mm, struct mm_struct *oldmm)
{
	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
	struct rb_node **rb_link, *rb_parent;
	int retval;
	unsigned long charge;
//	flush_cache_dup_mm(oldmm);
	mm->locked_vm = 0;
	mm->mmap = NULL;
	mm->vmacache_seqnum = 0;
	mm->map_count = 0;
	cpumask_clear_tlx(mm_cpumask_tlx(mm));
	mm->mm_rb = RB_ROOT;
	rb_link = &mm->mm_rb.rb_node;
	rb_parent = NULL;
	pprev = &mm->mmap;
//	retval = ksm_fork(mm, oldmm);
//	retval = khugepaged_fork(mm, oldmm);
	prev = NULL;
	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
		struct file *file;
		charge = 0;
		tmp = slab_alloc_tlx(vm_area_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
//		mem_cache_alloc(vm_area_cachep_tlx, GFP_KERNEL);
//		*tmp = *mpnt; //memcpy
		memcpy_tlx(tmp, mpnt, sizeof (*mpnt));
		INIT_LIST_HEAD(&tmp->anon_vma_chain);
		tmp->vm_mm = mm;
		tmp->vm_flags &= ~VM_LOCKED;
		tmp->vm_next = tmp->vm_prev = NULL;
		file = tmp->vm_file;
		if (file) {
			struct inode *inode = file_inode_tlx(file);
			struct address_space *mapping = file->f_mapping;
//			get_file(file);
			if (tmp->vm_flags & VM_DENYWRITE)
				atomic_dec_tlx(&inode->i_writecount);
			if (tmp->vm_flags & VM_SHARED)
				mapping->i_mmap_writable++;
					struct vm_area_struct *node = tmp;
					struct vm_area_struct *prev = mpnt;
					struct rb_root *root = &mapping->i_mmap;
						struct rb_node **link;
						struct vm_area_struct *parent;
						unsigned long last = node->vm_pgoff + ((node->vm_end - node->vm_start) >> PAGE_SHIFT) - 1;
						if (!prev->shared.linear.rb.rb_right) {
							parent = prev;
							link = &prev->shared.linear.rb.rb_right;
						} else {
							parent = rb_entry(prev->shared.linear.rb.rb_right,
										struct vm_area_struct, shared.linear.rb);
							if (parent->shared.linear.rb_subtree_last < last)
								parent->shared.linear.rb_subtree_last = last;
							while (parent->shared.linear.rb.rb_left) {
								parent = rb_entry(parent->shared.linear.rb.rb_left,
									struct vm_area_struct, shared.linear.rb);
								if (parent->shared.linear.rb_subtree_last < last)
									parent->shared.linear.rb_subtree_last = last;
							}
							link = &parent->shared.linear.rb.rb_left;
						}

						node->shared.linear.rb_subtree_last = last;
						rb_link_node_tlx(&node->shared.linear.rb, &parent->shared.linear.rb, link);
		}

	*pprev = tmp;
		pprev = &tmp->vm_next;
		tmp->vm_prev = prev;
		prev = tmp;
		rb_link_node_tlx(&tmp->vm_rb, rb_parent, rb_link);
		tmp->rb_subtree_gap = 0;
		rb_link = &tmp->vm_rb.rb_right;
		rb_parent = &tmp->vm_rb;
		mm->map_count++;
		struct mm_struct *dst_mm = mm;
		struct mm_struct *src_mm = oldmm;
		struct vm_area_struct *vma = mpnt;
			pgd_t *src_pgd, *dst_pgd;
			unsigned long next;
			unsigned long addr = vma->vm_start;
			unsigned long end = vma->vm_end;
			unsigned long mmun_start;	/* For mmu_notifiers */
			unsigned long mmun_end;		/* For mmu_notifiers */
			mmun_start = addr;
			mmun_end   = end;
			dst_pgd = pgd_offset(dst_mm, addr);
			src_pgd = pgd_offset(src_mm, addr);
			do {
				next = pgd_addr_end(addr, end);
					copy_pud_range_tlx(dst_mm, src_mm, dst_pgd, src_pgd,
									vma, addr, next);
			} while (dst_pgd++, src_pgd++, addr = next, addr != end);

		if (tmp->vm_ops && tmp->vm_ops->open)
			tmp->vm_ops->open(tmp);
	}
	unsigned long asid = (unsigned long)ASID(oldmm) << 48;
	dsb(ishst);
	asm("tlbi       aside1is, %0" : : "r" (asid));
	dsb(ish);
	return 0;
}

unsigned long default_dump_filter_tlx = MMF_DUMP_FILTER_DEFAULT;

struct mm_struct *mm_init_tlx_tlx(struct mm_struct *mm, struct task_struct *p)
{
	atomic_set(&mm->mm_users, 1);
	atomic_set(&mm->mm_count, 1);
	init_rwsem(&mm->mmap_sem);
	INIT_LIST_HEAD(&mm->mmlist);
	mm->core_state = NULL;
	atomic_long_set_tlx(&mm->nr_ptes, 0);
	memset_tlx(&mm->rss_stat, 0, sizeof(mm->rss_stat));
	spin_lock_init(&mm->page_table_lock);
//	clear_tlb_flush_pending(mm);
	barrier();
	mm->tlb_flush_pending = false;
	if (current->mm) {
		mm->flags = current->mm->flags & MMF_INIT_MASK;
		mm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;
	} else {
		mm->flags = default_dump_filter_tlx;
		mm->def_flags = 0;
	}
//		mm_alloc_pgd(mm);
	mm->pgd = pgd_alloc_tlx(mm);
	return mm;
}

#define allocate_mm()   (slab_alloc_tlx(mm_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_))
static inline unsigned long get_mm_counter(struct mm_struct *mm, int member)
 {
         long val = atomic_long_read_tlx(&mm->rss_stat.count[member]);
         return (unsigned long)val;
 }

struct mm_struct *dup_mm_tlx(struct task_struct *tsk)
{
	struct mm_struct *mm, *oldmm = current->mm;
	int err;

	mm = allocate_mm();
	memcpy_tlx(mm, oldmm, sizeof(*mm));
//	mm_init_cpumask(mm);
		mm_init_tlx_tlx(mm, tsk);
//		init_new_context(tsk, mm);
//	dup_mm_exe_file(oldmm, mm);
	mm->exe_file = oldmm->exe_file;
	dup_mmap_tlx(mm, oldmm);
	mm->hiwater_rss = get_mm_counter(mm, MM_FILEPAGES) +
								get_mm_counter(mm, MM_ANONPAGES);
	mm->hiwater_vm = mm->total_vm;
	return mm;
}



void *kzalloc_tlx(size_t size, gfp_t flags);

#define NICE_0_LOAD             SCHED_LOAD_SCALE
#define root_task_group_tlx_LOAD    NICE_0_LOAD




static inline void set_task_rq(struct task_struct *p, unsigned int cpu)
{
#if defined(CONFIG_FAIR_GROUP_SCHED) || defined(CONFIG_RT_GROUP_SCHED)
	struct task_group *tg =  p->sched_task_group;
#endif
#ifdef CONFIG_FAIR_GROUP_SCHED
	p->se.cfs_rq = tg->cfs_rq[cpu];
	p->se.parent = tg->se[cpu];
#endif
#ifdef CONFIG_RT_GROUP_SCHED
	p->rt.rt_rq  = tg->rt_rq[cpu];
	p->rt.parent = tg->rt_se[cpu];
#endif
}

void set_task_cpu_tlx(struct task_struct *p, unsigned int new_cpu)
{
	if (task_cpu_tlx(p) != new_cpu) {
		if (p->sched_class->migrate_task_rq)
			p->sched_class->migrate_task_rq(p, new_cpu);
		p->se.nr_migrations++;
//		perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, NULL, 0);
	}
//	__set_task_cpu(p, new_cpu);
	set_task_rq(p, new_cpu);
	smp_wmb();
	task_thread_info(p)->cpu = new_cpu;
	p->wake_cpu = new_cpu;
}

#define PREEMPT_ENABLED (0)
#define PREEMPT_DISABLED        PREEMPT_ENABLED

int sched_fork_tlx(unsigned long clone_flags, struct task_struct *p)
{
	unsigned long flags;
	int cpu = smp_processor_id();
		p->on_rq			= 0;
		p->se.on_rq			= 0;
		p->se.exec_start		= 0;
		p->se.sum_exec_runtime		= 0;
		p->se.prev_sum_exec_runtime	= 0;
		p->se.nr_migrations		= 0;
		p->se.vruntime			= 0;
		INIT_LIST_HEAD(&p->se.group_node);
		RB_CLEAR_NODE(&p->dl.rb_node);
		__hrtimer_init_tlx(&p->dl.dl_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
		p->dl.dl_runtime = p->dl.runtime = 0;
		p->dl.dl_deadline = p->dl.deadline = 0;
		p->dl.dl_period = 0;
		p->dl.flags = 0;

		INIT_LIST_HEAD(&p->rt.run_list);

	p->state = TASK_RUNNING;
	p->prio = current->normal_prio;
	if (unlikely(p->sched_reset_on_fork)) {
		if ((p->static_prio - DEFAULT_PRIO) < 0)
			p->static_prio = ((0) + DEFAULT_PRIO);

		p->prio = p->normal_prio =  p->static_prio;
//		set_load_weight(p);
		int prio = p->static_prio - MAX_RT_PRIO;
		struct load_weight *load = &p->se.load;
		if (p->policy == SCHED_IDLE) {
			load->weight = (WEIGHT_IDLEPRIO) << SCHED_LOAD_RESOLUTION ;
			load->inv_weight = WMULT_IDLEPRIO;
			goto have_weight;
		}
		load->weight = (prio_to_weight_tlx[prio]) << SCHED_LOAD_RESOLUTION;
		load->inv_weight = prio_to_wmult_tlx[prio];
have_weight:
		p->sched_reset_on_fork = 0;
	}

		p->sched_class = &fair_sched_class_tlx;

	if (p->sched_class->task_fork)
		p->sched_class->task_fork(p);

//	raw_spin_lock_irqsave(&p->pi_lock, flags);
	set_task_cpu_tlx(p, cpu);
//	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
	task_thread_info(p)->preempt_count = PREEMPT_DISABLED;
#ifdef CONFIG_SMP
//	plist_node_init(&p->pushable_tasks, MAX_PRIO);
	struct plist_node *node = &p->pushable_tasks;
	node->prio = MAX_PRIO;
	INIT_LIST_HEAD(&node->prio_list);
	INIT_LIST_HEAD(&node->node_list);
	RB_CLEAR_NODE(&p->pushable_dl_tasks);
#endif

//	put_cpu();
	return 0;
}



#define THREAD_SIZE_ORDER       2
# define THREADINFO_GFP         (GFP_KERNEL | __GFP_NOTRACK)
#define TIF_USER_RETURN_NOTIFY  11      /* notify kernel of userspace return */



struct task_struct *dup_task_struct_tlx(struct task_struct *orig)
{
	struct task_struct *tsk;
	struct thread_info *ti;
	unsigned long *stackend;
	int node = NUMA_NO_NODE;
	int err;

//	tsk = kmem_cache_alloc_node(task_struct_cachep_tlx, GFP_KERNEL, node);
	tsk = slab_alloc_tlx(task_struct_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
//	ti = alloc_thread_info_node(tsk, node);
	struct page *page = alloc_pages_node_tlx(node, THREADINFO_GFP,
							THREAD_SIZE_ORDER);

	ti =  page_address(page);

//	*dst = *src;
//	*tsk =  *orig; //memcpy
	memcpy_tlx(tsk, orig, sizeof(*tsk));
	tsk->stack = ti;
//	setup_thread_stack(tsk, orig);
	*task_thread_info(tsk) = *task_thread_info(orig);
	task_thread_info(tsk)->task = tsk;
//	clear_user_return_notifier(tsk);
	clear_tsk_thread_flag_tlx(tsk, TIF_USER_RETURN_NOTIFY);
//	clear_tsk_need_resched(tsk);
	clear_tsk_thread_flag_tlx(tsk,TIF_NEED_RESCHED);
	stackend =  (unsigned long *)(task_thread_info(tsk) + 1);
	*stackend = STACK_END_MAGIC;	/* for overflow detection */
	atomic_set(&tsk->usage, 2);
#ifdef CONFIG_BLK_DEV_IO_TRACE
	tsk->btrace_seq = 0;
#endif
	tsk->splice_pipe = NULL;
	tsk->task_frag.page = NULL;
//	account_kernel_stack(ti, 1);
//	struct thread_info *ti, int account)

	struct zone *zone = page_zone_tlx(virt_to_page(ti));
		mod_zone_page_state_tlx(zone, NR_KERNEL_STACK, 1);
	return tsk;
}

#define SIGNAL_UNKILLABLE       0x00000040 /* for init: ignore fatal signals */
#define CLONE_PARENT    0x00008000      /* set if we want to have the same parent as the cloner */
#define CLONE_THREAD    0x00010000      /* Same thread group? */
#define CLONE_PARENT_SETTID     0x00100000      /* set the TID in the parent */
#define CLONE_CHILD_CLEARTID    0x00200000      /* clear the TID in the child */
#define PSR_MODE_EL1h   0x00000005
#define CLONE_SETTLS    0x00080000      /* create a new TLS for the child */
#define CLONE_NEWIPC            0x08000000      /* New ipcs */
// 28 #define CLONE_NEWUSER           0x10000000      /* New user namespace */
#define CLONE_NEWPID            0x20000000      /* New pid namespace */
#define CLONE_NEWNET            0x40000000      /* New network namespace */
#define CLONE_NEWUTS            0x04000000      /* New utsname group? */
#define CLONE_NEWNS     0x00020000      /* New namespace group? */
#define PF_SUPERPRIV    0x00000100      /* used super-user privileges */
#define CLONE_CHILD_SETTID      0x01000000      /* set the TID in the child */


#define current_pt_regs() task_pt_regs(current)
#define task_pt_regs(p) \
			((struct pt_regs *)(THREAD_START_SP + p->stack) - 1)

static inline void list_add_tail_rcu(struct list_head *new,
																				struct list_head *head)
{
				__list_add_rcu_tlx(new, head->prev, head);
}

# define __this_cpu_add(pcp, val)                                        \
do { __this_cpu_preempt_check_tlx("add");                                   \
				__pcpu_size_call(raw_cpu_add_, (pcp), (val));                   \
} while (0)

struct task_struct *copy_process_tlx(unsigned long clone_flags,
					unsigned long stack_start,
					unsigned long stack_size,
					int __user *child_tidptr,
					struct pid *pid,
					int trace)
{
	int retval;
	struct task_struct *p;


//	retval = security_task_create(clone_flags);
	p = dup_task_struct_tlx(current);


	current->flags &= ~PF_NPROC_EXCEEDED;
	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
	p->flags |= PF_FORKNOEXEC;
	INIT_LIST_HEAD(&p->children);
	INIT_LIST_HEAD(&p->sibling);
//	rcu_copy_process_tlx(p);
	p->vfork_done = NULL;
//	spin_lock_init(&p->alloc_lock);

//	init_sigpending(&p->pending);

	p->utime = p->stime = p->gtime = 0;
	p->utimescaled = p->stimescaled = 0;
	p->default_timer_slack_ns = current->timer_slack_ns;
//	posix_cpu_timers_init(p);
	p->real_start_time = p->start_time;
	p->io_context = NULL;
	p->audit_context = NULL;
//	cgroup_fork(p);
	retval = sched_fork_tlx(clone_flags, p);
	memset_tlx(p->perf_event_ctxp, 0, sizeof(p->perf_event_ctxp));
//	mutex_init(&p->perf_event_mutex);
	INIT_LIST_HEAD(&p->perf_event_list);

//	retval = copy_files(clone_flags, p);
//	unsigned long clone_flags, struct task_struct *tsk)
//	{
		struct files_struct *oldf, *newf;
		struct nsproxy *old_ns = p->nsproxy;
		struct mm_struct *mm, *oldmm;
		oldf = current->files;
		if (clone_flags & CLONE_FILES) {
			atomic_inc_tlx(&oldf->count);
			goto out_fs;
		}
//		newf = dup_fd(oldf, NULL);
//		p->files = newf;
out_fs:
		p->min_flt = p->maj_flt = 0;
		p->nvcsw = p->nivcsw = 0;
		p->mm = NULL;
		p->active_mm = NULL;
		oldmm = current->mm;
		if (!oldmm)
			goto out_mm;
//		vmacache_flush(p);
		if (clone_flags & CLONE_VM) {
			atomic_inc_tlx(&oldmm->mm_users);
			mm = oldmm;
			goto good_mm;
		}
		retval = -ENOMEM;
		mm = dup_mm_tlx(p);
good_mm:
		p->mm = mm;
		p->active_mm = mm;
out_mm:
		if (likely(!(clone_flags & (CLONE_NEWNS | CLONE_NEWUTS | CLONE_NEWIPC |
							CLONE_NEWPID | CLONE_NEWNET)))) {
			atomic_inc_tlx(&old_ns->count);
		}
		unsigned long stk_sz = stack_size;
		struct pt_regs *childregs = task_pt_regs(p);
		unsigned long tls = p->thread.tp_value;

		memset_tlx(&p->thread.cpu_context, 0, sizeof(struct cpu_context));

		if (likely(!(p->flags & PF_KTHREAD))) {
//			*childregs = *current_pt_regs(); //memcpy
			memcpy_tlx(childregs, current_pt_regs(), sizeof(*childregs));
			childregs->regs[0] = 0;
				asm("mrs %0, tpidr_el0" : "=r" (tls));
				if (stack_start) {
					childregs->sp = stack_start;
			}
			if (clone_flags & CLONE_SETTLS)
				tls = childregs->regs[3];
		} else {
			memset_tlx(childregs, 0, sizeof(struct pt_regs));
			childregs->pstate = PSR_MODE_EL1h;
			p->thread.cpu_context.x19 = stack_start;
			p->thread.cpu_context.x20 = stk_sz;
		}
		p->thread.cpu_context.pc = (unsigned long)ret_from_fork_tlx;
		p->thread.cpu_context.sp = (unsigned long)childregs;
		p->thread.tp_value = tls;
		memset_tlx(&p->thread.debug, 0, sizeof(struct debug_info));

	if (pid != &init_struct_pid_tlx) {
		retval = -ENOMEM;
		pid = alloc_pid_tlx(p->nsproxy->pid_ns_for_children);
	}
	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;
	if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
		p->sas_ss_sp = p->sas_ss_size = 0;
	p->pid =  pid->numbers[0].nr;
	if (clone_flags & CLONE_THREAD) {
		p->exit_signal = -1;
		p->group_leader = current->group_leader;
		p->tgid = current->tgid;
	} else {
		if (clone_flags & CLONE_PARENT)
			p->exit_signal = current->group_leader->exit_signal;
		else
			p->exit_signal = (clone_flags & CSIGNAL);
		p->group_leader = p;
		p->tgid = p->pid;
	}

	p->nr_dirtied = 0;
	p->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
	p->dirty_paused_when = 0;

	p->pdeath_signal = 0;
	INIT_LIST_HEAD(&p->thread_group);
	p->task_works = NULL;
//	write_lock_irq(&tasklist_lock_tlx);
	if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
		p->real_parent = current->real_parent;
		p->parent_exec_id = current->parent_exec_id;
	} else {
		p->real_parent = current;
		p->parent_exec_id = current->self_exec_id;
	}

//	spin_lock(&current->sighand->siglock);
	if (likely(p->pid)) {
		struct task_struct *child = p;
		INIT_LIST_HEAD(&child->ptrace_entry);
		INIT_LIST_HEAD(&child->ptraced);
		child->jobctl = 0;
		child->ptrace = 0;
		child->parent = child->real_parent;
		p->pids[PIDTYPE_PID].pid = pid;
		if (p->exit_signal >= 0) {
			p->pids[PIDTYPE_PGID].pid = current->group_leader->pids[PIDTYPE_PGID].pid;
			p->pids[PIDTYPE_SID].pid =  current->group_leader->pids[PIDTYPE_SID].pid;

			if (pid->numbers[pid->level].nr == 1) {
				(pid->numbers[pid->level].ns)->child_reaper = p;
				p->signal->flags |= SIGNAL_UNKILLABLE;
			}

			p->signal->leader_pid = pid;
			p->signal->tty = current->signal->tty;
			list_add_tail(&p->sibling, &p->real_parent->children);
			list_add_tail_rcu(&p->tasks, &init_task_tlx.tasks);
			struct pid_link *link = &p->pids[PIDTYPE_PGID];
//			hlist_add_head_rcu(&link->node, &link->pid->tasks[PIDTYPE_PGID]);
			link = &p->pids[PIDTYPE_SID];
//			hlist_add_head_rcu(&link->node, &link->pid->tasks[PIDTYPE_SID]);

			__this_cpu_add(process_counts_tlx,1);
		} else {
			current->signal->nr_threads_tlx++;
			atomic_inc_tlx(&current->signal->live);
			atomic_inc_tlx(&current->signal->sigcnt);
			list_add_tail_rcu(&p->thread_group,
						&p->group_leader->thread_group);
			list_add_tail_rcu(&p->thread_node,
						&p->signal->thread_head);
		}
//		attach_pid(p, PIDTYPE_PID);
		struct pid_link *link = &p->pids[PIDTYPE_PID];
		hlist_add_head_rcu_tlx(&link->node, &link->pid->tasks[PIDTYPE_PID]);
		nr_threads_tlx++;
	}

	total_forks_tlx++;
//	spin_unlock(&current->sighand->siglock);
//	write_unlock_irq(&tasklist_lock_tlx);
	return p;
}

long do_fork_tlx(unsigned long clone_flags,
				unsigned long stack_start,
				unsigned long stack_size,
				int __user *parent_tidptr,
				int __user *child_tidptr)
{
	struct task_struct *p;
	int trace = 0;
	long nr;
	if (!(clone_flags & CLONE_UNTRACED)) {
		if (clone_flags & CLONE_VFORK)
			trace = PTRACE_EVENT_VFORK;
		else if ((clone_flags & CSIGNAL) != SIGCHLD)
			trace = PTRACE_EVENT_CLONE;
		else
			trace = PTRACE_EVENT_FORK;
	}

	p = copy_process_tlx(clone_flags, stack_start, stack_size,
			child_tidptr, NULL, trace);
//	if (!IS_ERR(p)) {
		struct completion vfork;
		struct pid *pid;
		pid = p->pids[PIDTYPE_PID].pid;
		atomic_inc_tlx(&pid->count);
		struct pid_namespace *ns = pid->numbers[(current->pids[PIDTYPE_PID].pid)->level].ns; //task_active_pid_ns(current);
		struct upid *upid;
		if (pid && ns->level <= pid->level) {
								upid = &pid->numbers[ns->level];
								if (upid->ns == ns)
												nr = upid->nr;
		}
		if (clone_flags & CLONE_VFORK) {
			p->vfork_done = &vfork;
		}
		unsigned long flags;
		struct rq *rq;
		rq =  &per_cpu(runqueues_tlx, (0));
		if ((p->state & TASK_UNINTERRUPTIBLE) != 0 && (p->flags & PF_FROZEN) == 0)
									rq->nr_uninterruptible--;
		p->sched_class->enqueue_task(rq, p, 0);
		p->on_rq = 1;
		const struct sched_class *class;
		if (p->sched_class == rq->curr->sched_class) {
			rq->curr->sched_class->check_preempt_curr(rq, p, flags);
		} else {
			for_each_class(class) {
				if (class == rq->curr->sched_class)
					break;
				if (class == p->sched_class) {
						struct task_struct *tmp_p = rq->curr;
						if (test_bit_tlx(TIF_NEED_RESCHED, (unsigned long *)&(task_thread_info(tmp_p))->flags))
								break;
									set_bit_tlx(TIF_NEED_RESCHED, (unsigned long *)&(task_thread_info(tmp_p))->flags);
						break;
				}
			}
		}
		if (rq->curr->on_rq &&  test_bit_tlx(TIF_NEED_RESCHED, (unsigned long *)&(task_thread_info(rq->curr))->flags))
			rq->skip_clock_update = 1;
		put_pid_tlx(pid);
//	} else {
//		nr = PTR_ERR(p);
//	}
	return nr;
}





void __sched_fork_tlx(unsigned long clone_flags, struct task_struct *p)
{
	p->on_rq			= 0;

	p->se.on_rq			= 0;
	p->se.exec_start		= 0;
	p->se.sum_exec_runtime		= 0;
	p->se.prev_sum_exec_runtime	= 0;
	p->se.nr_migrations		= 0;
	p->se.vruntime			= 0;
	INIT_LIST_HEAD(&p->se.group_node);
	RB_CLEAR_NODE(&p->dl.rb_node);
	__hrtimer_init_tlx(&p->dl.dl_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
	p->dl.dl_runtime = p->dl.runtime = 0;
	p->dl.dl_deadline = p->dl.deadline = 0;
	p->dl.dl_period = 0;
	p->dl.flags = 0;
	INIT_LIST_HEAD(&p->rt.run_list);
}



void __init sched_init_tlx(void)
{
		int i;


		list_add(&root_task_group_tlx.list, &task_groups_tlx);
		INIT_LIST_HEAD(&root_task_group_tlx.children);
		INIT_LIST_HEAD(&root_task_group_tlx.siblings);
			autogroup_default_tlx.tg = &root_task_group_tlx;
		kref_init_tlx(&autogroup_default_tlx.kref);
		init_rwsem(&autogroup_default_tlx.lock);
		init_task_tlx.signal->autogroup = &autogroup_default_tlx;
		atomic_inc_tlx(&init_mm_tlx.mm_count);
		struct rq *rq = cpu_rq(smp_processor_id());
		__sched_fork_tlx(0, current);
		current->state = TASK_RUNNING;
		current->se.exec_start = 0;
		if (current->sched_class && current->sched_class->set_cpus_allowed)
									current->sched_class->set_cpus_allowed(current, cpumask_of(smp_processor_id()));
		cpumask_copy_tlx(&current->cpus_allowed, cpumask_of(smp_processor_id()));
		current->nr_cpus_allowed = cpumask_weight_tlx(cpumask_of(smp_processor_id()));
		set_task_rq(current, smp_processor_id());
		smp_wmb();
		task_thread_info(current)->cpu = smp_processor_id();
		current->wake_cpu = smp_processor_id();
		rq->curr = rq->idle = current;
		current->on_rq = 1;
		current->on_cpu = 1;
		current->sched_class = &idle_sched_class_tlx;
//		sprintf(current->comm, "%s/%d", INIT_TASK_COMM, smp_processor_id());
}


#define for_each_matching_node_and_match(dn, matches, match) \
				for (dn = of_find_matching_node_and_match_tlx(NULL, matches, match); \
						dn; dn = of_find_matching_node_and_match_tlx(dn, matches, match))

#define for_each_matching_node(dn, matches) \
         for (dn = of_find_matching_node_and_match_tlx(NULL, matches, NULL); dn; \
              dn = of_find_matching_node_and_match_tlx(dn, matches, NULL))

extern struct of_device_id_tlx __clk_of_table;

struct intc_desc {
         struct list_head        list;
         struct device_node      *dev;
         struct device_node      *interrupt_parent;
};

typedef int (*of_irq_init_cb_t)(struct device_node *, struct device_node *);

#define of_irq_dflt_pic (NULL)
#define OF_IMAP_NO_PHANDLE      0x00000002
#define of_irq_workarounds (0)

void __init of_irq_init_tlx(const struct of_device_id_tlx *matches)
{
	struct device_node *np, *parent = NULL;
	struct intc_desc *desc, *temp_desc;
	struct list_head intc_desc_list, intc_parent_list;

	INIT_LIST_HEAD(&intc_desc_list);
	INIT_LIST_HEAD(&intc_parent_list);

	for_each_matching_node(np, matches) {
		desc = kzalloc_tlx(sizeof(*desc), GFP_KERNEL);
		desc->dev = np;
		struct device_node *child = np;
			struct device_node *p;
			const __be32 *parp;
			if (child)
									kobject_get_tlx(&child->kobj);
			if (!child) {
				p =  NULL;
				goto have_chld;
			}

			do {
				parp = of_get_property_tlx(child, "interrupt-parent", NULL);
				if (parp == NULL) {
            p = NULL;
						if (child) p = child->parent;
				}
				else {
					if (of_irq_workarounds & OF_IMAP_NO_PHANDLE){
							if (of_irq_dflt_pic)
													kobject_get_tlx(&( (struct device_node *)of_irq_dflt_pic)->kobj);
							p = of_irq_dflt_pic;
					}
					else
						{
							for (p = of_allnodes_tlx; p; p = p->allnext)
								if (p->phandle == be32_to_cpup(parp))
												break;
						}
				}
				child = p;
			} while (p && of_get_property_tlx(p, "#interrupt-cells", NULL) == NULL);

have_chld:
		desc->interrupt_parent = p ;
		if (desc->interrupt_parent == np)
			desc->interrupt_parent = NULL;
		list_add_tail(&desc->list, &intc_desc_list);
	}
	while (!list_empty(&intc_desc_list)) {
		list_for_each_entry_safe(desc, temp_desc, &intc_desc_list, list) {
			const struct of_device_id_tlx *match;
			int ret;
			of_irq_init_cb_t irq_init_cb;

			if (desc->interrupt_parent != parent)
				continue;

			list_del(&desc->list);
			const struct of_device_id_tlx *best_match = NULL;
			int score, best_score = 0;
				for (; matches->name[0] || matches->type[0] || matches->compatible[0]; matches++) {
						const struct device_node *device = desc->dev;
						const char *compat = matches->compatible;
						const char *type =  matches->type;
						const char *name = matches->name;
						struct property *prop;
						const char *cp;
						int index = 0;
						if (compat && compat[0]) {
								struct property *pp;
								for (pp = device->properties; pp; pp = pp->next) {
									if (strcmp_tlx(pp->name, "compatible") == 0) {
										break;
									}
								}
							prop = pp;
							for (cp = of_prop_next_string_tlx(prop, NULL); cp;
									cp = of_prop_next_string_tlx(prop, cp), index++) {
								if (strcasecmp_tlx(cp, compat) == 0) {
									score = INT_MAX/2 - (index << 2);
									break;
								}
							}
							if (!score)
								goto out2;
						}
						if (type && type[0]) {
							if (!device->type ||  strcasecmp_tlx(type, device->type))
								goto out2;
							score += 2;
						}
						if (name && name[0]) {
							if (!device->name ||  strcasecmp_tlx(name, device->name))
								return 0;
							score++;
						}

out2:
									if (score > best_score) {
													best_match = matches;
													best_score = score;
									}
					}
			match = best_match;
			irq_init_cb = (of_irq_init_cb_t)match->data;
			ret = irq_init_cb(desc->dev, desc->interrupt_parent);
			list_add_tail(&desc->list, &intc_parent_list);
		}
		desc = list_first_entry_or_null(&intc_parent_list,
						typeof(*desc), list);
		list_del(&desc->list);
		parent = desc->dev;
		kfree_tlx(desc);
	}

	list_for_each_entry_safe(desc, temp_desc, &intc_parent_list, list) {
		list_del(&desc->list);
		kfree_tlx(desc);
	}
err:
	list_for_each_entry_safe(desc, temp_desc, &intc_desc_list, list) {
		list_del(&desc->list);
		kfree_tlx(desc);
	}
}

# define SCHED_LOAD_RESOLUTION	0
# define scale_load(w)		(w)
# define scale_load_down(w)	(w)

#define SCHED_LOAD_SHIFT	(10 + SCHED_LOAD_RESOLUTION)
#define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)

#define per_cpu(var, cpu) \
          (*SHIFT_PERCPU_PTR(&(var), per_cpu_offset(cpu)))
#define cpu_rq(cpu)             (&per_cpu(runqueues_tlx, (cpu)))

#define NICE_0_LOAD		SCHED_LOAD_SCALE
#define ROOT_TASK_GROUP_LOAD	NICE_0_LOAD



void init_tg_cfs_entry_tlx(struct task_group *tg, struct cfs_rq *cfs_rq,
			struct sched_entity *se, int cpu,
			struct sched_entity *parent)
{
	struct rq *rq = cpu_rq(cpu);

	cfs_rq->tg = tg;
	cfs_rq->rq = rq;
//	init_cfs_rq_runtime(cfs_rq);
//	cfs_rq->runtime_enabled = 0;
 //	INIT_LIST_HEAD(&cfs_rq->throttled_list);
	tg->cfs_rq[cpu] = cfs_rq;
	tg->se[cpu] = se;
	if (!se)
		return;
	if (!parent) {
		se->cfs_rq = &rq->cfs;
		se->depth = 0;
	} else {
		se->cfs_rq = parent->my_q;
		se->depth = parent->depth + 1;
	}
	se->my_q = cfs_rq;
	struct load_weight *lw = &se->load;
	lw->weight =  NICE_0_LOAD;
  lw->inv_weight = 0;
	se->parent = parent;
}





void sched_init_tlx_tlx_tlx(void)
{
  int i, j;
  unsigned long alloc_size = 0, ptr;
  alloc_size += 2 * nr_cpu_ids_tlx_tlx * sizeof(void **);
  if (alloc_size) {
    ptr = (unsigned long)kzalloc_tlx(alloc_size, GFP_NOWAIT);
    root_task_group_tlx.se = (struct sched_entity **)ptr;
    ptr += nr_cpu_ids_tlx_tlx * sizeof(void **);
    root_task_group_tlx.cfs_rq = (struct cfs_rq **)ptr;
    ptr += nr_cpu_ids_tlx_tlx * sizeof(void **);
  }

  i = 0;
  struct rq *rq;
  rq = cpu_rq(i);
  root_task_group_tlx.shares = ROOT_TASK_GROUP_LOAD;
  INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);

  init_tg_cfs_entry_tlx(&root_task_group_tlx, &rq->cfs, NULL, i, NULL);
  rq = &(*({ unsigned long __ptr;					\
  __asm__ ("" : "=r"(__ptr) : "0"((typeof(*(&(runqueues_tlx))) __kernel __force *)(&(runqueues_tlx))));		\
  (typeof((typeof(*(&(runqueues_tlx))) __kernel __force *)(&(runqueues_tlx)))) (__ptr + (__per_cpu_offset_tlx[0])); }));
  INIT_LIST_HEAD(&rq->cfs_tasks);
//  INIT_LIST_HEAD(&rq->cfs_tasks);

//		}
//	init_idle(current, smp_processor_id());


}



void *__init alloc_large_system_hash_tlx(const char *tablename,
             unsigned long bucketsize,
             unsigned long numentries,
             int scale,
             int flags,
             unsigned int *_hash_shift,
             unsigned int *_hash_mask,
             unsigned long low_limit,
             unsigned long high_limit)
{
  unsigned long long max = high_limit;
  unsigned long log2qty, size;
  void *table = NULL;
  if (!numentries) {
    numentries = nr_kernel_pages_tlx;
    if (PAGE_SHIFT < 20)
      numentries = round_up(numentries, (1<<20)/PAGE_SIZE);
    if (scale > PAGE_SHIFT)
      numentries >>= (scale - PAGE_SHIFT);
    else
      numentries <<= (PAGE_SHIFT - scale);
    if (unlikely((numentries * bucketsize) < PAGE_SIZE))
      numentries = PAGE_SIZE / bucketsize;
  }
  numentries = roundup_pow_of_two(numentries);
  log2qty = ilog2(numentries);
  do {
    size = bucketsize << log2qty;
    table = memblock_tlx_virt_alloc_internal_tlx(size, 0, BOOTMEM_LOW_LIMIT,
                                               BOOTMEM_ALLOC_ACCESSIBLE,
                                               NUMA_NO_NODE);
  } while (!table && size > PAGE_SIZE && --log2qty);
  return table;
}


extern struct of_device_id_tlx __irqchip_of_table[];

  __visible void __init start_kernel(void)
{
	u64 i;
	char * command_line, *after_dashes;
	extern const struct kernel_param_tlx __start___param[], __stop___param[];
	int cpu = 0;
  set_bit_tlx(0, ((struct cpumask *) cpu_online_bits_tlx)->bits);
	set_bit_tlx(0, ((struct cpumask *) cpu_active_bits_tlx)->bits);
	set_bit_tlx(0, ((struct cpumask *) cpu_present_bits_tlx)->bits);
	set_bit_tlx(0, ((struct cpumask *) cpu_possible_bits_tlx)->bits);
	asm("msr        daifclr, #4" : : : "memory");
	initial_boot_params_tlx = (void *) __phys_to_virt_tlx(__fdt_pointer_tlx);
	struct memblock_tlx_type *mb_type = &memblock_tlx.memory;
	mb_type->regions[0].base = 1073741824;
	mb_type->regions[0].size = 536870912;
	mb_type->regions[0].flags = 0;
	mb_type->total_size = 536870912;
	init_mm_tlx.start_code = (unsigned long) _text;
	init_mm_tlx.end_code   = (unsigned long) _etext;
	init_mm_tlx.end_data   = (unsigned long) _edata;
	init_mm_tlx.brk	   = (unsigned long) _end;
	command_line = boot_command_line;
	memblock_reserve_region_tlx(__virt_to_phys_tlx((unsigned long)(_text)), _end - _text, MAX_NUMNODES, 0);
	memblock_reserve_region_tlx(__virt_to_phys_tlx((unsigned long)(swapper_pg_dir_tlx)), SWAPPER_DIR_SIZE, MAX_NUMNODES, 0);
	memblock_reserve_region_tlx(__virt_to_phys_tlx((unsigned long)(idmap_pg_dir_tlx)), IDMAP_DIR_SIZE, MAX_NUMNODES, 0);
	void *zero_page;
	struct memblock_tlx_region *reg;
	phys_addr_t limit;
	limit = PHYS_OFFSET + PGDIR_SIZE;
	memblock_tlx.current_limit = limit;
	for_each_memblock_tlx(memory, reg) {
		phys_addr_t start = reg->base;
		phys_addr_t end = start + reg->size;
		if (start >= end)
			break;
			pgd_t *pgd = pgd_offset_k_tlx(__phys_to_virt_tlx(start) & PAGE_MASK);
			phys_addr_t phys = start;
			unsigned long virt = __phys_to_virt_tlx(start);
			phys_addr_t size = end - start;
			int map_io = 0;
			unsigned long addr, length, end0, next;
				addr = virt & PAGE_MASK;
				length = PAGE_ALIGN(size + (virt & ~PAGE_MASK));
				end0 = addr + length;
				do {
					next = pgd_addr_end_tlx(addr, end0);
					alloc_init_pud_tlx(pgd, addr, next, phys, map_io);
					phys += next - addr;
				} while (pgd++, addr = next, addr != end0);
	}

	memblock_tlx.current_limit = MEMBLOCK_ALLOC_ANYWHERE;
//	flush_cache_all();
	flush_tlb_all();
	zero_page = __va(memblock_tlx_alloc_range_nid_tlx(PAGE_SIZE, PAGE_SIZE, 0, MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE));
	memset_tlx(zero_page, 0, PAGE_SIZE);
	bootmem_init_tlx();
	empty_zero_page_tlx = virt_to_page(zero_page);
	cpu_set_reserved_ttbr0();
	flush_tlb_all();
	void *blob = initial_boot_params_tlx;
	struct device_node **mynodes = &of_allnodes_tlx;
	unsigned long size1;
	int start1;
	void *mem;
	struct device_node **allnextp = mynodes;
/* First pass, scan for size */
	start1 = 0;
	size1 = (unsigned long)unflatten_dt_node_tlx(blob, NULL, &start1, NULL, NULL, 0);
	size1 = ALIGN(size1, 4);
	mem =
		__va(memblock_tlx_alloc_range_nid_tlx(size1 + 4, __alignof__(struct device_node), 0, MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE));
	memset_tlx(mem, 0, size1);
	*(__be32 *)(mem + size1) = cpu_to_be32(0xdeadbeef);
	start1 = 0;
	unflatten_dt_node_tlx(blob, mem, &start1, NULL, &allnextp, 0);
	saved_command_line =
			memblock_tlx_virt_alloc_internal_tlx(strlen_tlx(boot_command_line) + 1, 0,BOOTMEM_LOW_LIMIT,
                                             BOOTMEM_ALLOC_ACCESSIBLE,
                                             NUMA_NO_NODE);
	initcall_command_line =
		memblock_tlx_virt_alloc_internal_tlx(strlen_tlx(boot_command_line) + 1, 0,BOOTMEM_LOW_LIMIT,
																					BOOTMEM_ALLOC_ACCESSIBLE,
																					NUMA_NO_NODE);
	static_command_line =
	memblock_tlx_virt_alloc_internal_tlx(strlen_tlx(command_line) + 1, 0,BOOTMEM_LOW_LIMIT,
																				BOOTMEM_ALLOC_ACCESSIBLE,
																				NUMA_NO_NODE);
	strcpy_tlx (saved_command_line, boot_command_line);
	strcpy_tlx (static_command_line, command_line);
	unsigned long delta;
	int rc;
	size_t reserved_size = PERCPU_MODULE_RESERVE;
	size_t dyn_size = PERCPU_DYNAMIC_RESERVE;
	size_t atom_size = PAGE_SIZE;
	void *base0 = (void *)ULONG_MAX;
	void **areas = NULL;
	struct pcpu_alloc_info_tlx *ai;
	size_t size_sum, areas_size, max_distance;
	int group;
	static int group_map[NR_CPUS] __initdata;
	static int group_cnt[NR_CPUS] __initdata;
	const size_t static_size = __per_cpu_end - __per_cpu_start;
	int nr_groups = 1, nr_units = 0;
	size_t min_unit_size, alloc_size;
	int upa, max_upa, uninitialized_var(best_upa);	/* units_per_alloc */
	int last_allocs, unit;
	unsigned int tcpu;
	unsigned int *cpu_map;
	memset_tlx(group_map, 0, sizeof(group_map));
	memset_tlx(group_cnt, 0, sizeof(group_cnt));
	size_sum = PFN_ALIGN(static_size + reserved_size +
						max_t(size_t, dyn_size, PERCPU_DYNAMIC_EARLY_SIZE));
	dyn_size = size_sum - static_size - reserved_size;
	min_unit_size = max_t(size_t, size_sum, PCPU_MIN_UNIT_SIZE);

	alloc_size = roundup(min_unit_size, atom_size);
	upa = alloc_size / min_unit_size;
	while (alloc_size % upa || ((alloc_size / upa) & ~PAGE_MASK))
			upa--;
	max_upa = upa;
	for_each_possible_cpu(cpu) {
			group = 0;
		next_group:
			group_map[cpu] = group;
			group_cnt[group]++;
	}
	last_allocs = INT_MAX;
	for (upa = max_upa; upa; upa--) {
			int allocs = 0, wasted = 0;

		if (alloc_size % upa || ((alloc_size / upa) & ~PAGE_MASK))
				continue;

		for (group = 0; group < nr_groups; group++) {
				int this_allocs = DIV_ROUND_UP(group_cnt[group], upa);
				allocs += this_allocs;
				wasted += this_allocs * upa - group_cnt[group];
		}
		if (wasted > num_possible_cpus() / 3)
				continue;
		if (allocs > last_allocs)
				break;
		last_allocs = allocs;
		best_upa = upa;
	}
	upa = best_upa;
	for (group = 0; group < nr_groups; group++)
			nr_units += roundup(group_cnt[group], upa);
	size_t base_size, ai_size;
	void *ptr;
	base_size = ALIGN(sizeof(*ai) + nr_groups * sizeof(ai->groups[0]),
						__alignof__(ai->groups[0].cpu_map[0]));
	ai_size = base_size + nr_units * sizeof(ai->groups[0].cpu_map[0]);
	ptr =
			memblock_tlx_virt_alloc_internal_tlx(PFN_ALIGN(ai_size), 0, BOOTMEM_LOW_LIMIT,
																									BOOTMEM_ALLOC_ACCESSIBLE,
																									NUMA_NO_NODE);
	ai = ptr;
	ptr += base_size;
	ai->groups[0].cpu_map = ptr;
	for (unit = 0; unit < nr_units; unit++)
			ai->groups[0].cpu_map[unit] = NR_CPUS;
	ai->nr_groups = nr_groups;
	ai->__ai_size = PFN_ALIGN(ai_size);
  cpu_map = ai->groups[0].cpu_map;

	for (group = 0; group < nr_groups; group++) {
			ai->groups[group].cpu_map = cpu_map;
			cpu_map += roundup(group_cnt[group], upa);
	}
	ai->static_size = static_size;
	ai->reserved_size = reserved_size;
	ai->dyn_size = dyn_size;
	ai->unit_size = alloc_size / upa;
	ai->atom_size = atom_size;
	ai->alloc_size = alloc_size;

	for (group = 0, unit = 0; group_cnt[group]; group++) {
			struct pcpu_group_info_tlx *gi = &ai->groups[group];
			gi->base_offset = unit * ai->unit_size;
			for_each_possible_cpu(cpu)
				if (group_map[cpu] == group)
					gi->cpu_map[gi->nr_units++] = cpu;
			gi->nr_units = roundup(gi->nr_units, upa);
			unit += gi->nr_units;
	}

	size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;
	areas_size = PFN_ALIGN(ai->nr_groups * sizeof(void *));
	areas =
			memblock_tlx_virt_alloc_internal_tlx(areas_size, 0, BOOTMEM_LOW_LIMIT,
																									BOOTMEM_ALLOC_ACCESSIBLE,
																									NUMA_NO_NODE);
	for (group = 0; group < ai->nr_groups; group++) {
			struct pcpu_group_info_tlx *gi = &ai->groups[group];
			unsigned int cpu = NR_CPUS;
			void *ptr;
			for (i = 0; i < gi->nr_units && cpu == NR_CPUS; i++)
				cpu = gi->cpu_map[i];
			ptr = memblock_tlx_virt_alloc_internal_tlx(
					gi->nr_units * ai->unit_size, gi->nr_units * ai->unit_size,
          __virt_to_phys_tlx((unsigned long)(MAX_DMA_ADDRESS)),
          BOOTMEM_ALLOC_ACCESSIBLE, NUMA_NO_NODE);
			areas[group] = ptr;
			base0 = min(ptr, base0);
	}
	for (group = 0; group < ai->nr_groups; group++) {
			struct pcpu_group_info_tlx *gi = &ai->groups[group];
			void *ptr = areas[group];
			for (i = 0; i < gi->nr_units; i++, ptr += ai->unit_size) {
				if (gi->cpu_map[i] == NR_CPUS) {
				memblock_tlx_remove_range_tlx(&memblock_tlx.reserved, __virt_to_phys_tlx((unsigned long)(ptr)), ai->unit_size);
					continue;
				}
				memcpy_tlx(ptr, __per_cpu_load, ai->static_size);
				memblock_tlx_remove_range_tlx(&memblock_tlx.reserved, __virt_to_phys_tlx((unsigned long)(ptr + size_sum)), ai->unit_size - size_sum);
			}
	}
	static char cpus_buf[4096] __initdata;
	static int smap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
	static int dmap[PERCPU_DYNAMIC_EARLY_SLOTS] __initdata;
	dyn_size = ai->dyn_size;
	size_sum = ai->static_size + ai->reserved_size + dyn_size;
	struct pcpu_chunk *schunk, *dchunk = NULL;
	unsigned long *group_offsets;
	size_t *group_sizes;
	unsigned long *unit_off;
	int *unit_map;
	cpumask_scnprintf_tlx(cpus_buf, sizeof(cpus_buf), cpu_possible_mask_tlx);
	group_offsets =
		memblock_tlx_virt_alloc_internal_tlx(ai->nr_groups *
										sizeof(group_offsets[0]), 0,BOOTMEM_LOW_LIMIT,
																				BOOTMEM_ALLOC_ACCESSIBLE,
																				NUMA_NO_NODE);
	group_sizes =
			memblock_tlx_virt_alloc_internal_tlx(ai->nr_groups *
											sizeof(group_sizes[0]), 0,BOOTMEM_LOW_LIMIT,
																						BOOTMEM_ALLOC_ACCESSIBLE,
																						NUMA_NO_NODE);
	unit_map =
					memblock_tlx_virt_alloc_internal_tlx(nr_cpu_ids_tlx_tlx * sizeof(unit_map[0]), 0,BOOTMEM_LOW_LIMIT,
																								BOOTMEM_ALLOC_ACCESSIBLE,
																								NUMA_NO_NODE);
	unit_off =
			memblock_tlx_virt_alloc_internal_tlx(nr_cpu_ids_tlx_tlx * sizeof(unit_off[0]), 0,BOOTMEM_LOW_LIMIT,
																						BOOTMEM_ALLOC_ACCESSIBLE,
																						NUMA_NO_NODE);
	for (cpu = 0; cpu < nr_cpu_ids_tlx_tlx; cpu++)
				unit_map[cpu] = UINT_MAX;

	pcpu_low_unit_cpu_tlx = NR_CPUS;
	pcpu_high_unit_cpu_tlx = NR_CPUS;

	for (group = 0, unit = 0; group < ai->nr_groups; group++, unit += i) {
				const struct pcpu_group_info_tlx *gi = &ai->groups[group];

	group_offsets[group] = gi->base_offset;
	group_sizes[group] = gi->nr_units * ai->unit_size;

	for (i = 0; i < gi->nr_units; i++) {
			cpu = gi->cpu_map[i];
			if (cpu == NR_CPUS)
						continue;
			unit_map[cpu] = unit + i;
			unit_off[cpu] = gi->base_offset + i * ai->unit_size;
			if (pcpu_low_unit_cpu_tlx == NR_CPUS ||
							unit_off[cpu] < unit_off[pcpu_low_unit_cpu_tlx])
			pcpu_low_unit_cpu_tlx = cpu;
			if (pcpu_high_unit_cpu_tlx == NR_CPUS ||
							unit_off[cpu] > unit_off[pcpu_high_unit_cpu_tlx])
						pcpu_high_unit_cpu_tlx = cpu;
			}
	}
	pcpu_nr_units_tlx = unit;
	pcpu_nr_groups_tlx = ai->nr_groups;
	pcpu_group_offsets_tlx = group_offsets;
	pcpu_group_sizes_tlx = group_sizes;
	pcpu_unit_map_tlx = unit_map;
	pcpu_unit_offsets_tlx = unit_off;
	pcpu_unit_pages_tlx = ai->unit_size >> PAGE_SHIFT;
	pcpu_unit_size_tlx = pcpu_unit_pages_tlx << PAGE_SHIFT;
	pcpu_atom_size_tlx = ai->atom_size;
	pcpu_chunk_struct_size_tlx = sizeof(struct pcpu_chunk) +
				BITS_TO_LONGS(pcpu_unit_pages_tlx) * sizeof(unsigned long);
		int highbit = fls_tlx(pcpu_unit_size_tlx);        /* size is in bytes */
		pcpu_nr_slots_tlx = max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1) + 2;
	pcpu_slot_tlx =
		memblock_tlx_virt_alloc_internal_tlx(pcpu_nr_slots_tlx * sizeof(pcpu_slot_tlx[0]), 0,BOOTMEM_LOW_LIMIT,
																					BOOTMEM_ALLOC_ACCESSIBLE,
																					NUMA_NO_NODE);
	for (i = 0; i < pcpu_nr_slots_tlx; i++)
				INIT_LIST_HEAD(&pcpu_slot_tlx[i]);
	schunk =
			memblock_tlx_virt_alloc_internal_tlx(pcpu_chunk_struct_size_tlx, 0,BOOTMEM_LOW_LIMIT,
																						BOOTMEM_ALLOC_ACCESSIBLE,
																						NUMA_NO_NODE);
	INIT_LIST_HEAD(&schunk->list);
	void *base_addr = base0;
	schunk->base_addr = base_addr;
	schunk->map = smap;
	schunk->map_alloc = ARRAY_SIZE(smap);
	schunk->immutable = true;
	unsigned long *dst = schunk->populated;
	int nbits = pcpu_unit_pages_tlx;
         size_t nlongs = DIV_ROUND_UP(nbits, BITS_PER_BYTE * sizeof(long));
         if (!(__builtin_constant_p(nbits) && (nbits) <= BITS_PER_LONG)) {
                 int len = (nlongs - 1) * sizeof(unsigned long);
                 memset_tlx(dst, 0xff,  len);
         }
         dst[nlongs - 1] = ((nbits) % BITS_PER_LONG) ? (1UL<<((nbits) % BITS_PER_LONG))-1 : ~0UL;

	if (ai->reserved_size) {
				schunk->free_size = ai->reserved_size;
				pcpu_reserved_chunk_tlx = schunk;
				pcpu_reserved_chunk_tlx_limit_tlx = ai->static_size + ai->reserved_size;
	} else {
				schunk->free_size = dyn_size;
				dyn_size = 0;			/* dynamic area covered */
	}
	schunk->contig_hint = schunk->free_size;
	schunk->map[0] = 1;
	schunk->map[1] = ai->static_size;
	schunk->map_used = 1;
	if (schunk->free_size)
				schunk->map[++schunk->map_used] = 1 | (ai->static_size + schunk->free_size);
	else
				schunk->map[1] |= 1;
	if (dyn_size) {
				dchunk =
						memblock_tlx_virt_alloc_internal_tlx(pcpu_chunk_struct_size_tlx, 0,BOOTMEM_LOW_LIMIT,
																									BOOTMEM_ALLOC_ACCESSIBLE,
																									NUMA_NO_NODE);
				INIT_LIST_HEAD(&dchunk->list);
				dchunk->base_addr = base_addr;
				dchunk->map = dmap;
				dchunk->map_alloc = ARRAY_SIZE(dmap);
				dchunk->immutable = true;
				unsigned long *dst = dchunk->populated;
				int nbits = pcpu_unit_pages_tlx;
							size_t nlongs = DIV_ROUND_UP(nbits, BITS_PER_BYTE * sizeof(long));
							if (!(__builtin_constant_p(nbits) && (nbits) <= BITS_PER_LONG)) {
											int len = (nlongs - 1) * sizeof(unsigned long);
											memset_tlx(dst, 0xff,  len);
							}
							dst[nlongs - 1] = ((nbits) % BITS_PER_LONG) ? (1UL<<((nbits) % BITS_PER_LONG))-1 : ~0UL;
				dchunk->contig_hint = dchunk->free_size = dyn_size;
				dchunk->map[0] = 1;
				dchunk->map[1] = pcpu_reserved_chunk_tlx_limit_tlx;
				dchunk->map[2] = (pcpu_reserved_chunk_tlx_limit_tlx + dchunk->free_size) | 1;
				dchunk->map_used = 2;
			}
	pcpu_first_chunk_tlx = dchunk ?: schunk;
	int nslot;
	if (pcpu_first_chunk_tlx->free_size  == pcpu_unit_size_tlx)
	{
						nslot = pcpu_nr_slots_tlx - 1;
 	} else {
				int highbit = pcpu_first_chunk_tlx->free_size ? sizeof(pcpu_first_chunk_tlx->free_size) * 8 - __builtin_clz(pcpu_first_chunk_tlx->free_size) : 0;; /* size is in bytes */

					nslot = max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1);
	 }
	if (pcpu_first_chunk_tlx != pcpu_reserved_chunk_tlx && (-1) != nslot) {
						list_move(&pcpu_first_chunk_tlx->list, &pcpu_slot_tlx[nslot]);
	}
	pcpu_base_addr_tlx = base_addr;
	memblock_tlx_remove_range_tlx(&memblock_tlx.reserved, __virt_to_phys_tlx((unsigned long)(ai)), ai->__ai_size);
	if (areas)
			memblock_tlx_remove_range_tlx(&memblock_tlx.reserved, __virt_to_phys_tlx((unsigned long)(areas)), areas_size);
	delta = (unsigned long)pcpu_base_addr_tlx - (unsigned long)__per_cpu_start;
	cpu = smp_processor_id();
		__per_cpu_offset_tlx[cpu] = delta + pcpu_unit_offsets_tlx[cpu];

	asm volatile("msr tpidr_el1, %0" :: "r" (__per_cpu_offset_tlx[cpu]) : "memory");
	struct zone *zone = NULL;
	pg_data_t *pgdat = NODE_DATA(0);
	int node, local_node;
	enum zone_type j;
	struct zonelist *zonelist;
	local_node = pgdat->node_id;
	zonelist = &pgdat->node_zonelists[0];
	enum zone_type zone_type = MAX_NR_ZONES;
	zone = pgdat->node_zones;
	zonelist->_zonerefs[0].zone = zone;
	zonelist->_zonerefs[0].zone_idx = zone_idx(zone);
	zonelist->_zonerefs[1].zone = NULL;
	zonelist->_zonerefs[1].zone_idx = 0;
	struct per_cpu_pages *pcp;
	int migratetype;
	memset_tlx(&per_cpu(boot_pageset_tlx, 0), 0, sizeof(per_cpu(boot_pageset_tlx, 0)));
	pcp = &(&per_cpu(boot_pageset_tlx, 0))->pcp;
	pcp->count = 0;
	for (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)
			INIT_LIST_HEAD(&pcp->lists[migratetype]);
	pcp->batch = 1;
	pcp->high = 0;
	pcp->batch = 1UL;
	if (vm_total_pages_tlx < (pageblock_nr_pages * MIGRATE_TYPES))
		page_group_by_mobility_disabled_tlx = 1;
	else
		page_group_by_mobility_disabled_tlx = 0;

	unsigned long bucketsize = 8;
	unsigned long numentries = 0;
	int scale = 18;
	unsigned int *_hash_shift = &pidhash_shift_tlx;
	unsigned long log2qty, size;
	void *table = NULL;

	if (!numentries) {
		numentries = nr_kernel_pages_tlx;
		if (PAGE_SHIFT < 20)
			numentries = round_up(numentries, (1<<20)/PAGE_SIZE);
		if (scale > PAGE_SHIFT)
			numentries >>= (scale - PAGE_SHIFT);
		else
			numentries <<= (PAGE_SHIFT - scale);
			if (!(numentries >> *_hash_shift)) {
				numentries = 1UL << *_hash_shift;
			}
	}
	numentries = roundup_pow_of_two(numentries);
	log2qty = ilog2(numentries);

	do {
		size = bucketsize << log2qty;
			table =
				memblock_tlx_virt_alloc_internal_tlx(size, 0, BOOTMEM_LOW_LIMIT,
																										BOOTMEM_ALLOC_ACCESSIBLE,
																										NUMA_NO_NODE);
	} while (!table && size > PAGE_SIZE && --log2qty);

	pid_hash_tlx = table;
	scale = 13;
	table = NULL;
	if (!dhash_entries_tlx) {
		dhash_entries_tlx = nr_kernel_pages_tlx;
		if (PAGE_SHIFT < 20)
			dhash_entries_tlx = round_up(dhash_entries_tlx, (1<<20)/PAGE_SIZE);
		if (scale > PAGE_SHIFT)
			dhash_entries_tlx >>= (scale - PAGE_SHIFT);
		else
			dhash_entries_tlx <<= (PAGE_SHIFT - scale);
			if (unlikely((dhash_entries_tlx * sizeof(struct hlist_bl_head)) < PAGE_SIZE))
				dhash_entries_tlx = PAGE_SIZE / sizeof(struct hlist_bl_head);
	}
	dhash_entries_tlx = roundup_pow_of_two(dhash_entries_tlx);
	log2qty = ilog2(dhash_entries_tlx);

	do {
		size =  dhash_entries_tlx << log2qty;
		table =
						memblock_tlx_virt_alloc_internal_tlx(size, 0, BOOTMEM_LOW_LIMIT,
																												BOOTMEM_ALLOC_ACCESSIBLE,
																												NUMA_NO_NODE);
	} while (!table && size > PAGE_SIZE && --log2qty);

	dentry_hashtable_tlx = table;
	struct zone *z;
	for_each_online_pgdat(pgdat)
		for (z = pgdat->node_zones; z < pgdat->node_zones + MAX_NR_ZONES; z++)
			z->managed_pages = 0;

	#define for_each_mem_range(i, type_a, type_b, nid,			\
			   p_start, p_end, p_nid)			\
		for (i = 0, __next_mem_range_tlx(&i, nid, type_a, type_b,		\
				     p_start, p_end, p_nid);		\
	     i != (u64)ULLONG_MAX;					\
	     __next_mem_range_tlx(&i, nid, type_a, type_b,			\
			      p_start, p_end, p_nid))

	#define for_each_free_mem_range(i, nid, p_start, p_end, p_nid)		\
		for_each_mem_range(i, &memblock_tlx.memory, &memblock_tlx.reserved,	\
			   nid, p_start, p_end, p_nid)

	unsigned long count = 0;
	phys_addr_t start, end;
	for_each_free_mem_range(i, NUMA_NO_NODE, &start, &end, NULL) {
		unsigned long start_pfn = PFN_UP(start);
		unsigned long end_pfn = min_t(unsigned long,
							PFN_DOWN(end), max_low_pfn_tlx);
		if (start_pfn > end_pfn) continue;
		__free_pages_memory_tlx(start_pfn, end_pfn);
	count += end_pfn - start_pfn;
	}
	totalram_pages_tlx += count;
	static __initdata struct kmem_cache boot_kmem_cache,
	boot_kmem_cache_node_tlx;
 	 kmem_cache_node_tlx = &boot_kmem_cache_node_tlx;
	 kmem_cache_tlx = &boot_kmem_cache;
	 kmem_cache_node_tlx->name = "kmem_cache_node_tlx";
   kmem_cache_node_tlx->size = kmem_cache_node_tlx->object_size = sizeof(struct kmem_cache_node_tlx);
   kmem_cache_node_tlx->align = calculate_alignment_tlx(SLAB_HWCACHE_ALIGN, ARCH_KMALLOC_MINALIGN, sizeof(struct kmem_cache_node_tlx));
	__kmem_cache_create_tlx(kmem_cache_node_tlx, SLAB_HWCACHE_ALIGN);
	kmem_cache_tlx->name = "kmem_cache";
	kmem_cache_tlx->size = kmem_cache_tlx->object_size = offsetof(struct kmem_cache, node) + \
					nr_node_ids * sizeof(struct kmem_cache_node_tlx *);
	kmem_cache_tlx->align = calculate_alignment_tlx(SLAB_HWCACHE_ALIGN, ARCH_KMALLOC_MINALIGN, kmem_cache_tlx->size);
	__kmem_cache_create_tlx(kmem_cache_tlx, SLAB_HWCACHE_ALIGN);

	for (i = 8; i < KMALLOC_MIN_SIZE; i += 8) {
		int elem = (i - 1) / 8;

		if (elem >= ARRAY_SIZE(size_index_tlx))
			break;
		size_index_tlx[elem] = KMALLOC_SHIFT_LOW;
	}
	if (KMALLOC_MIN_SIZE >= 64) {
		for (i = 64 + 8; i <= 96; i += 8)
			size_index_tlx[(i - 1) / 8] = 7;
	}
	for (i = KMALLOC_SHIFT_LOW; i <= KMALLOC_SHIFT_HIGH; i++) {
		if (!kmalloc_caches_tlx[i]) {
				struct kmem_cache *s =
				slab_alloc_tlx(kmem_cache_tlx, GFP_NOWAIT | __GFP_ZERO, _RET_IP_);
				s->name = NULL;
				s->size = s->object_size = 1 << i;
				s->align = calculate_alignment_tlx(0, ARCH_KMALLOC_MINALIGN, 1 << i);
				__kmem_cache_create_tlx(s, 0);
				list_add(&s->list, &slab_caches_tlx);
				s->refcount = 1;
				kmalloc_caches_tlx[i] = s;
		}
		if (KMALLOC_MIN_SIZE <= 32 && !kmalloc_caches_tlx[1] && i == 6)
			kmalloc_caches_tlx[1] = create_kmalloc_cache_tlx(NULL, 96, 0);

		if (KMALLOC_MIN_SIZE <= 64 && !kmalloc_caches_tlx[2] && i == 7)
			kmalloc_caches_tlx[2] = create_kmalloc_cache_tlx(NULL, 192, 0);
	}
	slab_state_tlx = UP;
	struct pcpu_chunk *target_chunks[] =
		{ pcpu_first_chunk_tlx, pcpu_reserved_chunk_tlx, NULL };
	struct pcpu_chunk *chunk;
	unsigned long flags;
	for (i = 0; (chunk = target_chunks[i]); i++) {
		int *map;
		const size_t size = PERCPU_DYNAMIC_EARLY_SLOTS * sizeof(map[0]);
//		if (size <= PAGE_SIZE)
                 map = kzalloc_tlx(size, GFP_KERNEL);


		memcpy_tlx(map, chunk->map, size);
		chunk->map = map;

	}
		sched_init_tlx_tlx_tlx();
 //		sched_init();
		sched_init_tlx();

	idr_layer_cache = kmem_cache_create_tlx("idr_layer_cache",
				sizeof(struct idr_layer_tlx), 0, SLAB_PANIC, NULL);
	rcu_init_one_tlx(&rcu_tlx_bh_state, &rcu_bh_data_tlx);
	rcu_init_one_tlx(&rcu_tlx_sched_state, &rcu_sched_data_tlx);
	softirq_vec_tlx[RCU_SOFTIRQ].action = rcu_process_callbacks_tlx;
	radix_tree_node_cachep_tlx = kmem_cache_create_tlx("radix_tree_node",
		576, 0,
		SLAB_PANIC | SLAB_RECLAIM_ACCOUNT,
		NULL);

	for (i = 0; i < ARRAY_SIZE(height_to_maxindex_tlx_tlx); i++){
 		  unsigned int height = i;
			unsigned int width = height * RADIX_TREE_MAP_SHIFT;
			int shift = RADIX_TREE_INDEX_BITS - width;
			if (shift < 0) {
				height_to_maxindex_tlx_tlx[i] =  ~0UL;
				continue;
			}
			if (shift >= BITS_PER_LONG) {
				height_to_maxindex_tlx_tlx[i] =  0UL;
				continue;
			}
			height_to_maxindex_tlx_tlx[i] =  ~0UL >> shift;
}

 	of_irq_init_tlx(__irqchip_of_table);

	int j0;
	struct tvec_base *base1;
	static char tvec_base_done[NR_CPUS];

		base1 = &boot_tvec_bases_tlx;
		spin_lock_init(&base1->lock);
		tvec_base_done[cpu] = 1;

	for (j0 = 0; j0 < TVN_SIZE; j0++) {
		INIT_LIST_HEAD(base1->tv5.vec + j0);
		INIT_LIST_HEAD(base1->tv4.vec + j0);
		INIT_LIST_HEAD(base1->tv3.vec + j0);
		INIT_LIST_HEAD(base1->tv2.vec + j0);
	}
	for (j0 = 0; j0 < TVR_SIZE; j0++)
		INIT_LIST_HEAD(base1->tv1.vec + j0);

	base1->timer_jiffies_tlx = jiffies_tlx;
	base1->next_timer = base1->timer_jiffies_tlx;
	base1->active_timers = 0;
	base1->all_timers = 0;

	softirq_vec_tlx[TIMER_SOFTIRQ].action = run_timer_softirq_tlx;
	struct hrtimer_cpu_base_tlx  *cpu_base = &per_cpu(hrtimer_bases_tlx, 0);

	for (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {
		cpu_base->clock_base[i].cpu_base = cpu_base;
			(&cpu_base->clock_base[i].active)->head = RB_ROOT;
			(&cpu_base->clock_base[i].active)->next = NULL;
	}
	struct timekeeper *tk = &timekeeper_tlx;
	struct clocksource_tlx *clock;
	struct timespec now;
  (&now)->tv_sec = 0;
  (&now)->tv_nsec = 0;
	time_adjust_tlx	= 0;		/* stop active adjtime() */
	time_status_tlx	|= STA_UNSYNC;
	time_maxerror_tlx	= NTP_PHASE_LIMIT;
	time_esterror_tlx	= NTP_PHASE_LIMIT;
	u64 second_length;
	u64 new_base;
	second_length		 = (u64)(tick_usec_tlx * NSEC_PER_USEC * USER_HZ)
						<< NTP_SCALE_SHIFT;
	second_length		+= ntp_tick_adj_tlx;
	second_length		+= time_freq_tlx;
	tick_nsec_tlx		 = div_u64_tlx(second_length, HZ) >> NTP_SCALE_SHIFT;
	new_base		 = div_u64_tlx(second_length, NTP_INTERVAL_FREQ);
	tick_length_tlx		+= new_base - tick_length_tlx_base;
	tick_length_tlx_base	 = new_base;
	tick_length_tlx	= tick_length_tlx_base;
	time_offset_tlx	= 0;

	clock = &clocksource_jiffies_tlx;
	cycle_t interval;
	u64 tmp, ntpinterval;
	tk->clock = clock;
	tk->cycle_last = clock->cycle_last = jiffies_tlx;
	tmp = NTP_INTERVAL_LENGTH;
	tmp <<= clock->shift;
	ntpinterval = tmp;
	tmp += clock->mult/2;
	do_div(tmp, clock->mult);
	if (tmp == 0)
			tmp = 1;
	interval = (cycle_t) tmp;
	tk->cycle_interval = interval;
	tk->xtime_interval = (u64) interval * clock->mult;
	tk->xtime_remainder = ntpinterval - tk->xtime_interval;
	tk->raw_interval =
			((u64) interval * clock->mult) >> clock->shift;
	tk->shift = clock->shift;
	tk->ntp_error = 0;
	tk->ntp_error_shift = NTP_SCALE_SHIFT - clock->shift;
	tk->mult = clock->mult;
	memcpy_tlx(&shadow_timekeeper_tlx, &timekeeper_tlx, sizeof(timekeeper_tlx));
  struct of_device_id_tlx *match;
	struct device_node *np;
	struct clock_provider *clk_provider, *next;
	bool is_init_done;
	bool force = false;
	struct of_device_id_tlx 	*matches = &__clk_of_table;
	for_each_matching_node_and_match(np, matches, &match) {
		struct clock_provider *parent =
			kzalloc_tlx(sizeof(struct clock_provider),	GFP_KERNEL);

		parent->clk_init_cb = match->data;
		parent->np = np;
		list_add_tail(&parent->node, &clk_provider_list_tlx);
	}

	while (!list_empty(&clk_provider_list_tlx)) {
		is_init_done = false;
		list_for_each_entry_safe(clk_provider, next,
					&clk_provider_list_tlx, node) {
			if (force || parent_ready_tlx(clk_provider->np)) {
				clk_provider->clk_init_cb(clk_provider->np);
				list_del(&clk_provider->node);
				kfree_tlx(clk_provider);
				is_init_done = true;
			}
		}
		if (!is_init_done)
			force = true;

	}
	of_init_fn_1 init_func;
	unsigned clocksources = 0;
	for_each_matching_node_and_match(np, __clksrc_of_table_tlx, &match) {
		init_func = match->data;
		init_func(np);
		clocksources++;
	}

	INIT_LIST_HEAD(&per_cpu(rotation_list_tlx, 0));
	early_boot_irqs_disabled = false;
	local_irq_enable();
	tty_ldiscs_tlx[N_TTY] = &tty_ldisc_N_TTY_tlx;
	(&tty_ldisc_N_TTY_tlx)->num = N_TTY;
	(&tty_ldisc_N_TTY_tlx)->refcount = 0;
	sched_clock_running_tlx = 1;
	init_pid_ns_tlx.pid_cachep = kmem_cache_create_tlx("pid", 80,
		0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);
	anon_vma_cachep_tlx = kmem_cache_create_tlx("anon_vma", sizeof(struct anon_vma),
		0, SLAB_DESTROY_BY_RCU|SLAB_PANIC, anon_vma_ctor_tlx);
	anon_vma_chain_cachep_tlx	= kmem_cache_create_tlx("anon_vma_chain", sizeof(struct anon_vma_chain),
		0, SLAB_PANIC, NULL);
	cred_jar_tlx_tlx = kmem_cache_create_tlx("cred_jar_tlx_tlx", sizeof(struct cred),
					0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
	task_struct_cachep_tlx =
	kmem_cache_create_tlx("task_struct", 2656 ,
		L1_CACHE_BYTES, SLAB_PANIC | SLAB_NOTRACK, NULL);

	max_threads_tlx = totalram_pages_tlx / (8 * THREAD_SIZE / PAGE_SIZE);

	sighand_cachep_tlx = kmem_cache_create_tlx("sighand_cache",
		2088, 0,
		SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU|
		SLAB_NOTRACK, sighand_ctor_tlx);
	signal_cachep_tlx = kmem_cache_create_tlx("signal_cache",
		936, 0,
		SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
	files_cachep_tlx = kmem_cache_create_tlx("files_cache",
		640, 0,
		SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
	fs_cachep_tlx = kmem_cache_create_tlx("fs_cache",
		56, 0,
		SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
	mm_cachep_tlx = kmem_cache_create_tlx("mm_struct",
			760, 0,
			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
	vm_area_cachep_tlx = kmem_cache_create_tlx("vm_area_struct",
			176, 0, SLAB_PANIC, NULL);

  (&vm_committed_as_tlx)->count = 0;
  (&vm_committed_as_tlx)->counters = alloc_percpu(s32);
	unsigned long reserve;
	reserve = min((totalram_pages_tlx - nr_free_pages()) * 3/2, totalram_pages_tlx - 1);
	totalram_pages_tlx -= reserve;

	names_cachep_tlx = kmem_cache_create_tlx("names_cache", 4096, 0,
		SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
	dentry_cache_tlx = kmem_cache_create_tlx("dentry", 192, 0,
		SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|SLAB_MEM_SPREAD, NULL);

	inode_cachep_tlx = kmem_cache_create_tlx("inode_cache",
			552, 0, (SLAB_RECLAIM_ACCOUNT|SLAB_PANIC| SLAB_MEM_SPREAD), init_once_tlx_tlx);
	inode_hashtable_tlx =
		alloc_large_system_hash_tlx("Inode-cache",
			8, ihash_entries_tlx, 14, 0, &i_hash_shift_tlx, &i_hash_mask_tlx, 0, 0);
	filp_cachep_tlx = kmem_cache_create_tlx("filp", 248, 0,
		SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);
	(&nr_files_tlx)->count = 0;
	(&nr_files_tlx)->counters = alloc_percpu(s32);
	unsigned u;
	int err;
	mnt_cache_tlx = kmem_cache_create_tlx("mnt_cache_tlx", 288,
			0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);

	mount_hashtable_tlx = alloc_large_system_hash_tlx("Mount-cache",
				sizeof(struct hlist_head),
				mhash_entries_tlx, 19,
				0,
				&m_hash_shift_tlx, &m_hash_mask_tlx, 0, 0);
	mountpoint_hashtable_tlx = alloc_large_system_hash_tlx("Mountpoint-cache",
				sizeof(struct hlist_head),
				mphash_entries_tlx, 19,
				0,
				&mp_hash_shift_tlx, &mp_hash_mask_tlx, 0, 0);
	for (u = 0; u <= m_hash_mask_tlx; u++)
		INIT_HLIST_HEAD(&mount_hashtable_tlx[u]);
	for (u = 0; u <= mp_hash_mask_tlx; u++)
		INIT_HLIST_HEAD(&mountpoint_hashtable_tlx[u]);
	kernfs_node_cache_tlx = kmem_cache_create_tlx("kernfs_node_cache_tlx",
							120,
							0, SLAB_PANIC, NULL);
	struct kernfs_node_tlx *kn;
	sysfs_root_tlx = kzalloc_tlx(sizeof(*sysfs_root_tlx), GFP_KERNEL);
	INIT_LIST_HEAD(&sysfs_root_tlx->supers);
	char *dup_name = NULL;
	kn = kmem_cache_zalloc_tlx(kernfs_node_cache_tlx, GFP_KERNEL);
	atomic_set(&kn->count, 1);
	atomic_set(&kn->active, KN_DEACTIVATED_BIAS);
	RB_CLEAR_NODE(&kn->rb);
	kn->name = "";
	kn->mode = S_IFDIR | S_IRUGO | S_IXUGO;
	kn->flags = KERNFS_DIR;
	kn->priv = NULL;
	kn->dir.root = sysfs_root_tlx;
	sysfs_root_tlx->syscall_ops = NULL;
	sysfs_root_tlx->flags = KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK;
	sysfs_root_tlx->kn = kn;
	sysfs_root_kn_tlx = sysfs_root_tlx->kn;
	fs_kobj_tlx_tlx = kobject_create_and_add_tlx("fs", NULL);
  struct file_system_type ** pt;
  for (pt=&file_systems_tlx; *pt; pt=&(*pt)->next)
                  if (strlen_tlx((*pt)->name) == strlen_tlx((&rootfs_fs_type_tlx)->name) &&
                      strncmp_tlx((*pt)->name, (&rootfs_fs_type_tlx)->name, strlen_tlx((&rootfs_fs_type_tlx)->name)) == 0)
                          break;
	*pt = &rootfs_fs_type_tlx;
	shmem_inode_cachep_tlx = kmem_cache_create_tlx("shmem_inode_cache",
			624,
			0, SLAB_PANIC, shmem_init_inode_tlx);

	is_tmpfs_tlx = true;
	struct vfsmount *mnt;
	struct mnt_namespace *ns;
	struct path root;
	struct file_system_type *type;
	struct file_system_type **pf;
	for (pf=&file_systems_tlx; *pf; pf=&(*pf)->next)
					if (strlen_tlx((*pf)->name) == strlen_tlx("rootfs") &&
							strncmp_tlx((*pf)->name, "rootfs", strlen_tlx("rootfs")) == 0)
									break;

 	type = *(pf);
	mnt = vfs_kern_mount_rd(type, 0, "rootfs", NULL);
	root.mnt = mnt;
	root.dentry = mnt->mnt_root;
	current->fs->pwd = root;
	current->fs->root = root;
	struct kobj_map *p = kmalloc_tlx(sizeof(struct kobj_map), GFP_KERNEL);
	struct probe *base = kzalloc_tlx(sizeof(*base), GFP_KERNEL);
	base->dev = 1;
	base->range = ~0;
	base->get = base_probe_tlx;
	for (i = 0; i < 255; i++)
		p->probes[i] = base;
	p->lock = &chrdevs_lock_tlx;
	cdev_map_tlx = p;
	proc_inode_cachep_tlx_tlx = kmem_cache_create_tlx("proc_inode_cache",
						616,
						0, (SLAB_RECLAIM_ACCOUNT|
					SLAB_MEM_SPREAD|SLAB_PANIC),
						NULL);

	int pid;
  do_fork_tlx(CLONE_FS|CLONE_VM|CLONE_UNTRACED, (unsigned long)kernel_init, NULL, NULL, NULL);
	pid = do_fork_tlx(CLONE_FS|CLONE_FILES|CLONE_VM|CLONE_UNTRACED, (unsigned long)kthreadd, NULL, NULL, NULL);

	struct pid *tmp_pid;
  struct upid *pnr;
  hlist_for_each_entry_rcu(pnr,
                       &pid_hash_tlx[pid_hash_tlxfn(pid, &init_pid_ns_tlx)], pid_chain)
  if (pnr->nr == pid && pnr->ns == &init_pid_ns_tlx) {
       tmp_pid = container_of(pnr, struct pid, numbers[(&init_pid_ns_tlx)->level]);
			 break;
  }
  struct hlist_node *first;
  first = rcu_dereference_check(hlist_first_rcu(&tmp_pid->tasks[PIDTYPE_PID]),
  								lockdep_tasklist_lock_tlx_is_held_tlx());
 	kthreadd_task_tlx = hlist_entry(first, struct task_struct, pids[(PIDTYPE_PID)].node);

	(&kthreadd_done)->done++;
	wait_queue_head_t *q = &(&kthreadd_done)->wait;
	unsigned int mode = TASK_NORMAL;
	int nr_exclusive = 1;
	int wake_flags = 0;
	void *key = NULL;
	wait_queue_t *curr, *next_tsk;
	list_for_each_entry_safe(curr, next_tsk, &q->task_list, task_list) {
			unsigned flags = curr->flags;
			if (curr->func(curr, mode, wake_flags, key) &&
					(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
				break;
	}
	current->sched_class = &idle_sched_class_tlx;
	__schedule_tlx();
	cpu_idle_loop_tlx();
}


static void __init do_ctors(void)
{
#ifdef CONFIG_CONSTRUCTORS
	ctor_fn_t *fn = (ctor_fn_t *) __ctors_start;

	for (; fn < (ctor_fn_t *) __ctors_end; fn++)
		(*fn)();
#endif
}

bool initcall_debug;
//core_param(initcall_debug, initcall_debug, bool, 0644);

//static __initdata_or_module LIST_HEAD(blacklisted_initcalls);
int __init_or_module do_one_initcall(initcall_t fn)
{

}

static noinline void __init kernel_init_freeable(void);


struct device platform_bus = {
	.init_name	= "platform",
};

struct bus_type platform_bus_type = {
	.name		= "platform",
	.dev_groups	= NULL,
	.match		= NULL,
	.uevent		= NULL,
	.pm		= NULL,
};

static struct dentry *sockfs_mount_tlx(struct file_system_type *fs_type,
                        int flags, const char *dev_name, void *data)
{
         return mount_pseudo_tlx(fs_type, "socket:", &sockfs_ops_tlx,
                 &sockfs_dentry_operations_tlx, SOCKFS_MAGIC);
}

static struct file_system_type sock_fs_type = {
  .name =		"sockfs",
  .mount =	sockfs_mount_tlx,
  .kill_sb =	NULL,
};


struct kmem_cache *sock_inode_cachep_tlx ;




static void init_once(void *foo)
{
	  struct socket_alloc *ei = (struct socket_alloc *)foo;
 	  struct inode *inode = &ei->vfs_inode;
         memset_tlx(inode, 0, sizeof(*inode));
         INIT_HLIST_NODE(&inode->i_hash);
         INIT_LIST_HEAD(&inode->i_devices);
         INIT_LIST_HEAD(&inode->i_wb_list);
         INIT_LIST_HEAD(&inode->i_lru);
         address_space_init_once_tlx(&inode->i_data);
         i_size_ordered_init(inode);
#ifdef CONFIG_FSNOTIFY
         INIT_HLIST_HEAD(&inode->i_fsnotify_marks);
#endif
}




#define for_each_cpu_worker_pool(pool, cpu)				\
	for ((pool) = &per_cpu(cpu_worker_pools_tlx, cpu)[0];		\
			(pool) < &per_cpu(cpu_worker_pools_tlx, cpu)[NR_STD_WORKER_POOLS]; \
			(pool)++)

struct kmem_cache *pwq_cache_tlx;


#define PER_CPU_SHARED_ALIGNED_SECTION ""
DEFINE_PER_CPU_SECTION(struct worker_pool [NR_STD_WORKER_POOLS], cpu_worker_pools_tlx, PER_CPU_SHARED_ALIGNED_SECTION) \
				____cacheline_aligned_in_smp;

typedef int (*decompress_fn) (unsigned char *inbuf, int len,
						int(*fill)(void*, unsigned int),
						int(*flush)(void*, unsigned int),
						unsigned char *outbuf,
						int *posp,
						void(*error)(char *x));

extern char __initramfs_start[];
extern char __irf_start_tlx[];
extern unsigned long __initramfs_size_tlx;
char *message_tlx;
char *header_buf_tlx, *symlink_buf_tlx, *name_buf_tlx;
#define N_ALIGN(len) ((((len) + 1) & ~3) + 2)

static enum state {
	Start,
	Collect,
	GotHeader,
	SkipIt,
	GotName,
	CopyFile,
	GotSymlink,
	Reset
} state, next_state;

__initdata loff_t this_header_tlx, next_header_tlx;
unsigned my_inpt_tlxr;   /* index of next byte to be processed in inbuf */
void __init error_tlx(char *x)
{
}


struct kobj_type ktype_cdev_default_tlx;
int exact_lock_tlx(dev_t dev, void *data)
{
         struct cdev *p = data;
  			 struct kobject *kobj;
				 kobj = kobject_get_tlx(&p->kobj);
				 return kobj ? 0 : -1;
}
struct kobject *exact_match_tlx(dev_t dev, int *part, void *data)
{
       struct cdev *p = data;
       return &p->kobj;
}


struct device *consdev_tlx;



#define TTY_IO_ERROR            1       /* Cause an I/O error (may be no ldisc too) */

struct tty_ldisc {
         struct tty_ldisc_ops *ops;
         struct tty_struct *tty;
};

struct tty_file_private {
	struct tty_struct *tty;
	struct file *file;
	struct list_head list;
};




struct ldsem_waiter {
         struct list_head list;
         struct task_struct *task;
};

void __ldsem_wake_tlx(struct ld_semaphore *sem)
{
	if (!list_empty(&sem->write_wait)) {
		struct ldsem_waiter *waiter;
		waiter = list_entry(sem->write_wait.next, struct ldsem_waiter, list);
		wake_up_process_tlx(waiter->task);
	}
//		__ldsem_wake_writer(sem);
//	else if (!list_empty(&sem->read_wait))
//		__ldsem_wake_readers(sem);
}

#define LDSEM_ACTIVE_BIAS       1L
#define LDSEM_READ_BIAS         LDSEM_ACTIVE_BIAS
# define LDSEM_ACTIVE_MASK      0xffffffffL

static inline long atomic64_add_return_tlx(long i, atomic64_t *v)
 {
         long result;
         unsigned long tmp;

         asm volatile("// atomic64_add_return\n"
 "1:     ldxr    %0, %2\n"
 "       add     %0, %0, %3\n"
 "       stlxr   %w1, %0, %2\n"
 "       cbnz    %w1, 1b"
         : "=&r" (result), "=&r" (tmp), "+Q" (v->counter)
         : "Ir" (i)
         : "memory");

         smp_mb();
         return result;
 }

struct ld_semaphore  *
down_read_failed_tlx(struct ld_semaphore *sem, long count, long timeout);


int __ldsem_down_read_nested_tlx(struct ld_semaphore *sem,
						int subclass, long timeout)
{
	long count;

//	lockdep_acquire_read(sem, subclass, 0, _RET_IP_);

//	count = ldsem_atomic_update(LDSEM_READ_BIAS, sem);
//	count = atomic_long_add_return(LDSEM_READ_BIAS, (atomic_long_t *)&sem->count);
	atomic64_t *v = (atomic64_t *)(atomic_long_t *)&sem->count;
	count =  (long)atomic64_add_return_tlx(LDSEM_READ_BIAS, v);
	if (count <= 0) {
//		lock_stat(sem, contended);
		if (!down_read_failed_tlx(sem, count, timeout)) {
//			lockdep_release(sem, 1, _RET_IP_);
			return 0;
		}
	}
//	lock_stat(sem, acquired);
	return 1;
}

void ldsem_up_read_tlx(struct ld_semaphore *sem)
{
         long count;

//         lockdep_release(sem, 1, _RET_IP_);

//         count = ldsem_atomic_update(-LDSEM_READ_BIAS, sem);
					atomic64_t *v = (atomic64_t *)(atomic_long_t *)&sem->count;
       	count =  (long)atomic64_add_return_tlx(-LDSEM_READ_BIAS, v);
//				 count = atomic_long_add_return(-LDSEM_READ_BIAS, ;
         if (count < 0 && (count & LDSEM_ACTIVE_MASK) == 0)
							 __ldsem_wake_tlx(sem);
//               ldsem_wake(sem);
 }




#define atomic_long_cmpxchg(l, old, new) \
         (atomic_cmpxchg_tlx((atomic_t *)(l), (old), (new)))

static inline int ldsem_cmpxchg_tlx(long *old, long new, struct ld_semaphore *sem)
 {
         long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
         if (tmp == *old) {
                 *old = new;
                 return 1;
         } else {
                 *old = tmp;
                 return 0;
         }
 }


#define LDSEM_WAIT_BIAS         (-LDSEM_ACTIVE_MASK-1)

struct ld_semaphore  *
down_read_failed_tlx(struct ld_semaphore *sem, long count, long timeout)
{
	struct ldsem_waiter waiter;
	struct task_struct *tsk = current;
	long adjust = -LDSEM_ACTIVE_BIAS + LDSEM_WAIT_BIAS;

	/* set up my own style of waitqueue */
	raw_spin_lock_irq(&sem->wait_lock);

	/* Try to reverse the lock attempt but if the count has changed
	* so that reversing fails, check if there are are no waiters,
	* and early-out if not */
	do {
		if (ldsem_cmpxchg_tlx(&count, count + adjust, sem))
			break;
		if (count > 0) {
			raw_spin_unlock_irq(&sem->wait_lock);
			return sem;
		}
	} while (1);

	list_add_tail(&waiter.list, &sem->read_wait);
	sem->wait_readers++;

	waiter.task = tsk;
	get_task_struct(tsk);

	/* if there are no active locks, wake the new lock owner(s) */
	if ((count & LDSEM_ACTIVE_MASK) == 0)
		__ldsem_wake_tlx(sem);

	raw_spin_unlock_irq(&sem->wait_lock);

	/* wait to be given the lock */
	for (;;) {
//		set_task_state(tsk, TASK_UNINTERRUPTIBLE);
		do { (tsk)->state = (TASK_UNINTERRUPTIBLE); } while (0);
		if (!waiter.task)
			break;
		if (!timeout)
			break;
		timeout = schedule_timeout_tlx(timeout);
	}

//	__set_task_state(tsk, TASK_RUNNING);
	do { (tsk)->state = (TASK_RUNNING); } while (0);
	if (!timeout) {
		/* lock timed out but check if this task was just
		* granted lock ownership - if so, pretend there
		* was no timeout; otherwise, cleanup lock wait */
		raw_spin_lock_irq(&sem->wait_lock);
		if (waiter.task) {
	//		ldsem_atomic_update(-LDSEM_WAIT_BIAS, sem);
			atomic64_t *v = (atomic64_t *)(atomic_long_t *)&sem->count;
			count =  (long)atomic64_add_return_tlx(-LDSEM_WAIT_BIAS, v);
			list_del(&waiter.list);
			raw_spin_unlock_irq(&sem->wait_lock);
//			put_task_struct(waiter.task);
			return NULL;
		}
		raw_spin_unlock_irq(&sem->wait_lock);
	}

	return sem;
}

int ldsem_down_read_trylock_tlx(struct ld_semaphore *sem)
{
         long count = sem->count;

         while (count >= 0) {
                 if (ldsem_cmpxchg_tlx(&count, count + LDSEM_READ_BIAS, sem)) {
//                         lockdep_acquire_read(sem, 0, 1, _RET_IP_);
                         return 1;
                 }
         }
         return 0;
 }

#define TTY_NO_WRITE_SPLIT      17      /* Preserve write boundaries to driver */

ssize_t do_tty_write_tlx(
	ssize_t (*write)(struct tty_struct *, struct file *, const unsigned char *, size_t),
	struct tty_struct *tty,
	struct file *file,
	const char __user *buf,
	size_t count)
{
	ssize_t ret, written = 0;
	unsigned int chunk;

	ret = 0;
//	tty_write_lock(tty, file->f_flags & O_NDELAY);
	chunk = 2048;
	if (test_bit_tlx(TTY_NO_WRITE_SPLIT, &tty->flags))
		chunk = 65536;
	if (count < chunk)
		chunk = count;

	/* write_buf/write_cnt is protected by the atomic_write_lock mutex */
	if (tty->write_cnt < chunk) {
		unsigned char *buf_chunk;

		if (chunk < 1024)
			chunk = 1024;

		buf_chunk = kmalloc_tlx(chunk, GFP_KERNEL);
		if (!buf_chunk) {
			ret = -ENOMEM;
			goto out;
		}
		kfree_tlx(tty->write_buf);
		tty->write_cnt = chunk;
		tty->write_buf = buf_chunk;
	}

	/* Do the write .. */
	for (;;) {
		size_t size = count;
		if (size > chunk)
			size = chunk;
		ret = -EFAULT;
		if (copy_from_user(tty->write_buf, buf, size))
			break;
		ret = write(tty, file, tty->write_buf, size);
		if (ret <= 0)
			break;
		written += ret;
		buf += ret;
		count -= ret;
		if (!count)
			break;
		ret = -ERESTARTSYS;
		if (signal_pending_tlx(current))
			break;
		cond_resched();
	}
	if (written) {
		ret = written;
	}
out:
//	tty_write_unlock(tty);
	return ret;
}

#define TIOCPKT_STOP             4
#define TIOCPKT_START            8

void start_tty_tlx(struct tty_struct *tty)
{
	unsigned long flags;
//	spin_lock_irqsave(&tty->ctrl_lock, flags);
	tty->stopped = 0;
	if (tty->link && tty->link->packet) {
		tty->ctrl_status &= ~TIOCPKT_STOP;
		tty->ctrl_status |= TIOCPKT_START;
//		wake_up_interruptible_poll(&tty->link->read_wait, POLLIN);
		wait_queue_head_t *q = &tty->read_wait;
		unsigned int mode = TASK_INTERRUPTIBLE;
		int nr_exclusive = 1;
		int wake_flags = 0;
		void *key = (void *) (POLLIN);
		wait_queue_t *curr, *next;

		list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
					unsigned flags = curr->flags;

				if (curr->func(curr, mode, wake_flags, key) &&
													(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
									break;
	}
	}
//	spin_unlock_irqrestore(&tty->ctrl_lock, flags);
	if (tty->ops->start)
		(tty->ops->start)(tty);
	/* If we have a running line discipline it may need kicking */
//	tty_wakeup(tty);
		struct tty_ldisc *ld;

		if (test_bit_tlx(TTY_DO_WRITE_WAKEUP, &tty->flags)) {
//			ld = tty_ldisc_ref(tty);
			struct tty_ldisc *ld = NULL;

         if (ldsem_down_read_trylock_tlx(&tty->ldisc_sem)) {
                 ld = tty->ldisc;
                 if (!ld)
                         ldsem_up_read_tlx(&tty->ldisc_sem);
         }


			if (ld) {
				if (ld->ops->write_wakeup)
					ld->ops->write_wakeup(tty);
				//tty_ldisc_deref(ld);
					ldsem_up_read_tlx(&ld->tty->ldisc_sem);
			}
		}
//		wake_up_interruptible_poll(&tty->write_wait, POLLOUT);
		wait_queue_head_t *q = &tty->write_wait;
		unsigned int mode = TASK_INTERRUPTIBLE;
		int nr_exclusive = 1;
		int wake_flags = 0;
		void *key = (void *) (POLLOUT);
		wait_queue_t *curr, *next;

		list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
					unsigned flags = curr->flags;

				if (curr->func(curr, mode, wake_flags, key) &&
													(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
									break;
		}
}

ssize_t tty_write_tlx(struct file *file, const char __user *buf,
						size_t count, loff_t *ppos)
{
	struct tty_struct *tty = ((struct tty_file_private *)file->private_data)->tty;
	struct tty_ldisc *ld;
	ssize_t ret;
	if (!tty || !tty->ops->write ||
		(test_bit_tlx(TTY_IO_ERROR, &tty->flags)))
			return -EIO;
//	ldsem_down_read(&tty->ldisc_sem, MAX_SCHEDULE_TIMEOUT);
//	might_sleep();
	__ldsem_down_read_nested_tlx(&tty->ldisc_sem, 0, MAX_SCHEDULE_TIMEOUT);
	ld = tty->ldisc;
		ret = do_tty_write_tlx(ld->ops->write, tty, file, buf, count);
//	tty_ldisc_deref(ld);
	ldsem_up_read_tlx(&ld->tty->ldisc_sem);
	return ret;
}
#define TTY_LDISC_OPEN          11      /* Line discipline is open */
#define TTY_MAGIC               0x5401
struct tty_driver {
         int     magic;          /* magic number for this structure */
         struct kref kref;       /* Reference management */
         struct cdev *cdevs;
         struct module   *owner;
         const char      *driver_name;
         const char      *name;
         int     name_base;      /* offset of printed name */
         int     major;          /* major device number */
         int     minor_start;    /* start of minor device number */
         unsigned int    num;    /* number of devices allocated */
         short   type;           /* type of tty driver */
         short   subtype;        /* subtype of tty driver */
         struct ktermios init_termios; /* Initial termios */
         unsigned long   flags;          /* tty driver flags */
         struct proc_dir_entry *proc_entry; /* /proc fs entry */
         struct tty_driver *other; /* only used for the PTY driver */

         /*
          * Pointer to the tty data structures
          */
         struct tty_struct **ttys;
         struct tty_port **ports;
         struct ktermios **termios;
         void *driver_state;

         /*
          * Driver methods
          */

         const struct tty_operations *ops;
         struct list_head tty_drivers_tlx;
};

struct llist_head {
         struct llist_node *first;
};

struct tty_buffer {
         union {
                 struct tty_buffer *next;
                 struct llist_node free;
         };
         int used;
         int size;
         int commit;
         int read;
         int flags;
         /* Data points here */
         unsigned long data[0];
 };

struct tty_bufhead {
         struct tty_buffer *head;        /* Queue head */
         struct work_struct work;
         struct mutex       lock;
          atomic_t           priority;
        struct tty_buffer sentinel;
         struct llist_head free;         /* Free queue head */
         atomic_t           mem_used;    /* In-use buffers excluding free list */
         int                mem_limit;
         struct tty_buffer *tail;        /* Active buffer */
 };

struct tty_port {
         struct tty_bufhead      buf;            /* Locked internally */
         struct tty_struct       *tty;           /* Back pointer */
         struct tty_struct       *itty;          /* internal back ptr */
         const struct tty_port_operations *ops;  /* Port operations */
         spinlock_t              lock;           /* Lock protecting tty field */
         int                     blocked_open;   /* Waiting to open */
         int                     count;          /* Usage count */
         wait_queue_head_t       open_wait;      /* Open waiters */
         wait_queue_head_t       close_wait;     /* Close waiters */
         wait_queue_head_t       delta_msr_wait; /* Modem status change */
       unsigned long           flags;          /* TTY flags ASY_*/
       unsigned char           console:1,      /* port is a console */
                                 low_latency:1;  /* optional: tune for latency */
         struct mutex            mutex;          /* Locking */
         struct mutex            buf_mutex;      /* Buffer alloc lock */
         unsigned char           *xmit_buf;      /* Optional buffer */
         unsigned int            close_delay;    /* Close port delay */
         unsigned int            closing_wait;   /* Delay for output */
         int                     drain_delay;    /* Set to zero if no pure time
                                                    based drain is needed else
                                                    set to size of fifo */
         struct kref             kref;           /* Ref counter */
 };

struct tty_struct *tty_init_dev_tlx(struct tty_driver *driver, int idx)
{
	struct tty_struct *tty;
	int retval;
	tty =  kzalloc_tlx(sizeof(struct tty_struct), GFP_KERNEL);
//	initialize_tty_struct(tty, driver, idx);
	memset_tlx(tty, 0, sizeof(struct tty_struct));
	kref_init_tlx(&tty->kref);
	tty->magic = TTY_MAGIC;
//	tty_ldisc_init(tty);
//	struct tty_ldisc *ld_ = tty_ldisc_get(tty, N_TTY);
	struct tty_ldisc *ld_;
	struct tty_ldisc_ops *ldops;
	ldops = tty_ldiscs_tlx[N_TTY];
	ld_ = kmalloc_tlx(sizeof(struct tty_ldisc), GFP_KERNEL);
	ld_->ops = ldops;
	ld_->tty = tty;
	tty->ldisc = ld_;
	tty->session = NULL;
	tty->pgrp = NULL;
	mutex_init(&tty->legacy_mutex);
	mutex_init(&tty->throttle_mutex);
	init_rwsem(&tty->termios_rwsem);
	mutex_init(&tty->winsize_mutex);
//	init_ldsem(&tty->ldisc_sem);
	init_waitqueue_head(&tty->write_wait);
	init_waitqueue_head(&tty->read_wait);
//	INIT_WORK(&tty->hangup_work, do_tty_hangup);
	mutex_init(&tty->atomic_write_lock);
	spin_lock_init(&tty->ctrl_lock);
	INIT_LIST_HEAD(&tty->tty_files);
//	INIT_WORK(&tty->SAK_work, do_SAK_work);

	tty->driver = driver;
	tty->ops = driver->ops;
	tty->index = idx;

//	tty_lock(tty);
//	retval = tty_driver_install_tty(driver, tty);
//	tty_standard_install(driver, tty);
//	tty_init_termios(tty);
	struct ktermios *tp;
	int idx_ = tty->index;
	tp = tty->driver->termios[idx_];
	if (tp != NULL)
			tty->termios = *tp;
	else
		tty->termios = tty->driver->init_termios;

	driver->ttys[tty->index] = tty;
	if (!tty->port)
		tty->port = driver->ports[idx];
	tty->port->itty = tty;
//	retval = tty_ldisc_setup(tty, tty->link);
	struct tty_ldisc *ld = tty->ldisc;
//	tty_ldisc_open(tty, ld);
	if (ld->ops->open) {
			int ret;
			ret = ld->ops->open(tty);
			if (ret)
						__clear_bit_tlx(TTY_LDISC_OPEN, &tty->flags);
  }
	return tty;
}

struct console *console_drivers_tlx;

#define for_each_console(con) \
       for (con = console_drivers_tlx; con != NULL; con = con->next)

struct console {
         char    name[16];
         void    (*write)(struct console *, const char *, unsigned);
         int     (*read)(struct console *, char *, unsigned);
         struct tty_driver *(*device)(struct console *, int *);
         void    (*unblank)(void);
         int     (*setup)(struct console *, char *);
         int     (*early_setup)(void);
         short   flags;
         short   index;
         int     cflag;
         void    *data;
         struct   console *next;
};

static struct tty_driver *tty_lookup_driver_tlx(dev_t device, struct file *filp,
		int *noctty, int *index)
{
//	struct tty_driver *driver;
	struct tty_driver *driver = NULL;
	switch (device) {
	case MKDEV(TTYAUX_MAJOR, 1): {
//		struct tty_driver *console_driver = console_device(index);
		struct console *c;

		for_each_console(c) {
								if (!c->device)
												continue;
								driver = c->device(c, index);
								if (driver)
												break;
		}
		struct tty_driver *console_driver = driver;



		if (console_driver) {
			driver = console_driver;
			if (driver) {
				/* Don't let /dev/console block */
				filp->f_flags |= O_NONBLOCK;
				*noctty = 1;
				break;
			}
		}
	}
	default:
		break;
	}
	return driver;
}



#define TTY_HUPPED              18      /* Post driver->hangup() */

int tty_open_tlx(struct inode *inode, struct file *filp)
{
	struct tty_struct *tty;
	int noctty, retval;
	struct tty_driver *driver = NULL;
	int index;
	dev_t device = inode->i_rdev;
	unsigned saved_flags = filp->f_flags;
	struct tty_file_private *priv_;
//	nonseekable_open(inode, filp);

retry_open:
//	retval = tty_alloc_file(filp);

	priv_ = kmalloc_tlx(sizeof(*priv_), GFP_KERNEL);
	filp->private_data = priv_;
	noctty = filp->f_flags & O_NOCTTY;
	index  = -1;
	retval = 0;
		driver = tty_lookup_driver_tlx(device, filp, &noctty, &index);
//		tty = tty_driver_lookup_tty(driver, inode, index);
		tty = tty_init_dev_tlx(driver, index);
//	tty_add_file(tty, filp);
	struct tty_file_private *priv = filp->private_data;
	priv->tty = tty;
	priv->file = filp;
	list_add(&priv->list, &tty->tty_files);
	if (tty->ops->open)
		retval = tty->ops->open(tty, filp);
	filp->f_flags = saved_flags;
	__clear_bit_tlx(TTY_HUPPED, &tty->flags);
//	tty_unlock(tty);


	return 0;
}


ssize_t tty_read_tlx(struct file *file, char __user *buf, size_t count,
			loff_t *ppos)
{
	int i;
	struct inode *inode = file->f_inode;
	struct tty_struct *tty = ((struct tty_file_private *)file->private_data)->tty;
	struct tty_ldisc *ld;
//	ldsem_down_read(&tty->ldisc_sem, MAX_SCHEDULE_TIMEOUT);
	__ldsem_down_read_nested_tlx(&tty->ldisc_sem, 0,MAX_SCHEDULE_TIMEOUT);

	ld = tty->ldisc;
	if (ld->ops->read)
		i = (ld->ops->read)(tty, file, buf, count);
	else
		i = -EIO;
//	tty_ldisc_deref(ld);
	ldsem_up_read_tlx(&ld->tty->ldisc_sem);
//	struct timekeeper *tk = &timekeeper;
//	unsigned long sec = tk->xtime_sec & ~7;
//	if (i > 0) {
//		unsigned long sec = get_seconds() & ~7;
//				if ((long)(sec - (&inode->i_atime)->tv_sec) > 0)
//								(&inode->i_atime)->tv_sec = sec;
//	}
//		tty_update_time(&inode->i_atime);

	return i;
}




struct file_operations console_fops_tlx = {
	.llseek		= NULL,
	.read		= tty_read_tlx,
	.write		= tty_write_tlx,
	.poll		= NULL,
	.unlocked_ioctl	= NULL,
	.compat_ioctl	= NULL,
	.open		= tty_open_tlx,
	.release	= NULL,
	.fasync		= NULL,
};

struct cdev  console_cdev_tlx; //tty_cdev,

typedef u32 phandle;

struct clk_ops {
	int		(*prepare)(struct clk_hw *hw);
	void		(*unprepare)(struct clk_hw *hw);
	int		(*is_prepared)(struct clk_hw *hw);
	void		(*unprepare_unused)(struct clk_hw *hw);
	int		(*enable)(struct clk_hw *hw);
	void		(*disable)(struct clk_hw *hw);
	int		(*is_enabled)(struct clk_hw *hw);
	void		(*disable_unused)(struct clk_hw *hw);
	unsigned long	(*recalc_rate)(struct clk_hw *hw,
					unsigned long parent_rate);
	long		(*round_rate)(struct clk_hw *hw, unsigned long rate,
					unsigned long *parent_rate);
	long		(*determine_rate)(struct clk_hw *hw, unsigned long rate,
					unsigned long *best_parent_rate,
					struct clk **best_parent_clk);
	int		(*set_parent)(struct clk_hw *hw, u8 index);
	u8		(*get_parent)(struct clk_hw *hw);
	int		(*set_rate)(struct clk_hw *hw, unsigned long rate,
				    unsigned long parent_rate);
	int		(*set_rate_and_parent)(struct clk_hw *hw,
				    unsigned long rate,
				    unsigned long parent_rate, u8 index);
	unsigned long	(*recalc_accuracy)(struct clk_hw *hw,
					   unsigned long parent_accuracy);
	void		(*init)(struct clk_hw *hw);
	int		(*debug_init)(struct clk_hw *hw, struct dentry *dentry);
};

#define AMBA_NR_IRQS    9

struct clk {
	const char		*name;
	const struct clk_ops	*ops;
	struct clk_hw		*hw;
	struct module		*owner;
	struct clk		*parent;
	const char		**parent_names;
	struct clk		**parents;
	u8			num_parents;
	u8			new_parent_index;
	unsigned long		rate;
	unsigned long		new_rate;
	struct clk		*new_parent;
	struct clk		*new_child;
	unsigned long		flags;
	unsigned int		enable_count;
	unsigned int		prepare_count;
	unsigned long		accuracy;
	struct hlist_head	children;
	struct hlist_node	child_node;
	unsigned int		notifier_count;
#ifdef CONFIG_DEBUG_FS
	struct dentry		*dentry;
#endif
	struct kref		ref;
};


struct amba_id_tlx {
				unsigned int            id;
				unsigned int            mask;
				void                    *data;
};

struct amba_device_tlx {
         struct device           dev;
         struct resource         res;
         struct clk              *pclk;
         unsigned int            periphid;
         unsigned int            irq[AMBA_NR_IRQS];
 };
struct amba_driver_tlx {
          struct device_driver    drv;
         int                     (*probe)(struct amba_device *, const struct amba_id_tlx *);
         int                     (*remove)(struct amba_device *);
         void                    (*shutdown)(struct amba_device *);
         int                     (*suspend)(struct amba_device *, pm_message_t);
         int                     (*resume)(struct amba_device *);
         const struct amba_id_tlx    *id_table;
};


int amba_probe_tlx(struct device *dev)
{
	struct amba_device_tlx *pcdev =  container_of(dev, struct amba_device_tlx, dev);
	struct amba_driver_tlx *pcdrv = container_of(dev->driver, struct amba_driver_tlx, drv);
	const struct amba_id_tlx *table = pcdrv->id_table;
	int ret = 0;
	while (table->mask) {
			ret = (pcdev->periphid & table->mask) == table->id;
			if (ret)
								break;
			table++;
	}
	const struct amba_id_tlx *id =  ret ? table : NULL;
	ret = 0;
	struct clk *clk;
	do {
			struct device *dev__ = &pcdev->dev;
			const char *con_id =  "apb_pclk";
			const char *dev_id = dev__ ? dev_name(dev__) : NULL;
			if (dev__) {
							struct device_node *np = dev__->of_node;
							const char *name = con_id;
								while (np) {
									int index = 0;
									if (name) {
										const char *propname = "clock-names";
										const char *string = name;
											struct property *prop;
											struct property *pp;
											for (pp = np->properties; pp; pp = pp->next) {
																if (strcmp_tlx(pp->name, propname) == 0) {
																				break;
																}
											}
											prop = pp;
											size_t l;
											int i;
											const char *p, *end;
											p = prop->value;
											end = p + prop->length;
											for (i = 0; p < end; i++, p += l) {
												l = strlen_tlx(p) + 1;
												if (strcmp_tlx(string, p) == 0) {
													index = i; /* Found it; return index */
													break;
												}
											}
										}
										struct of_phandle_args clkspec;
										const char *list_name = "clocks";
										const char *cells_name ="#clock-cells";
										int cell_count = 0;
										struct of_phandle_args *out_args = &clkspec;
										const __be32 *list, *list_end;
										int rc = 0, size, cur_index = 0;
										uint32_t count = 0;
										struct device_node *node = NULL;
										phandle phandle;
										struct property *pp;
										for (pp = np->properties; pp; pp = pp->next) {
											if (strcmp_tlx(pp->name, list_name) == 0) {
																			size = pp->length;
																			break;
															}
										}
										list = pp ? pp->value : NULL;
										list_end = list + size / sizeof(*list);
										while (list < list_end) {
													rc = -EINVAL;
													count = 0;
													phandle = be32_to_cpup(list++);
													if (phandle) {
														if (cells_name || cur_index == index) {
															for (node = of_allnodes_tlx; node; node = node->allnext)
																			if (node->phandle == phandle)
																							break;
															if (node)
                									kobject_get_tlx(&node->kobj);
														}
														if (cells_name) {
														} else {
															count = cell_count;
														}
													}
													rc = -ENOENT;
													if (cur_index == index) {
														if (out_args) {
															int i;
															out_args->np = node;
															out_args->args_count = count;
															for (i = 0; i < count; i++)
																out_args->args[i] = be32_to_cpup(list++);
														} else
														break;
													}
													node = NULL;
													list += count;
													cur_index++;
												}
											struct of_clk_provider *provider;
											list_for_each_entry(provider, &of_clk_providers_tlx, link) {
												if (provider->node == clkspec.np)
													clk = provider->get(&clkspec, provider->data);
											}
									if (!IS_ERR_tlx(clk))
										break;
										for (pp = np->properties; pp; pp = pp->next) {
															if (strcmp_tlx(pp->name, name) == 0) {
																			break;
															}
										}
										pcdev->pclk = pp ? pp->value : NULL;
										goto have_clk;
										break;
								}
								pcdev->pclk = clk;
					}
have_clk:
		clk = pcdev->pclk;
		if (clk->prepare_count == 0) {
			if (clk->ops->prepare) {
				ret = clk->ops->prepare(clk->hw);
			}
		}
		clk->prepare_count++;

		if (ret)
			break;
		ret = pcdrv->probe(pcdev, id);
		if (ret == 0)
			break;
		if (--pcdev->pclk->enable_count > 0)
								continue;
		if (pcdev->pclk->ops->disable)
							pcdev->pclk->ops->disable(pcdev->pclk->hw);
	} while (0);

	return ret;
}


#define AMBA_CID	0xb105f00d

struct amba_id {
       unsigned int            id;
       unsigned int            mask;
       void                    *data;
};

#define UART011_IFLS_RX4_8      (2 << 3)
#define UART011_IFLS_TX4_8      (2 << 0)
#define UART011_LCRH            0x2c    /* Line control register. */

struct vendor_data {
         unsigned int            ifls;
         unsigned int            lcrh_tx;
         unsigned int            lcrh_rx;
         bool                    oversampling;
         bool                    dma_threshold;
         bool                    cts_event_workaround;

       unsigned int (*get_fifosize)(struct amba_device *dev);
 };

struct amba_device {
         struct device           dev;
         struct resource         res;
         struct clk              *pclk;
         unsigned int            periphid;
        unsigned int            irq[AMBA_NR_IRQS];
 };

#define AMBA_REV_BITS(a) (((a) >> 20) & 0x0f)

static unsigned int get_fifosize_arm_tlx(struct amba_device *dev)
 {
         return AMBA_REV_BITS((dev)->periphid) < 3 ? 16 : 32;
 }

struct vendor_data vendor_arm_tlx = {
	.ifls			= UART011_IFLS_RX4_8|UART011_IFLS_TX4_8,
	.lcrh_tx		= UART011_LCRH,
	.lcrh_rx		= UART011_LCRH,
	.oversampling		= false,
	.dma_threshold		= false,
	.cts_event_workaround	= false,
	.get_fifosize		= get_fifosize_arm_tlx,
};;

static struct amba_id pl011_ids_tlx[] = {
	{
		.id	= 0x00041011,
		.mask	= 0x000fffff,
		.data	= &vendor_arm_tlx,
	},
	{
		.id	= 0x00380802,
		.mask	= 0x00ffffff,
	},
	{ 0, 0 },
};

typedef unsigned int __bitwise__ upf_t;


struct uart_ops {
	unsigned int	(*tx_empty)(struct uart_port *);
	void		(*set_mctrl)(struct uart_port *, unsigned int mctrl);
	unsigned int	(*get_mctrl)(struct uart_port *);
	void		(*stop_tx)(struct uart_port *);
	void		(*start_tx)(struct uart_port *);
	void		(*throttle)(struct uart_port *);
	void		(*unthrottle)(struct uart_port *);
	void		(*send_xchar)(struct uart_port *, char ch);
	void		(*stop_rx)(struct uart_port *);
	void		(*enable_ms)(struct uart_port *);
	void		(*break_ctl)(struct uart_port *, int ctl);
	int		(*startup)(struct uart_port *);
	void		(*shutdown)(struct uart_port *);
	void		(*flush_buffer)(struct uart_port *);
	void		(*set_termios)(struct uart_port *, struct ktermios *new,
				       struct ktermios *old);
	void		(*set_ldisc)(struct uart_port *, int new);
	void		(*pm)(struct uart_port *, unsigned int state,
			      unsigned int oldstate);

	/*
	 * Return a string describing the type of the port
	 */
	const char	*(*type)(struct uart_port *);

	/*
	 * Release IO and memory resources used by the port.
	 * This includes iounmap if necessary.
	 */
	void		(*release_port)(struct uart_port *);

	/*
	 * Request IO and memory resources used by the port.
	 * This includes iomapping the port if necessary.
	 */
	int		(*request_port)(struct uart_port *);
	void		(*config_port)(struct uart_port *, int);
	int		(*verify_port)(struct uart_port *, struct serial_struct *);
	int		(*ioctl)(struct uart_port *, unsigned int, unsigned long);
#ifdef CONFIG_CONSOLE_POLL
	int		(*poll_init)(struct uart_port *);
	void		(*poll_put_char)(struct uart_port *, unsigned char);
	int		(*poll_get_char)(struct uart_port *);
#endif
};

struct uart_icount {
				__u32   cts;
				__u32   dsr;
				__u32   rng;
				__u32   dcd;
				__u32   rx;
				__u32   tx;
				__u32   frame;
				__u32   overrun;
				__u32   parity;
				__u32   brk;
				__u32   buf_overrun;
};

struct uart_port {
	spinlock_t		lock;			/* port lock */
	unsigned long		iobase;			/* in/out[bwl] */
	unsigned char __iomem	*membase;		/* read/write[bwl] */
	unsigned int		(*serial_in)(struct uart_port *, int);
	void			(*serial_out)(struct uart_port *, int, int);
	void			(*set_termios)(struct uart_port *,
				               struct ktermios *new,
				               struct ktermios *old);
	int			(*handle_irq)(struct uart_port *);
	void			(*pm)(struct uart_port *, unsigned int state,
				      unsigned int old);
	void			(*handle_break)(struct uart_port *);
	unsigned int		irq;			/* irq number */
	unsigned long		irqflags;		/* irq flags  */
	unsigned int		uartclk;		/* base uart clock */
	unsigned int		fifosize;		/* tx fifo size */
	unsigned char		x_char;			/* xon/xoff char */
	unsigned char		regshift;		/* reg offset shift */
	unsigned char		iotype;			/* io access style */
	unsigned char		unused1;

#define UPIO_PORT		(0)
#define UPIO_HUB6		(1)
#define UPIO_MEM		(2)
#define UPIO_MEM32		(3)
#define UPIO_AU			(4)			/* Au1x00 and RT288x type IO */
#define UPIO_TSI		(5)			/* Tsi108/109 type IO */

	unsigned int		read_status_mask;	/* driver specific */
	unsigned int		ignore_status_mask;	/* driver specific */
	struct uart_state	*state;			/* pointer to parent state */
	struct uart_icount	icount;			/* statistics */

	struct console		*cons;			/* struct console, if any */
#if defined(CONFIG_SERIAL_CORE_CONSOLE) || defined(SUPPORT_SYSRQ)
	unsigned long		sysrq;			/* sysrq timeout */
#endif

	upf_t			flags;

#define UPF_FOURPORT		((__force upf_t) (1 << 1))
#define UPF_SAK			((__force upf_t) (1 << 2))
#define UPF_SPD_MASK		((__force upf_t) (0x1030))
#define UPF_SPD_HI		((__force upf_t) (0x0010))
#define UPF_SPD_VHI		((__force upf_t) (0x0020))
#define UPF_SPD_CUST		((__force upf_t) (0x0030))
#define UPF_SPD_SHI		((__force upf_t) (0x1000))
#define UPF_SPD_WARP		((__force upf_t) (0x1010))
#define UPF_SKIP_TEST		((__force upf_t) (1 << 6))
#define UPF_AUTO_IRQ		((__force upf_t) (1 << 7))
#define UPF_HARDPPS_CD		((__force upf_t) (1 << 11))
#define UPF_LOW_LATENCY		((__force upf_t) (1 << 13))
#define UPF_BUGGY_UART		((__force upf_t) (1 << 14))
#define UPF_NO_TXEN_TEST	((__force upf_t) (1 << 15))
#define UPF_MAGIC_MULTIPLIER	((__force upf_t) (1 << 16))
/* Port has hardware-assisted h/w flow control (iow, auto-RTS *not* auto-CTS) */
#define UPF_HARD_FLOW		((__force upf_t) (1 << 21))
/* Port has hardware-assisted s/w flow control */
#define UPF_SOFT_FLOW		((__force upf_t) (1 << 22))
#define UPF_CONS_FLOW		((__force upf_t) (1 << 23))
#define UPF_SHARE_IRQ		((__force upf_t) (1 << 24))
#define UPF_EXAR_EFR		((__force upf_t) (1 << 25))
#define UPF_BUG_THRE		((__force upf_t) (1 << 26))
/* The exact UART type is known and should not be probed.  */
#define UPF_FIXED_TYPE		((__force upf_t) (1 << 27))
#define UPF_BOOT_AUTOCONF	((__force upf_t) (1 << 28))
#define UPF_FIXED_PORT		((__force upf_t) (1 << 29))
#define UPF_DEAD		((__force upf_t) (1 << 30))
#define UPF_IOREMAP		((__force upf_t) (1 << 31))

#define UPF_CHANGE_MASK		((__force upf_t) (0x17fff))
#define UPF_USR_MASK		((__force upf_t) (UPF_SPD_MASK|UPF_LOW_LATENCY))

	unsigned int		mctrl;			/* current modem ctrl settings */
	unsigned int		timeout;		/* character-based timeout */
	unsigned int		type;			/* port type */
	const struct uart_ops	*ops;
	unsigned int		custom_divisor;
	unsigned int		line;			/* port index */
	resource_size_t		mapbase;		/* for ioremap */
	struct device		*dev;			/* parent device */
	unsigned char		hub6;			/* this should be in the 8250 driver */
	unsigned char		suspended;
	unsigned char		irq_wake;
	unsigned char		unused[2];
	void			*private_data;		/* generic platform data pointer */
};

int device_add_tlx_tlx(struct device *dev);

int sprintf_tlx(char *buf, const char *fmt, ...)
{
       va_list args;
       int i;

       va_start(args, fmt);
       i = vsnprintf_tlx(buf, INT_MAX, fmt, args);
       va_end(args);

       return i;
}

#define CON_CONSDEV     (2) /* Last on the command line */
#define CON_ENABLED     (4)

int unregister_console_tlx(struct console *console)
{
        struct console *a, *b;
  int res;

  res = 1;
  if (console_drivers_tlx == console) {
    console_drivers_tlx=console->next;
    res = 0;
  } else if (console_drivers_tlx) {
    for (a=console_drivers_tlx->next, b=console_drivers_tlx ;
         a; b=a, a=b->next) {
      if (a == console) {
        b->next = a->next;
        res = 0;
        break;
      }
    }
  }
  if (console_drivers_tlx != NULL && console->flags & CON_CONSDEV)
    console_drivers_tlx->flags |= CON_CONSDEV;

  console->flags &= ~CON_ENABLED;
  return res;
}

#define CON_BOOT        (8)
int preferred_console_tlx = -1;
int console_set_on_cmdline_tlx;
int selected_console_tlx = -1;
#define MAX_CMDLINECONSOLES 8
#define CON_PRINTBUFFER (1)
raw_spinlock_t logbuf_lock_tlx = __RAW_SPIN_LOCK_UNLOCKED(logbuf_lock_tlx);
u64 console_seq_tlx;
u32 console_idx_tlx;
u64 syslog_seq_tlx;
u32 syslog_idx_tlx;

enum log_flags {
	LOG_NOCONS	= 1,	/* already flushed, do not print to console */
	LOG_NEWLINE	= 2,	/* text ended with a newline */
	LOG_PREFIX	= 4,	/* text started with a prefix */
	LOG_CONT	= 8,	/* text is a fragment of a continuation line */
};

enum log_flags syslog_prev_tlx;
enum log_flags console_prev_tlx;
struct console *exclusive_console_tlx;
int  keep_bootcon_tlx;

struct console_cmdline_tlx
{
       char    name[8];                        /* Name of the driver       */
       int     index;                          /* Minor dev. to use        */
       char    *options;                       /* Options for the driver   */
#ifdef CONFIG_A11Y_BRAILLE_CONSOLE
         char    *brl_options;                   /* Options for braille driver */
#endif
};


struct console_cmdline_tlx console_cmdline_tlx[MAX_CMDLINECONSOLES];

void register_console_tlx(struct console *newcon)
{
	int i;
	unsigned long flags;
	struct console *bcon = NULL;
	struct console_cmdline_tlx *c;


	if (console_drivers_tlx && console_drivers_tlx->flags & CON_BOOT)
		bcon = console_drivers_tlx;

	if (preferred_console_tlx < 0 || bcon || !console_drivers_tlx)
		preferred_console_tlx = selected_console_tlx;

	if (preferred_console_tlx < 0) {
		if (newcon->index < 0)
			newcon->index = 0;
		if (newcon->setup == NULL ||
				newcon->setup(newcon, NULL) == 0) {
			newcon->flags |= CON_ENABLED;
			if (newcon->device) {
				newcon->flags |= CON_CONSDEV;
				preferred_console_tlx = 0;
			}
		}
	}

	/*
	*	See if this console matches one we selected on
	*	the command line.
	*/
	for (i = 0, c = console_cmdline_tlx;
			i < MAX_CMDLINECONSOLES && c->name[0];
			i++, c++) {
		if (strcmp_tlx(c->name, newcon->name) != 0)
			continue;
		if (newcon->index >= 0 &&
				newcon->index != c->index)
			continue;
		if (newcon->index < 0)
			newcon->index = c->index;
		if (newcon->setup &&
				newcon->setup(newcon, console_cmdline_tlx[i].options) != 0)
			break;
		newcon->flags |= CON_ENABLED;
		newcon->index = c->index;
		if (i == selected_console_tlx) {
			newcon->flags |= CON_CONSDEV;
			preferred_console_tlx = selected_console_tlx;
		}
		break;
	}

	if (!(newcon->flags & CON_ENABLED))
		return;
	if (bcon && ((newcon->flags & (CON_CONSDEV | CON_BOOT)) == CON_CONSDEV))
		newcon->flags &= ~CON_PRINTBUFFER;
	if ((newcon->flags & CON_CONSDEV) || console_drivers_tlx == NULL) {
		newcon->next = console_drivers_tlx;
		console_drivers_tlx = newcon;
		if (newcon->next)
			newcon->next->flags &= ~CON_CONSDEV;
	} else {
		newcon->next = console_drivers_tlx->next;
		console_drivers_tlx->next = newcon;
	}
	if (newcon->flags & CON_PRINTBUFFER) {
		__raw_spin_lock_irq_tlx(&logbuf_lock_tlx);
		console_seq_tlx = syslog_seq_tlx;
		console_idx_tlx = syslog_idx_tlx;
		console_prev_tlx = syslog_prev_tlx;
		__raw_spin_unlock_irq_tlx(&logbuf_lock_tlx);
		exclusive_console_tlx = newcon;
	}
	if (bcon &&
			((newcon->flags & (CON_CONSDEV | CON_BOOT)) == CON_CONSDEV) &&
			!keep_bootcon_tlx) {
		/* We need to iterate through all boot consoles, to make
		* sure we print everything out, before we unregister them.
		*/
		for_each_console(bcon)
			if (bcon->flags & CON_BOOT)
				unregister_console_tlx(bcon);
	}
}



#define TIOCM_DTR       0x002
#define CON_ENABLED     (4)
#define UART_CONFIG_TYPE        (1 << 0)
#define UART_CONFIG_IRQ         (1 << 1)
#define PORT_UNKNOWN    0

struct tracepoint {
         const char *name;               /* Tracepoint name */
         struct static_key key;
         void (*regfunc)(void);
         void (*unregfunc)(void);
         struct tracepoint_func __rcu *funcs;
};

struct tracepoint __tracepoint_workqueue_queue_work_tlx;


struct workqueue_struct {
	struct list_head	pwqs;		/* WR: all pwqs of this wq */
	struct list_head	list;		/* PL: list of all workqueues */
	struct mutex		mutex;		/* protects this wq */
	int			work_color;	/* WQ: current work color */
	int			flush_color;	/* WQ: current flush color */
	atomic_t		nr_pwqs_to_flush; /* flush in progress */
	struct wq_flusher	*first_flusher;	/* WQ: first flusher */
	struct list_head	flusher_queue;	/* WQ: flush waiters */
	struct list_head	flusher_overflow; /* WQ: flush overflow list */
	struct list_head	maydays;	/* MD: pwqs requesting rescue */
	struct worker		*rescuer;	/* I: rescue worker */
	int			nr_drainers;	/* WQ: drain in progress */
	int			saved_max_active; /* WQ: saved pwq max_active */
	struct workqueue_attrs_tlx	*unbound_attrs;	/* WQ: only for unbound wqs */
	struct pool_workqueue	*dfl_pwq;	/* WQ: only for unbound wqs */
#ifdef CONFIG_SYSFS
	struct wq_device	*wq_dev;	/* I: for sysfs interface */
#endif
#ifdef CONFIG_LOCKDEP
	struct lockdep_map	lockdep_map;
#endif
	char			name[WQ_NAME_LEN]; /* I: workqueue name */
	unsigned int		flags ____cacheline_aligned; /* WQ: WQ_* flags */
	struct pool_workqueue __percpu *cpu_pwqs; /* I: per-cpu pwqs */
	struct pool_workqueue __rcu *numa_pwq_tbl[]; /* FR: unbound pwqs indexed by node */
};

struct pool_workqueue {
	struct worker_pool	*pool;		/* I: the associated pool */
	struct workqueue_struct *wq;		/* I: the owning workqueue */
	int			work_color;	/* L: current color */
	int			flush_color;	/* L: flushing color */
	int			refcnt;		/* L: reference count */
	int			nr_in_flight[WORK_NR_COLORS];
						/* L: nr of in_flight works */
	int			nr_active;	/* L: nr of active works */
	int			max_active;	/* L: max active works */
	struct list_head	delayed_works;	/* L: delayed works */
	struct list_head	pwqs_node;	/* WR: node on wq->pwqs */
	struct list_head	mayday_node;	/* MD: node on wq->maydays */
	struct work_struct	unbound_release_work;
	struct rcu_head		rcu;
} __aligned(1 << WORK_STRUCT_FLAG_BITS);


struct tracepoint_func {
         void *func;
         void *data;
};

#define __to_kthread(vfork)     \
          container_of(vfork, struct kthread, exited)

void *kthread_data_tlx(struct task_struct *task)
{
         return __to_kthread(task->vfork_done)->data;
}

int tracepoint_probe_register_tlx(struct tracepoint *tp, void *probe, void *data)
{
         struct tracepoint_func tp_func;
         int ret;
         tp_func.func = probe;
         tp_func.data = data;
         ret = -1;
         return ret;
}

int tracepoint_probe_unregister_tlx(struct tracepoint *tp, void *probe, void *data)
 {
         struct tracepoint_func tp_func;
         int ret;
         tp_func.func = probe;
         tp_func.data = data;
         ret = -1;
         return ret;
 }


static inline struct worker *current_wq_worker(void)
{
        if (current->flags & PF_WQ_WORKER)
                 return kthread_data_tlx(current);
         return NULL;
}

static __always_inline bool static_key_false(struct static_key *key)
{
       return arch_static_branch_tlx(key);
}

#define TP_PROTO(args...)       args
#define TP_ARGS(args...)        args
#define PARAMS(args...) args
#define TP_CONDITION(args...)   args


#define rcu_dereference_sched_check(p, c) \
       __rcu_dereference_check((p), rcu_read_lock_sched_held() || (c), \
                                 __rcu)

#define rcu_dereference_sched(p) rcu_dereference_sched_check(p, 0)

static inline notrace void rcu_read_unlock_sched_notrace(void)
{
       __release(RCU_SCHED);
         preempt_enable();
}

static inline notrace void rcu_read_lock_sched_notrace(void)
{
         preempt_disable_notrace();
       __acquire(RCU_SCHED);
}

#define __DO_TRACE(tp, proto, args, cond, prercu, postrcu)		\
	do {								\
		struct tracepoint_func *it_func_ptr;			\
		void *it_func;						\
		void *__data;						\
									\
		if (!(cond))						\
			return;						\
		prercu;							\
		rcu_read_lock_sched_notrace();				\
		it_func_ptr = rcu_dereference_sched((tp)->funcs);	\
		if (it_func_ptr) {					\
			do {						\
				it_func = (it_func_ptr)->func;		\
				__data = (it_func_ptr)->data;		\
				((void(*)(proto))(it_func))(args);	\
			} while ((++it_func_ptr)->func);		\
		}							\
		rcu_read_unlock_sched_notrace();			\
		postrcu;						\
	} while (0)

#define __DECLARE_TRACE(name, proto, args, cond, data_proto, data_args) \
	struct tracepoint __tracepoint_##name;			\
	static inline void trace_##name(proto)				\
	{								\
		if (static_key_false(&__tracepoint_##name.key))		\
			__DO_TRACE(&__tracepoint_##name,		\
				TP_PROTO(data_proto),			\
				TP_ARGS(data_args),			\
				TP_CONDITION(cond),,);			\
	}								\
	static inline int						\
	register_trace_##name(void (*probe)(data_proto), void *data)	\
	{								\
		return tracepoint_probe_register_tlx(&__tracepoint_##name,	\
						(void *)probe, data);	\
	}								\
	static inline int						\
	unregister_trace_##name(void (*probe)(data_proto), void *data)	\
	{								\
		return tracepoint_probe_unregister_tlx(&__tracepoint_##name,\
						(void *)probe, data);	\
	}								\
	static inline void						\
	check_trace_callback_type_##name(void (*cb)(data_proto))	\
	{								\
	}								\
	static inline bool						\
	trace_##name##_enabled(void)					\
	{								\
		return static_key_false(&__tracepoint_##name.key);	\
	}


#define DECLARE_TRACE(name, proto, args)				\
		__DECLARE_TRACE(name, PARAMS(proto), PARAMS(args), 1,	\
				PARAMS(void *__data, proto),		\
				PARAMS(__data, args))

#define TRACE_EVENT(name, proto, args, struct, assign, print)	\
	DECLARE_TRACE(name, PARAMS(proto), PARAMS(args))


TRACE_EVENT(workqueue_queue_work_tlx,

	TP_PROTO(unsigned int req_cpu, struct pool_workqueue *pwq,
		struct work_struct *work),

	TP_ARGS(req_cpu, pwq, work),

	TP_STRUCT__entry(
		__field( void *,	work	)
		__field( void *,	function)
		__field( void *,	workqueue)
		__field( unsigned int,	req_cpu	)
		__field( unsigned int,	cpu	)
	),

	TP_fast_assign(
		__entry->work		= work;
		__entry->function	= work->func;
		__entry->workqueue	= pwq->wq;
		__entry->req_cpu	= req_cpu;
		__entry->cpu		= pwq->pool->cpu;
	),

	TP_printk_tlx("work struct=%p function=%pf workqueue=%p req_cpu=%u cpu=%u",
			__entry->work, __entry->function, __entry->workqueue,
			__entry->req_cpu, __entry->cpu)
);


#define IDR_INIT(name)                                                  \
 {                                                                       \
         .lock                   = __SPIN_LOCK_UNLOCKED(name.lock),      \
 }

struct idr worker_pool_idr_tlx = IDR_INIT(worker_pool_idr_tlx);
#define __WARN_printf(arg...)   do {  __WARN(); } while (0)


void *idr_find_slowpath_tlx(struct idr *idp, int id)
{
	int n;
	struct idr_layer *p;

	if (id < 0)
		return NULL;

	p = rcu_dereference_raw(idp->top);
	if (!p)
		return NULL;
	n = (p->layer+1) * IDR_BITS;

	if (id > idr_max(p->layer + 1))
		return NULL;

	while (n > 0 && p) {
		n -= IDR_BITS;
		p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
	}
	return((void *)p);
}

static inline void *idr_find(struct idr *idr, int id)
{
      struct idr_layer *hint = rcu_dereference_raw(idr->hint);

       if (hint && (id & ~IDR_MASK) == hint->prefix)
                 return rcu_dereference_raw(hint->ary[id & IDR_MASK]);

       return idr_find_slowpath_tlx(idr, id);
}

#define WARN(condition, format...) ({                                           \
         int __ret_warn_on = !!(condition);                              \
         if (unlikely(__ret_warn_on))                                    \
                 __WARN_printf(format);                                  \
         unlikely(__ret_warn_on);                                        \
 })


#define WARN_ONCE(condition, format...) ({                      \
         static bool __section(.data.unlikely) __warned;         \
         int __ret_warn_once = !!(condition);                    \
                                                                 \
         if (unlikely(__ret_warn_once))                          \
                 if (WARN(!__warned, format))                    \
                         __warned = true;                        \
         unlikely(__ret_warn_once);                              \
})



void __queue_work_tlx(int cpu, struct workqueue_struct *wq,
			struct work_struct *work)
{
	struct pool_workqueue *pwq;
	struct worker_pool *last_pool;
	struct list_head *worklist;
	unsigned int work_flags;
	unsigned int req_cpu = cpu;
	struct worker *worker;
	worker = current_wq_worker();

		if (unlikely(wq->flags & __WQ_DRAINING) &&
			!(worker && worker->current_pwq->wq == wq))
		return;
retry:
		cpu = raw_smp_processor_id();
		pwq = per_cpu_ptr(wq->cpu_pwqs, cpu);
//	last_pool = get_work_pool(work);
	unsigned long data = atomic_long_read_tlx(&work->data);
	int pool_id;
//   assert_rcu_or_pool_mutex();
		if (data & WORK_STRUCT_PWQ) {
							last_pool = ((struct pool_workqueue *)
												(data & WORK_STRUCT_WQ_DATA_MASK))->pool;
							goto have_lp;
		}
		pool_id = data >> WORK_OFFQ_POOL_SHIFT;
			if (pool_id == WORK_OFFQ_POOL_NONE) {
			last_pool = NULL;
				goto have_lp;
	}
	last_pool = idr_find(&worker_pool_idr_tlx, pool_id);
have_lp:
	if (last_pool && last_pool != pwq->pool) {
		struct worker *worker;

		spin_lock_tlx(&last_pool->lock);

//		worker = find_worker_executing_work(last_pool, work);
			hash_for_each_possible(last_pool->busy_hash, worker, hentry,
															(unsigned long)work)
							if (worker->current_work == work &&
									worker->current_func == work->func) break;

		if (worker && worker->current_pwq->wq == wq) {
			pwq = worker->current_pwq;
		} else {
			/* meh... not running there, queue here */
			spin_unlock_tlx(&last_pool->lock);
			spin_lock_tlx(&pwq->pool->lock);
		}
	} else {
		spin_lock_tlx(&pwq->pool->lock);
	}

	/*
	* pwq is determined and locked.  For unbound pools, we could have
	* raced with pwq release and it could already be dead.  If its
	* refcnt is zero, repeat pwq selection.  Note that pwqs never die
	* without another pwq replacing it in the numa_pwq_tbl or while
	* work items are executing on it, so the retrying is guaranteed to
	* make forward-progress.
	*/
	if (unlikely(!pwq->refcnt)) {
		if (wq->flags & WQ_UNBOUND) {
			spin_unlock_tlx(&pwq->pool->lock);
			cpu_relax();
			goto retry;
		}
		/* oops */
		WARN_ONCE(true, "workqueue: per-cpu pwq for %s on cpu%d has 0 refcnt",
				wq->name, cpu);
	}

	/* pwq determined, queue */
	trace_workqueue_queue_work_tlx(req_cpu, pwq, work);
	pwq->nr_in_flight[pwq->work_color]++;
	work_flags = (pwq->work_color) << WORK_STRUCT_COLOR_SHIFT;;

	if (likely(pwq->nr_active < pwq->max_active)) {
//		trace_workqueue_activate_work_tlx_tlx(work);
		pwq->nr_active++;
		worklist = &pwq->pool->worklist;
	} else {
		work_flags |= WORK_STRUCT_DELAYED;
		worklist = &pwq->delayed_works;
	}

//	insert_work(pwq, work, worklist, work_flags);
	struct list_head *head = worklist;
	unsigned int extra_flags = work_flags;
	struct worker_pool *pool = pwq->pool;
//	set_work_pwq(work, pwq, extra_flags);
//	set_work_data(work, (unsigned long)pwq,
//                       WORK_STRUCT_PENDING | WORK_STRUCT_PWQ | extra_flags);
		atomic_long_set_tlx(&work->data,
					(unsigned long)pwq | WORK_STRUCT_PENDING | WORK_STRUCT_PWQ | extra_flags);
		list_add_tail(&work->entry, head);
//				get_pwq(pwq);
	smp_mb();
	if (!atomic_read(&pool->nr_running)) {
		struct worker *worker = list_first_entry(&pool->idle_list, struct worker, entry);
		if (likely(worker))
						wake_up_process_tlx(worker->task);
	}
//          wake_up_worker(pool);

	spin_unlock_tlx(&pwq->pool->lock);
}



struct device *tty_register_device_attr_tlx(struct tty_driver *driver,
					unsigned index, struct device *device,
					void *drvdata,
					const struct attribute_group **attr_grp)
{
	char name[64];
	dev_t devt = MKDEV(driver->major, driver->minor_start) + index;
	struct device *dev = NULL;
	int retval = -ENODEV;
	bool cdev = false;
//		tty_line_name(driver, index, name);
	sprintf_tlx(name, "%s%d", driver->name,
																index + driver->name_base);

	dev = kzalloc_tlx(sizeof(*dev), GFP_KERNEL);
	dev->devt = devt;
	dev->class = tty_class_tlx;
	dev->parent = device;
//	dev->release = tty_device_create_release;
//	dev_set_name_tlx(dev, "%s", name);
	(&dev->kobj)->name = name;
	dev->groups = attr_grp;
//	dev_set_drvdata(dev, drvdata);
//	retval = device_register(dev);
//	device_initialize(dev);
	device_add_tlx_tlx(dev);
	return dev;
}

struct attribute_group {
         const char              *name;
         umode_t                 (*is_visible)(struct kobject *,
                                               struct attribute *, int);
         struct attribute        **attrs;
         struct bin_attribute    **bin_attrs;
 };

struct attribute_group tty_dev_attr_group_tlx = {
         .attrs = NULL,
         };

struct attribute_group *tty_dev_attr_groups_tlx[] = {
         &tty_dev_attr_group_tlx,
         NULL
         };

struct circ_buf {
         char *buf;
         int head;
         int tail;
};

enum uart_pm_state {
	UART_PM_STATE_ON = 0,
	UART_PM_STATE_OFF = 3, /* number taken from ACPI */
	UART_PM_STATE_UNDEFINED,
};

struct uart_state {
	struct tty_port		port;

	enum uart_pm_state	pm_state;
	struct circ_buf		xmit;

	struct uart_port	*uart_port;
};



struct uart_driver {
	struct module		*owner;
	const char		*driver_name;
	const char		*dev_name;
	int			 major;
	int			 minor;
	int			 nr;
	struct console		*cons;

	/*
	 * these are private; the low level driver should not
	 * touch these; they should be initialised to NULL
	 */
	struct uart_state	*state;
	struct tty_driver	*tty_driver;
};

struct lock_class_key port_lock_key_tlx;


int uart_add_one_port_tlx(struct uart_driver *drv, struct uart_port *uport)
{
	struct uart_state *state;
	struct tty_port *port;
	int ret = 0;
	struct device *tty_dev;
	state = drv->state + uport->line;
	port = &state->port;
	state->uart_port = uport;
	state->pm_state = UART_PM_STATE_UNDEFINED;
	uport->cons = drv->cons;
	uport->state = state;
	if (!(((uport)->cons && (uport)->cons->index == (uport)->line) && (uport->cons->flags & CON_ENABLED))) {
		spin_lock_init(&uport->lock);
		lockdep_set_class(&uport->lock, &port_lock_key_tlx);
	}

//	uart_configure_port(drv, state, uport);
//	struct uart_driver *drv, struct uart_state *state,
	//	struct uart_port *port = uport;
//	{
		unsigned int flags;
		if (!uport->iobase && !uport->mapbase && !uport->membase)
			return;
		flags = 0;
		if (uport->flags & UPF_AUTO_IRQ)
			flags |= UART_CONFIG_IRQ;
		if (uport->flags & UPF_BOOT_AUTOCONF) {
			if (!(uport->flags & UPF_FIXED_TYPE)) {
				uport->type = PORT_UNKNOWN;
				flags |= UART_CONFIG_TYPE;
			}
			uport->ops->config_port(uport, flags);
		}
		if (uport->type != PORT_UNKNOWN) {
			unsigned long flags;
			uport->ops->set_mctrl(uport, uport->mctrl & TIOCM_DTR);
			if (uport->cons && !(uport->cons->flags & CON_ENABLED))
				register_console_tlx(uport->cons);
		}

//	tty_dev = tty_port_register_device_attr(port, drv->tty_driver,
//			uport->line, uport->dev, port, tty_dev_attr_groups_tlx);
		struct tty_driver *driver = drv->tty_driver;
		unsigned index = uport->line;
		struct device *device = uport->dev;
		void *drvdata = port;
const struct attribute_group **attr_grp = tty_dev_attr_groups_tlx;
//	tty_port_link_device(port, driver, index);
	driver->ports[index] = port;
	tty_dev = tty_register_device_attr_tlx(driver, index, device, drvdata,
										attr_grp);


	uport->flags &= ~UPF_DEAD;
out:
	return ret;
}

#define llist_entry(ptr, type, member)          \
         container_of(ptr, type, member)

static inline struct llist_node *llist_del_all(struct llist_head *head)
{
         return xchg(&head->first, NULL);
}

#define llist_for_each_entry_safe(pos, n, node, member)                        \
         for (pos = llist_entry((node), typeof(*pos), member);                  \
              &pos->member != NULL &&                                           \
               (n = llist_entry(pos->member.next, typeof(*n), member), true); \
            pos = n)

void tty_buffer_free_all_tlx(struct tty_port *port)
{
	struct tty_bufhead *buf = &port->buf;
	struct tty_buffer *p, *next;
	struct llist_node *llist;

	while ((p = buf->head) != NULL) {
		buf->head = p->next;
		if (p->size > 0)
			kfree_tlx(p);
	}
	llist = llist_del_all(&buf->free);
	llist_for_each_entry_safe(p, next, llist, free)
		kfree_tlx(p);

//	tty_buffer_reset(&buf->sentinel, 0);
	struct tty_buffer *p_ = &buf->sentinel;
	size_t size = 0;

			p_->used = 0;
			p_->size = size;
			p_->next = NULL;
			p_->commit = 0;
			p_->read = 0;
			p_->flags = 0;

	buf->head = &buf->sentinel;
	buf->tail = &buf->sentinel;

	atomic_set(&buf->mem_used, 0);
}


struct klist_iter {
         struct klist            *i_klist;
         struct klist_node       *i_cur;
};

struct klist_node *klist_next_tlx(struct klist_iter *i);

void klist_iter_init_node_tlx(struct klist *k, struct klist_iter *i,
			  struct klist_node *n)
{
	i->i_klist = k;
	i->i_cur = n;
//	if (n)
//		kref_get(&n->n_ref);
}


struct subsys_private {
	struct kset subsys;
	struct kset *devices_kset_tlx;
	struct list_head interfaces;
	struct mutex mutex;

	struct kset *drivers_kset;
	struct klist klist_devices;
	struct klist klist_drivers;
	struct blocking_notifier_head_tlx bus_notifier;
	unsigned int drivers_autoprobe:1;
	struct bus_type *bus;

	struct kset glue_dirs;
	struct class *class;
};

struct class {
	const char		*name;
	struct module		*owner;

	struct class_attribute		*class_attrs;
	const struct attribute_group	**dev_groups;
	struct kobject			*dev_kobj;

	int (*dev_uevent)(struct device *dev, struct kobj_uevent_env *env);
	char *(*devnode)(struct device *dev, umode_t *mode);

	void (*class_release)(struct class *class);
	void (*dev_release)(struct device *dev);

	int (*suspend)(struct device *dev, pm_message_t state);
	int (*resume)(struct device *dev);

	const struct kobj_ns_type_operations *ns_type;
	const void *(*namespace)(struct device *dev);

	const struct dev_pm_ops *pm;

	struct subsys_private *p;
};



struct class_dev_iter {
         struct klist_iter               ki;
         const struct device_type        *type;
};

void class_dev_iter_init_tlx(struct class_dev_iter *iter, struct class *class,
                          struct device *start, const struct device_type *type)
{
         struct klist_node *start_knode = NULL;

         if (start)
                 start_knode = &start->knode_class;
         klist_iter_init_node_tlx(&class->p->klist_devices, &iter->ki, start_knode);
         iter->type = type;
}

struct device *class_dev_iter_next_tlx(struct class_dev_iter *iter)
{
         struct klist_node *knode;
         struct device *dev;

         while (1) {
                 knode = klist_next_tlx(&iter->ki);
                 if (!knode)
                         return NULL;
                 dev = container_of(knode, struct device, knode_class);
                 if (!iter->type || iter->type == dev->type)
                         return dev;
         }
}

struct device *class_find_device_tlx(struct class *class, struct device *start,
				const void *data,
				int (*match)(struct device *, const void *))
{
	struct class_dev_iter iter;
	struct device *dev;
	class_dev_iter_init_tlx(&iter, class, start, NULL);
	while ((dev = class_dev_iter_next_tlx(&iter))) {
		if (match(dev, data)) {
			kobject_get_tlx(&dev->kobj);
			break;
		}
	}
//	class_dev_iter_exit(&iter);
	return dev;
}

static int __match_devt(struct device *dev, const void *data)
{
				const dev_t *devt = data;

				return dev->devt == *devt;
}


int kobj_map_tlx(struct kobj_map *domain, dev_t dev, unsigned long range,
	     struct module *module, kobj_probe_t *probe,
	     int (*lock)(dev_t, void *), void *data)
{
	unsigned n = MAJOR(dev + range - 1) - MAJOR(dev) + 1;
	unsigned index = MAJOR(dev);
	unsigned i;
	struct probe *p;
	if (n > 255)
		n = 255;
	p = kmalloc_tlx(sizeof(struct probe) * n, GFP_KERNEL);
	for (i = 0; i < n; i++, p++) {
		p->owner = module;
		p->get = probe;
		p->lock = lock;
		p->dev = dev;
		p->range = range;
		p->data = data;
	}
//	mutex_lock(domain->lock);
	for (i = 0, p -= n; i < n; i++, p++, index++) {
		struct probe **s = &domain->probes[index % 255];
		while (*s && (*s)->range < range)
			s = &(*s)->next;
		p->next = *s;
		*s = p;
	}
//	mutex_unlock(domain->lock);
	return 0;
}


static struct kobject *exact_match(dev_t dev, int *part, void *data)
{
				struct cdev *p = data;
				return &p->kobj;
}

static int exact_lock(dev_t dev, void *data)
{
				struct cdev *p = data;
				return 0;
}

struct mutex_waiter {
	struct list_head	list;
	struct task_struct	*task;
#ifdef CONFIG_DEBUG_MUTEXES
	void			*magic;
#endif
};

struct ww_acquire_ctx {
	struct task_struct *task;
	unsigned long stamp;
	unsigned acquired;
};

struct ww_mutex {
	struct mutex base;
	struct ww_acquire_ctx *ctx;
};

#define	MUTEX_SHOW_NO_WAITER(mutex)	(atomic_read(&(mutex)->count) >= 0)

#define spin_lock_mutex(lock, flags) \
		do { spin_lock_tlx(lock); (void)(flags); } while (0)
#define spin_unlock_mutex(lock, flags) \
		do { spin_unlock_tlx(lock); (void)(flags); } while (0)

static inline int signal_pending_state(long state, struct task_struct *p)
{
	if (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))
		return 0;
	if (!signal_pending_tlx(p))
		return 0;

	return (state & TASK_INTERRUPTIBLE);
}

#define atomic_xchg(v, new) (xchg(&((v)->counter), new))

static inline bool owner_running(struct mutex *lock, struct task_struct *owner)
{
	if (lock->owner != owner)
		return false;

	/*
	 * Ensure we emit the owner->on_cpu, dereference _after_ checking
	 * lock->owner still matches owner, if that fails, owner might
	 * point to free()d memory, if it still matches, the rcu_read_lock()
	 * ensures the memory stays valid.
	 */
	barrier();

	return owner->on_cpu;
}



static __always_inline int __sched
__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
				struct lockdep_map *nest_lock, unsigned long ip,
				struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)
{
	struct task_struct *task = current;
	struct mutex_waiter waiter;
	unsigned long flags;
	int ret;
	preempt_disable();
#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
	struct task_struct *owner;
	int retval = 1;
	if (need_resched_tlx()) {
		retval = 0;
		goto how;
	}
	rcu_read_lock_tlx();
	owner = ACCESS_ONCE(lock->owner);
	if (owner)
		retval = owner->on_cpu;
	rcu_read_unlock_tlx();
how:
	if (need_resched_tlx())
		__schedule_tlx();
#endif
	spin_lock_mutex(&lock->wait_lock, flags);
	if (MUTEX_SHOW_NO_WAITER(lock) && (atomic_xchg(&lock->count, 0) == 1))
		goto skip_wait;
	list_add_tail(&waiter.list, &lock->wait_list);
	waiter.task = task;
//	lock_contended(&lock->dep_map, ip);
	for (;;) {
		if (MUTEX_SHOW_NO_WAITER(lock) &&
				(atomic_xchg(&lock->count, -1) == 1))
			break;
		if (unlikely(signal_pending_state(state, task))) {
			ret = -EINTR;
			goto err;
		}
		if (use_ww_ctx && ww_ctx->acquired > 0) {
				struct ww_mutex *ww = container_of(lock, struct ww_mutex, base);
				struct ww_acquire_ctx *hold_ctx = ACCESS_ONCE(ww->ctx);
				if (unlikely(ww_ctx == hold_ctx))
					goto err;
				if (ww_ctx->stamp - hold_ctx->stamp <= LONG_MAX &&
						(ww_ctx->stamp != hold_ctx->stamp || ww_ctx > hold_ctx)) {
					goto err;
				}

		}
//		__set_task_state(task, state);
		do { (task)->state = (state); } while (0);
		spin_unlock_mutex(&lock->wait_lock, flags);
		__schedule_tlx();
		spin_lock_mutex(&lock->wait_lock, flags);
	}
		__list_del((&waiter)->list.prev, (&waiter)->list.next);

	if (likely(list_empty(&lock->wait_list)))
		atomic_set(&lock->count, 0);
skip_wait:
//	lock_acquired(&lock->dep_map, ip);
	lock->owner = current;
	spin_unlock_mutex(&lock->wait_lock, flags);
	preempt_enable();
	return 0;

err:
	return ret;
}


__mutex_lock_slowpath_tlx(atomic_t *lock_count)
{
	struct mutex *lock = container_of(lock_count, struct mutex, count);

	__mutex_lock_common(lock, TASK_UNINTERRUPTIBLE, 0,
					NULL, _RET_IP_, NULL, 0);
}


static inline void
__mutex_unlock_common_slowpath(atomic_t *lock_count, int nested)
{
	struct mutex *lock = container_of(lock_count, struct mutex, count);
	unsigned long flags;
	spin_lock_mutex(&lock->wait_lock, flags);
	if (!list_empty(&lock->wait_list)) {
		/* get the first entry from the wait-list: */
		struct mutex_waiter *waiter =
				list_entry(lock->wait_list.next,
						struct mutex_waiter, list);

		wake_up_process_tlx(waiter->task);
	}

	spin_unlock_mutex(&lock->wait_lock, flags);
}


LIST_HEAD(tty_drivers_tlx);


#define TTY_DRIVER_DYNAMIC_ALLOC        0x0040
#define TTY_DRIVER_DYNAMIC_DEV          0x0008
#define TTY_DRIVER_INSTALLED            0x0001
struct mutex tty_mutex_tlx;

typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);

typedef struct poll_table_struct {
         poll_queue_proc _qproc;
         unsigned long _key;
} poll_table;

int tty_fasync_tlx(int fd, struct file *filp, int on)
{

	return 0;
}

int tty_release_tlx(struct inode *inode, struct file *filp)
{
	return 0;
}

long tty_compat_ioctl_tlx(struct file *file, unsigned int cmd,
				unsigned long arg)
{
	return 0;
}

long tty_ioctl_tlx(struct file *file, unsigned int cmd, unsigned long arg)
{

	return 0;
}

unsigned int tty_poll_tlx(struct file *filp, poll_table *wait)
{
	struct tty_struct *tty = ((struct tty_file_private *)filp->private_data)->tty;
	struct tty_ldisc *ld;
	int ret = 0;
	ld = tty->ldisc;
	if (ld->ops->poll)
		ret = (ld->ops->poll)(tty, filp, wait);
	return ret;
}


#define ESPIPE          29      /* Illegal seek */

loff_t no_llseek_tlx(struct file *file, loff_t offset, int whence)
{
         return -ESPIPE;
}

static const struct file_operations tty_fops_tlx = {
	.llseek		= no_llseek_tlx,
	.read		= tty_read_tlx,
	.write		= tty_write_tlx,
	.poll		= tty_poll_tlx,
	.unlocked_ioctl	= tty_ioctl_tlx,
	.compat_ioctl	= tty_compat_ioctl_tlx,
	.open		= tty_open_tlx,
	.release	= tty_release_tlx,
	.fasync		= tty_fasync_tlx,
};

#define SIZE_MAX        (~(size_t)0)

void *kcalloc_tlx(size_t n, size_t size, gfp_t flags)
{
	//     return kmalloc_array(n, size, flags | __GFP_ZERO);
					if (size != 0 && n > SIZE_MAX / size)
							return NULL;
			return __kmalloc_tlx(n * size, flags | __GFP_ZERO);
}



int tty_register_driver_tlx(struct tty_driver *driver)
{
	int error = 0;
	int i;
	dev_t dev;
	struct device *d;

	if (!driver->major) {
//		error = alloc_chrdev_region(&dev, driver->minor_start,
//						driver->num, driver->name);
		if (!error) {
			driver->major = MAJOR(dev);
			driver->minor_start = MINOR(dev);
		}
	} else {
		dev = MKDEV(driver->major, driver->minor_start);
//		error = register_chrdev_region(dev, driver->num, driver->name);
	}
	if (error < 0)
		goto err;

	if (driver->flags & TTY_DRIVER_DYNAMIC_ALLOC) {
//		error = tty_cdev_add(driver, dev, 0, driver->num);
				unsigned int index = 0;
				unsigned int count = driver->num;
//        cdev_init(&driver->cdevs[index], &tty_fops);
				struct cdev *cdev = &driver->cdevs[index];
				const struct file_operations *fops = &tty_fops_tlx;
				memset_tlx(cdev, 0, sizeof *cdev);
				INIT_LIST_HEAD(&cdev->list);
				kobject_init_tlx(&cdev->kobj, NULL);
				cdev->ops = fops;

				driver->cdevs[index].owner = driver->owner;
//        cdev_add(&driver->cdevs[index], dev, count);
				struct cdev *p = &driver->cdevs[index];
				p->dev = dev;
				p->count = count;

				kobj_map_tlx(cdev_map_tlx, dev, count, NULL,
									exact_match, exact_lock, p);
	}

//	mutex_lock(&tty_mutex_tlx);
	list_add(&driver->tty_drivers_tlx, &tty_drivers_tlx);
//	mutex_unlock(&tty_mutex_tlx);

	if (!(driver->flags & TTY_DRIVER_DYNAMIC_DEV)) {
		for (i = 0; i < driver->num; i++) {
//			d = tty_register_device(driver, i, NULL);
			d = tty_register_device_attr_tlx(driver, i, NULL, NULL, NULL);
			if (d) {
				goto err_unreg_devs;
			}
		}
	}
	driver->flags |= TTY_DRIVER_INSTALLED;
	return 0;

err_unreg_devs:
	for (i--; i >= 0; i--) {
//			device_destroy(tty_class,
//             MKDEV(driver->major, driver->minor_start) + i);
			struct class *class = tty_class_tlx;
			dev_t devt = MKDEV(driver->major, driver->minor_start) + i;
			struct device *dev;
			dev = class_find_device_tlx(class, NULL, &devt, __match_devt);
			if (dev) {
	//               put_device(dev);
					//       device_unregister(dev);
//					 device_del(dev);
			}
	}
//		tty_unregister_device(driver, i);

//	mutex_lock(&tty_mutex_tlx);
	might_sleep();
//	__mutex_fastpath_lock(&(&tty_mutex_tlx)->count, __mutex_lock_slowpath);
	__mutex_lock_slowpath_tlx(&(&tty_mutex_tlx)->count);
//  mutex_set_owner(&tty_mutex_tlx);
	list_del(&driver->tty_drivers_tlx);
//	__mutex_fastpath_unlock(&(&tty_mutex_tlx)->count, __mutex_unlock_slowpath)
	__mutex_unlock_common_slowpath((&(&tty_mutex_tlx)->count),1);
//	mutex_unlock(&tty_mutex_tlx);

err_unreg_char:
//	unregister_chrdev_region(dev, driver->num);
err:
	return error;
}


#define TTY_DRIVER_UNNUMBERED_NODE      0x0080
#define TTY_DRIVER_MAGIC                0x5402
#define TTY_DRIVER_DEVPTS_MEM           0x0010

struct tty_driver *__tty_alloc_driver_tlx(unsigned int lines, struct module *owner,
		unsigned long flags)
{
	struct tty_driver *driver;
	unsigned int cdevs = 1;
	int err;

	if (!lines || (flags & TTY_DRIVER_UNNUMBERED_NODE && lines > 1))
		return -EINVAL;

	driver = kzalloc_tlx(sizeof(struct tty_driver), GFP_KERNEL);
	if (!driver)
		return -ENOMEM;

	kref_init_tlx(&driver->kref);
	driver->magic = TTY_DRIVER_MAGIC;
	driver->num = lines;
	driver->owner = owner;
	driver->flags = flags;

	if (!(flags & TTY_DRIVER_DEVPTS_MEM)) {
		driver->ttys = kcalloc_tlx(lines, sizeof(*driver->ttys),
				GFP_KERNEL);
		driver->termios = kcalloc_tlx(lines, sizeof(*driver->termios),
				GFP_KERNEL);
	}

	if (!(flags & TTY_DRIVER_DYNAMIC_ALLOC)) {
		driver->ports = kcalloc_tlx(lines, sizeof(*driver->ports),
				GFP_KERNEL);
		cdevs = lines;
	}

	driver->cdevs = kcalloc_tlx(cdevs, sizeof(*driver->cdevs), GFP_KERNEL);
	return driver;
}

struct tty_port_operations {
         /* Return 1 if the carrier is raised */
         int (*carrier_raised)(struct tty_port *port);
         /* Control the DTR line */
         void (*dtr_rts)(struct tty_port *port, int raise);
         /* Called when the last close completes or a hangup finishes
            IFF the port was initialized. Do not use to free resources. Called
            under the port mutex to serialize against activate/shutdowns */
         void (*shutdown)(struct tty_port *port);
         /* Called under the port mutex from tty_port_open, serialized using
            the port mutex */
         /* FIXME: long term getting the tty argument *out* of this would be
          good for consoles */
         int (*activate)(struct tty_port *port, struct tty_struct *tty);
         /* Called on the final put of a port */
         void (*destruct)(struct tty_port *port);
 };

struct tty_port_operations uart_port_ops_tlx = {};

static inline unsigned char *char_buf_ptr(struct tty_buffer *b, int ofs)
{
       return ((unsigned char *)b->data) + ofs;
}

static inline char *flag_buf_ptr(struct tty_buffer *b, int ofs)
{
         return (char *)char_buf_ptr(b, ofs) + b->size;
}


#define TTY_BUFFER_PAGE (((PAGE_SIZE - sizeof(struct tty_buffer)) / 2) & ~0xFF)



#define TTYB_ALIGN_MASK 255
#define MIN_TTYB_SIZE   256
#define __ALIGN_MASK(x, mask)   __ALIGN_KERNEL_MASK((x), (mask))

struct llist_node *llist_del_first_tlx(struct llist_head *head)
{
	struct llist_node *entry, *old_entry, *next;

	entry = head->first;
	for (;;) {
		if (entry == NULL)
			return NULL;
		old_entry = entry;
		next = entry->next;
		entry = cmpxchg(&head->first, old_entry, next);
		if (entry == old_entry)
			break;
	}

	return entry;
}

struct tty_buffer *tty_buffer_alloc_tlx(struct tty_port *port, size_t size)
{
	struct llist_node *free;
	struct tty_buffer *p;

	/* Round the buffer size out */
	size = __ALIGN_MASK(size, TTYB_ALIGN_MASK);

	if (size <= MIN_TTYB_SIZE) {
		free = llist_del_first_tlx(&port->buf.free);
		if (free) {
			p = llist_entry(free, struct tty_buffer, free);
			goto found;
		}
	}

	/* Should possibly check if this fails for the largest buffer we
		have queued and recycle that ? */
	if (atomic_read(&port->buf.mem_used) > port->buf.mem_limit)
		return NULL;
	p = kmalloc_tlx(sizeof(struct tty_buffer) + 2 * size, GFP_ATOMIC);


found:
//	tty_buffer_reset(p, size);
	p->used = 0;
	p->size = size;
	p->next = NULL;
	p->commit = 0;
	p->read = 0;
	p->flags = 0;
	atomic_add(size, &port->buf.mem_used);
	return p;
}

#define TTYB_NORMAL     1       /* buffer has no flags buffer */

int __tty_buffer_request_room_tlx(struct tty_port *port, size_t size,
						int flags)
{
	struct tty_bufhead *buf = &port->buf;
	struct tty_buffer *b, *n;
	int left, change;

	b = buf->tail;
	if (b->flags & TTYB_NORMAL)
		left = 2 * b->size - b->used;
	else
		left = b->size - b->used;

	change = (b->flags & TTYB_NORMAL) && (~flags & TTYB_NORMAL);
	if (change || left < size) {
		/* This is the slow path - looking for new buffers to use */
		if ((n = tty_buffer_alloc_tlx(port, size)) != NULL) {
			n->flags = flags;
			buf->tail = n;
			b->commit = b->used;
			/* paired w/ barrier in flush_to_ldisc(); ensures the
			* latest commit value can be read before the head is
			* advanced to the next buffer
			*/
			smp_wmb();
			b->next = n;
		} else if (change)
			size = 0;
		else
			size = left;
	}
	return size;
}


int tty_insert_flip_string_flags_tlx(struct tty_port *port,
								const unsigned char *chars, const char *flags, size_t size)
{
				int copied = 0;
				do {
								int goal = min_t(size_t, size - copied, TTY_BUFFER_PAGE);
								int space = __tty_buffer_request_room_tlx(port, goal, 0);
								struct tty_buffer *tb = port->buf.tail;
								if (unlikely(space == 0))
												break;
								memcpy_tlx(char_buf_ptr(tb, tb->used), chars, space);
								memcpy_tlx(flag_buf_ptr(tb, tb->used), flags, space);
								tb->used += space;
								copied += space;
								chars += space;
								flags += space;
								/* There is a small chance that we need to split the data over
										several buffers. If this is the case we must loop */
				} while (unlikely(size > copied));
				return copied;
}


int tty_insert_flip_char_tlx(struct tty_port *port,
																				unsigned char ch, char flag)
{
				struct tty_buffer *tb = port->buf.tail;
				int change;

				change = (tb->flags & TTYB_NORMAL) && (flag != TTY_NORMAL);
					if (!change && tb->used < tb->size) {
								if (~tb->flags & TTYB_NORMAL)
												*flag_buf_ptr(tb, tb->used) = flag;
								*char_buf_ptr(tb, tb->used++) = ch;
							return 1;
					}
				return tty_insert_flip_string_flags_tlx(port, &ch, &flag, 1);
}


#define TTY_OVERRUN     4

void uart_insert_char_tlx(struct uart_port *port, unsigned int status,
		unsigned int overrun, unsigned int ch, unsigned int flag)
{
	struct tty_port *tport = &port->state->port;

	if ((status & port->ignore_status_mask & ~overrun) == 0)
		if (tty_insert_flip_char_tlx(tport, ch, flag) == 0)
			++port->icount.buf_overrun;

	/*
	* Overrun is special.  Since it's reported immediately,
	* it doesn't affect the current character.
	*/
	if (status & ~port->ignore_status_mask & overrun)
		if (tty_insert_flip_char_tlx(tport, 0, TTY_OVERRUN) == 0)
			++port->icount.buf_overrun;
}




int
receive_buf_tlx(struct tty_struct *tty, struct tty_buffer *head, int count)
{
	struct tty_ldisc *disc = tty->ldisc;
	unsigned char *p = char_buf_ptr(head, head->read);
	char	      *f = NULL;

	if (~head->flags & TTYB_NORMAL)
		f = (char *)char_buf_ptr(head, head->read) + head->size;;

	if (disc->ops->receive_buf2)
		count = disc->ops->receive_buf2(tty, p, f, count);
	head->read += count;
	return count;
}


void flush_to_ldisc_tlx(struct work_struct *work)
{
	struct tty_port *port = container_of(work, struct tty_port, buf.work);
	struct tty_bufhead *buf = &port->buf;
	struct tty_struct *tty;
	struct tty_ldisc *disc;

	tty = port->itty;
	disc = tty->ldisc;
	while (1) {
		struct tty_buffer *head = buf->head;
		struct tty_buffer *next;
		int count;

		/* Ldisc or user is trying to gain exclusive access */
		if (atomic_read(&buf->priority))
			break;

		next = head->next;
		/* paired w/ barrier in __tty_buffer_request_room();
		* ensures commit value read is not stale if the head
		* is advancing to the next buffer
		*/
		smp_rmb();
		count = head->commit - head->read;
		if (!count) {
			if (next == NULL)
				break;
			buf->head = next;
//			tty_buffer_free(port, head);
			continue;
		}

		count = receive_buf_tlx(tty, head, count);
		if (!count)
			break;
	}
}




static inline void init_llist_head(struct llist_head *list)
{
         list->first = NULL;
}

#define CIRC_SPACE_TO_END(head,tail,size) \
       ({int end = (size) - 1 - (head); \
           int n = (end + (tail)) & ((size)-1); \
           n <= end ? n : end+1;})

#define UART_XMIT_SIZE  PAGE_SIZE

int uart_write_tlx(struct tty_struct *tty,
					const unsigned char *buf, int count)
{
	struct uart_state *state = tty->driver_data;
	struct uart_port *port;
	struct circ_buf *circ;
	unsigned long flags;
	int c, ret = 0;
	port = state->uart_port;
	circ = &state->xmit;

	while (1) {
		c = CIRC_SPACE_TO_END(circ->head, circ->tail, UART_XMIT_SIZE);
		if (count < c)
			c = count;
		if (c <= 0)
			break;
		memcpy_tlx(circ->buf + circ->head, buf, c);
		circ->head = (circ->head + c) & (UART_XMIT_SIZE - 1);
		buf += c;
		count -= c;
		ret += c;
	}
	if (!tty->stopped && !tty->hw_stopped)
		port->ops->start_tx(port);
	return ret;
}

#define ASYNCB_INITIALIZED      31 /* Serial port was initialized */

int uart_startup_tlx(struct tty_struct *tty, struct uart_state *state,
		int init_hw)
{
	struct tty_port *port = &state->port;
	int retval;
	set_bit_tlx(TTY_IO_ERROR, &tty->flags);
//	retval = uart_port_startup(tty, state, init_hw);
	struct uart_port *uport = state->uart_port;
	unsigned long page;
//	int retval = 0;
	if (!state->xmit.buf) {
//		page = __get_free_pages(GFP_KERNEL | __GFP_ZERO, 0);
		page = alloc_pages(GFP_KERNEL | __GFP_ZERO, 0);
		page = (unsigned long) page_address(page);
		state->xmit.buf = (unsigned char *) page;
	}

	retval = uport->ops->startup(uport);

	if (!retval) {
		set_bit_tlx(ASYNCB_INITIALIZED, &port->flags);
		__clear_bit_tlx(TTY_IO_ERROR, &tty->flags);
	} else if (retval > 0)
		retval = 0;

	return retval;
}


int uart_open_tlx(struct tty_struct *tty, struct file *filp)
{
	struct uart_driver *drv = (struct uart_driver *)tty->driver->driver_state;
	int retval, line = tty->index;
	struct uart_state *state = drv->state + line;
	struct tty_port *port = &state->port;
	port->count++;
	tty->driver_data = state;
	state->uart_port->state = state;
	state->port.low_latency =
		(state->uart_port->flags & UPF_LOW_LATENCY) ? 1 : 0;
//	tty_port_tty_set(port, tty);
	port->tty = tty;
	retval = uart_startup_tlx(tty, state, 0);
end:
	return retval;
}

struct tty_operations uart_ops_tlx = {
	.open		= uart_open_tlx,
	.write		= uart_write_tlx,
};

#define TTY_DRIVER_REAL_RAW             0x0004
#define CLOCAL  0004000
#define HUPCL   0002000
#define CREAD   0000200
#define   CS8   0000060
#define  B9600  0000015
#define SERIAL_TYPE_NORMAL      1
#define TTY_DRIVER_TYPE_SERIAL          0x0003
#define TTYB_DEFAULT_MEM_LIMIT  65536


#define INIT_C_CC "\003\034\177\025\004\0\1\0\021\023\032\0\022\017\027\026\0"
#define  B38400 0000017

struct ktermios tty_std_termios_tlx =  {     /* for the benefit of tty drivers  */
         .c_iflag = ICRNL | IXON,
         .c_oflag = OPOST | ONLCR,
         .c_cflag = B38400 | CS8 | CREAD | HUPCL,
         .c_lflag = ISIG | ICANON | ECHO | ECHOE | ECHOK |
                    ECHOCTL | ECHOKE | IEXTEN,
         .c_cc = INIT_C_CC,
         .c_ispeed = 38400,
         .c_ospeed = 38400
};

int uart_register_driver_tlx(struct uart_driver *drv)
{
	struct tty_driver *normal;
	int i, retval;

//	BUG_ON(drv->state);
	drv->state = kzalloc_tlx(sizeof(struct uart_state) * drv->nr, GFP_KERNEL);
	normal =  __tty_alloc_driver_tlx(drv->nr, THIS_MODULE, 0); //alloc_tty_driver(drv->nr);
	drv->tty_driver = normal;
	normal->driver_name	= drv->driver_name;
	normal->name		= drv->dev_name;
	normal->major		= drv->major;
	normal->minor_start	= drv->minor;
	normal->type		= TTY_DRIVER_TYPE_SERIAL;
	normal->subtype		= SERIAL_TYPE_NORMAL;
	normal->init_termios	= tty_std_termios_tlx;
	normal->init_termios.c_cflag = B9600 | CS8 | CREAD | HUPCL | CLOCAL;
	normal->init_termios.c_ispeed = normal->init_termios.c_ospeed = 9600;
	normal->flags		= TTY_DRIVER_REAL_RAW | TTY_DRIVER_DYNAMIC_DEV;
	normal->driver_state    = drv;
//	tty_set_operations(normal, &uart_ops);
	normal->ops = &uart_ops_tlx;
	for (i = 0; i < drv->nr; i++) {
		struct uart_state *state = drv->state + i;
		struct tty_port *port = &state->port;

//		tty_port_init(port);
		memset_tlx(port, 0, sizeof(*port));
	//  tty_buffer_init(port);
		struct tty_bufhead *buf = &port->buf;
		mutex_init(&buf->lock);
	//  tty_buffer_reset(&buf->sentinel, 0);
		buf->head = &buf->sentinel;
		buf->tail = &buf->sentinel;
		init_llist_head(&buf->free);
		atomic_set(&buf->mem_used, 0);
		atomic_set(&buf->priority, 0);
		(&buf->work)->data = (atomic_long_t) ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL);
	INIT_LIST_HEAD(&(&buf->work)->entry);
	(&buf->work)->func = (flush_to_ldisc_tlx);

		buf->mem_limit = TTYB_DEFAULT_MEM_LIMIT;
		init_waitqueue_head(&port->open_wait);
		init_waitqueue_head(&port->close_wait);
		init_waitqueue_head(&port->delta_msr_wait);
		mutex_init(&port->mutex);
		mutex_init(&port->buf_mutex);
		spin_lock_init(&port->lock);
		port->close_delay = (50 * HZ) / 100;
		port->closing_wait = (3000 * HZ) / 100;
		kref_init_tlx(&port->kref);
		port->ops = &uart_port_ops_tlx;
		port->close_delay     = HZ / 2;	/* .5 seconds */
		port->closing_wait    = 30 * HZ;/* 30 seconds */
	}
	retval = tty_register_driver_tlx(normal);
	if (retval >= 0)
		return retval;

	for (i = 0; i < drv->nr; i++) {
	//			cancel_work_sync(&(&drv->state[i].port)->buf.work);
				tty_buffer_free_all_tlx(&drv->state[i].port);
	}
//		tty_port_destroy(&drv->state[i].port);
	return -ENOMEM;
}

static inline resource_size_t resource_size_tlx(const struct resource *res)
{
				return res->end - res->start + 1;
}

typedef void (*dr_release_t)(struct device *dev, void *res);

struct devres_node {
				struct list_head                entry;
				dr_release_t                    release;
#ifdef CONFIG_DEBUG_DEVRES
				const char                      *name;
				size_t                          size;
#endif
};

struct devres {
				struct devres_node              node;
				/* -- 3 pointers */
				unsigned long long              data[]; /* guarantee ull alignment */
};

#define UART_NR                 14

struct uart_amba_port *amba_ports_tlx[UART_NR];
#define EBUSY           16      /* Device or resource busy */

struct uart_amba_port {
	struct uart_port	port;
	struct clk		*clk;
	const struct vendor_data *vendor;
	unsigned int		dmacr;		/* dma control reg */
	unsigned int		im;		/* interrupt mask */
	unsigned int		old_status;
	unsigned int		fifosize;	/* vendor-specific */
	unsigned int		lcrh_tx;	/* vendor-specific */
	unsigned int		lcrh_rx;	/* vendor-specific */
	unsigned int		old_cr;		/* state during shutdown */
	bool			autorts;
	char			type[12];
#ifdef CONFIG_DMA_ENGINE
	/* DMA stuff */
	bool			using_tx_dma;
	bool			using_rx_dma;
	struct pl011_dmarx_data dmarx;
	struct pl011_dmatx_data	dmatx;
#endif
};

#define PORT_AMBA       32

void pl011_config_port_tlx(struct uart_port *port, int flags)
{
	if (flags & UART_CONFIG_TYPE) {
		port->type = PORT_AMBA;
//		pl011_request_port(port);
	}
}

#define UART011_IBRD            0x24    /* Integer baud rate divisor register. */
#define UART011_FBRD            0x28    /* Fractional baud rate divisor register. */

static inline void __raw_writew(u16 val, volatile void __iomem *addr)
{
				asm volatile("strh %w0, [%1]" : : "r" (val), "r" (addr));
}

static inline u16 __raw_readw(const volatile void __iomem *addr)
{
         u16 val;
         asm volatile("ldrh %w0, [%1]" : "=r" (val) : "r" (addr));
         return val;
}


#ifdef __LITTLE_ENDIAN
#define __cpu_to_le16(x) ((__force __le16)(__u16)(x))
#define __le16_to_cpu(x) ((__force __u16)(__le16)(x))
#else
#define __cpu_to_le16(x) ((__force __le16)__swab16((x)))
#define __le16_to_cpu(x) __swab16((__force __u16)(__le16)(x))
#endif

#define UART011_CR              0x30    /* Control register. */
#define UART011_DR_OE           (1 << 11)

#define wmb()           dsb(st)
#define cpu_to_le16 __cpu_to_le16
#define __iowmb()               wmb()
#define writew_relaxed(v,c)     ((void)__raw_writew((__force u16)cpu_to_le16(v),(c)))
#define writew(v,c)             ({ __iowmb(); writew_relaxed((v),(c)); })
#define DIV_ROUND_CLOSEST(x, divisor)(                  \
{                                                       \
     typeof(x) __x = x;                              \
       typeof(divisor) __d = divisor;                  \
       (((typeof(x))-1) > 0 ||                         \
          ((typeof(divisor))-1) > 0 || (__x) > 0) ?      \
                 (((__x) + ((__d) / 2)) / (__d)) :       \
                 (((__x) - ((__d) / 2)) / (__d));        \
}                                                       \
 )

static inline u16 readw(const volatile void __iomem *addr)
{
         return __le16_to_cpu(__raw_readw(addr));
}



void
pl011_set_termios_tlx(struct uart_port *port, struct ktermios *termios,
				struct ktermios *old)
{
	struct uart_amba_port *uap = (struct uart_amba_port *)port;
	unsigned int lcr_h, old_cr;
	unsigned long flags;
	unsigned int baud, quot, clkdiv;

	if (uap->vendor->oversampling)
		clkdiv = 8;
	else
		clkdiv = 16;
	baud = 9600;

	if (baud > port->uartclk/16)
		quot = DIV_ROUND_CLOSEST(port->uartclk * 8, baud);
	else
		quot = DIV_ROUND_CLOSEST(port->uartclk * 4, baud);

	port->read_status_mask = UART011_DR_OE | 255;
	port->ignore_status_mask = 0;
	old_cr = readw(port->membase + UART011_CR);
	writew(0, port->membase + UART011_CR);
	writew(quot & 0x3f, port->membase + UART011_FBRD);
	writew(quot >> 6, port->membase + UART011_IBRD);
	writew(old_cr, port->membase + UART011_CR);
}

#define TIOCM_OUT1      0x2000
#define TIOCM_OUT2      0x4000
#define UART011_CR_RTS          0x0800  /* RTS */
#define UART011_CR_DTR          0x0400  /* DTR */

#define UART011_CR_OUT1         0x1000  /* OUT1 */
#define UART011_CR_OUT2         0x2000  /* OUT2 */
#define UART011_CR_RTSEN        0x4000  /* RTS hardware flow control */
#define UART011_CR_LBE          0x0080  /* loopback enable */
#define TIOCM_LOOP      0x8000
#define TIOCM_RTS       0x004

void pl011_set_mctrl_tlx(struct uart_port *port, unsigned int mctrl)
{
	struct uart_amba_port *uap = (struct uart_amba_port *)port;
	unsigned int cr;

	cr = readw(uap->port.membase + UART011_CR);

#define	TIOCMBIT(tiocmbit, uartbit)		\
	if (mctrl & tiocmbit)		\
		cr |= uartbit;		\
	else				\
		cr &= ~uartbit

	TIOCMBIT(TIOCM_RTS, UART011_CR_RTS);
	TIOCMBIT(TIOCM_DTR, UART011_CR_DTR);
	TIOCMBIT(TIOCM_OUT1, UART011_CR_OUT1);
	TIOCMBIT(TIOCM_OUT2, UART011_CR_OUT2);
	TIOCMBIT(TIOCM_LOOP, UART011_CR_LBE);

	if (uap->autorts) {
		/* We need to disable auto-RTS if we want to turn RTS off */
		TIOCMBIT(TIOCM_RTS, UART011_CR_RTSEN);
	}
#undef TIOCMBIT

	writew(cr, uap->port.membase + UART011_CR);
}

#define UART011_IMSC            0x38    /* Interrupt mask. */
#define UART011_ICR             0x44    /* Interrupt clear register. */
#define UART011_TXIM            (1 << 5)        /* transmit interrupt mask */

void pl011_start_tx_tlx(struct uart_port *port)
{
	struct uart_amba_port *uap = (struct uart_amba_port *)port;

//	if (!pl011_dma_tx_start(uap)) {
		uap->im |= UART011_TXIM;
		writew(uap->im, uap->port.membase + UART011_IMSC);
//	}
}

#define UART01x_FR_DCD          0x004
#define UART01x_FR_DSR          0x002
#define UART01x_FR_CTS          0x001
#define UART011_OEIS            (1 << 10)       /* overrun error interrupt status */
#define UART011_BEIS            (1 << 9)        /* break error interrupt status */
#define UART011_PEIS            (1 << 8)        /* parity error interrupt status */
#define UART011_FEIS            (1 << 7)        /* framing error interrupt status */
#define UART011_RTIS            (1 << 6)        /* receive timeout interrupt status */
#define UART011_RXIS            (1 << 4)        /* receive interrupt status */
#define UART011_RTIM            (1 << 6)        /* receive timeout interrupt mask */
#define UART011_RXIM            (1 << 4)        /* receive interrupt mask */
#define UART011_IFLS            0x34    /* Interrupt fifo level select. */
#define UART01x_CR_UARTEN       0x0001  /* UART enable */
#define UART011_CR_TXE          0x0100  /* transmit enable */
#define UART01x_DR              0x00    /* Data read or written from the interface. */
#define UART01x_FR              0x18    /* Flag register (Read only). */
#define UART01x_FR_BUSY         0x008
#define UART011_CR_RXE          0x0200  /* receive enable */
#define UART01x_FR_MODEM_ANY    (UART01x_FR_DCD|UART01x_FR_DSR|UART01x_FR_CTS)
#define UART_DUMMY_DR_RX        (1 << 16)
#define UART_DR_ERROR           (UART011_DR_OE|UART011_DR_BE|UART011_DR_PE|UART011_DR_FE)
#define UART011_DR_BE           (1 << 10)
#define UART011_DR_FE           (1 << 8)
#define UART011_DR_PE           (1 << 9)
#define UART01x_FR_RXFE         0x010
#define TTY_BREAK       1
#define TTY_FRAME       2
#define TTY_PARITY      3

enum irqreturn {
     IRQ_NONE                = (0 << 0),
     IRQ_HANDLED             = (1 << 0),
     IRQ_WAKE_THREAD         = (1 << 1),
};

typedef enum irqreturn irqreturn_t;

typedef irqreturn_t (*irq_handler_t)(int, void *);



int request_threaded_irq_tlx(unsigned int irq, irq_handler_t handler,
                          irq_handler_t thread_fn, unsigned long irqflags,
                          const char *devname, void *dev_id);




int pl011_fifo_to_tty_tlx(struct uart_amba_port *uap)
{
	u16 status, ch;
	unsigned int flag, max_count = 256;
	int fifotaken = 0;

	while (max_count--) {
		status = readw(uap->port.membase + UART01x_FR);
		if (status & UART01x_FR_RXFE)
			break;

		/* Take chars from the FIFO and update status */
		ch = readw(uap->port.membase + UART01x_DR) |
			UART_DUMMY_DR_RX;
		flag = TTY_NORMAL;
		uap->port.icount.rx++;
		fifotaken++;

		if (unlikely(ch & UART_DR_ERROR)) {
			if (ch & UART011_DR_BE) {
				ch &= ~(UART011_DR_FE | UART011_DR_PE);
				uap->port.icount.brk++;
//				if (uart_handle_break(&uap->port))
//					continue;
			} else if (ch & UART011_DR_PE)
				uap->port.icount.parity++;
			else if (ch & UART011_DR_FE)
				uap->port.icount.frame++;
			if (ch & UART011_DR_OE)
				uap->port.icount.overrun++;

			ch &= uap->port.read_status_mask;

			if (ch & UART011_DR_BE)
				flag = TTY_BREAK;
			else if (ch & UART011_DR_PE)
				flag = TTY_PARITY;
			else if (ch & UART011_DR_FE)
				flag = TTY_FRAME;
		}

	//	if (uart_handle_sysrq_char(&uap->port, ch & 255))
	//		continue;

		uart_insert_char_tlx(&uap->port, ch, UART011_DR_OE, ch, flag);
	}

	return fifotaken;
}

void pl011_rx_chars_tlx(struct uart_amba_port *uap)
{
	pl011_fifo_to_tty_tlx(uap);

	spin_unlock_tlx(&uap->port.lock);
//	tty_flip_buffer_push(&uap->port.state->port);
	struct tty_port *port = &uap->port.state->port;
	struct tty_bufhead *buf = &port->buf;
	buf->tail->commit = buf->tail->used;
//  schedule_work(&buf->work);
//	queue_work(system_wq_tlx, &buf->work);
	if (!test_and_set_bit_tlx(WORK_STRUCT_PENDING_BIT, work_data_bits(&buf->work))) {
								__queue_work_tlx(WORK_CPU_UNBOUND, system_wq_tlx, &buf->work);
	}
	spin_lock_tlx(&uap->port.lock);
}

void pl011_stop_tx_tlx(struct uart_port *port)
{
	struct uart_amba_port *uap = (struct uart_amba_port *)port;

	uap->im &= ~UART011_TXIM;
	writew(uap->im, uap->port.membase + UART011_IMSC);
//	pl011_dma_tx_stop(uap);
}


int uart_tx_stopped_tlx(struct uart_port *port)
{
				struct tty_struct *tty = port->state->port.tty;
				if(tty->stopped || tty->hw_stopped)
							return 1;
				return 0;
}

#define uart_circ_empty(circ)           ((circ)->head == (circ)->tail)



void pl011_tx_chars_tlx(struct uart_amba_port *uap)
{
	struct circ_buf *xmit = &uap->port.state->xmit;
	int count;

	if (uap->port.x_char) {
		writew(uap->port.x_char, uap->port.membase + UART01x_DR);
		uap->port.icount.tx++;
		uap->port.x_char = 0;
		return;
	}
	if (uart_circ_empty(xmit) || uart_tx_stopped_tlx(&uap->port)) {
		pl011_stop_tx_tlx(&uap->port);
		return;
	}
	count = uap->fifosize >> 1;
	do {
		writew(xmit->buf[xmit->tail], uap->port.membase + UART01x_DR);
		xmit->tail = (xmit->tail + 1) & (UART_XMIT_SIZE - 1);
		uap->port.icount.tx++;
		if (uart_circ_empty(xmit))
			break;
	} while (--count > 0);


}

#define AMBA_ISR_PASS_LIMIT     256
#define UART011_MIS             0x40    /* Masked interrupt status. */
#define UART011_TXIS            (1 << 5)        /* transmit interrupt status */
#define IRQ_RETVAL(x)   ((x) ? IRQ_HANDLED : IRQ_NONE)


irqreturn_t pl011_int_tlx(int irq, void *dev_id)
{
	struct uart_amba_port *uap = dev_id;
	unsigned long flags;
	unsigned int status, pass_counter = AMBA_ISR_PASS_LIMIT;
	int handled = 0;
	unsigned int dummy_read;

	raw_spin_lock_irqsave(&uap->port.lock, flags);
	status = readw(uap->port.membase + UART011_MIS);
	if (status) {
		do {
			if (uap->vendor->cts_event_workaround) {
				/* workaround to make sure that all bits are unlocked.. */
				writew(0x00, uap->port.membase + UART011_ICR);
				dummy_read = readw(uap->port.membase + UART011_ICR);
				dummy_read = readw(uap->port.membase + UART011_ICR);
			}
			writew(status & ~(UART011_TXIS|UART011_RTIS|
						UART011_RXIS),
						uap->port.membase + UART011_ICR);

			if (status & (UART011_RTIS|UART011_RXIS)) {
					pl011_rx_chars_tlx(uap);
			}
			if (status & UART011_TXIS)
				pl011_tx_chars_tlx(uap);

			if (pass_counter-- == 0)
				break;

			status = readw(uap->port.membase + UART011_MIS);
		} while (status != 0);
		handled = 1;
	}

	raw_spin_unlock_irqrestore(&uap->port.lock, flags);

	return IRQ_RETVAL(handled);
}



int pl011_startup_tlx(struct uart_port *port)
{
	struct uart_amba_port *uap = (struct uart_amba_port *)port;
	unsigned int cr, lcr_h, fbrd, ibrd;
	int retval;

//	retval = pl011_hwinit(port);
//	struct uart_amba_port *uap = (struct uart_amba_port *)port;
	writew(UART011_OEIS | UART011_BEIS | UART011_PEIS | UART011_FEIS |
				UART011_RTIS | UART011_RXIS, uap->port.membase + UART011_ICR);
	uap->im = readw(uap->port.membase + UART011_IMSC);
	writew(UART011_RTIM | UART011_RXIM, uap->port.membase + UART011_IMSC);
	writew(uap->im, uap->port.membase + UART011_IMSC);
	request_threaded_irq_tlx(uap->port.irq, pl011_int_tlx, NULL,  0, "uart-pl011", uap);
	writew(uap->vendor->ifls, uap->port.membase + UART011_IFLS);
	fbrd = readw(uap->port.membase + UART011_FBRD);
	ibrd = readw(uap->port.membase + UART011_IBRD);
	lcr_h = readw(uap->port.membase + uap->lcrh_rx);
	cr = UART01x_CR_UARTEN | UART011_CR_TXE | UART011_CR_LBE;
	writew(cr, uap->port.membase + UART011_CR);
	writew(0, uap->port.membase + UART011_FBRD);
	writew(1, uap->port.membase + UART011_IBRD);
//	pl011_write_lcr_h(uap, 0);
	writew(0, uap->port.membase + UART01x_DR);
	while (readw(uap->port.membase + UART01x_FR) & UART01x_FR_BUSY)
		barrier();
	writew(fbrd, uap->port.membase + UART011_FBRD);
	writew(ibrd, uap->port.membase + UART011_IBRD);
//	pl011_write_lcr_h(uap, lcr_h);

	/* restore RTS and DTR */
	cr = uap->old_cr & (UART011_CR_RTS | UART011_CR_DTR);
	cr |= UART01x_CR_UARTEN | UART011_CR_RXE | UART011_CR_TXE;
	writew(cr, uap->port.membase + UART011_CR);
	uap->old_status = readw(uap->port.membase + UART01x_FR) & UART01x_FR_MODEM_ANY;

	/* Startup DMA */
//	pl011_dma_startup(uap);
	/* Clear out any spuriously appearing RX interrupts */
	writew(UART011_RTIS | UART011_RXIS,
		uap->port.membase + UART011_ICR);
	uap->im = UART011_RTIM;
	uap->im |= UART011_RXIM;
	writew(uap->im, uap->port.membase + UART011_IMSC);

	return 0;

}

struct uart_ops amba_pl011_pops_tlx = {
	.set_mctrl	= pl011_set_mctrl_tlx,
	.start_tx	= pl011_start_tx_tlx,
	.startup	= pl011_startup_tlx,
//	.flush_buffer	= pl011_dma_flush_buffer,
	.set_termios	= pl011_set_termios_tlx,
//	.request_port	= pl011_request_port,
	.config_port	= pl011_config_port_tlx,
};

struct clk *clk_get_tlx(struct device *dev, const char *con_id);




int snprintf_tlx(char *buf, size_t size, const char *fmt, ...)
 {
         va_list args;
         int i;

         va_start(args, fmt);
         i = vsnprintf_tlx(buf, size, fmt, args);
         va_end(args);

         return i;
}

#define amba_rev(d)     AMBA_REV_BITS((d)->periphid)

struct tty_driver *uart_console_device_tlx(struct console *co, int *index)
{
         struct uart_driver *p = co->data;
         *index = co->index;
         return p->tty_driver;
}


#define SERIAL_AMBA_MAJOR       204
#define SERIAL_AMBA_MINOR       64

static struct uart_driver amba_reg_tlx;

struct console amba_console_tlx = {
	.name		= "ttyAMA",
	.device		= uart_console_device_tlx,
	.flags		= CON_PRINTBUFFER,
	.index		= -1,
	.data		= &amba_reg_tlx,
};

static struct uart_driver amba_reg_tlx = {
         .owner                  = THIS_MODULE,
         .driver_name            = "ttyAMA",
         .dev_name               = "ttyAMA",
         .major                  = SERIAL_AMBA_MAJOR,
         .minor                  = SERIAL_AMBA_MINOR,
         .nr                     = UART_NR,
         .cons                   = &amba_console_tlx,
};



int pl011_probe_tlx(struct amba_device *dev, const struct amba_id *id)
{
	struct uart_amba_port *uap;
	struct vendor_data *vendor = id->data;
	void __iomem *base;
	int i, ret;

	for (i = 0; i < ARRAY_SIZE(amba_ports_tlx); i++)
		if (amba_ports_tlx[i] == NULL)
			break;


//	uap = devm_kzalloc(&dev->dev, sizeof(struct uart_amba_port),
	//		   GFP_KERNEL);
	struct devres *dr;
	size_t tot_size = sizeof(struct devres) + sizeof(struct uart_amba_port);
	dr = kzalloc_tlx(tot_size, GFP_KERNEL | __GFP_ZERO);
//	 dr = alloc_dr(devm_kmalloc_release, sizeof(struct uart_amba_port), GFP_KERNEL | __GFP_ZERO);
		uap = dr->data;
	if (uap == NULL) {
		ret = -ENOMEM;
		goto out;
	}

//	i = pl011_probe_dt_alias(i, &dev->dev);

//	base = devm_ioremap(&dev->dev, dev->res.start,
//			    resource_size(&dev->res));
	base =  __ioremap_tlx(dev->res.start,
						resource_size_tlx(&dev->res), __pgprot(PROT_DEVICE_nGnRE));
//	ioremap(dev->res.start,
//					resource_size(&dev->res));
//	uap->clk = devm_clk_get(&dev->dev, NULL);
	struct device *dev_ = &dev->dev;
	const char *id_ = NULL;
	struct clk **ptr, *clk;
//  ptr = devres_alloc(devm_clk_release, sizeof(*ptr), GFP_KERNEL);
	uap->clk = clk_get_tlx(dev_, id_);
	uap->vendor = vendor;
	uap->lcrh_rx = vendor->lcrh_rx;
	uap->lcrh_tx = vendor->lcrh_tx;
	uap->old_cr = 0;
	uap->fifosize = vendor->get_fifosize(dev);
	uap->port.dev = &dev->dev;
	uap->port.mapbase = dev->res.start;
	uap->port.membase = base;
	uap->port.iotype = UPIO_MEM;
	uap->port.irq = dev->irq[0];
	uap->port.fifosize = uap->fifosize;
	uap->port.ops = &amba_pl011_pops_tlx;
	uap->port.flags = UPF_BOOT_AUTOCONF;
	uap->port.line = i;
	/* Ensure interrupts from this UART are masked and cleared */
	writew(0, uap->port.membase + UART011_IMSC);
	writew(0xffff, uap->port.membase + UART011_ICR);

	snprintf_tlx(uap->type, sizeof(uap->type), "PL011 rev%u", amba_rev(dev));

	amba_ports_tlx[i] = uap;

//	amba_set_drvdata(dev, uap);
	(&dev->dev)->driver_data = uap;
	if (!amba_reg_tlx.state) {
		ret = uart_register_driver_tlx(&amba_reg_tlx);
	}

	ret = uart_add_one_port_tlx(&amba_reg_tlx, &uap->port);
out:
#ifndef MODULE
//	printk_tlx(KERN_ERR "WIth out module=================== \n");
#endif
	return ret;
}



struct amba_driver_tlx pl011_driver_tlx = {
	.drv = {
		.name	= "uart-pl011",
	},
	.id_table	= pl011_ids_tlx,
	.probe		= pl011_probe_tlx,
	.remove		= NULL,
};

const struct of_device_id_tlx of_default_bus_match_table_tlx[]  = {
	{ .compatible = "simple-bus", },
#ifdef CONFIG_ARM_AMBA
#endif /* CONFIG_ARM_AMBA */
	{} /* Empty terminated list */
};

struct clk *clk_get_tlx(struct device *dev, const char *con_id)
{
	const char *dev_id = dev ? dev_name(dev) : NULL;
	struct device_node *np =dev->of_node;
	const char *name = con_id;
		struct clk *clk = ERR_PTR_tlx(-ENOENT);
		while (np) {
			int index = 0;
			if (name) {
				const char *propname = "clock-names";
				const char *string = name;
					struct property *prop;
					struct property *pp;
					for (pp = np->properties; pp; pp = pp->next) {
										if (strcmp_tlx(pp->name, propname) == 0) {
														break;
										}
					}
					prop = pp;
					size_t l;
					int i;
					const char *p, *end;
					p = prop->value;
					end = p + prop->length;
					for (i = 0; p < end; i++, p += l) {
						l = strlen_tlx(p) + 1;
						if (strcmp_tlx(string, p) == 0) {
							index = i; /* Found it; return index */
							break;
						}
					}
				}
				struct of_phandle_args clkspec;
				const char *list_name = "clocks";
				const char *cells_name ="#clock-cells";
				int cell_count = 0;
				struct of_phandle_args *out_args = &clkspec;
				const __be32 *list, *list_end;
				int rc = 0, size, cur_index = 0;
				uint32_t count = 0;
				struct device_node *node = NULL;
				phandle phandle;
				struct property *pp;
				for (pp = np->properties; pp; pp = pp->next) {
					if (strcmp_tlx(pp->name, list_name) == 0) {
													size = pp->length;
													break;
									}
				}
				list = pp ? pp->value : NULL;
				list_end = list + size / sizeof(*list);
						/* Loop over the phandles until all the requested entry is found */
				while (list < list_end) {
							rc = -EINVAL;
							count = 0;
							phandle = be32_to_cpup(list++);
							if (phandle) {
								if (cells_name || cur_index == index) {
									for (node = of_allnodes_tlx; node; node = node->allnext)
													if (node->phandle == phandle)
																	break;
										if (node)
                 				kobject_get_tlx(&node->kobj);
								}
								if (cells_name) {
								} else {
									count = cell_count;
								}
							}
							rc = -ENOENT;
							if (cur_index == index) {
								if (out_args) {
									int i;
									out_args->np = node;
									out_args->args_count = count;
									for (i = 0; i < count; i++)
										out_args->args[i] = be32_to_cpup(list++);
								} else
								break;
							}
							node = NULL;
							list += count;
							cur_index++;
						}
					struct of_clk_provider *provider;
					list_for_each_entry(provider, &of_clk_providers_tlx, link) {
						if (provider->node == clkspec.np)
							clk = provider->get(&clkspec, provider->data);
					}
			if (!IS_ERR_tlx(clk))
				break;
				for (pp = np->properties; pp; pp = pp->next) {
									if (strcmp_tlx(pp->name, name) == 0) {
													break;
									}
				}
				return pp ? pp->value : NULL;
				break;
		}
	return clk;
}


struct device_node *of_get_next_child_tlx(const struct device_node *node,
						struct device_node *prev)
{
	struct device_node *next;

	if (!node)
		return NULL;
	next = prev ? prev->sibling : node->child;
	for (; next; next = next->sibling) {
		if (next)
			break;
	}
	return next;
}

#define for_each_child_of_node(parent, child) \
         for (child = of_get_next_child_tlx(parent, NULL); child != NULL; \
              child = of_get_next_child_tlx(parent, child))

struct vdso_data {
       __u64 cs_cycle_last;    /* Timebase at clocksource init */
       __u64 xtime_clock_sec;  /* Kernel time */
       __u64 xtime_clock_nsec;
       __u64 xtime_coarse_sec; /* Coarse time */
       __u64 xtime_coarse_nsec;
       __u64 wtm_clock_sec;    /* Wall to monotonic time */
       __u64 wtm_clock_nsec;
       __u32 tb_seq_count;     /* Timebase sequence counter */
       __u32 cs_mult;          /* Clocksource multiplier */
       __u32 cs_shift;         /* Clocksource shift */
       __u32 tz_minuteswest;   /* Whacky timezone stuff */
       __u32 tz_dsttime;
       __u32 use_syscall;
};

#define __page_aligned_data     __section(.data..page_aligned) __aligned(PAGE_SIZE)

static union {
         struct vdso_data        data;
          u8                      page[PAGE_SIZE];
} vdso_data_store_tlx __page_aligned_data;

 struct vdso_data *vdso_data_tlx = &vdso_data_store_tlx.data;


extern char vdso_start_tlx, vdso_end_tlx;
unsigned long vdso_pages_tlx;
struct page **vdso_pagelist_tlx;



struct amba_driver {
          struct device_driver    drv;
         int                     (*probe)(struct amba_device *, const struct amba_id *);
         int                     (*remove)(struct amba_device *);
         void                    (*shutdown)(struct amba_device *);
         int                     (*suspend)(struct amba_device *, pm_message_t);
         int                     (*resume)(struct amba_device *);
         const struct amba_id    *id_table;
 };

struct amba_id *
amba_lookup_tlx(const struct amba_id *table, struct amba_device *dev)
{
				int ret = 0;

				while (table->mask) {
								ret = (dev->periphid & table->mask) == table->id;
								if (ret)
												break;
								table++;
				}

				return ret ? table : NULL;
}

static int amba_match_tlx(struct device *dev, struct device_driver *drv)
{
         struct amba_device *pcdev =  container_of(dev, struct amba_device, dev);
         struct amba_driver *pcdrv = container_of(drv, struct amba_driver, drv);

         return amba_lookup_tlx(pcdrv->id_table, pcdev) != NULL;
}


struct bus_type amba_bustype_tlx = {
	.name		= "amba",
	.match		= amba_match_tlx,
};

struct device_private {
	struct klist klist_children;
	struct klist_node knode_parent;
	struct klist_node knode_driver;
	struct klist_node knode_bus;
	struct list_head deferred_probe;
	struct device *device;
};

void klist_devices_get_tlx(struct klist_node *n)
{
	struct device_private *dev_prv = container_of(n, struct device_private, knode_bus);
	struct device *dev = dev_prv->device;
	kobject_get_tlx(&dev->kobj);
}

void klist_devices_put_tlx(struct klist_node *n)
{
	struct device_private *dev_prv = container_of(n, struct device_private, knode_bus);
	struct device *dev = dev_prv->device;
	if (dev)
								kobject_put_tlx(&dev->kobj);
}

struct kset *bus_kset_tlx;
struct kobj_type bus_ktype_tlx;





#define KNODE_DEAD              1LU
static struct klist_node *to_klist_node_tlx(struct list_head *n)
 {
         return container_of(n, struct klist_node, n_node);
 }



struct klist_node *klist_next_tlx(struct klist_iter *i)
{
	void (*put)(struct klist_node *) = i->i_klist->put;
	struct klist_node *last = i->i_cur;
	struct klist_node *next;

	spin_lock_tlx(&i->i_klist->k_lock);

	if (last) {
		next = to_klist_node_tlx(last->n_node.next);
	//	if (!klist_dec_and_del(last))
	//		if (kref_put(&last->n_ref, klist_release))
			//	if (kref_sub(&last->n_ref, 1, klist_release))
				if (atomic_sub_and_test((int) 1, &(&last->n_ref)->refcount)) {
//					klist_release(&last->n_ref);
				} else {
					put = NULL;
				}
	} else
		next = to_klist_node_tlx(i->i_klist->k_list.next);

	i->i_cur = NULL;
	while (next != to_klist_node_tlx(&i->i_klist->k_list)) {
		if (likely(!((unsigned long)next->n_klist & KNODE_DEAD))) {
			atomic_inc_return(&(&next->n_ref)->refcount);
			i->i_cur = next;
			break;
		}
		next = to_klist_node_tlx(next->n_node.next);
	}

	spin_unlock_tlx(&i->i_klist->k_lock);

	if (put && last)
		put(last);
	return i->i_cur;
}


static struct device *next_device_tlx(struct klist_iter *i)
{
	struct klist_node *n = klist_next_tlx(i);
	struct device *dev = NULL;
	struct device_private *dev_prv;

	if (n) {
		dev_prv = container_of(n, struct device_private, knode_bus);
		dev = dev_prv->device;
	}
	return dev;
}

int bus_for_each_dev_tlx(struct bus_type *bus, struct device *start,
				void *data, int (*fn)(struct device *, void *))
{
	struct klist_iter i;
	struct device *dev;
	int error = 0;
	struct klist *k = &bus->p->klist_devices;
	struct klist_node *n = start ? &start->p->knode_bus : NULL;
	i.i_klist = k;
	i.i_cur = n;
	while ((dev = next_device_tlx(&i)) && !error)
		error = fn(dev, data);
	return error;
}


int __driver_attach_tlx(struct device *dev, void *data)
{
	struct device_driver *drv = data;
	if (!(drv->bus->match ? drv->bus->match(dev, drv) : 1))
		return 0;
	if (!dev->driver) {
				dev->driver = drv;
				if (dev->bus->probe) {
					dev->bus->probe(dev);
				} else if (drv->probe) {
					drv->probe(dev);
				}
	}
//		really_probe(dev, drv);
	return 0;
}

struct kobj_type driver_ktype_tlx;// = {
//	.sysfs_ops	= &driver_sysfs_ops,
//	.release	= driver_release,
//};

int driver_register_tlx(struct device_driver *drv)
{
	struct bus_type *bus;
	struct driver_private *priv;
	int error = 0;

	bus = drv->bus;
	priv = kzalloc_tlx(sizeof(*priv), GFP_KERNEL);
	klist_init_tlx(&priv->klist_devices, NULL, NULL);
	priv->driver = drv;
	drv->p = priv;
	priv->kobj.kset = bus->p->drivers_kset;
	error = kobject_init_and_add_tlx(&priv->kobj, &driver_ktype_tlx, NULL,
						"%s", drv->name);
	klist_add_tail_tlx(&priv->knode_bus, &bus->p->klist_drivers);
	if (drv->bus->p->drivers_autoprobe) {
//		error = driver_attach(drv);
			bus_for_each_dev_tlx(drv->bus, NULL, drv, __driver_attach_tlx);
	}
};

int bus_register_tlx(struct bus_type *bus)
{
	int retval;
	struct subsys_private *priv;
	struct lock_class_key *key = &bus->lock_key;

	priv = kzalloc_tlx(sizeof(struct subsys_private), GFP_KERNEL);
	priv->bus = bus;
	bus->p = priv;
	BLOCKING_INIT_NOTIFIER_HEAD(&priv->bus_notifier);
	retval = kobject_set_name_tlx(&priv->subsys.kobj, "%s", bus->name);
	priv->subsys.kobj.kset = bus_kset_tlx;
	priv->subsys.kobj.ktype = &bus_ktype_tlx;
	priv->drivers_autoprobe = 1;
	retval = kset_register_tlx(&priv->subsys);
	priv->devices_kset_tlx = kset_create_and_add_tlx("devices", NULL,
						&priv->subsys.kobj);
	priv->drivers_kset = kset_create_and_add_tlx("drivers", NULL,
						&priv->subsys.kobj);
	INIT_LIST_HEAD(&priv->interfaces);
	__mutex_init_tlx(&priv->mutex, "subsys mutex", key);
	klist_init_tlx(&priv->klist_devices, klist_devices_get_tlx, klist_devices_put_tlx);
	klist_init_tlx(&priv->klist_drivers, NULL, NULL);
	return 0;

}


#define MAX_ARG_STRLEN (PAGE_SIZE * 32)
#define EXSTACK_DEFAULT   0     /* Whatever the arch defaults to */
#define EXSTACK_DISABLE_X 1     /* Disable executable stacks */
#define EXSTACK_ENABLE_X  2     /* Enable executable stacks */
#define BINPRM_FLAGS_EXECFD_BIT 1
#define BINPRM_FLAGS_EXECFD (1 << BINPRM_FLAGS_EXECFD_BIT)
#define BINPRM_BUF_SIZE 128

struct linux_binprm {
	char buf[BINPRM_BUF_SIZE];
#ifdef CONFIG_MMU
	struct vm_area_struct *vma;
	unsigned long vma_pages;
#else
# define MAX_ARG_PAGES	32
	struct page *page[MAX_ARG_PAGES];
#endif
	struct mm_struct *mm;
	unsigned long p; /* current top of mem */
	unsigned int
		cred_prepared:1,/* true if creds already prepared (multiple
				* preps happen for interpreters) */
		cap_effective:1;/* true if has elevated effective capabilities,
				* false if not; except for init which inherits
				* its parent's caps anyway */
#ifdef __alpha__
	unsigned int taso:1;
#endif
	unsigned int recursion_depth; /* only for search_binary_handler() */
	struct file * file;
	struct cred *cred;	/* new credentials */
	int unsafe;		/* how unsafe this exec is (mask of LSM_UNSAFE_*) */
	unsigned int per_clear;	/* bits to clear in current->personality */
	int argc, envc;
	const char * filename;	/* Name of binary as seen by procps */
	const char * interp;	/* Name of the binary really executed. Most
					of the time same as filename, but could be
					different for binfmt_{misc,script} */
	unsigned interp_flags;
	unsigned interp_data;
	unsigned long loader, exec;
};

struct linux_binfmt {
	struct list_head lh;
	struct module *module;
	int (*load_binary)(struct linux_binprm *);
	int (*load_shlib)(struct file *);
	int (*core_dump)(struct coredump_params *cprm);
	unsigned long min_coredump;	/* minimal dump size */
};

int load_elf_binary_tlx(struct linux_binprm *bprm);

struct linux_binfmt elf_format_tlx = {
	.module		= THIS_MODULE,
	.load_binary	= load_elf_binary_tlx,
	.load_shlib	= NULL,
	.core_dump	= NULL,
	.min_coredump	= ELF_EXEC_PAGESIZE,
};





void process_one_work_tlx(struct worker *worker, struct work_struct *work)
__releases(&pool->lock)
__acquires(&pool->lock)
{
	struct pool_workqueue *pwq  = NULL;// = get_work_pwq(work);
	unsigned long data = atomic_long_read_tlx(&work->data);
	if (data & WORK_STRUCT_PWQ)
			pwq  = (void *)(data & WORK_STRUCT_WQ_DATA_MASK);
					struct worker_pool *pool = worker->pool;
	bool cpu_intensive = pwq->wq->flags & WQ_CPU_INTENSIVE;
	struct worker *collision;
	struct worker *worker_ = NULL;
	hash_for_each_possible(pool->busy_hash, worker_, hentry,
															(unsigned long)work)
				if (worker_->current_work == work &&
							worker_->current_func == work->func)
												break;
	collision = worker_;
	if (unlikely(collision)) {
		struct list_head *head = &collision->scheduled;
		struct work_struct **nextp = NULL;
			struct work_struct *n;
			list_for_each_entry_safe_from(work, n, NULL, entry) {
				list_move_tail(&work->entry, head);
				if (!(*((unsigned long *)(&(work)->data)) & WORK_STRUCT_LINKED))
					break;
			}
			if (nextp)
				*nextp = n;
		return;
	}
	hash_add(pool->busy_hash, &worker->hentry, (unsigned long)work);
	worker->current_work = work;
	worker->current_func = work->func;
	worker->current_pwq = pwq;
	list_del_init(&work->entry);
	smp_wmb();
	atomic_long_set_tlx(&work->data, (unsigned long)pool->id << WORK_OFFQ_POOL_SHIFT |
								*(unsigned long *)(&(work)->data) & WORK_STRUCT_STATIC);
	spin_unlock_irq_tlx(&pool->lock);
	lock_map_acquire_read(&pwq->wq->lockdep_map);
	lock_map_acquire(&lockdep_map);
	worker->current_func(work);
	lock_map_release(&lockdep_map);
	lock_map_release(&pwq->wq->lockdep_map);
	cond_resched();
	spin_lock_irq_tlx(&pool->lock);
	hash_del(&worker->hentry);
	worker->current_work = NULL;
	worker->current_func = NULL;
	worker->current_pwq = NULL;
	worker->desc_valid = false;
}

int worker_thread_tlx(void *__worker)
{
	struct worker *worker = __worker;
	struct worker_pool *pool = worker->pool;
	worker->task->flags |= PF_WQ_WORKER;
woke_up:
	spin_lock_irq_tlx(&pool->lock);
	pool = worker->pool;
	worker->flags &= ~(WORKER_IDLE);
	pool->nr_idle--;
	list_del_init(&worker->entry);

recheck:
	if (!(!list_empty(&pool->worklist) && !atomic_read(&pool->nr_running)))
		goto sleep;
	pool = worker->pool;
	unsigned int oflags = worker->flags;
	worker->flags &= ~(worker, WORKER_PREP | WORKER_REBOUND);
	do {
		struct work_struct *work =
			list_first_entry(&pool->worklist,
					struct work_struct, entry);
		if (likely(!(*work_data_bits(work) & WORK_STRUCT_LINKED))) {
			process_one_work_tlx(worker, work);
			if (unlikely(!list_empty(&worker->scheduled)))
					while (!list_empty(&worker->scheduled)) {
						struct work_struct *work = list_first_entry(&worker->scheduled,
										struct work_struct, entry);
						process_one_work_tlx(worker, work);
					}
		} else {
				struct list_head *head = &worker->scheduled;
				struct work_struct **nextp = NULL;
					struct work_struct *n;
					list_for_each_entry_safe_from(work, n, NULL, entry) {
						list_move_tail(&work->entry, head);
						if (!(*work_data_bits(work) & WORK_STRUCT_LINKED))
							break;
					}
					if (nextp)
						*nextp = n;

				while (!list_empty(&worker->scheduled)) {
					struct work_struct *work = list_first_entry(&worker->scheduled,
									struct work_struct, entry);
					process_one_work_tlx(worker, work);
				}
		}
	} while (!list_empty(&pool->worklist) &&
								atomic_read(&pool->nr_running) <= 1);
		worker->flags |= WORKER_PREP;
sleep:
	pool = worker->pool;

	if (WARN_ON_ONCE(worker->flags & WORKER_IDLE) ||
			WARN_ON_ONCE(!list_empty(&worker->entry) &&
			(worker->hentry.next || worker->hentry.pprev)))
		goto out;
	worker->flags |= WORKER_IDLE;
	pool->nr_idle++;
	worker->last_active = jiffies_tlx;
	list_add(&worker->entry, &pool->idle_list);
out:
	__set_current_state(TASK_INTERRUPTIBLE);
	spin_unlock_irq_tlx(&pool->lock);
	__schedule_tlx();
	goto woke_up;
}

static void idle_worker_timeout_tlx(unsigned long __pool)
{

}
static void pool_mayday_timeout_tlx(unsigned long __pool)
{

}


void pwq_unbound_release_workfn_tlx(struct work_struct *work)
{

};

bool workqueue_freezing_tlx;




struct device_attribute dev_attr_irq0_tlx;
struct device_attribute dev_attr_irq1_tlx;

int __of_device_is_compatible_tlx(const struct device_node *device,
						const char *compat, const char *type, const char *name)
{
	struct property *prop;
	const char *cp;
	int index = 0, score = 0;
	if (compat && compat[0]) {
		struct property *pp;
		for (pp = device->properties; pp; pp = pp->next) {
			if (strcmp_tlx(pp->name, "compatible") == 0) {
				break;
			}
		}
	prop = pp;
		for (cp = of_prop_next_string_tlx(prop, NULL); cp;
				cp = of_prop_next_string_tlx(prop, cp), index++) {
			if (strcasecmp_tlx(cp, compat) == 0) {
				score = INT_MAX/2 - (index << 2);
				break;
			}
		}
		if (!score)
			return 0;
	}
	if (type && type[0]) {
		if (!device->type || strcasecmp_tlx(type, device->type))
			return 0;
		score += 2;
	}
	if (name && name[0]) {
		if (!device->name || strcasecmp_tlx(name, device->name))
			return 0;
		score++;
	}
	return score;
}



void klist_children_get_tlx(struct klist_node *n)
{
	struct device_private *dev_prv = container_of(n, struct device_private, knode_parent);
	struct device *dev = dev_prv->device;
	kobject_get_tlx(&dev->kobj);
}

void klist_children_put_tlx(struct klist_node *n)
{
	struct device_private *dev_prv = container_of(n, struct device_private, knode_parent);
	struct device *dev = dev_prv->device;
	if (dev)
								kobject_put_tlx(&dev->kobj);
}


struct kset *devices_kset_tlx;


static const struct sysfs_ops dev_sysfs_ops_tlx = {
};

struct kobj_type device_ktype_tlx = {
         .sysfs_ops      = &dev_sysfs_ops_tlx,
};

int bus_add_device_tlx_tlx(struct device *dev)
{
	struct bus_type *bus;
	if (dev->bus) {
			kset_get_tlx(&(dev->bus)->p->subsys);
			bus = dev->bus;
	}
	if (bus) {
			klist_add_tail_tlx(&dev->p->knode_bus, &bus->p->klist_devices);
	}
	return 0;
}

int device_add_tlx_tlx(struct device *dev)
{
	struct device *parent = NULL;
	struct kobject *kobj;
	struct class_interface *class_intf;
//	container_of(kobject_get_tlx(&dev->kobj), struct device, kobj);
//	kobj_to_dev()
	dev = dev ? container_of(kobject_get_tlx(&dev->kobj), struct device, kobj) : NULL;
	if (!dev->p) {
			dev->p = kzalloc_tlx(sizeof(*dev->p), GFP_KERNEL);
			dev->p->device = dev;
			klist_init_tlx(&dev->p->klist_children, klist_children_get_tlx,
					klist_children_put_tlx);
			INIT_LIST_HEAD(&dev->p->deferred_probe);

	}
	kobject_add_tlx(&dev->kobj, dev->kobj.parent, NULL);
	bus_add_device_tlx_tlx(dev);
done:
//	put_device(dev);
	return 0;
}

#define OF_BAD_ADDR     ((u64)-1)
#define OF_MAX_ADDR_CELLS       4


static int of_translate_one_tlx(struct device_node *parent, struct of_bus *bus,
					struct of_bus *pbus, __be32 *addr,
					int na, int ns, int pna, const char *rprop)
{
	const __be32 *ranges;
	unsigned int rlen;
	int rone;
	u64 offset = OF_BAD_ADDR;
	struct property *pp;
	for (pp = parent->properties; pp; pp = pp->next) {
		if (!strcmp_tlx(pp->name, rprop)) {
				*(&rlen) = pp->length;
			break;
		}
	}
	ranges  =  pp ? pp->value : NULL;
	rlen /= 4;
	rone = na + pna + ns;
	for (; rlen >= rone; rlen -= rone, ranges += rone) {
			u64 cp, s, da;
			const __be32 *cell = ranges;
			int num =  na;
			while (num--)
								cp = (cp << 32) | be32_to_cpu(*(cell++));
			cell = ranges + na + pna;
			num =  ns;
			while (num--)
								s = (s << 32) | be32_to_cpu(*(cell++));
			cell = addr;
			num =  na;
			while (num--)
								da = (da << 32) | be32_to_cpu(*(cell++));
			if (da < cp || da >= (cp + s))
				continue;
			offset = da - cp;
			break;
	}
	memcpy_tlx(addr, ranges + na, 4 * pna);
	u64 a;
	const __be32 *cell = addr;
	int num  =  pna;
	while (num--)
							a = (a << 32) | be32_to_cpu(*(cell++));
	memset_tlx(addr, 0, na * 4);
	a += offset;
	if (na > 1)
			addr[na - 2] = cpu_to_be32(a >> 32);
	addr[na - 1] = cpu_to_be32(a & 0xffffffffu);
	return 0;

}

static u64 of_translate_address_tlx(struct device_node *dev,
					const __be32 *in_addr)
{
	const char *rprop = "ranges";
	struct device_node *parent = NULL;
	struct of_bus *bus, *pbus;
	__be32 addr[OF_MAX_ADDR_CELLS];
	int na, ns, pna, pns;
	u64 result = OF_BAD_ADDR;
	parent = dev->parent;
	if (parent == NULL)
		goto bail;
	struct device_node *tmp_np =  dev;
	struct property *pp;
	const __be32 *ip;
	do {
		if (tmp_np->parent)
			tmp_np = tmp_np->parent;
			for (pp = tmp_np->properties; pp; pp = pp->next)
								if (!strcmp_tlx(pp->name, "#address-cells")) break;
			ip  = pp ? pp->value : NULL;
			if (ip) {
				na = be32_to_cpup(ip);
				break;
			}
	} while (tmp_np->parent);
	do {
		if (tmp_np->parent)
			tmp_np = tmp_np->parent;
		for (pp = tmp_np->properties; pp; pp = pp->next)
							if (!strcmp_tlx(pp->name, "#size-cells")) break;
		ip  = pp ? pp->value : NULL;
		if (ip) {
			ns = be32_to_cpup(ip);
			break;
		}
	} while (tmp_np->parent);
	memcpy_tlx(addr, in_addr, na * 4);
	for (;;) {
		dev = parent;
		parent = dev->parent;
		if (parent == NULL) {
				const __be32 *cell = addr;
				int num =  na;
				while (num--)
									result = (result << 32) | be32_to_cpu(*(cell++));

			break;
		}
		tmp_np =  dev;
		do {
			if (tmp_np->parent)
				tmp_np = tmp_np->parent;
				for (pp = tmp_np->properties; pp; pp = pp->next)
									if (!strcmp_tlx(pp->name, "#address-cells")) break;
				ip  = pp ? pp->value : NULL;
				if (ip) {
					pna = be32_to_cpup(ip);
					break;
				}
		} while (tmp_np->parent);
		do {
			if (tmp_np->parent)
				tmp_np = tmp_np->parent;
			for (pp = tmp_np->properties; pp; pp = pp->next)
								if (!strcmp_tlx(pp->name, "#size-cells")) break;
			ip  = pp ? pp->value : NULL;
			if (ip) {
				pns = be32_to_cpup(ip);
				break;
			}
		} while (tmp_np->parent);

		if (of_translate_one_tlx(dev, bus, pbus, addr, na, ns, pna, rprop))
			break;
		na = pna;
		ns = pns;
	}
bail:
	return result;
}

int of_address_to_resource_tlx(struct device_node *dev, int index,
				struct resource *r)
{
	const __be32	*addrp;
	u64		size;
	unsigned int	flags;
	const char	*name = NULL;
	struct property *pp;
	struct device_node *np = dev;
	const char **output =  &name;
	u64 taddr;
	struct property *prop;
		const __be32 *tmp_prop;
		unsigned int psize;
		struct device_node *parent;
		int onesize, i, na, ns;
		parent = dev->parent;
		struct device_node *tmp_np =  dev;
		const __be32 *ip;
		do {
			if (tmp_np->parent)
				tmp_np = tmp_np->parent;
				for (pp = tmp_np->properties; pp; pp = pp->next)
									if (!strcmp_tlx(pp->name, "#address-cells")) break;
				ip  = pp ? pp->value : NULL;
				if (ip) {
					na = be32_to_cpup(ip);
					break;
				}
		} while (tmp_np->parent);
	do {
			if (tmp_np->parent)
				tmp_np = tmp_np->parent;
			for (pp = tmp_np->properties; pp; pp = pp->next)
								if (!strcmp_tlx(pp->name, "#size-cells")) break;
			ip  = pp ? pp->value : NULL;
			if (ip) {
				ns = be32_to_cpup(ip);
				break;
			}
	} while (tmp_np->parent);
	if (!dev)
			goto out;
		for (pp = dev->properties; pp; pp = pp->next) {
			if (!strcmp_tlx(pp->name, "reg")) {
					*(&psize) = pp->length;
				break;
			}
		}
		tmp_prop =  pp ? pp->value : NULL;
		psize /= 4;
		onesize = na + ns;
		for (i = 0; psize >= onesize; psize -= onesize, tmp_prop += onesize, i++)
			if (i == index) {
					const __be32 *cell = tmp_prop + na;
					int num =  ns;
					while (num--)
										size = (size << 32) | be32_to_cpu(*(cell++));

					flags = IORESOURCE_MEM;
					addrp = tmp_prop;
					goto out;
			}
	addrp = NULL;
out:
	taddr = of_translate_address_tlx(dev, addrp);
	memset_tlx(r, 0, sizeof(struct resource));
	if (flags & IORESOURCE_IO) {
			unsigned long port;
			port = (unsigned long) taddr;
			r->start = port;
			r->end = port + size - 1;
	} else {
			r->start = taddr;
			r->end = taddr + size - 1;
	}
	r->flags = flags;
	r->name = name ? name : dev->full_name;
	return 0;
}

#define of_irq_workarounds (0)
#define OF_IMAP_NO_PHANDLE      0x00000002
#define of_irq_dflt_pic (NULL)
struct irq_domain *irq_default_domain_tlx;

struct device_node *of_irq_find_parent_tlx(struct device_node *child)
{
	struct device_node *p;
	const __be32 *parp;
	struct property *pp;
	if (child)
							kobject_get_tlx(&child->kobj);
	if (!child)
		return NULL;
	do {
		pp = NULL;
		for (pp = child->properties; pp; pp = pp->next) {
			if (strcmp_tlx(pp->name, "interrupt-parent") == 0) {
				break;
			}
		}
		parp = pp ? pp->value : NULL;
		if (parp == NULL) {
				p = NULL;
				if (child) {
					if (child->parent)
											kobject_get_tlx(&child->parent->kobj);
					p = child->parent;
					}
		}
		else {
			if (of_irq_workarounds & OF_IMAP_NO_PHANDLE)
				p = NULL;
			else {
					phandle handle = be32_to_cpup(parp);
						struct device_node *np;
						unsigned long flags;
						for (np = of_allnodes_tlx; np; np = np->allnext)
							if (np->phandle == handle)
								break;
						if (np)
												kobject_get_tlx(&np->kobj);
						p = np;
			}
		}
		child = p;
		pp = NULL;
		if(p)
			for (pp = p->properties; pp; pp = pp->next) {
				if (strcmp_tlx(pp->name, "#interrupt-cells") == 0) {
					break;
				}
			}
	} while (p && (pp ? pp->value : NULL) == NULL);
	return p;
}

int of_irq_parse_raw_tlx(const __be32 *addr, struct of_phandle_args *out_irq)
{
	struct device_node *ipar, *tnode, *old = NULL, *newpar = NULL;
	__be32 initial_match_array[MAX_PHANDLE_ARGS];
	const __be32 *match_array = initial_match_array;
	const __be32 *tmp, *imap, *imask, dummy_imask[] = { [0 ... MAX_PHANDLE_ARGS] = ~0 };
	u32 intsize = 1, addrsize, newintsize = 0, newaddrsize = 0;
	int imaplen, match, i;
	ipar = out_irq->np;
	do {
		tmp = of_get_property_tlx(ipar, "#interrupt-cells", NULL);
		if (tmp != NULL) {
			intsize = be32_to_cpu(*tmp);
			break;
		}
		tnode = ipar;
		ipar = of_irq_find_parent_tlx(ipar);
		if (tnode)
								kobject_put_tlx(&tnode->kobj);
	} while (ipar);
	old = ipar;
	do {
		tmp = of_get_property_tlx(old, "#address-cells", NULL);
			tnode = NULL;
			if (old) tnode = old->parent;
		if (old)
								kobject_put_tlx(&old->kobj);
		old = tnode;
	} while (old && tmp == NULL);
	if (old)
							kobject_put_tlx(&old->kobj);
	old = NULL;
	addrsize = (tmp == NULL) ? 2 : be32_to_cpu(*tmp);
	for (i = 0; i < addrsize; i++)
		initial_match_array[i] = addr ? addr[i] : 0;
	for (i = 0; i < intsize; i++)
		initial_match_array[addrsize + i] = cpu_to_be32(out_irq->args[i]);
	while (ipar != NULL) {
		if (of_get_property_tlx(ipar, "interrupt-controller", NULL) !=
			NULL) {
			return 0;
		}
		imap = of_get_property_tlx(ipar, "interrupt-map", &imaplen);
		/* No interrupt map, check for an interrupt parent */
		imaplen /= sizeof(u32);
		imask = of_get_property_tlx(ipar, "interrupt-map-mask", NULL);
		if (!imask)
			imask = dummy_imask;
		match = 0;
		while (imaplen > (addrsize + intsize + 1) && !match) {
			match = 1;
			for (i = 0; i < (addrsize + intsize); i++, imaplen--)
				match &= !((match_array[i] ^ *imap++) & imask[i]);
			if (of_irq_workarounds & OF_IMAP_NO_PHANDLE)
				newpar = of_irq_dflt_pic;
			else {
//				newpar = of_find_node_by_phandle(be32_to_cpup(imap));
					phandle handle = be32_to_cpup(imap);
						struct device_node *np;
						unsigned long flags;
						for (np = of_allnodes_tlx; np; np = np->allnext)
							if (np->phandle == handle)
								break;
						if (np)
												kobject_get_tlx(&np->kobj);
						newpar = np;
			}
			imap++;
			--imaplen;
			match = 1;
			const struct device_node *device = newpar;
				const char *status;
				int statlen;
				const struct device_node *np = device;
				const char *name = "status";
				int *lenp =  &statlen;
					struct property *pp;
					if (!np)
						goto out1;
					for (pp = np->properties; pp; pp = pp->next) {
						if (strcmp_tlx((pp->name), (name)) == 0) {
							if (lenp)
								*lenp = pp->length;
							break;
						}
					}
out1:
				status = pp ? pp->value : NULL;
				if (status == NULL)
					goto have_match;
				if (statlen > 0) {
					if (!strcmp_tlx(status, "okay") || !strcmp_tlx(status, "ok"))
						goto have_match;
				}
			match = 0;
have_match:
			tmp = of_get_property_tlx(newpar, "#interrupt-cells", NULL);
			newintsize = be32_to_cpu(*tmp);
			tmp = of_get_property_tlx(newpar, "#address-cells", NULL);
			newaddrsize = (tmp == NULL) ? 0 : be32_to_cpu(*tmp);
			imap += newaddrsize + newintsize;
			imaplen -= newaddrsize + newintsize;
		}
		out_irq->np = newpar;
		match_array = imap - newaddrsize - newintsize;
		for (i = 0; i < newintsize; i++)
			out_irq->args[i] = be32_to_cpup(imap - newintsize + i);
		out_irq->args_count = intsize = newintsize;
		addrsize = newaddrsize;

	skiplevel:
	if (ipar)
						kobject_put_tlx(&ipar->kobj);
		ipar = newpar;
		newpar = NULL;
	}
}


struct radix_tree_root irq_desc_tree_tlx;
struct irq_domain_ops {
         int (*match)(struct irq_domain *d, struct device_node *node);
         int (*map)(struct irq_domain *d, unsigned int virq, irq_hw_number_t hw);
         void (*unmap)(struct irq_domain *d, unsigned int virq);
         int (*xlate)(struct irq_domain *d, struct device_node *node,
                      const u32 *intspec, unsigned int intsize,
                      unsigned long *out_hwirq, unsigned int *out_type);
 };



 struct irq_domain {
          struct list_head link;
         const char *name;
         const struct irq_domain_ops *ops;
         void *host_data;

         /* Optional data */
         struct device_node *of_node;
         struct irq_domain_chip_generic *gc;

         /* reverse map data. The linear map gets appended to the irq_domain */
         irq_hw_number_t hwirq_max;
         unsigned int revmap_direct_max_irq;
         unsigned int revmap_size;
         struct radix_tree_root revmap_tree;
         unsigned int linear_revmap[];
};


#define IRQ_DEFAULT_INIT_FLAGS 0

enum {
	IRQD_TRIGGER_MASK		= 0xf,
	IRQD_SETAFFINITY_PENDING	= (1 <<  8),
	IRQD_NO_BALANCING		= (1 << 10),
	IRQD_PER_CPU			= (1 << 11),
	IRQD_AFFINITY_SET		= (1 << 12),
	IRQD_LEVEL			= (1 << 13),
	IRQD_WAKEUP_STATE		= (1 << 14),
	IRQD_MOVE_PCNTXT		= (1 << 15),
	IRQD_IRQ_DISABLED		= (1 << 16),
	IRQD_IRQ_MASKED			= (1 << 17),
	IRQD_IRQ_INPROGRESS		= (1 << 18),
};

enum {
	IRQ_TYPE_NONE		= 0x00000000,
	IRQ_TYPE_EDGE_RISING	= 0x00000001,
	IRQ_TYPE_EDGE_FALLING	= 0x00000002,
	IRQ_TYPE_EDGE_BOTH	= (IRQ_TYPE_EDGE_FALLING | IRQ_TYPE_EDGE_RISING),
	IRQ_TYPE_LEVEL_HIGH	= 0x00000004,
	IRQ_TYPE_LEVEL_LOW	= 0x00000008,
	IRQ_TYPE_LEVEL_MASK	= (IRQ_TYPE_LEVEL_LOW | IRQ_TYPE_LEVEL_HIGH),
	IRQ_TYPE_SENSE_MASK	= 0x0000000f,
	IRQ_TYPE_DEFAULT	= IRQ_TYPE_SENSE_MASK,

	IRQ_TYPE_PROBE		= 0x00000010,

	IRQ_LEVEL		= (1 <<  8),
	IRQ_PER_CPU		= (1 <<  9),
	IRQ_NOPROBE		= (1 << 10),
	IRQ_NOREQUEST		= (1 << 11),
	IRQ_NOAUTOEN		= (1 << 12),
	IRQ_NO_BALANCING	= (1 << 13),
	IRQ_MOVE_PCNTXT		= (1 << 14),
	IRQ_NESTED_THREAD	= (1 << 15),
	IRQ_NOTHREAD		= (1 << 16),
	IRQ_PER_CPU_DEVID	= (1 << 17),
	IRQ_IS_POLLED		= (1 << 18),
};

#define IRQF_MODIFY_MASK        \
         (IRQ_TYPE_SENSE_MASK | IRQ_NOPROBE | IRQ_NOREQUEST | \
          IRQ_NOAUTOEN | IRQ_MOVE_PCNTXT | IRQ_LEVEL | IRQ_NO_BALANCING | \
          IRQ_PER_CPU | IRQ_NESTED_THREAD | IRQ_NOTHREAD | IRQ_PER_CPU_DEVID | \
          IRQ_IS_POLLED)

typedef	void (*irq_flow_handler_t)(unsigned int irq,
							struct irq_desc *desc);

# define IRQ_BITMAP_BITS        NR_IRQS

struct irq_chip no_irq_chip_tlx;
unsigned long allocated_irqs_tlx[BITS_TO_LONGS(IRQ_BITMAP_BITS)];	//allocated_irqs_tlx, IRQ_BITMAP_BITS
struct mutex sparse_irq_lock_tlx;


struct irq_data {
	u32			mask;
	unsigned int		irq;
	unsigned long		hwirq;
	unsigned int		node;
	unsigned int		state_use_accessors;
	struct irq_chip		*chip;
	struct irq_domain	*domain;
	void			*handler_data;
	void			*chip_data;
	struct msi_desc		*msi_desc;
	cpumask_var_t		affinity;
};

struct irq_desc {
	struct irq_data		irq_data;
	unsigned int __percpu	*kstat_tlx_irqs;
	irq_flow_handler_t	handle_irq;
#ifdef CONFIG_IRQ_PREFLOW_FASTEOI
	irq_preflow_handler_t	preflow_handler;
#endif
	struct irqaction	*action;	/* IRQ action list */
	unsigned int		status_use_accessors;
	unsigned int		core_internal_state__do_not_mess_with_it;
	unsigned int		depth;		/* nested irq disables */
	unsigned int		wake_depth;	/* nested wake enables */
	unsigned int		irq_count;	/* For detecting broken IRQs */
	unsigned long		last_unhandled;	/* Aging timer for unhandled count */
	unsigned int		irqs_unhandled;
	atomic_t		threads_handled;
	int			threads_handled_last;
	raw_spinlock_t		lock;
	struct cpumask		*percpu_enabled;
#ifdef CONFIG_SMP
	const struct cpumask	*affinity_hint;
	struct irq_affinity_notify *affinity_notify;
#ifdef CONFIG_GENERIC_PENDING_IRQ
	cpumask_var_t		pending_mask;
#endif
#endif
	unsigned long		threads_oneshot;
	atomic_t		threads_active;
	wait_queue_head_t       wait_for_threads;
#ifdef CONFIG_PROC_FS
	struct proc_dir_entry	*dir;
#endif
	int			parent_irq;
	struct module		*owner;
	const char		*name;
} ____cacheline_internodealigned_in_smp;

struct irq_chip {
	const char	*name;
	unsigned int	(*irq_startup)(struct irq_data *data);
	void		(*irq_shutdown)(struct irq_data *data);
	void		(*irq_enable)(struct irq_data *data);
	void		(*irq_disable)(struct irq_data *data);

	void		(*irq_ack)(struct irq_data *data);
	void		(*irq_mask)(struct irq_data *data);
	void		(*irq_mask_ack)(struct irq_data *data);
	void		(*irq_unmask)(struct irq_data *data);
	void		(*irq_eoi)(struct irq_data *data);

	int		(*irq_set_affinity)(struct irq_data *data, const struct cpumask *dest, bool force);
	int		(*irq_retrigger)(struct irq_data *data);
	int		(*irq_set_type)(struct irq_data *data, unsigned int flow_type);
	int		(*irq_set_wake)(struct irq_data *data, unsigned int on);

	void		(*irq_bus_lock)(struct irq_data *data);
	void		(*irq_bus_sync_unlock)(struct irq_data *data);

	void		(*irq_cpu_online)(struct irq_data *data);
	void		(*irq_cpu_offline)(struct irq_data *data);

	void		(*irq_suspend)(struct irq_data *data);
	void		(*irq_resume)(struct irq_data *data);
	void		(*irq_pm_shutdown)(struct irq_data *data);

	void		(*irq_calc_mask)(struct irq_data *data);

	void		(*irq_print_chip)(struct irq_data *data, struct seq_file *p);
	int		(*irq_request_resources)(struct irq_data *data);
	void		(*irq_release_resources)(struct irq_data *data);

	unsigned long	flags;
};

void irqd_clear_tlx(struct irq_data *d, unsigned int mask)
{
			d->state_use_accessors &= ~mask;
}


struct irqaction {
         irq_handler_t           handler;
         void                    *dev_id;
         void __percpu           *percpu_dev_id;
         struct irqaction        *next;
         irq_handler_t           thread_fn;
         struct task_struct      *thread;
         unsigned int            irq;
         unsigned int            flags;
         unsigned long           thread_flags;
         unsigned long           thread_mask;
         const char              *name;
       struct proc_dir_entry   *dir;
} ____cacheline_internodealigned_in_smp;

enum {
	IRQTF_RUNTHREAD,
	IRQTF_WARNED,
	IRQTF_AFFINITY,
	IRQTF_FORCED_THREAD,
};

#define __to_kthread(vfork)     \
         container_of(vfork, struct kthread, exited)

int irq_wait_for_interrupt_tlx(struct irqaction *action)
{
       set_current_state(TASK_INTERRUPTIBLE);

       while (!test_bit_tlx(KTHREAD_SHOULD_STOP, &__to_kthread(current->vfork_done)->flags)) {

               if (test_and_clear_bit_tlx(IRQTF_RUNTHREAD,
                                      &action->thread_flags)) {
                       __set_current_state(TASK_RUNNING);
                         return 0;
               }
               __schedule_tlx();
               set_current_state(TASK_INTERRUPTIBLE);
       }
       __set_current_state(TASK_RUNNING);
       return -1;
}

irqreturn_t irq_thread_fn_tlx(struct irq_desc *desc,
		struct irqaction *action)
{
	irqreturn_t ret;

	ret = action->thread_fn(action->irq, action->dev_id);
//	irq_finalize_oneshot(desc, action);
	return ret;
}

int irq_thread_tlx(void *data)
{
	struct callback_head on_exit_work;
	struct irqaction *action = data;
	struct irq_desc *desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, action->irq);
	irqreturn_t (*handler_fn)(struct irq_desc *desc,
			struct irqaction *action);

		handler_fn = irq_thread_fn_tlx;

	struct task_struct *task = current;
	struct callback_head *work = &on_exit_work;
	struct callback_head *head;
	do {
								head = ACCESS_ONCE(task->task_works);
								work->next = head;
	} while (cmpxchg(&task->task_works, head, work) != head);
	while (!irq_wait_for_interrupt_tlx(action)) {
		irqreturn_t action_ret;
		action_ret = handler_fn(desc, action);
		if (action_ret == IRQ_HANDLED)
			atomic_inc_tlx(&desc->threads_handled);
	}
return 0;
}




enum {
	IRQCHIP_SET_TYPE_MASKED		= (1 <<  0),
	IRQCHIP_EOI_IF_HANDLED		= (1 <<  1),
	IRQCHIP_MASK_ON_SUSPEND		= (1 <<  2),
	IRQCHIP_ONOFFLINE_ENABLED	= (1 <<  3),
	IRQCHIP_SKIP_SET_WAKE		= (1 <<  4),
	IRQCHIP_ONESHOT_SAFE		= (1 <<  5),
	IRQCHIP_EOI_THREADED		= (1 <<  6),
};

#define IRQF_ONESHOT            0x00002000

enum {
	IRQS_AUTODETECT		= 0x00000001,
	IRQS_SPURIOUS_DISABLED	= 0x00000002,
	IRQS_POLL_INPROGRESS	= 0x00000008,
	IRQS_ONESHOT		= 0x00000020,
	IRQS_REPLAY		= 0x00000040,
	IRQS_WAITING		= 0x00000080,
	IRQS_PENDING		= 0x00000200,
	IRQS_SUSPENDED		= 0x00000800,
};

int
__setup_irq_tlx(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
{
	struct irqaction *old, **old_ptr;
	unsigned long flags, thread_mask = 0;
	int ret, nested, shared = 0;
	cpumask_var_t mask;
//		if (irq_settings_can_thread(desc))
//			irq_setup_forced_threading(new);
	if (new->thread_fn && !nested) {
		struct task_struct *t;
		static const struct sched_param param = {
			.sched_priority = MAX_USER_RT_PRIO/2,
		};

//		t = kthread_create(irq_thread, new, "irq/%d-%s", irq,
//				   new->name);
			t = kthread_create_on_node_tlx(irq_thread_tlx, new, -1,  "irq/%d-%s", irq, new->name);
//		sched_setscheduler_nocheck(t, SCHED_FIFO, &param);
//		get_task_struct(t);
		new->thread = t;
		set_bit_tlx(IRQTF_AFFINITY, &new->thread_flags);
	}

//	if (!alloc_cpumask_var(&mask, GFP_KERNEL)) {
//	}
	if (desc->irq_data.chip->flags & IRQCHIP_ONESHOT_SAFE)
		new->flags &= ~IRQF_ONESHOT;
	raw_spin_lock_irqsave(&desc->lock, flags);
	old_ptr = &desc->action;
	old = *old_ptr;
	if (old) {
		do {
			thread_mask |= old->thread_mask;
			old_ptr = &old->next;
			old = *old_ptr;
		} while (old);
		shared = 1;
	}

	if (!shared) {
//		ret = irq_request_resources(desc);
//		desc->istate &= ~(IRQS_AUTODETECT | IRQS_SPURIOUS_DISABLED | \
//					IRQS_ONESHOT | IRQS_WAITING);
// 	  irq_startup(desc, true);
			desc->depth = 0;
//			irq_stat_tlxe_clr_disabled(desc);
			irqd_clear_tlx(&desc->irq_data, IRQD_IRQ_DISABLED);
			if (desc->irq_data.chip->irq_enable)
							desc->irq_data.chip->irq_enable(&desc->irq_data);
			else
			desc->irq_data.chip->irq_unmask(&desc->irq_data);
//      irq_stat_tlxe_clr_masked(desc);

	}

	new->irq = irq;
	*old_ptr = new;
	desc->irq_count = 0;
	desc->irqs_unhandled = 0;
	raw_spin_unlock_irqrestore(&desc->lock, flags);
//	register_irq_proc(irq, desc);
	new->dir = NULL;
//	register_handler_proc(irq, new);
//	free_cpumask_var(mask);

	return 0;

}

int request_threaded_irq_tlx(unsigned int irq, irq_handler_t handler,
			irq_handler_t thread_fn, unsigned long irqflags,
			const char *devname, void *dev_id)
{
	struct irqaction *action;
	struct irq_desc *desc;
	int retval;
	desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, irq);;
	action = kzalloc_tlx(sizeof(struct irqaction), GFP_KERNEL);
	action->handler = handler;
	action->thread_fn = thread_fn;
	action->flags = irqflags;
	action->name = devname;
	action->dev_id = dev_id;
	retval = __setup_irq_tlx(irq, desc, action);
	return retval;
}


# define IRQ_BITMAP_BITS        NR_IRQS

static inline void irqd_set(struct irq_data *d, unsigned int mask)
{
			d->state_use_accessors |= mask;
}

LIST_HEAD(irq_domain_list_tlx);

void handle_bad_irq_tlx(unsigned int irq, struct irq_desc *desc)
{
}

#define PCPU_DFL_MAP_ALLOC              16      /* start a map with 16 ents */

int pcpu_need_to_extend_tlx(struct pcpu_chunk *chunk)
{
	int new_alloc;

	if (chunk->map_alloc >= chunk->map_used + 3)
		return 0;

	new_alloc = PCPU_DFL_MAP_ALLOC;
	while (new_alloc < chunk->map_used + 3)
		new_alloc *= 2;

	return new_alloc;
}



void *pcpu_mem_zalloc_tlx(size_t size)
{
	if (size <= PAGE_SIZE)
		return kzalloc_tlx(size, GFP_KERNEL);
}

int pcpu_size_to_slot_tlx(int size)
{
	if (size == pcpu_unit_size_tlx)
		return pcpu_nr_slots_tlx - 1;
//	return __pcpu_size_to_slot_tlx(size);
	int highbit = fls_tlx(size);	/* size is in bytes */
	return max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1);
}

int pcpu_chunk_slot_tlx(const struct pcpu_chunk *chunk)
{
	if (chunk->free_size < sizeof(int) || chunk->contig_hint < sizeof(int))
		return 0;

	return pcpu_size_to_slot_tlx(chunk->free_size);
}
int pcpu_alloc_area_tlx(struct pcpu_chunk *chunk, int size, int align)
{
	int oslot = pcpu_chunk_slot_tlx(chunk);
	int max_contig = 0;
	int i, off;
	bool seen_free = false;
	int *p;

	for (i = chunk->first_free, p = chunk->map + i; i < chunk->map_used; i++, p++) {
		int head, tail;
		int this_size;

		off = *p;
		if (off & 1)
			continue;

		/* extra for alignment requirement */
		head = ALIGN(off, align) - off;

		this_size = (p[1] & ~1) - off;
		if (this_size < head + size) {
			if (!seen_free) {
				chunk->first_free = i;
				seen_free = true;
			}
			max_contig = max(this_size, max_contig);
			continue;
		}

		/*
		* If head is small or the previous block is free,
		* merge'em.  Note that 'small' is defined as smaller
		* than sizeof(int), which is very small but isn't too
		* uncommon for percpu allocations.
		*/
		if (head && (head < sizeof(int) || !(p[-1] & 1))) {
			*p = off += head;
			if (p[-1] & 1)
				chunk->free_size -= head;
			else
				max_contig = max(*p - p[-1], max_contig);
			this_size -= head;
			head = 0;
		}

		/* if tail is small, just keep it around */
		tail = this_size - head - size;
		if (tail < sizeof(int)) {
			tail = 0;
			size = this_size - head;
		}

		/* split if warranted */
		if (head || tail) {
			int nr_extra = !!head + !!tail;

			/* insert new subblocks */
			memmove_tlx(p + nr_extra + 1, p + 1,
				sizeof(chunk->map[0]) * (chunk->map_used - i));
			chunk->map_used += nr_extra;

			if (head) {
				if (!seen_free) {
					chunk->first_free = i;
					seen_free = true;
				}
				*++p = off += head;
				++i;
				max_contig = max(head, max_contig);
			}
			if (tail) {
				p[1] = off + size;
				max_contig = max(tail, max_contig);
			}
		}

		if (!seen_free)
			chunk->first_free = i + 1;

		/* update hint and mark allocated */
		if (i + 1 == chunk->map_used)
			chunk->contig_hint = max_contig; /* fully scanned */
		else
			chunk->contig_hint = max(chunk->contig_hint,
						max_contig);

		chunk->free_size -= size;
		*p |= 1;

//		pcpu_chunk_relocate(chunk, oslot);
		int nslot = pcpu_chunk_slot_tlx(chunk);

		if (chunk != pcpu_reserved_chunk_tlx && oslot != nslot) {
			if (oslot < nslot)
				list_move(&chunk->list, &pcpu_slot_tlx[nslot]);
			else
				list_move_tail(&chunk->list, &pcpu_slot_tlx[nslot]);
		}
		return off;
	}

	chunk->contig_hint = max_contig;	/* fully scanned */
//	pcpu_chunk_relocate(chunk, oslot);
	int nslot = pcpu_chunk_slot_tlx(chunk);

	if (chunk != pcpu_reserved_chunk_tlx && oslot != nslot) {
		if (oslot < nslot)
			list_move(&chunk->list, &pcpu_slot_tlx[nslot]);
		else
			list_move_tail(&chunk->list, &pcpu_slot_tlx[nslot]);
	}

	/* tell the upper layer that this chunk has no matching area */
	return -1;
}

#define order_base_2(n) ilog2(roundup_pow_of_two(n))

#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))

static void pcpu_mem_free_tlx(void *ptr, size_t size)
{
	if (size <= PAGE_SIZE)
		kfree_tlx(ptr);

}

int pcpu_extend_area_map_tlx(struct pcpu_chunk *chunk, int new_alloc)
{
	int *old = NULL, *new = NULL;
	size_t old_size = 0, new_size = new_alloc * sizeof(new[0]);
	unsigned long flags;

	new = pcpu_mem_zalloc_tlx(new_size);
	old_size = chunk->map_alloc * sizeof(chunk->map[0]);
	old = chunk->map;
	memcpy_tlx(new, old, old_size);
	chunk->map_alloc = new_alloc;
	chunk->map = new;
	new = NULL;
out_unlock:
	pcpu_mem_free_tlx(old, old_size);
	pcpu_mem_free_tlx(new, new_size);
	return 0;
}

void *pcpu_alloc_tlx(size_t size, size_t align, bool reserved)
{
	static int warn_limit = 10;
	struct pcpu_chunk *chunk;
	const char *err;
	int slot, off, new_alloc;
	unsigned long flags;
	void __percpu *ptr;
	if (unlikely(align < 2))
		align = 2;

	if (unlikely(size & 1))
		size++;

//	mutex_lock_tlx(&pcpu_alloc_mutex);
//	spin_lock_irqsave(&pcpu_lock, flags);
	if (reserved && pcpu_reserved_chunk_tlx) {
		chunk = pcpu_reserved_chunk_tlx;
		while ((new_alloc = pcpu_need_to_extend_tlx(chunk))) {
//			spin_unlock_irqrestore(&pcpu_lock, flags);
			if (pcpu_extend_area_map_tlx(chunk, new_alloc) < 0) {
				err = "failed to extend area map of reserved chunk";
			}
//			spin_lock_irqsave(&pcpu_lock, flags);
		}
		off = pcpu_alloc_area_tlx(chunk, size, align);
		if (off >= 0)
			goto area_found;
	}

restart:
	/* search through normal chunks */
	for (slot = pcpu_size_to_slot_tlx(size); slot < pcpu_nr_slots_tlx; slot++) {
		list_for_each_entry(chunk, &pcpu_slot_tlx[slot], list) {
			if (size > chunk->contig_hint)
				continue;

			new_alloc = pcpu_need_to_extend_tlx(chunk);
			if (new_alloc) {
//				spin_unlock_irqrestore(&pcpu_lock, flags);
				if (pcpu_extend_area_map_tlx(chunk,
							new_alloc) < 0) {
					err = "failed to extend area map";
				}
//				spin_lock_irqsave(&pcpu_lock, flags);
				goto restart;
			}

			off = pcpu_alloc_area_tlx(chunk, size, align);
			if (off >= 0)
				goto area_found;
		}
	}

	/* hmmm... no space left, create a new chunk */
//	spin_unlock_irqrestore(&pcpu_lock, flags);
//	chunk = pcpu_create_chunk();
	const int nr_pages = pcpu_group_sizes_tlx[0] >> PAGE_SHIFT;
//	struct pcpu_chunk *chunk;
	struct page *pages;
	int i;

//	chunk = pcpu_alloc_chunk();
	chunk = pcpu_mem_zalloc_tlx(pcpu_chunk_struct_size_tlx);
	chunk->map = pcpu_mem_zalloc_tlx(PCPU_DFL_MAP_ALLOC *
						sizeof(chunk->map[0]));
	chunk->map_alloc = PCPU_DFL_MAP_ALLOC;
	chunk->map[0] = 0;
	chunk->map[1] = pcpu_unit_size_tlx | 1;
	chunk->map_used = 1;

	INIT_LIST_HEAD(&chunk->list);
	chunk->free_size = pcpu_unit_size_tlx;
	chunk->contig_hint = pcpu_unit_size_tlx;

	pages = alloc_pages(GFP_KERNEL, order_base_2(nr_pages));
	for (i = 0; i < nr_pages; i++)
		nth_page(pages, i)->index = (unsigned long)chunk;
//		pcpu_set_page_chunk(nth_page(pages, i), chunk);

	chunk->data = pages;
	chunk->base_addr = page_address(pages) - pcpu_group_offsets_tlx[0];

//	spin_lock_irqsave(&pcpu_lock, flags);
//	pcpu_chunk_relocate(chunk, -1);
	int nslot = pcpu_chunk_slot_tlx(chunk);
		if (chunk != pcpu_reserved_chunk_tlx && -1 != nslot) {
							if (-1 < nslot)
											list_move(&chunk->list, &pcpu_slot_tlx[nslot]);
							else
											list_move_tail(&chunk->list, &pcpu_slot_tlx[nslot]);
		}

	goto restart;

area_found:
//	spin_unlock_irqrestore(&pcpu_lock, flags);

	/* populate, map and clear the area */
//		pcpu_populate_chunk(chunk, off, size);
//	mutex_unlock_tlx(&pcpu_alloc_mutex);
//	ptr = __addr_to_pcpu_ptr(chunk->base_addr + off);
	ptr =  (void __percpu *)((unsigned long)(chunk->base_addr + off) -                       \
													(unsigned long)pcpu_base_addr_tlx +               \
													(unsigned long)__per_cpu_start);

	return ptr;
}

unsigned int irq_create_of_mapping_tlx(struct of_phandle_args *irq_data)
{
	struct irq_domain *domain;
	irq_hw_number_t hwirq;
	unsigned int type = 0;
	unsigned int virq;
		struct irq_domain *h, *found = NULL;
		int rc;
		list_for_each_entry(h, &irq_domain_list_tlx, link) {
			if (h->ops->match)
				rc = h->ops->match(h, irq_data->np);
			else
				rc = (h->of_node != NULL) && (h->of_node == irq_data->np);
			if (rc) {
				found = h;
				break;
			}
		}
	domain = irq_data->np ? found  : irq_default_domain_tlx;
		if (domain->ops->xlate(domain, irq_data->np, irq_data->args,
					irq_data->args_count, &hwirq, &type))
			return 0;
	unsigned int hint;
	if (domain == NULL)
		domain = irq_default_domain_tlx;
		struct irq_data *data;
		if (hwirq < domain->revmap_direct_max_irq) {
			struct irq_desc *desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, hwirq);
			data = desc ? &desc->irq_data : NULL;
			if (data && (data->domain == domain) && (data->hwirq == hwirq)) {
				virq = hwirq;
				goto have_virq;
			}
		}
		if (hwirq < domain->revmap_size) {
			virq = domain->linear_revmap[hwirq];
		goto have_virq;
	}

		rcu_read_lock_tlx();
		data = radix_tree_lookup_tlx(&domain->revmap_tree, hwirq);
		rcu_read_unlock_tlx();
		virq = data ? data->irq : 0;
have_virq:

	hint = hwirq % nr_irqs_tlx;
	if (hint == 0)
		hint++;
		int irq = -1;
		unsigned int from = 1;
		unsigned int cnt = 1;
		int node = numa_node_id_tlx();
		struct module *owner = THIS_MODULE;
			int start, ret;
			if (irq >= 0) {
				from = irq;
			}
			start = bitmap_find_next_zero_area_tlx(allocated_irqs_tlx, IRQ_BITMAP_BITS,
								from, cnt, 0);
			if (start + cnt > nr_irqs_tlx) {
					nr_irqs_tlx = start + cnt;
			}
			bitmap_set_tlx(allocated_irqs_tlx, start, cnt);
			struct irq_desc *desc;
			int i;
			for (i = 0; i < cnt; i++) {
					irq = start + i;
					gfp_t gfp = GFP_KERNEL;
					desc = kzalloc_tlx(sizeof(*desc), gfp);
					desc->kstat_tlx_irqs = alloc_percpu(unsigned int);
//					zalloc_cpumask_var_node_tlx(&desc->irq_data.affinity, gfp, node);
					raw_spin_lock_init(&desc->lock);
						int cpu;
						desc->irq_data.irq = irq;
						desc->irq_data.chip = &no_irq_chip_tlx;
						desc->irq_data.chip_data = NULL;
						desc->irq_data.handler_data = NULL;
						desc->irq_data.msi_desc = NULL;
						desc->status_use_accessors &= ~(~0 & IRQF_MODIFY_MASK);
						desc->status_use_accessors |= (IRQ_DEFAULT_INIT_FLAGS & IRQF_MODIFY_MASK);
						(&desc->irq_data)->state_use_accessors |= IRQD_IRQ_DISABLED;
						desc->handle_irq = handle_bad_irq_tlx;
						desc->depth = 1;
						desc->irq_count = 0;
						desc->irqs_unhandled = 0;
						desc->name = NULL;
						desc->owner = owner;
						for_each_possible_cpu(cpu)
							*per_cpu_ptr(desc->kstat_tlx_irqs, cpu) = 0;
						desc->irq_data.node = node;
						cpumask_copy_tlx(desc->irq_data.affinity, irq_default_affinity_tlx);
							radix_tree_insert_tlx(&irq_desc_tree_tlx, start + i, desc);
			}
			virq = start;
		desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, virq);
		struct irq_data *_irq_data = desc ? &desc->irq_data : NULL;
		_irq_data->hwirq = hwirq;
		_irq_data->domain = domain;
		if (domain->ops->map) {
			ret = domain->ops->map(domain, virq, hwirq);
			if (ret != 0) {
				_irq_data->domain = NULL;
				_irq_data->hwirq = 0;
//				irq_free_desc(virq);
//				unsigned int from)
//				{
						int i;
					for (i = 0; i < 1; i++) {
									unsigned int irq = virq + i;
									struct irq_desc *desc =  radix_tree_lookup_tlx(&irq_desc_tree_tlx, irq);
									mutex_lock_tlx(&sparse_irq_lock_tlx);
									radix_tree_delete_tlx(&irq_desc_tree_tlx, irq);
									mutex_unlock_tlx(&sparse_irq_lock_tlx);
									free_cpumask_var_tlx(desc->irq_data.affinity);
									free_percpu_tlx(desc->kstat_tlx_irqs);
									kfree_tlx(desc);
					}
					mutex_lock_tlx(&sparse_irq_lock_tlx);
					bitmap_clear_tlx(allocated_irqs_tlx, from, 1);
					mutex_unlock_tlx(&sparse_irq_lock_tlx);
				return 0;
			}
			if (!domain->name && _irq_data->chip)
				domain->name = _irq_data->chip->name;
		}

		if (hwirq < domain->revmap_size) {
			domain->linear_revmap[hwirq] = virq;
		} else {
			radix_tree_insert_tlx(&domain->revmap_tree, hwirq, _irq_data);
		}
		desc =   radix_tree_lookup_tlx(&irq_desc_tree_tlx, irq);
			desc->status_use_accessors &= ~(IRQ_NOREQUEST & IRQF_MODIFY_MASK);
			desc->status_use_accessors |= (0 & IRQF_MODIFY_MASK);
			irqd_clear_tlx(&desc->irq_data, IRQD_NO_BALANCING | IRQD_PER_CPU |
				IRQD_TRIGGER_MASK | IRQD_LEVEL | IRQD_MOVE_PCNTXT);
			irqd_set(&desc->irq_data, IRQD_NO_BALANCING);
			irqd_set(&desc->irq_data, IRQD_PER_CPU);
			irqd_set(&desc->irq_data, IRQD_LEVEL);

		irqd_set(&desc->irq_data,  desc->status_use_accessors & IRQ_TYPE_SENSE_MASK);
out:
	return virq;
	if (!virq)
		return virq;
}

unsigned int irq_of_parse_and_map_tlx(struct device_node *dev, int index)
{
	struct of_phandle_args oirq;

		struct device_node *device = dev;
		struct of_phandle_args *out_irq =  &oirq;
			struct device_node *p;
			const __be32 *intspec, *tmp, *addr;
			u32 intsize, intlen;
			int i, res = -EINVAL;
			addr = of_get_property_tlx(device, "reg", NULL);
			intspec = of_get_property_tlx(device, "interrupts", &intlen);
			intlen /= sizeof(*intspec);
			p = of_irq_find_parent_tlx(device);
			tmp = of_get_property_tlx(p, "#interrupt-cells", NULL);
			intsize = be32_to_cpu(*tmp);
			if ((index + 1) * intsize > intlen)
				goto out;
			intspec += index * intsize;
			out_irq->np = p;
			out_irq->args_count = intsize;
			for (i = 0; i < intsize; i++)
				out_irq->args[i] = be32_to_cpup(intspec++);
			res = of_irq_parse_raw_tlx(addr, out_irq);
		out:
			if (p)
                  kobject_put_tlx(&p->kobj);
			if (res) return 0;
	return irq_create_of_mapping_tlx(&oirq);
}

int of_platform_bus_create_tlx(struct device_node *bus,
					const struct of_device_id_tlx *matches,
					const struct of_dev_auxdata *lookup,
					struct device *parent, bool strict)
{
	const struct of_dev_auxdata *auxdata;
	struct device_node *child;
	struct platform_device *dev;
	const char *bus_id = NULL;
	void *platform_data = NULL;
	int rc = 0;
	if (__of_device_is_compatible_tlx(bus, "arm,primecell", NULL, NULL)) {
		struct device_node *node = bus;
								const char *bus_id = NULL;
								void *platform_data = NULL;
			struct amba_device_tlx *dev;
			const void *prop;
			int i, ret;
			dev = kzalloc_tlx(sizeof(*dev), GFP_KERNEL);
			if (dev) {
				(&dev->dev)->kobj.kset = devices_kset_tlx;
				kobject_init_tlx(&(&dev->dev)->kobj, &device_ktype_tlx);
				INIT_LIST_HEAD(&(&dev->dev)->dma_pools);
				mutex_init(&(&dev->dev)->mutex);
				spin_lock_init(&(&dev->dev)->devres_lock);
				INIT_LIST_HEAD(&(&dev->dev)->devres_head);
				set_dev_node(&dev->dev, -1);

				dev->dev.release = NULL;
				dev->dev.bus = &amba_bustype_tlx;
				dev->dev.dma_mask = &dev->dev.coherent_dma_mask;
				dev->res.name = dev_name(&dev->dev);
				dev->res.start = 0;
				dev->res.end = - 1;
				dev->res.flags = IORESOURCE_MEM;
			}
			dev->dev.coherent_dma_mask = ~0;
			dev->dev.of_node = node;
			dev->dev.parent = parent;
			dev->dev.platform_data = platform_data;
			struct device *tmp_dev = &dev->dev;
				struct device_node *tmp_node = tmp_dev->of_node;
				const __be32 *reg;
				u64 addr;
				while (tmp_node->parent) {
					reg = of_get_property_tlx(tmp_node, "reg", NULL);
					if (reg && (addr = of_translate_address_tlx(node, reg)) != OF_BAD_ADDR) {
						dev_set_name_tlx(tmp_dev, dev_name(tmp_dev) ? "%llx.%s:%s" : "%llx.%s",
									(unsigned long long)addr, node->name,
									dev_name(tmp_dev));
						break;
					}
				}

			for (i = 0; i < AMBA_NR_IRQS; i++)
				dev->irq[i] = irq_of_parse_and_map_tlx(node, i);


			ret = of_address_to_resource_tlx(node, 0, &dev->res);
			parent = &iomem_resource;
				u32 size;
				void __iomem *tmp;
				ret = __request_resource_tlx(parent, &dev->res);
				size = resource_size(&dev->res);
				tmp = ioremap(dev->res.start, size);
				struct amba_device_tlx *pcdev = dev;
					pcdev->pclk = clk_get_tlx(&pcdev->dev, "apb_pclk");
				pcdev->pclk->prepare_count++;
					pcdev->pclk->enable_count++;
					u32 pid, cid;
					for (pid = 0, i = 0; i < 4; i++)
						pid |= (readl(tmp + size - 0x20 + 4 * i) & 255) <<
							(i * 8);
					for (cid = 0, i = 0; i < 4; i++)
						cid |= (readl(tmp + size - 0x10 + 4 * i) & 255) <<
							(i * 8);
						pcdev->pclk->enable_count++;
						pcdev->pclk->prepare_count--;
					if (cid == AMBA_CID)
						dev->periphid = pid;
				iounmap_tlx(tmp);
				ret = device_add_tlx_tlx(&dev->dev);
				if (dev->irq[0])
					ret = device_create_file_tlx(&dev->dev, &dev_attr_irq0_tlx);
				if (ret == 0 && dev->irq[1])
					ret = device_create_file_tlx(&dev->dev, &dev_attr_irq1_tlx);
		return 0;
	}

	return rc;
}

typedef u32             compat_uptr_t;

struct user_arg_ptr {
	bool is_compat;
	union {
		const char __user *const __user *native;
		const compat_uptr_t __user *compat;
	} ptr;
};










struct compress_format {
	unsigned char magic[2];
	const char *name;
	decompress_fn decompressor;
};

void device_create_release_tlx(struct device *dev)
{
//        pr_debug("device: '%s': %s\n", dev_name(dev), __func__);
         kfree_tlx(dev);
}



#define LOOKUP_OPEN             0x0100
#define LOOKUP_FOLLOW           0x0001
#define LOOKUP_RCU              0x0040


enum {
	READ_IMPLIES_EXEC =	0x0400000,
	ADDR_NO_RANDOMIZE = 	0x0040000,
};
#define PROT_READ       0x1             /* page can be read */
#define PROT_WRITE      0x2             /* page can be written */
#define PROT_EXEC       0x4             /* page can be executed */

#define MAP_PRIVATE     0x02            /* Changes are private */
#define MAP_DENYWRITE   0x0800          /* ETXTBSY */
#define MAP_EXECUTABLE  0x1000          /* mark it as an executable */


#if ELF_EXEC_PAGESIZE > PAGE_SIZE
#define ELF_MIN_ALIGN	ELF_EXEC_PAGESIZE
#else
#define ELF_MIN_ALIGN	PAGE_SIZE
#endif


#define ELF_PAGESTART(_v) ((_v) & ~(unsigned long)(ELF_MIN_ALIGN-1))
#define ELF_PAGEOFFSET(_v) ((_v) & (ELF_MIN_ALIGN-1))
#define ELF_PAGEALIGN(_v) (((_v) + ELF_MIN_ALIGN - 1) & ~(ELF_MIN_ALIGN - 1))




struct word_at_a_time {
				const unsigned long one_bits, high_bits;
};

#define REPEAT_BYTE(x)  ((~0ul / 0xff) * (x))
static inline unsigned long has_zero_tlx(unsigned long a, unsigned long *bits,
																			const struct word_at_a_time *c)
{
				unsigned long mask = ((a - c->one_bits) & ~a) & c->high_bits;
				*bits = mask;
				return mask;
}

static inline unsigned long create_zero_mask_tlx(unsigned long bits)
{
			bits = (bits - 1) & ~bits;
			return bits >> 7;
}

static inline unsigned long find_zero_tlx(unsigned long mask)
{
				return fls64_tlx(mask) >> 3;
}

static inline unsigned int fold_hash_tlx(unsigned long hash)
{
				hash += hash >> (8*sizeof(int));
				return hash;
}


#define WORD_AT_A_TIME_CONSTANTS { REPEAT_BYTE(0x01), REPEAT_BYTE(0x80) }

unsigned long hash_name_tlx(const char *name, unsigned int *hashp)
{
	unsigned long a, b, adata, bdata, mask, hash, len;
	const struct word_at_a_time constants = WORD_AT_A_TIME_CONSTANTS;

	hash = a = 0;
	len = -sizeof(unsigned long);
	do {
		hash = (hash + a) * 9;
		len += sizeof(unsigned long);
		const void *addr = name+len;
			unsigned long ret, offset;

			asm(
			"1:	ldr	%0, %3\n"
			"2:\n"
			"	.pushsection .fixup,\"ax\"\n"
			"	.align 2\n"
			"3:	and	%1, %2, #0x7\n"
			"	bic	%2, %2, #0x7\n"
			"	ldr	%0, [%2]\n"
			"	lsl	%1, %1, #0x3\n"
		#ifndef __AARCH64EB__
			"	lsr	%0, %0, %1\n"
		#else
			"	lsl	%0, %0, %1\n"
		#endif
			"	b	2b\n"
			"	.popsection\n"
			"	.pushsection __ex_table,\"a\"\n"
			"	.align	3\n"
			"	.quad	1b, 3b\n"
			"	.popsection"
			: "=&r" (ret), "=&r" (offset)
			: "r" (addr), "Q" (*(unsigned long *)addr));
		a = ret;
		b = a ^ REPEAT_BYTE('/');
	} while (!(has_zero_tlx(a, &adata, &constants) | has_zero_tlx(b, &bdata, &constants)));

	adata = adata;
	bdata = bdata;

	mask = create_zero_mask_tlx(adata | bdata);

	hash += a & mask;
	*hashp = fold_hash_tlx(hash);

	return len + find_zero_tlx(mask);
}

int link_path_walk_tlx(const char *name, struct nameidata *nd);



int
follow_link_tlx(struct path *link, struct nameidata *nd, void **p)
{
	struct dentry *dentry = link->dentry;
	int error;
	char *s;

//	if (link->mnt == nd->path.mnt)
//		mntget(link->mnt);
	error = -ELOOP;
	cond_resched();
	current->total_link_count++;
	touch_atime_tlx(link);
	nd->saved_names[nd->depth] = NULL;
	nd->last_type = LAST_BIND;
	*p = dentry->d_inode->i_op->follow_link(dentry, nd);
	error = PTR_ERR_tlx(*p);
	error = 0;
	s =nd->saved_names[nd->depth];
	if (s) {
		if (*s == '/') {
			if (!nd->root.mnt) {
//                 get_fs_root(current->fs, &nd->root);
								struct path *root =&nd->root;
								*root = current->fs->root;
								path_get_tlx(root);
			}
			path_put_tlx(&nd->path);
			nd->path = nd->root;
			path_get_tlx(&nd->root);
			nd->flags |= LOOKUP_JUMPED;
		}
		nd->inode = nd->path.dentry->d_inode;
		error = link_path_walk_tlx(s, nd);
	}
	return error;
}


int lookup_fast_tlx(struct nameidata *nd,
					struct path *path, struct inode **inode);



bool legitimize_mnt_tlx(struct vfsmount *bastard, unsigned seq)
{
	struct mount *mnt;
	if (read_seqretry_tlx(&mount_lock_tlx, seq))
		return false;
	mnt = real_mount(bastard);
	this_cpu_add(mnt->mnt_pcp->mnt_count, 1);
	if (likely(!read_seqretry_tlx(&mount_lock_tlx, seq)))
		return true;
	rcu_read_unlock_tlx();
//	clk_get_tlx(bastard);
	rcu_read_lock_tlx();
	return false;
}

int follow_managed_tlx(struct path *path, unsigned flags);
struct dentry *__d_lookup_tlx_tlx(const struct dentry *parent, const struct qstr *name);


int lookup_slow_tlx(struct nameidata *nd, struct path *path)
{
	struct dentry *dentry, *parent;
	int err;

	parent = nd->path.dentry;
//	BUG_ON(nd->inode != parent->d_inode);

	mutex_lock_tlx(&parent->d_inode->i_mutex);
	struct qstr *name = &nd->last;
	struct dentry *base = parent;
	unsigned int flags =  nd->flags;
		bool need_lookup;
		struct dentry *dir = base;
			need_lookup = false;
			unsigned seq;
			do {
                 seq = read_seqbegin_tlx(&rename_lock_tlx);
                 dentry = __d_lookup_tlx_tlx(parent, name);
                 if (dentry)
                         break;
       } while (read_seqretry_tlx(&rename_lock_tlx, seq));
			if (dentry) {
				if (dentry->d_flags & DCACHE_OP_REVALIDATE) {
							dentry->d_op->d_revalidate(dentry, flags);
							dput_tlx(dentry);
							dentry = NULL;
				}
			}
			if (!dentry) {
				dentry = d_alloc_tlx(dir, name);
				need_lookup = true;
			}

		if (!need_lookup)
			goto have_dentry;
			base->d_inode->i_op->lookup(dir, dentry, flags);
have_dentry:
	mutex_unlock_tlx(&parent->d_inode->i_mutex);
	if (IS_ERR_tlx(dentry))
		return PTR_ERR_tlx(dentry);
	path->mnt = nd->path.mnt;
	path->dentry = dentry;
	err = follow_managed_tlx(path, nd->flags);
	if (unlikely(err < 0)) {
		dput_tlx(path->dentry);
//		if (path->mnt != nd->path.mnt)
//			clk_get_tlx(path->mnt);
		return err;
	}
	if (err)
		nd->flags |= LOOKUP_JUMPED;
	return 0;
}

int walk_component_tlx(struct nameidata *nd, struct path *path,
		int follow)
{
	struct inode *inode;
	int err;
	if (unlikely(nd->last_type != LAST_NORM)) {
		return 0;
	}
	err = lookup_fast_tlx(nd, path, &inode);
	if (unlikely(err)) {
		if (err < 0)
			goto out_err;
		err = lookup_slow_tlx(nd, path);
		if (err < 0)
			goto out_err;
		inode = path->dentry->d_inode;
	}
	err = -ENOENT;
	if (!inode || d_is_negative_tlx(path->dentry))
		goto out_path_put;
	if (unlikely(d_is_symlink_tlx(path->dentry)) ? follow : 0) {
		if (nd->flags & LOOKUP_RCU) {
				struct dentry *dentry = path->dentry;
					struct fs_struct *fs = current->fs;
					struct dentry *parent = nd->path.dentry;

					legitimize_mnt_tlx(nd->path.mnt, nd->m_seq);
					nd->flags &= ~LOOKUP_RCU;
						lockref_get_not_dead_tlx(&parent->d_lockref);
					if (!dentry) {
							read_seqcount_retry_tlx(&parent->d_seq, nd->seq);
					} else {
							lockref_get_not_dead_tlx(&dentry->d_lockref);
					}
					if (nd->root.mnt && !(nd->flags & LOOKUP_ROOT)) {
						spin_lock_tlx(&fs->lock);
						path_get_tlx(&nd->root);
						spin_unlock_tlx(&fs->lock);
					}
					rcu_read_unlock_tlx();
		}
		return 1;
	}
		if (!(nd->flags & LOOKUP_RCU)) {
			dput_tlx(nd->path.dentry);
//			if (nd->path.mnt != path->mnt)
//				clk_get_tlx(nd->path.mnt);
		}
		nd->path.mnt = path->mnt;
		nd->path.dentry = path->dentry;
	nd->inode = inode;
	return 0;

out_path_put:
		if (!(nd->flags & LOOKUP_RCU)) {
			dput_tlx(nd->path.dentry);
//			if (nd->path.mnt != path->mnt)
//				mntput_tlx(nd->path.mnt);
		}
		nd->path.mnt = path->mnt;
		nd->path.dentry = path->dentry;
out_err:
	if (!(nd->flags & LOOKUP_RCU)) {
		path_put_tlx(&nd->path);
	} else {
		nd->flags &= ~LOOKUP_RCU;
		if (!(nd->flags & LOOKUP_ROOT))
			nd->root.mnt = NULL;
		rcu_read_unlock_tlx();
	}
	return err;
}



int link_path_walk_tlx(const char *name, struct nameidata *nd)
{
	struct path next;
	int err;
	while (*name=='/')
		name++;
	if (!*name)
		return 0;
	for(;;) {
		struct qstr this;
		long len;
		int type;
		len = hash_name_tlx(name, &this.hash);
		this.name = name;
		this.len = len;
		type = LAST_NORM;
		if (name[0] == '.') switch (len) {
			case 2:
				if (name[1] == '.') {
					type = LAST_DOTDOT;
					nd->flags |= LOOKUP_JUMPED;
				}
				break;
			case 1:
				type = LAST_DOT;
		}
		if (likely(type == LAST_NORM)) {
			struct dentry *parent = nd->path.dentry;
			nd->flags &= ~LOOKUP_JUMPED;
			if (unlikely(parent->d_flags & DCACHE_OP_HASH)) {
				err = parent->d_op->d_hash(parent, &this);
				if (err < 0)
					break;
			}
		}

		nd->last = this;
		nd->last_type = type;

		if (!name[len])
			return 0;
		do {
			len++;
		} while (unlikely(name[len] == '/'));
		if (!name[len])
			return 0;

		name += len;
		err = walk_component_tlx(nd, &next, LOOKUP_FOLLOW);
		if (err < 0)
			return err;
		if (err) {
			struct path *path = &next;
				int res;
				if (unlikely(current->link_count >= MAX_NESTED_LINKS)) {
					dput_tlx(path->dentry);
//					if (path->mnt != nd->path.mnt)
//						mntput_tlx(path->mnt);
					path_put_tlx(&nd->path);
					return -ELOOP;
				}
				nd->depth++;
				current->link_count++;
				do {
					struct path link = *path;
					void *cookie;
					res = follow_link_tlx(&link, nd, &cookie);
					if (res)
						break;
					res = walk_component_tlx(nd, path, LOOKUP_FOLLOW);
						struct inode *inode = link.dentry->d_inode;
						if (inode->i_op->put_link)
							inode->i_op->put_link(link.dentry, nd, cookie);
						path_put_tlx(&link);
				} while (res > 0);
				current->link_count--;
				nd->depth--;
				err = res;
			if (err)
				return err;
		}
	}
	if (!(nd->flags & LOOKUP_RCU)) {
		path_put_tlx(&nd->path);
	} else {
		nd->flags &= ~LOOKUP_RCU;
		if (!(nd->flags & LOOKUP_ROOT))
			nd->root.mnt = NULL;
		rcu_read_unlock_tlx();
	}
	return err;
}

#define FDPUT_POS_UNLOCK 2

unsigned long __fget_light_tlx(unsigned int fd, fmode_t mask)
{
  struct files_struct *files = current->files;
  struct file *file;

  if (atomic_read(&files->count) == 1) {
    struct fdtable *fdt = rcu_dereference_raw(files->fdt);
    if (fd < fdt->max_fds)
                   file = rcu_dereference_raw(fdt->fd[fd]);

    if (!file || unlikely(file->f_mode & mask))
      return 0;
    return (unsigned long)file;
  } else {
      struct files_struct *files = current->files;
      rcu_read_lock_tlx();
      struct fdtable *fdt = rcu_dereference_raw(files->fdt);
      if (fd < fdt->max_fds)
                     file = rcu_dereference_raw(fdt->fd[fd]);

      if (file) {
        /* File object ref couldn't be taken */
        if ((file->f_mode & mask) ||
            !atomic_long_inc_not_zero(&file->f_count))
          file = NULL;
      }
      rcu_read_unlock_tlx();
    if (!file)
      return 0;
    return FDPUT_FPUT | (unsigned long)file;
  }
}

int path_init_tlx(int dfd, const char *name, unsigned int flags,
				struct nameidata *nd, struct file **fp)
{
	int retval = 0;

	nd->last_type = LAST_ROOT; /* if there are only slashes... */
	nd->flags = flags | LOOKUP_JUMPED;
	nd->depth = 0;
	if (flags & LOOKUP_ROOT) {
		struct dentry *root = nd->root.dentry;
		struct inode *inode = root->d_inode;
		nd->path = nd->root;
		nd->inode = inode;
		if (flags & LOOKUP_RCU) {
			rcu_read_lock_tlx();
			nd->seq = __read_seqcount_begin_tlx(&nd->path.dentry->d_seq);
			nd->m_seq = read_seqbegin_tlx(&mount_lock_tlx);
		} else {
			path_get_tlx(&nd->path);
		}
		return 0;
	}
	nd->root.mnt = NULL;
	nd->m_seq = read_seqbegin_tlx(&mount_lock_tlx);
	if (*name=='/') {
		if (flags & LOOKUP_RCU) {
			rcu_read_lock_tlx();
				if (!nd->root.mnt) {
					struct fs_struct *fs = current->fs;
					unsigned seq;

					do {
						seq = read_seqcount_begin_tlx(&fs->seq);
						nd->root = fs->root;
						nd->seq = __read_seqcount_begin_tlx(&nd->root.dentry->d_seq);
					} while (read_seqcount_retry_tlx(&fs->seq, seq));
				}
		} else {
			if (!nd->root.mnt) {
//				       get_fs_root(current->fs, &nd->root);
						struct path *root =&nd->root;
         		*root = current->fs->root;
         		path_get_tlx(root);
			}
			path_get_tlx(&nd->root);
		}
		nd->path = nd->root;
	} else if (dfd == AT_FDCWD) {
		if (flags & LOOKUP_RCU) {
			struct fs_struct *fs = current->fs;
			unsigned seq;
			rcu_read_lock_tlx();
			do {
				seq = read_seqcount_begin_tlx(&fs->seq);
				nd->path = fs->pwd;
				nd->seq = __read_seqcount_begin_tlx(&nd->path.dentry->d_seq);
			} while (read_seqcount_retry_tlx(&fs->seq, seq));
		} else {
//			get_fs_pwd(current->fs, &nd->path);
			 	 struct path *pwd = &nd->path;
        *pwd = current->fs->pwd;
         path_get_tlx(pwd);
		}
	} else {
		struct fd f;
    f = __to_fd( __fget_light_tlx(dfd,0));
		struct dentry *dentry;
		dentry = f.file->f_path.dentry;
		nd->path = f.file->f_path;
		if (flags & LOOKUP_RCU) {
			if (f.flags & FDPUT_FPUT)
				*fp = f.file;
			nd->seq = __read_seqcount_begin_tlx(&nd->path.dentry->d_seq);
			rcu_read_lock_tlx();
		} else {
			path_get_tlx(&nd->path);
			fdput(f);
		}
	}
	nd->inode = nd->path.dentry->d_inode;
	return 0;
}

struct llist_head_tlx delayed_fput_list_tlx;

int
__mod_timer_tlx(struct timer_list *timer, unsigned long expires,
						bool pending_only, int pinned)
{
	struct tvec_base *base, *new_base;
	unsigned long flags;
	int ret = 0 , cpu;
			for (;;) {
				struct tvec_base *prelock_base = timer->base;
				base = ((struct tvec_base *)((unsigned long)prelock_base & ~TIMER_FLAG_MASK));
				if (likely(base != NULL)) {
					spin_lock_irqsave(&base->lock, flags);
					if (likely(prelock_base == timer->base))
						goto have_base;
					spin_unlock_irqrestore_tlx(&base->lock, flags);
				}
				cpu_relax();
			}
have_base:
	if (!ret && pending_only)
		goto out_unlock;
	cpu = smp_processor_id();
	new_base = per_cpu(tvec_bases_tlx, cpu);
	if (base != new_base) {
		if (likely(base->running_timer != timer)) {
			unsigned long flags = (unsigned long)timer->base & TIMER_FLAG_MASK;
			timer->base = (struct tvec_base *)((unsigned long)(NULL) | flags);
			spin_unlock_tlx(&base->lock);
			base = new_base;
			spin_lock_tlx(&base->lock);
			flags = (unsigned long)timer->base & TIMER_FLAG_MASK;
			timer->base = (struct tvec_base *)((unsigned long)(base) | flags);
		}
	}
	timer->expires = expires;
		if (!base->all_timers)
								base->timer_jiffies_tlx = jiffies_tlx;
			unsigned long idx = expires - base->timer_jiffies_tlx;
			struct list_head *vec;
			if (idx < TVR_SIZE) {
				int i = expires & TVR_MASK;
				vec = base->tv1.vec + i;
			} else if (idx < 1 << (TVR_BITS + TVN_BITS)) {
				int i = (expires >> TVR_BITS) & TVN_MASK;
				vec = base->tv2.vec + i;
			} else if (idx < 1 << (TVR_BITS + 2 * TVN_BITS)) {
				int i = (expires >> (TVR_BITS + TVN_BITS)) & TVN_MASK;
				vec = base->tv3.vec + i;
			} else if (idx < 1 << (TVR_BITS + 3 * TVN_BITS)) {
				int i = (expires >> (TVR_BITS + 2 * TVN_BITS)) & TVN_MASK;
				vec = base->tv4.vec + i;
			} else if ((signed long) idx < 0) {
				vec = base->tv1.vec + (base->timer_jiffies_tlx & TVR_MASK);
			} else {
				int i;
				if (idx > MAX_TVAL) {
					idx = MAX_TVAL;
					expires = idx + base->timer_jiffies_tlx;
				}
				i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;
				vec = base->tv5.vec + i;
			}
			list_add_tail(&timer->entry, vec);
	if (!((unsigned int)(unsigned long)timer->base & TIMER_DEFERRABLE)) {
				if (!base->active_timers++ ||
							((long)((timer->expires) - (base->next_timer)) < 0))
											base->next_timer = timer->expires;
	}
	base->all_timers++;

out_unlock:
	spin_unlock_irqrestore_tlx(&base->lock, flags);
	return ret;
}

void __queue_delayed_work_tlx(int cpu, struct workqueue_struct *wq,
				struct delayed_work *_dwork, unsigned long delay)
{
									struct delayed_work *dwork = &_dwork->work;
									struct timer_list *timer = &dwork->timer;
									struct work_struct *work = &dwork->work;
									dwork->wq = system_wq_tlx;
									dwork->cpu = WORK_CPU_UNBOUND;
									timer->expires = jiffies_tlx + 1;
										unsigned long expires = timer->expires;
											unsigned long expires_limit, mask;
											int bit;
											if (timer->slack >= 0) {
												expires_limit = expires + timer->slack;
											} else {
												long delta = expires - jiffies_tlx;
												if (delta < 256)
													goto have_expires;
												expires_limit = expires + delta / 256;
											}
											mask = expires ^ expires_limit;
											if (mask == 0)
												goto have_expires;
											bit = find_last_bit_tlx(&mask, BITS_PER_LONG);
											mask = (1UL << bit) - 1;
											expires = expires_limit & ~(mask);
have_expires:
										__mod_timer_tlx(timer, expires, false, TIMER_NOT_PINNED);

}

struct callback_head work_exited_tlx;



int lookup_open_tlx(struct nameidata *nd, struct path *path,
			struct file *file,
			const struct open_flags *op,
			bool got_write, int *opened)
{
	struct dentry *dir = nd->path.dentry;
	struct inode *dir_inode = dir->d_inode;
	struct dentry *dentry;
	int error;
	bool need_lookup;

	*opened &= ~FILE_CREATED;
		struct qstr *name = &nd->last;
		unsigned int flags = nd->flags;
		need_lookup = false;
		dentry = __d_lookup_tlx_tlx(dir, name);
		if (dentry) {
			if (dentry->d_flags & DCACHE_OP_REVALIDATE) {
				error = dentry->d_op->d_revalidate(dentry, flags);
						dput_tlx(dentry);
						dentry = NULL;
			}
		}
		if (!dentry) {
			dentry = d_alloc_tlx(dir, name);
			need_lookup = true;
		}

	if (!need_lookup && dentry->d_inode)
		goto out_no_open;
	if ((nd->flags & LOOKUP_OPEN) && dir_inode->i_op->atomic_open) {
			struct inode *dir =  nd->path.dentry->d_inode;
			unsigned open_flag = op->open_flag;
			if ((open_flag & O_ACCMODE) == 3)
                 open_flag--;
			umode_t mode;
			int error;
			int acc_mode;
//			int create_error = 0;
			struct dentry *const DENTRY_NOT_SET = (void *) -1UL;
			bool excl;
			mode = op->mode;
			if ((open_flag & O_CREAT) && !IS_POSIXACL(dir))
				mode &= ~current->fs->umask;
			excl = (open_flag & (O_EXCL | O_CREAT)) == (O_EXCL | O_CREAT);
			if (excl)
				open_flag &= ~O_TRUNC;
			if (nd->flags & LOOKUP_DIRECTORY)
				open_flag |= O_DIRECTORY;
			file->f_path.dentry = DENTRY_NOT_SET;
			file->f_path.mnt = nd->path.mnt;
			error = dir->i_op->atomic_open(dir, dentry, file, open_flag, mode,
									opened);
			acc_mode = op->acc_mode;
			if (*opened & FILE_CREATED) {
				acc_mode = MAY_OPEN;
			}
			dput_tlx(dentry);
	}
	if (need_lookup) {
		dir_inode->i_op->lookup(dir_inode, dentry, nd->flags);
	}
	if (!dentry->d_inode && (op->open_flag & O_CREAT)) {
		umode_t mode = op->mode;
		if (!IS_POSIXACL(dir->d_inode))
			mode &= ~current->fs->umask;
		*opened |= FILE_CREATED;
		struct inode *__dir = dir->d_inode;
		bool want_excl = nd->flags & LOOKUP_EXCL;
			mode &= S_IALLUGO;
			mode |= S_IFREG;
			error = __dir->i_op->create(__dir, dentry, mode, want_excl);
	}
out_no_open:
	path->dentry = dentry;
	path->mnt = nd->path.mnt;
	return 1;

}


int complete_walk_tlx(struct nameidata *nd)
{
	struct dentry *dentry = nd->path.dentry;
	int status;

	if (nd->flags & LOOKUP_RCU) {
		nd->flags &= ~LOOKUP_RCU;
		if (!(nd->flags & LOOKUP_ROOT))
			nd->root.mnt = NULL;
				struct lockref *lockref = &dentry->d_lockref;
				struct vfsmount *bastard = nd->path.mnt;
				unsigned seq = nd->m_seq;
					struct mount *mnt;
					smp_rmb();
					if (read_seqretry_tlx(&mount_lock_tlx, seq))
						goto ok;
					mnt = real_mount(bastard);
					this_cpu_add(mnt->mnt_pcp->mnt_count, 1);
					smp_rmb();
					if (likely(!read_seqretry_tlx(&mount_lock_tlx, seq)))
						goto ok;
					rcu_read_unlock_tlx();
//					mntput_tlx(bastard);
					rcu_read_lock_tlx();
ok:
				if ((int) lockref->count >= 0) {
								lockref->count++;
				}
out:
			smp_rmb();
	}
	return 0;

}

struct mount *__lookup_mnt_tlx(struct vfsmount *mnt, struct dentry *dentry)
{
	struct hlist_head *head;// = m_hash(mnt, dentry);
	unsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);
	tmp += ((unsigned long)dentry / L1_CACHE_BYTES);
	tmp = tmp + (tmp >> m_hash_shift_tlx);
	head =  &mount_hashtable_tlx[tmp & m_hash_mask_tlx];
	struct mount *p;
	hlist_for_each_entry_rcu(p, head, mnt_hash)
	if (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)
			return p;
	return NULL;
}

enum slow_d_compare {
	D_COMP_OK,
	D_COMP_NOMATCH,
	D_COMP_SEQRETRY,
};



struct hlist_bl_head *d_hash_tlx(const struct dentry *parent,
					unsigned int hash)
{
	hash += (unsigned long) parent / L1_CACHE_BYTES;
	hash = hash + (hash >> d_hash_shift_tlx);
	return dentry_hashtable_tlx + (hash & d_hash_mask_tlx);
}

struct mnt_namespace {
         atomic_t                count;
         unsigned int            proc_inum;
         struct mount *  root;
         struct list_head        list;
         struct user_namespace   *user_ns;
         u64                     seq;    /* Sequence number to prevent loops */
         wait_queue_head_t poll;
         u64 event;
};


static inline struct hlist_head *m_hash(struct vfsmount *mnt, struct dentry *dentry)
{
          unsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);
         tmp += ((unsigned long)dentry / L1_CACHE_BYTES);
         tmp = tmp + (tmp >> m_hash_shift_tlx);
          return &mount_hashtable_tlx[tmp & m_hash_mask_tlx];
}

void commit_tree_tlx(struct mount *mnt, struct mount *shadows)
{
	struct mount *parent = mnt->mnt_parent;
	struct mount *m;
	LIST_HEAD(head);
	struct mnt_namespace *n = parent->mnt_ns;
	list_add_tail(&head, &mnt->mnt_list);
	list_for_each_entry(m, &head, mnt_list)
		m->mnt_ns = n;
	list_splice(&head, n->list.prev);
	if (shadows)
		hlist_add_after_rcu_tlx(&shadows->mnt_hash, &mnt->mnt_hash);
	else
		hlist_add_head_rcu_tlx(&mnt->mnt_hash,
				m_hash(&parent->mnt, mnt->mnt_mountpoint));
	list_add_tail(&mnt->mnt_child, &parent->mnt_mounts);
}

struct mount *next_mnt_tlx(struct mount *p, struct mount *root)
{
	struct list_head *next = p->mnt_mounts.next;
	if (next == &p->mnt_mounts) {
		while (1) {
			if (p == root)
				return NULL;
			next = p->mnt_child.next;
			if (next != &p->mnt_parent->mnt_mounts)
				break;
			p = p->mnt_parent;
		}
	}
	return list_entry(next, struct mount, mnt_child);
}

#define IS_MNT_SHARED(m) ((m)->mnt.mnt_flags & MNT_SHARED)
#define SET_MNT_MARK(m) ((m)->mnt.mnt_flags |= MNT_MARKED)
#define CL_UNPRIVILEGED         0x40
#define CL_SLAVE                0x02
#define CL_MAKE_SHARED          0x08
#define IS_MNT_MARKED(m) ((m)->mnt.mnt_flags & MNT_MARKED)
#define IS_MNT_NEW(m)  (!(m)->mnt_ns)
#define CL_COPY_MNT_NS_FILE     0x80
#define IS_MNT_UNBINDABLE(m) ((m)->mnt.mnt_flags & MNT_UNBINDABLE)
#define CL_COPY_UNBINDABLE      0x04
#define CL_EXPIRE               0x01
#define IS_MNT_SLAVE(m) ((m)->mnt_master)
#define CLEAR_MNT_SHARED(m) ((m)->mnt.mnt_flags &= ~MNT_SHARED)
#define CL_PRIVATE              0x10
#define CL_SHARED_TO_SLAVE      0x20


struct mountpoint {
         struct hlist_node m_hash;
         struct dentry *m_dentry;
         int m_count;
};

struct mount *last_dest_tlx;
struct mount *last_source_tlx;
struct mount *dest_master_tlx;
struct user_namespace *user_ns_tlx;
struct hlist_head *my_list_tlx;
struct mountpoint *my_mp_tlx;


#define CLEAR_MNT_MARK(m) ((m)->mnt.mnt_flags &= ~MNT_MARKED)
struct mount *next_group_tlx(struct mount *m, struct mount *origin)
{
	while (1) {
		while (1) {
			struct mount *next;
			if (!IS_MNT_NEW(m) && !list_empty(&m->mnt_slave_list))
				return  list_entry(m->mnt_slave_list.next, struct mount, mnt_slave);
			next = list_entry(m->mnt_share.next, struct mount, mnt_share);
			if (m->mnt_group_id == origin->mnt_group_id) {
				if (next == origin)
					return NULL;
			} else if (m->mnt_slave.next != &next->mnt_slave)
				break;
			m = next;
		}
		while (1) {
			struct mount *master = m->mnt_master;
			if (m->mnt_slave.next != &master->mnt_slave_list)
				return  list_entry(m->mnt_slave_list.next, struct mount, mnt_slave);
			m = list_entry(master->mnt_share.next, struct mount, mnt_share);
			if (master->mnt_group_id == origin->mnt_group_id)
				break;
			if (master->mnt_slave.next == &m->mnt_slave)
				break;
			m = master;
		}
		if (m == origin)
			return NULL;
	}
}

bool is_mnt_ns_file(struct dentry *dentry);

struct mount *clone_mnt_tlx(struct mount *old, struct dentry *root,
					int flag)
{
	struct super_block *sb = old->mnt.mnt_sb;
	struct mount *mnt;
	int err;
	mnt = kmem_cache_zalloc_tlx(mnt_cache_tlx, GFP_KERNEL);
	if (mnt) {
		int err;
		if (old->mnt_devname) {
			mnt->mnt_devname = kstrdup_tlx(old->mnt_devname, GFP_KERNEL);
		}

		mnt->mnt_pcp = alloc_percpu(struct mnt_pcp);
		this_cpu_add(mnt->mnt_pcp->mnt_count, 1);
		INIT_HLIST_NODE(&mnt->mnt_hash);
		INIT_LIST_HEAD(&mnt->mnt_child);
		INIT_LIST_HEAD(&mnt->mnt_mounts);
		INIT_LIST_HEAD(&mnt->mnt_list);
		INIT_LIST_HEAD(&mnt->mnt_expire);
		INIT_LIST_HEAD(&mnt->mnt_share);
		INIT_LIST_HEAD(&mnt->mnt_slave_list);
		INIT_LIST_HEAD(&mnt->mnt_slave);
	}

	if (flag & (CL_SLAVE | CL_PRIVATE | CL_SHARED_TO_SLAVE))
		mnt->mnt_group_id = 0; /* not a peer of original */
	else
		mnt->mnt_group_id = old->mnt_group_id;
	mnt->mnt.mnt_flags = old->mnt.mnt_flags & ~(MNT_WRITE_HOLD|MNT_MARKED);
	/* Don't allow unprivileged users to change mount flags */
	if ((flag & CL_UNPRIVILEGED) && (mnt->mnt.mnt_flags & MNT_READONLY))
		mnt->mnt.mnt_flags |= MNT_LOCK_READONLY;
	/* Don't allow unprivileged users to reveal what is under a mount */
	if ((flag & CL_UNPRIVILEGED) && list_empty(&old->mnt_expire))
		mnt->mnt.mnt_flags |= MNT_LOCKED;
	atomic_inc_tlx(&sb->s_active);
	mnt->mnt.mnt_sb = sb;
	mnt->mnt.mnt_root = dget_tlx(root);
	mnt->mnt_mountpoint = mnt->mnt.mnt_root;
	mnt->mnt_parent = mnt;
	list_add_tail(&mnt->mnt_instance, &sb->s_mounts);
	if ((flag & CL_SLAVE) ||
			((flag & CL_SHARED_TO_SLAVE) && IS_MNT_SHARED(old))) {
		list_add(&mnt->mnt_slave, &old->mnt_slave_list);
		mnt->mnt_master = old;
		CLEAR_MNT_SHARED(mnt);
	} else if (!(flag & CL_PRIVATE)) {
		if ((flag & CL_MAKE_SHARED) || IS_MNT_SHARED(old))
			list_add(&mnt->mnt_share, &old->mnt_share);
		if (IS_MNT_SLAVE(old))
			list_add(&mnt->mnt_slave, &old->mnt_slave);
		mnt->mnt_master = old->mnt_master;
	}
	if (flag & CL_MAKE_SHARED) {
//		set_mnt_shared(mnt);
			mnt->mnt.mnt_flags &= ~MNT_SHARED_MASK;
    	mnt->mnt.mnt_flags |= MNT_SHARED;
	}
	/* stick the duplicate mount on the same expiry list
	* as the original if that was on one */
	if (flag & CL_EXPIRE) {
		if (!list_empty(&old->mnt_expire))
			list_add(&mnt->mnt_expire, &old->mnt_expire);
	}
	return mnt;
}

int propagate_one_tlx(struct mount *m)
{
	struct mount *child;
	int type;
	if (IS_MNT_NEW(m))
		return 0;
	if (!is_subdir_tlx(my_mp_tlx->m_dentry, m->mnt.mnt_root))
		return 0;
	if (m->mnt_group_id == last_dest_tlx->mnt_group_id) {
		type = CL_MAKE_SHARED;
	} else {
		struct mount *n, *p;
		for (n = m; ; n = p) {
			p = n->mnt_master;
			if (p == dest_master_tlx || IS_MNT_MARKED(p)) {
				while (last_dest_tlx->mnt_master != p) {
					last_source_tlx = last_source_tlx->mnt_master;
					last_dest_tlx = last_source_tlx->mnt_parent;
				}
				if (n->mnt_group_id != last_dest_tlx->mnt_group_id) {
					last_source_tlx = last_source_tlx->mnt_master;
					last_dest_tlx = last_source_tlx->mnt_parent;
				}
				break;
			}
		}
		type = CL_SLAVE;
		if (IS_MNT_SHARED(m))
			type |= CL_MAKE_SHARED;
	}
	if (m->mnt_ns->user_ns != user_ns_tlx)
		type |= CL_UNPRIVILEGED;
//	child = copy_tree(last_source_tlx, last_source_tlx->mnt.mnt_root, type);
	struct mount *mnt = last_source_tlx;
	struct dentry *dentry = last_source_tlx->mnt.mnt_root;
	int flag = type;
//	{
		struct mount *res, *p, *q, *r, *parent;
		res = q = clone_mnt_tlx(mnt, dentry, flag);
		q->mnt.mnt_flags &= ~MNT_LOCKED;
		q->mnt_mountpoint = mnt->mnt_mountpoint;
		p = mnt;
		list_for_each_entry(r, &mnt->mnt_mounts, mnt_child) {
			struct mount *s;
			if (!is_subdir_tlx(r->mnt_mountpoint, dentry))
				continue;
			for (s = r; s; s = next_mnt_tlx(s, r)) {
				if (!(flag & CL_COPY_UNBINDABLE) &&
						IS_MNT_UNBINDABLE(s)) {
//					s = skip_mnt_tree(s);
						struct list_head *prev = s->mnt_mounts.prev;
						while (prev != &s->mnt_mounts) {
							s = list_entry(prev, struct mount, mnt_child);
							prev = s->mnt_mounts.prev;
						}
					continue;
				}
				while (p != s->mnt_parent) {
					p = p->mnt_parent;
					q = q->mnt_parent;
				}
				p = s;
				parent = q;
				q = clone_mnt_tlx(p, p->mnt.mnt_root, flag);
				list_add_tail(&q->mnt_list, &res->mnt_list);
					p->mnt_mp->m_count++;
						this_cpu_add(parent->mnt_pcp->mnt_count, 1);
					q->mnt_mountpoint = dget_tlx(p->mnt_mp->m_dentry);
					q->mnt_parent = parent;
					q->mnt_mp = p->mnt_mp;

				hlist_add_head_rcu_tlx(&mnt->mnt_hash, m_hash(&parent->mnt, p->mnt_mp->m_dentry));
				list_add_tail(&q->mnt_child, &parent->mnt_mounts);
			}
		}
		child = res;

	if (IS_ERR_tlx(child))
		return PTR_ERR_tlx(child);
			my_mp_tlx->m_count++;
			this_cpu_add(m->mnt_pcp->mnt_count, 1);
			child->mnt_mountpoint = dget_tlx(my_mp_tlx->m_dentry);
			child->mnt_parent = m;
			child->mnt_mp = my_mp_tlx;

	last_dest_tlx = m;
	last_source_tlx = child;
	if (m->mnt_master != dest_master_tlx) {
		read_seqlock_excl_tlx(&mount_lock_tlx);
		SET_MNT_MARK(m->mnt_master);
		read_sequnlock_excl_tlx(&mount_lock_tlx);
	}
	hlist_add_head(&child->mnt_hash, my_list_tlx);
	return 0;
}

//int d_set_mounted(struct dentry *dentry);

int follow_automount_tlx(struct path *path, unsigned flags,
					bool *need_mntput)
{
	struct vfsmount *__mnt;
	int err;
	current->total_link_count++;
	__mnt = path->dentry->d_op->d_automount(path);
	if (!__mnt) /* mount collision */
		return 0;
	if (!*need_mntput) {
// 		 mntget(path->mnt);
		*need_mntput = true;
	}
			struct mount *newmnt = real_mount(__mnt);
			int mnt_flags = path->mnt->mnt_flags | MNT_SHRINKABLE;
				struct mountpoint *mp;
				struct mount *parent;
				mnt_flags &= ~MNT_INTERNAL_FLAGS;
					struct vfsmount *t_mnt;
					struct dentry *t_dentry = path->dentry;
					struct mount *child_mnt;
					struct vfsmount *m;
					unsigned seq;
retry:
            do {
                 seq = read_seqbegin_tlx(&mount_lock_tlx);
                 child_mnt = __lookup_mnt_tlx(path->mnt, path->dentry);
                 m = child_mnt ? &child_mnt->mnt : NULL;
           } while (!legitimize_mnt_tlx(m, seq));

					t_mnt = m;
					if (likely(!t_mnt)) {
						struct dentry *dentry = t_dentry;
								unsigned long tmp = ((unsigned long)dentry / L1_CACHE_BYTES);
								tmp = tmp + (tmp >> mp_hash_shift_tlx);
								struct hlist_head *chain = &mountpoint_hashtable_tlx[tmp & mp_hash_mask_tlx];
							hlist_for_each_entry(mp, chain, m_hash) {
								if (mp->m_dentry == dentry) {
									my_mp_tlx->m_count++;
									return mp;
								}
							}
							mp = kmalloc_tlx(sizeof(struct mountpoint), GFP_KERNEL);
							struct dentry *p;
							write_seqlock_tlx(&rename_lock_tlx);
							for (p = dentry->d_parent; !IS_ROOT(p); p = p->d_parent) {
								spin_lock_tlx(&p->d_lock);
								if (unlikely(d_unhashed_tlx(p))) {
									spin_unlock_tlx(&p->d_lock);
									goto out_m;
								}
								spin_unlock_tlx(&p->d_lock);
							}
							spin_lock_tlx(&dentry->d_lock);
							if (!d_unlinked_tlx(dentry)) {
								dentry->d_flags |= DCACHE_MOUNTED;
							}
							spin_unlock_tlx(&dentry->d_lock);
out_m:
							write_sequnlock_tlx(&rename_lock_tlx);

							mp->m_dentry = dentry;
							mp->m_count = 1;
							hlist_add_head(&mp->m_hash, chain);
						goto have_mp;
					}
					path->mnt = t_mnt;
					t_dentry = path->dentry = dget_tlx(t_mnt->mnt_root);
					goto retry;
have_mp:
				parent = real_mount(path->mnt);
				err = -EINVAL;
				newmnt->mnt.mnt_flags = mnt_flags;
				struct mount *mnt = newmnt;
				struct mount *__p =  parent;
							struct mount *source_mnt = mnt;
							struct mount *dest_mnt = __p;
							struct mountpoint *dest_mp = mp;
							struct path *parent_path = NULL;
								HLIST_HEAD(tree_list);
								struct mount *child, *p;
								struct hlist_node *n;
								if (IS_MNT_SHARED(dest_mnt)) {
											struct mount *p;
											for (p = source_mnt; p; p = true ? next_mnt_tlx(p, source_mnt) : NULL) {
												if (!p->mnt_group_id && !IS_MNT_SHARED(p)) {
												}
											}
											struct mount *m, *n;
											int ret = 0;
											user_ns_tlx = current->nsproxy->mnt_ns->user_ns;
											last_dest_tlx = dest_mnt;
											last_source_tlx = source_mnt;
											mp = dest_mp;
											my_list_tlx = &tree_list;
											dest_master_tlx = dest_mnt->mnt_master;
											for (n = list_entry(dest_mnt->mnt_share.next, struct mount, mnt_share); n != dest_mnt;
																n = list_entry(n->mnt_share.next, struct mount, mnt_share)) {
												ret = propagate_one_tlx(n);
												if (ret)
													goto out_;
											}
											for (m = next_group_tlx(dest_mnt, dest_mnt); m;
													m = next_group_tlx(m, dest_mnt)) {
												n = m;
												do {
													ret = propagate_one_tlx(n);
													if (ret)
														goto out_;
													n = list_entry(n->mnt_share.next, struct mount, mnt_share);;
												} while (n != m);
											}
										out_:
											read_seqlock_excl_tlx(&mount_lock_tlx);
											hlist_for_each_entry(n, &tree_list, mnt_hash) {
												m = n->mnt_parent;
												if (m->mnt_master != dest_mnt->mnt_master)
													CLEAR_MNT_MARK(m->mnt_master);
											}
											read_sequnlock_excl_tlx(&mount_lock_tlx);
									for (p = source_mnt; p; p = next_mnt_tlx(p, source_mnt)){
														mnt->mnt.mnt_flags &= ~MNT_SHARED_MASK;
														mnt->mnt.mnt_flags |= MNT_SHARED;
									}
								} else {
								}
								if (parent_path) {
         					parent_path->dentry = mnt->mnt_mountpoint;
         					parent_path->mnt = &source_mnt->mnt_parent->mnt;
         					source_mnt->mnt_parent = source_mnt;
         					source_mnt->mnt_mountpoint = source_mnt->mnt.mnt_root;
         					list_del_init(&source_mnt->mnt_child);
         					hlist_del_init_rcu_tlx(&source_mnt->mnt_hash);
									 if (!--source_mnt->mnt_mp->m_count) {
                 			struct dentry *dentry = source_mnt->mnt_mp->m_dentry;
               				dentry->d_flags &= ~DCACHE_MOUNTED;
                 			hlist_del(&source_mnt->mnt_mp->m_hash);
               				kfree_tlx(source_mnt->mnt_mp);
         					}
         					source_mnt->mnt_mp = NULL;
									struct mountpoint *mp = (struct mountpoint *) parent_path;
         					my_mp_tlx->m_count++;
									this_cpu_add(dest_mnt->mnt_pcp->mnt_count, 1);
         					source_mnt->mnt_mountpoint = dget_tlx(mp->m_dentry);
         					source_mnt->mnt_parent = dest_mnt;
         					source_mnt->mnt_mp = mp;
         					hlist_add_head_rcu_tlx(&source_mnt->mnt_hash, m_hash(&dest_mnt->mnt, dest_mp->m_dentry));
         					list_add_tail(&source_mnt->mnt_child, &dest_mnt->mnt_mounts);
								} else {
         					dest_mp->m_count++;
         					this_cpu_add(dest_mnt->mnt_pcp->mnt_count, 1);
         					source_mnt->mnt_mountpoint = dget_tlx(dest_mp->m_dentry);
         					source_mnt->mnt_parent = dest_mnt;
         					source_mnt->mnt_mp = dest_mp;
									commit_tree_tlx(source_mnt, NULL);
								}
								hlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {
									struct mount *q;
									hlist_del_init(&child->mnt_hash);
									struct vfsmount *_mnt = &child->mnt_parent->mnt;
									struct dentry *dentry = child->mnt_mountpoint;
										struct mount *_p, *res;
										res = _p = __lookup_mnt_tlx(_mnt, dentry);
										if (!_p)
											goto out;
										hlist_for_each_entry_continue(_p, mnt_hash) {
											if (&_p->mnt_parent->mnt != _mnt || _p->mnt_mountpoint != dentry)
												break;
											res = _p;
										}
							out:
										q= res;
									commit_tree_tlx(child, q);
								}
			unlock:
		path_put_tlx(path);
		path->mnt = __mnt;
		path->dentry = dget_tlx(__mnt->mnt_root);
		return 0;
}


int follow_managed_tlx(struct path *path, unsigned flags)
{
	struct vfsmount *mnt = path->mnt; /* held by caller, must be left alone */
	unsigned managed;
	bool need_mntput = false;
	int ret = 0;
	while (managed = ACCESS_ONCE(path->dentry->d_flags),
				managed &= DCACHE_MANAGED_DENTRY,
				unlikely(managed != 0)) {
		if (managed & DCACHE_MANAGE_TRANSIT) {
			ret = path->dentry->d_op->d_manage(path->dentry, false);
			if (ret < 0)
				break;
		}
		if (managed & DCACHE_MOUNTED) {
			struct mount *child_mnt;
			struct vfsmount *m;
			unsigned seq;
			rcu_read_lock_tlx();
			do {
				seq = read_seqbegin_tlx(&mount_lock_tlx);
				child_mnt = __lookup_mnt_tlx(path->mnt, path->dentry);
				m = child_mnt ? &child_mnt->mnt : NULL;
			} while (!legitimize_mnt_tlx(m, seq));
			rcu_read_unlock_tlx();
			struct vfsmount *mounted = m;
			if (mounted) {
				dput_tlx(path->dentry);
//				if (need_mntput)
//					mntput_tlx(path->mnt);
				path->mnt = mounted;
				path->dentry = dget_tlx(mounted->mnt_root);
				need_mntput = true;
				continue;
			}
		}
		if (managed & DCACHE_NEED_AUTOMOUNT) {
			ret = follow_automount_tlx(path, flags, &need_mntput);
			if (ret < 0)
				break;
			continue;
		}
		break;
	}
//	if (need_mntput && path->mnt == mnt)
//		mntput_tlx(path->mnt);
	return ret < 0 ? ret : need_mntput;
}

struct dentry *__d_lookup_rcu_tlx_tlx(const struct dentry *parent,
				const struct qstr *name,
				unsigned *seqp)
{
	u64 hashlen = name->hash_len;
	const unsigned char *str = name->name;
	struct hlist_bl_head *b = d_hash_tlx(parent, hashlen_hash(hashlen));
	struct hlist_bl_node *node;
	struct dentry *dentry;
	hlist_bl_for_each_entry_rcu(dentry, node, b, d_hash) {
		unsigned seq;
seqretry:
		seq = raw_seqcount_begin_tlx(&dentry->d_seq);
		if (dentry->d_parent != parent)
			continue;
		if (d_unhashed_tlx(dentry))
			continue;
		if (unlikely(parent->d_flags & DCACHE_OP_COMPARE)) {
			if (dentry->d_name.hash != hashlen_hash(hashlen))
				continue;
			*seqp = seq;

		int tlen = dentry->d_name.len;
		const char *tname = dentry->d_name.name;

		if (read_seqcount_retry_tlx(&dentry->d_seq, seq)) {
			cpu_relax();
			goto seqretry;
		}
		if (parent->d_op->d_compare(parent, dentry, tlen, tname, name))
			continue;
		return dentry;
	}

		if (dentry->d_name.hash_len != hashlen)
			continue;
		*seqp = seq;
	}
	return NULL;
}

struct dentry *__d_lookup_tlx_tlx(const struct dentry *parent, const struct qstr *name)
{
	unsigned int len = name->len;
	unsigned int hash = name->hash;
	const unsigned char *str = name->name;
	struct hlist_bl_head *b = d_hash_tlx(parent, hash);
	struct hlist_bl_node *node;
	struct dentry *found = NULL;
	struct dentry *dentry;
	rcu_read_lock_tlx();
	hlist_bl_for_each_entry_rcu(dentry, node, b, d_hash) {
		if (dentry->d_name.hash != hash)
			continue;
		spin_lock_tlx(&dentry->d_lock);
		if (dentry->d_parent != parent)
			goto next;
		if (d_unhashed_tlx(dentry))
			goto next;
		if (parent->d_flags & DCACHE_OP_COMPARE) {
			int tlen = dentry->d_name.len;
			const char *tname = dentry->d_name.name;
			if (parent->d_op->d_compare(parent, dentry, tlen, tname, name))
				goto next;
		} else {
			if (dentry->d_name.len != len)
				goto next;
		}
		dentry->d_lockref.count++;
		found = dentry;
		spin_unlock_tlx(&dentry->d_lock);
		break;
next:
		spin_unlock_tlx(&dentry->d_lock);
	}
	rcu_read_unlock_tlx();

	return found;
}


int lookup_fast_tlx(struct nameidata *nd,
					struct path *path, struct inode **inode)
{
	struct vfsmount *mnt = nd->path.mnt;
	struct dentry *dentry, *parent = nd->path.dentry;
	int need_reval = 1;
	int status = 1;
	int err;
	if (nd->flags & LOOKUP_RCU) {
		unsigned seq;
		dentry = __d_lookup_rcu_tlx_tlx(parent, &nd->last, &seq);
		if (!dentry)
			goto unlazy;
		*inode = dentry->d_inode;
		if (read_seqcount_retry_tlx(&dentry->d_seq, seq))
			return -ECHILD;
		if (__read_seqcount_retry_tlx(&parent->d_seq, nd->seq))
			return -ECHILD;
		nd->seq = seq;
		if (unlikely(dentry->d_flags & DCACHE_OP_REVALIDATE)) {
			status = dentry->d_op->d_revalidate(dentry, nd->flags); //(dentry, nd->flags);
			if (unlikely(status <= 0)) {
				if (status != -ECHILD)
					need_reval = 0;
				goto unlazy;
			}
		}
		path->mnt = mnt;
		path->dentry = dentry;
				for (;;) {
					struct mount *mounted;
					if (!d_mountpoint_tlx(path->dentry))
						return true;
					mounted = __lookup_mnt_tlx(path->mnt, path->dentry);
					if (!mounted)
						break;
					path->mnt = &mounted->mnt;
					path->dentry = mounted->mnt.mnt_root;
					nd->flags |= LOOKUP_JUMPED;
					nd->seq = read_seqcount_begin_tlx(&path->dentry->d_seq);
					*inode = path->dentry->d_inode;
				}
				if (unlikely(!read_seqretry_tlx(&mount_lock_tlx, nd->m_seq)))
							goto unlazy;

		if (unlikely(path->dentry->d_flags & DCACHE_NEED_AUTOMOUNT))
			goto unlazy;
		return 0;
unlazy:
				legitimize_mnt_tlx(nd->path.mnt, nd->m_seq);
				struct fs_struct *fs = current->fs;
				struct dentry *parent = nd->path.dentry;
				nd->flags &= ~LOOKUP_RCU;
					lockref_get_not_dead_tlx(&parent->d_lockref);
				if (!dentry) {
						read_seqcount_retry_tlx(&parent->d_seq, nd->seq);
				} else {
						lockref_get_not_dead_tlx(&dentry->d_lockref);
				}
				if (nd->root.mnt && !(nd->flags & LOOKUP_ROOT)) {
					spin_lock_tlx(&fs->lock);
					path_get_tlx(&nd->root);
					spin_unlock_tlx(&fs->lock);
				}
				rcu_read_unlock_tlx();
	} else {
		dentry = __d_lookup_tlx_tlx(parent, &nd->last);
	}
	if (unlikely(!dentry))
		goto need_lookup;
	if (unlikely(dentry->d_flags & DCACHE_OP_REVALIDATE) && need_reval)
		status = dentry->d_op->d_revalidate(dentry, nd->flags);
	if (unlikely(status <= 0)) {
		if (status < 0) {
			dput_tlx(dentry);
			return status;
		}
		if (!d_invalidate_tlx(dentry)) {
			dput_tlx(dentry);
			goto need_lookup;
		}
	}
	path->mnt = mnt;
	path->dentry = dentry;
	err = follow_managed_tlx(path, nd->flags);
	if (err)
		nd->flags |= LOOKUP_JUMPED;
	*inode = path->dentry->d_inode;
	return 0;
need_lookup:
	return 1;
}

int do_last_tlx(struct nameidata *nd, struct path *path,
			struct file *file, const struct open_flags *op,
			int *opened, struct filename *name)
{
	struct dentry *dir = nd->path.dentry;
	int open_flag = op->open_flag;
	bool will_truncate = (open_flag & O_TRUNC) != 0;
	bool got_write = false;
	int acc_mode = op->acc_mode;
	struct inode *inode;
	bool symlink_ok = false;
	struct path save_parent = { .dentry = NULL, .mnt = NULL };
	bool retried = false;
	int error;

	nd->flags &= ~LOOKUP_PARENT;
	nd->flags |= op->intent;

	if (nd->last_type != LAST_NORM) {
			if (nd->last_type == LAST_DOTDOT){
				if (!nd->root.mnt) {
					nd->root = current->fs->root;
					path_get_tlx(&nd->root);
				}
				while(1) {
					struct dentry *old = nd->path.dentry;
					if (nd->path.dentry == nd->root.dentry &&
							nd->path.mnt == nd->root.mnt) {
						break;
					}
					if (nd->path.dentry != nd->path.mnt->mnt_root) {
						nd->path.dentry = dget_parent_tlx(nd->path.dentry);
						dput_tlx(old);
						break;
					}
							struct path *path = &nd->path;
								struct mount *mnt = real_mount(path->mnt);
								struct mount *parent;
								struct dentry *mountpoint;
								read_seqlock_excl_tlx(&mount_lock_tlx);
								parent = mnt->mnt_parent;
								if (parent == mnt) {
									read_sequnlock_excl_tlx(&mount_lock_tlx);
									break;
								}
	//							mntget(&parent->mnt);
								mountpoint = dget_tlx(mnt->mnt_mountpoint);
								read_sequnlock_excl_tlx(&mount_lock_tlx);
								dput_tlx(path->dentry);
								path->dentry = mountpoint;
//								mntput_tlx(path->mnt);
								path->mnt = &parent->mnt;
				}
				struct path *path = &nd->path;
							while (d_mountpoint_tlx(path->dentry)) {
									struct mount *child_mnt = NULL;
									struct vfsmount *m;
									unsigned seq;
									do {
											seq = read_seqbegin_tlx(&mount_lock_tlx);
											struct vfsmount *mnt = path->mnt;
											struct dentry *dentry = path->dentry;
													unsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);
													tmp += ((unsigned long)dentry / L1_CACHE_BYTES);
													tmp = tmp + (tmp >> m_hash_shift_tlx);
													struct hlist_head *head =  &mount_hashtable_tlx[tmp & m_hash_mask_tlx];
												struct mount *p;
												hlist_for_each_entry_rcu(p, head, mnt_hash)
											if (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)
															child_mnt = p;
											m = child_mnt ? &child_mnt->mnt : NULL;
							} while (!legitimize_mnt_tlx(m, seq));
											if (!m)
															break;
											dput_tlx(path->dentry);
//											mntput_tlx(path->mnt);
											path->mnt = m;
											path->dentry = dget_tlx(m->mnt_root);
							}
				nd->inode = nd->path.dentry->d_inode;
			}
		goto finish_open;
	}

	if (!(open_flag & O_CREAT)) {
		if (nd->last.name[nd->last.len])
			nd->flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;
		if (open_flag & O_PATH && !(nd->flags & LOOKUP_FOLLOW))
			symlink_ok = true;
		error = lookup_fast_tlx(nd, path, &inode);
		if (likely(!error))
			goto finish_lookup;
	} else {
		error = complete_walk_tlx(nd);
		error = -EISDIR;
		if (nd->last.name[nd->last.len])
			goto out;
	}

retry_lookup:
	if (op->open_flag & (O_CREAT | O_TRUNC | O_WRONLY | O_RDWR)) {
//		error = mnt_want_write(nd->path.mnt);

//		if (!error)
			got_write = true;
	}
	mutex_lock_tlx(&dir->d_inode->i_mutex);
	error = lookup_open_tlx(nd, path, file, op, got_write, opened);
	mutex_unlock_tlx(&dir->d_inode->i_mutex);

	if (error <= 0) {
		if ((*opened & FILE_CREATED) ||
				!S_ISREG(file_inode_tlx(file)->i_mode))
			will_truncate = false;
		goto opened;
	}
	if (*opened & FILE_CREATED) {
		open_flag &= ~O_TRUNC;
		will_truncate = false;
		acc_mode = MAY_OPEN;
			if (!(nd->flags & LOOKUP_RCU)) {
				dput_tlx(nd->path.dentry);
//				if (nd->path.mnt != path->mnt)
//					mntput_tlx(nd->path.mnt);
			}
			nd->path.mnt = path->mnt;
			nd->path.dentry = path->dentry;
		goto finish_open_created;
	}
	if (got_write) {
		struct mount *mnt = nd->path.mnt;
		this_cpu_dec(container_of(mnt, struct mount, mnt)->mnt_pcp->mnt_writers);
		if (waitqueue_active_tlx(&(nd->path.mnt->mnt_sb)->s_writers.wait)) {
									wait_queue_head_t *q = &(nd->path.mnt->mnt_sb)->s_writers.wait;
									int nr_exclusive = 1;
									wait_queue_t *curr, *next;
									list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
										unsigned flags = curr->flags;
										if (curr->func(curr, TASK_NORMAL, 0, NULL) &&
																(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
												break;
									}
							}
		got_write = false;
	}

	error = -EEXIST;
	if ((open_flag & (O_EXCL | O_CREAT)) == (O_EXCL | O_CREAT))
		goto exit_dput;
	error = follow_managed_tlx(path, nd->flags);
	if (error < 0)
		goto exit_dput;
	if (error)
		nd->flags |= LOOKUP_JUMPED;
	inode = path->dentry->d_inode;
finish_lookup:
	error = -ENOENT;
	if (!inode || d_is_negative_tlx(path->dentry)) {
			if (!(nd->flags & LOOKUP_RCU)) {
				dput_tlx(nd->path.dentry);
//				if (nd->path.mnt != path->mnt)
//					mntput_tlx(nd->path.mnt);
			}
			nd->path.mnt = path->mnt;
			nd->path.dentry = path->dentry;
		goto out;
	}
	if (unlikely(d_is_symlink_tlx(path->dentry)) ? !symlink_ok : 0) {
		if (nd->flags & LOOKUP_RCU) {
					struct dentry *dentry = path->dentry;
					struct fs_struct *fs = current->fs;
					struct dentry *parent = nd->path.dentry;

					legitimize_mnt_tlx(nd->path.mnt, nd->m_seq);
					nd->flags &= ~LOOKUP_RCU;
						lockref_get_not_dead_tlx(&parent->d_lockref);
					if (!dentry) {
							read_seqcount_retry_tlx(&parent->d_seq, nd->seq);
					} else {
							lockref_get_not_dead_tlx(&dentry->d_lockref);
					}
					if (nd->root.mnt && !(nd->flags & LOOKUP_ROOT)) {
						spin_lock_tlx(&fs->lock);
						path_get_tlx(&nd->root);
						spin_unlock_tlx(&fs->lock);
					}
					rcu_read_unlock_tlx();
		}
		return 1;
	}

	if ((nd->flags & LOOKUP_RCU) || nd->path.mnt != path->mnt) {
				if (!(nd->flags & LOOKUP_RCU)) {
					dput_tlx(nd->path.dentry);
//					if (nd->path.mnt != path->mnt)
	//					mntput_tlx(nd->path.mnt);
				}
				nd->path.mnt = path->mnt;
				nd->path.dentry = path->dentry;
	} else {
		save_parent.dentry = nd->path.dentry;
		save_parent.mnt = path->mnt;
		nd->path.dentry = path->dentry;

	}
	nd->inode = inode;
finish_open:
	error = complete_walk_tlx(nd);
	error = -EISDIR;
	if ((open_flag & O_CREAT) && d_is_dir_tlx(nd->path.dentry))
		goto out;
	error = -ENOTDIR;
	if ((nd->flags & LOOKUP_DIRECTORY) && !d_can_lookup_tlx(nd->path.dentry))
		goto out;
	if (!S_ISREG(nd->inode->i_mode))
		will_truncate = false;

	if (will_truncate) {
	//	error = mnt_want_write(nd->path.mnt);
//		if (error)
//			goto out;
		got_write = true;
	}
finish_open_created:
	file->f_path.mnt = nd->path.mnt;
		struct dentry *dentry = nd->path.dentry;
		file->f_path.dentry = dentry;
		struct file *f =file;
		int (*open)(struct inode *, struct file *) = NULL;
		const struct cred *cred = current_cred();
			static const struct file_operations empty_fops = {};
			f->f_mode = OPEN_FMODE(f->f_flags) | FMODE_LSEEK |
						FMODE_PREAD | FMODE_PWRITE;
			path_get_tlx(&f->f_path);
			inode = f->f_inode = f->f_path.dentry->d_inode;
			f->f_mapping = inode->i_mapping;
			if (unlikely(f->f_flags & O_PATH)) {
				f->f_mode = FMODE_PATH;
				f->f_op = &empty_fops;
				return 0;
			}
			if (f->f_mode & FMODE_WRITE && !special_file(inode->i_mode)) {
				struct vfsmount *m = f->f_path.mnt;
					struct mount *mnt = real_mount(m);
					this_cpu_inc(mnt->mnt_pcp->mnt_writers);
					smp_mb();
					while (ACCESS_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)
						cpu_relax();
					smp_rmb();
					if (m->mnt_sb->s_readonly_remount)
                 goto is_rw;
         smp_rmb();
         if (m->mnt_flags & MNT_READONLY) goto is_ro;
			 	 if (m->mnt_sb->s_flags & MS_RDONLY) goto is_ro;
is_rw:
						this_cpu_dec(mnt->mnt_pcp->mnt_writers);
						error = -EROFS;
is_ro:
				f->f_mode |= FMODE_WRITER;
			}
			if (S_ISREG(inode->i_mode))
				f->f_mode |= FMODE_ATOMIC_POS;
			f->f_op = (inode->i_fop) ? (inode->i_fop) : NULL;// (inode->i_fop);
			smp_mb();
			if (!open)
				open = f->f_op->open;
			if (open) {
				error = open(inode, f);
			}
			if ((f->f_mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ)
				i_readcount_inc_tlx(inode);
			if ((f->f_mode & FMODE_READ) &&
					likely(f->f_op->read || f->f_op->aio_read || f->f_op->read_iter))
				f->f_mode |= FMODE_CAN_READ;
			if ((f->f_mode & FMODE_WRITE) &&
					likely(f->f_op->write || f->f_op->aio_write || f->f_op->write_iter))
				f->f_mode |= FMODE_CAN_WRITE;
			f->f_flags &= ~(O_CREAT | O_EXCL | O_NOCTTY | O_TRUNC);
			struct file_ra_state *ra = &f->f_ra;
			struct address_space *mapping = f->f_mapping->host->i_mapping;
			ra->ra_pages = mapping->backing_dev_info->ra_pages;
			ra->prev_pos = -1;
			error = 0;
		if (!error)
				*opened |= FILE_OPENED;
opened:
out:
	if (got_write) {
		struct mount *mnt = nd->path.mnt;
		this_cpu_dec(container_of(mnt, struct mount, mnt)->mnt_pcp->mnt_writers);
		if (waitqueue_active_tlx(&(nd->path.mnt->mnt_sb)->s_writers.wait)) {
									wait_queue_head_t *q = &(nd->path.mnt->mnt_sb)->s_writers.wait;
									int nr_exclusive = 1;
									wait_queue_t *curr, *next;
									list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
										unsigned flags = curr->flags;
										if (curr->func(curr, TASK_NORMAL, 0, NULL) &&
																(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
												break;
									}

							}
	}
	dput_tlx((&save_parent)->dentry);
//	mntput_tlx((&save_parent)->mnt);
	if (!(nd->flags & LOOKUP_RCU)) {
								path_put_tlx(&nd->path);
	} else {
		nd->flags &= ~LOOKUP_RCU;
		if (!(nd->flags & LOOKUP_ROOT))
												nd->root.mnt = NULL;
		rcu_read_unlock_tlx();
	}
	return error;

exit_dput:
	dput_tlx(path->dentry);
//	if (path->mnt != nd->path.mnt)
//							mntput_tlx(path->mnt);
	goto out;
exit_fput:
	if (atomic_long_dec_and_test_tlx(&file->f_count)) {
		struct task_struct *task = current;
		if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {
			(&file->f_u.fu_rcuhead)->func = NULL;
		}
		if (llist_add_tlx(&file->f_u.fu_llist, &delayed_fput_list_tlx)) {
			struct delayed_work *_dwork = &delayed_fput_work_tlx;
			__queue_delayed_work_tlx(WORK_CPU_UNBOUND,system_wq_tlx, &_dwork->work, 1);
		}
	}
	goto out;

stale_open:
	if (!save_parent.dentry || retried)
		goto out;
	dput_tlx((&nd->path)->dentry);
//	mntput_tlx((&nd->path)->mnt);
	nd->path = save_parent;
	nd->inode = dir->d_inode;
	save_parent.mnt = NULL;
	save_parent.dentry = NULL;
	if (got_write) {
		struct mount *mnt = nd->path.mnt;
		this_cpu_dec(container_of(mnt, struct mount, mnt)->mnt_pcp->mnt_writers);
		if (waitqueue_active_tlx(&(nd->path.mnt->mnt_sb)->s_writers.wait)) {
										wait_queue_head_t *q = &(nd->path.mnt->mnt_sb)->s_writers.wait;
										int nr_exclusive = 1;
										wait_queue_t *curr, *next;
										list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
											unsigned flags = curr->flags;
											if (curr->func(curr, TASK_NORMAL, 0, NULL) &&
																	(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
													break;
										}
								}
		got_write = false;
	}
	retried = true;
	goto retry_lookup;
}


#define RCU_NEXT_TAIL           3

typedef struct {
         unsigned int __softirq_pending;
} ____cacheline_aligned irq_cpustat_t;

irq_cpustat_t irq_stat_tlx[];                /* defined in asm/hardirq.h */
#define __IRQ_STAT(cpu, member) (irq_stat_tlx[cpu].member)
struct task_struct * ksoftirqd_tlx;


struct file *path_openat_tlx(int dfd, struct filename *pathname,
		struct nameidata *nd, const struct open_flags *op, int flags)
{
	struct file *base = NULL;
	struct file *file;
	struct path path;
	int opened = 0;
	int error;
	const struct cred *cred = current_cred();
	static long old_max;
	struct file *f;
	f = kmem_cache_zalloc_tlx(filp_cachep_tlx, GFP_KERNEL);
	percpu_counter_inc_tlx(&nr_files_tlx);
	f->f_cred = get_cred_tlx(cred);
	atomic_long_set_tlx(&f->f_count, 1);
	rwlock_init(&f->f_owner.lock);
	spin_lock_init(&f->f_lock);
	mutex_init(&f->f_pos_lock);
	INIT_LIST_HEAD(&f->f_ep_links);
	INIT_LIST_HEAD(&f->f_tfile_llink);
	file = f;
	file->f_flags = op->open_flag;

	error = path_init_tlx(dfd, pathname->name, flags | LOOKUP_PARENT, nd, &base);
	current->total_link_count = 0;
	error = link_path_walk_tlx(pathname->name, nd);
	if (unlikely(error))
		goto out;

	error = do_last_tlx(nd, &path, file, op, &opened, pathname);
	while (unlikely(error > 0)) { /* trailing symlink */
		struct path link = path;
		void *cookie;
		if (!(nd->flags & LOOKUP_FOLLOW)) {
			dput_tlx((&path)->dentry);
//			if ((&path)->mnt != nd->path.mnt)
//									mntput_tlx((&path)->mnt);
			dput_tlx((&nd->root)->dentry);
//			mntput_tlx((&nd->root)->mnt);
			error = -ELOOP;
			break;
		}
		nd->flags |= LOOKUP_PARENT;
		nd->flags &= ~(LOOKUP_OPEN|LOOKUP_CREATE|LOOKUP_EXCL);
		error = follow_link_tlx(&link, nd, &cookie);
		error = do_last_tlx(nd, &path, file, op, &opened, pathname);
		struct inode *inode = link.dentry->d_inode;
		if (inode->i_op->put_link)
			inode->i_op->put_link(link.dentry, nd, cookie);
		path_put_tlx(&link);

	}
out:
	if (nd->root.mnt && !(nd->flags & LOOKUP_ROOT)) {
			dput_tlx((&nd->root)->dentry);
//			clk_get_tlx((&nd->root)->mnt);
	}
	if (base) {
		if (atomic_long_dec_and_test_tlx(&base->f_count)) {
			struct task_struct *task = current;
			if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {
				(&base->f_u.fu_rcuhead)->func = NULL;
				bool rez = 0;
				struct callback_head *work = &base->f_u.fu_rcuhead;
				struct callback_head *head;
					do {
						head = ACCESS_ONCE(task->task_works);
						if (unlikely(head == &work_exited_tlx)) {
								rez = -ESRCH;
								break;
						}
						work->next = head;

					} while (cmpxchg(&task->task_works, head, work) != head);
				if (!rez)
					goto out1;
			}
			if (llist_add_tlx(&base->f_u.fu_llist, &delayed_fput_list_tlx)) {
					struct delayed_work *_dwork = &delayed_fput_work_tlx;
				__queue_delayed_work_tlx(WORK_CPU_UNBOUND,system_wq_tlx, &_dwork->work, 1);
				}
			}
		}
out1:
	if (!(opened & FILE_OPENED)) {
				if (atomic_long_dec_and_test_tlx(&file->f_count)) {
//          call_rcu_sched(&file->f_u.fu_rcuhead, file_free_rcu);
//					__call_rcu(&file->f_u.fu_rcuhead, file_free_rcu, &rcu_tlx_sched_state, -1, 0);
						struct rcu_head *head = &file->f_u.fu_rcuhead;
						struct rcu_state *rsp = &rcu_tlx_sched_state;
						unsigned long flags;
						struct rcu_data *rdp;
						head->func = file_free_rcu;
						head->next = NULL;
						rdp = this_cpu_ptr(rsp->rda);
						smp_mb();  /* Count before adding callback for rcu_barrier(). */
						*rdp->nxttail[RCU_NEXT_TAIL] = head;
						rdp->nxttail[RCU_NEXT_TAIL] = &head->next;
//						__call_rcu_core(rsp, rdp, head, flags);
//							raise_softirq(RCU_SOFTIRQ);
						unsigned long flags_;
         		local_irq_save(flags_);
         		__IRQ_STAT(smp_processor_id(), __softirq_pending) |= (1UL << RCU_SOFTIRQ);
						if (!in_interrupt()) {
									struct task_struct *tsk = __this_cpu_read(ksoftirqd_tlx);
									if (tsk && tsk->state != TASK_RUNNING)
													wake_up_process_tlx(tsk);
						}
//               			wakeup_softirqd();
         		local_irq_restore(flags_);
/*
	//					 __raise_softirq_irqoff(RCU_SOFTIRQ);
							__IRQ_STAT(smp_processor_id(), __softirq_pending) |= (RCU_SOFTIRQ);
//						wakeup_softirqd();
							struct task_struct *tsk = __this_cpu_read(ksoftirqd_tlx);
         			if (tsk && tsk->state != TASK_RUNNING)
                 			wake_up_process_tlx(tsk);
*/
			}
	}
	if (unlikely(error)) {
		if (error == -EOPENSTALE) {
			if (flags & LOOKUP_RCU)
				error = -ECHILD;
			else
				error = -ESTALE;
		}
		file = ERR_PTR_tlx(error);
	}
	return file;
}



long do_sys_open_tlx(int dfd, const char __user *filename, int flags, umode_t mode)
{
	struct open_flags op;
	int fd = build_open_flags_tlx(flags, mode, &op);
	struct filename *tmp;
	tmp = getname_tlx(filename);
	if (IS_ERR_tlx(tmp))
		return PTR_ERR_tlx(tmp);

//	fd = __alloc_fd(current->files, 0, rlimit(RLIMIT_NOFILE), flags);
	struct files_struct *files = current->files;
					unsigned start = 0;
//					unsigned end = rlimit(RLIMIT_NOFILE);
//		unsigned int fd;
		struct fdtable *fdt;

	//	spin_lock(&files->file_lock);
	repeat:
		fdt = rcu_dereference_check(((files)->fdt), lockdep_is_held(&(files)->file_lock));
		fd = start;
		if (fd < files->next_fd)
			fd = files->next_fd;

		if (fd < fdt->max_fds)
			fd = find_next_zero_bit_tlx(fdt->open_fds, fdt->max_fds, fd);
//		error = -EMFILE;
	//	error = expand_files(files, fd);
	//	if (nr >= fdt->max_fds)
		if (start <= files->next_fd)
			files->next_fd = fd + 1;
	//	__set_open_fd(fd, fdt);
		__set_bit_tlx(fd, fdt->open_fds);

	if (fd >= 0) {
		struct nameidata nd;
		int flags_ = op.lookup_flags;
		struct file *f = path_openat_tlx(dfd, tmp, &nd, &op, flags_ | LOOKUP_RCU);
//		do_filp_open(dfd, tmp, &op);
		if (IS_ERR_tlx(f)) {
//			put_unused_fd(fd);
			fd = PTR_ERR_tlx(f);
		} else {
//			fsnotify_open(f);
//			fd_install(fd, f);
//				__fd_install(current->files, fd, f);
	//			struct files_struct *files = current->files;
					struct fdtable *fdt;
					fdt = rcu_dereference_check(((files)->fdt), lockdep_is_held(&(files)->file_lock));
					rcu_assign_pointer(fdt->fd[fd], f);
		}
	}
	__putname(tmp);
	return fd;
}

SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode)
{
	int dfd = AT_FDCWD;
	struct open_flags op;
	int fd = build_open_flags_tlx(flags, mode, &op);
	struct filename *tmp;
	if (fd)
		return fd;
	tmp = getname_tlx(filename);
	struct filename *pathname = tmp;
	struct nameidata nd;
	struct file *f  = path_openat_tlx(dfd, pathname, &nd, &op, op.lookup_flags | LOOKUP_RCU);
	struct files_struct *files = current->files;
	struct fdtable *fdt = (files)->fdt;
	unsigned start = 0;
		fd = start;
		if (fd < files->next_fd)
			fd = files->next_fd;
		if (fd < fdt->max_fds)
			fd = find_next_zero_bit_tlx(fdt->open_fds, fdt->max_fds, fd);
		if (start <= files->next_fd)
			files->next_fd = fd + 1;
		__set_bit_tlx(fd, fdt->open_fds);
		fdt->fd[fd] = f;
		if (tmp->separate) {
				kmem_cache_free_tlx(names_cachep_tlx, (void *)(tmp->name));
				kfree_tlx(tmp);
		} else {
				kmem_cache_free_tlx(names_cachep_tlx, (void *)(tmp));
		}
		return fd;

}


#define ELF_BASE_PLATFORM NULL
#define STACK_ALLOC(sp, len) ({ sp -= len ; sp; })
#define STACK_ADD(sp, items) ((elf_addr_t __user *)(sp) - (items))
#define STACK_ROUND(sp, items) \
         (((unsigned long) (sp - items)) &~ 15UL)

int
create_elf_tables_tlx(struct linux_binprm *bprm, struct elfhdr *exec,
		unsigned long load_addr, unsigned long interp_load_addr)
{
	unsigned long p = bprm->p;
	int argc = bprm->argc;
	int envc = bprm->envc;
	elf_addr_t __user *argv;
	elf_addr_t __user *envp;
	elf_addr_t __user *sp;
	elf_addr_t __user *u_platform;
	elf_addr_t __user *u_base_platform;
	elf_addr_t __user *u_rand_bytes;
	const char *k_platform = ELF_PLATFORM;
	const char *k_base_platform = ELF_BASE_PLATFORM;
	unsigned char k_rand_bytes[16];
	int items;
	elf_addr_t *elf_info;
	int ei_index = 0;
	const struct cred *cred  = rcu_dereference_protected(current->cred, 1);
	struct vm_area_struct *vma;
	p = p & ~0xf;
	u_platform = NULL;
	if (k_platform) {
		size_t len = strlen_tlx(k_platform) + 1;
		u_platform = (elf_addr_t __user *)STACK_ALLOC(p, len);
		__copy_to_user_tlx(u_platform, k_platform, len);
	}
	u_base_platform = NULL;
	if (k_base_platform) {
		size_t len = strlen_tlx(k_base_platform) + 1;
		u_base_platform = (elf_addr_t __user *)STACK_ALLOC(p, len);
		__copy_to_user_tlx(u_base_platform, k_base_platform, len);
	}

	u_rand_bytes = (elf_addr_t __user *)
					STACK_ALLOC(p, sizeof(k_rand_bytes));
	__copy_to_user_tlx(u_rand_bytes, k_rand_bytes, sizeof(k_rand_bytes));
	elf_info = (elf_addr_t *)current->mm->saved_auxv;

#define NEW_AUX_ENT(id, val) \
	do { \
		elf_info[ei_index++] = id; \
		elf_info[ei_index++] = val; \
	} while (0)

	NEW_AUX_ENT(AT_HWCAP, elf_hwcap_tlx);
	NEW_AUX_ENT(AT_PAGESZ, ELF_EXEC_PAGESIZE);
	NEW_AUX_ENT(AT_CLKTCK, CLOCKS_PER_SEC);
	NEW_AUX_ENT(AT_PHDR, load_addr + exec->e_phoff);
	NEW_AUX_ENT(AT_PHENT, sizeof(struct elf_phdr));
	NEW_AUX_ENT(AT_PHNUM, exec->e_phnum);
	NEW_AUX_ENT(AT_BASE, interp_load_addr);
	NEW_AUX_ENT(AT_FLAGS, 0);
	NEW_AUX_ENT(AT_ENTRY, exec->e_entry);
	NEW_AUX_ENT(AT_UID, from_kuid_munged_tlx(cred->user_ns, cred->uid));
	NEW_AUX_ENT(AT_EUID, from_kuid_munged_tlx(cred->user_ns, cred->euid));
	NEW_AUX_ENT(AT_GID, from_kgid_munged_tlx(cred->user_ns, cred->gid));
	NEW_AUX_ENT(AT_EGID, from_kgid_munged_tlx(cred->user_ns, cred->egid));
	NEW_AUX_ENT(AT_SECURE, 0);
	NEW_AUX_ENT(AT_RANDOM, (elf_addr_t)(unsigned long)u_rand_bytes);
#ifdef elf_hwcap_tlx2
	NEW_AUX_ENT(AT_HWCAP2, elf_hwcap_tlx2);
#endif
	NEW_AUX_ENT(AT_EXECFN, bprm->exec);
	if (k_platform) {
		NEW_AUX_ENT(AT_PLATFORM,
					(elf_addr_t)(unsigned long)u_platform);
	}
	if (k_base_platform) {
		NEW_AUX_ENT(AT_BASE_PLATFORM,
					(elf_addr_t)(unsigned long)u_base_platform);
	}
	if (bprm->interp_flags & BINPRM_FLAGS_EXECFD) {
		NEW_AUX_ENT(AT_EXECFD, bprm->interp_data);
	}
#undef NEW_AUX_ENT
	memset_tlx(&elf_info[ei_index], 0,
				sizeof current->mm->saved_auxv - ei_index * sizeof elf_info[0]);

	ei_index += 2;
	sp = STACK_ADD(p, ei_index);
	items = (argc + 1) + (envc + 1) + 1;
	bprm->p = STACK_ROUND(sp, items);
	sp = (elf_addr_t __user *)bprm->p;
	struct mm_struct * mm = current->mm;
	unsigned long addr = bprm->p;
	unsigned long start;
		addr &= PAGE_MASK;
		vma = find_vma_tlx(mm,addr);
		if (vma->vm_start <= addr)
			goto have_vma;
		start = vma->vm_start;
			addr &= PAGE_MASK;
			if (addr < vma->vm_start) {
				unsigned long size, grow;
				size = vma->vm_end - addr;
				grow = (vma->vm_start - addr) >> PAGE_SHIFT;
				if (grow <= vma->vm_pgoff) {
						vma->vm_start = addr;
						vma->vm_pgoff -= grow;
				}
			}
have_vma:
	__put_user(argc, sp++);
	argv = sp;
	envp = argv + argc + 1;
	p = current->mm->arg_end = current->mm->arg_start;
	while (argc-- > 0) {
		size_t len;
		__put_user((elf_addr_t)p, argv++);
		len = strnlen_user((void __user *)p, MAX_ARG_STRLEN);
		p += len;
	}
	__put_user(0, argv);
	current->mm->arg_end = current->mm->env_start = p;
	while (envc-- > 0) {
		size_t len;
		if (__put_user((elf_addr_t)p, envp++))
			return -EFAULT;
		len = strnlen_user((void __user *)p, MAX_ARG_STRLEN);
		if (!len || len > MAX_ARG_STRLEN)
			return -EINVAL;
		p += len;
	}
	__put_user(0, envp);
	current->mm->env_end = p;
	sp = (elf_addr_t __user *)envp + 1;
		copy_to_user(sp, elf_info, ei_index * sizeof(elf_addr_t));
	return 0;
}

#define MAP_ANONYMOUS   0x20            /* don't use a file */
#define MAP_GROWSDOWN   0x0100          /* stack-like segment */
#define MAP_LOCKED      0x2000          /* pages are locked */


#define _calc_vm_trans(x, bit1, bit2) \
((bit1) <= (bit2) ? ((x) & (bit1)) * ((bit2) / (bit1)) \
: ((x) & (bit1)) / ((bit1) / (bit2)))

unsigned long mmap_region_tlx(struct file *file, unsigned long addr,
		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff);

unsigned long vm_mmap_tlx(struct file *file, unsigned long addr,
	unsigned long len, unsigned long prot,
	unsigned long flag, unsigned long offset)
{

//	return vm_mmap_pgoff(file, addr, len, prot, flag, offset >> PAGE_SHIFT);
			unsigned long pgoff = offset >> PAGE_SHIFT;
			unsigned long ret;
			struct mm_struct *mm = current->mm;
//						struct mm_struct * mm = current->mm;
						vm_flags_t vm_flags;
						len = PAGE_ALIGN(len);
//						addr = get_unmapped_area_tlx(file, addr, len, pgoff, flag);
							unsigned long (*get_area)(struct file *, unsigned long,
											unsigned long, unsigned long, unsigned long);
							get_area = current->mm->get_unmapped_area;
							if (file && file->f_op->get_unmapped_area)
								get_area = file->f_op->get_unmapped_area;
							addr = get_area(file, addr, len, pgoff, flag);
	//					vm_flags = calc_vm_prot_bits(prot)
						vm_flags = _calc_vm_trans(prot, PROT_READ,  VM_READ ) |
												_calc_vm_trans(prot, PROT_WRITE, VM_WRITE) |
												_calc_vm_trans(prot, PROT_EXEC,  VM_EXEC);

						vm_flags = vm_flags | _calc_vm_trans(flag, MAP_GROWSDOWN,  VM_GROWSDOWN ) |
								_calc_vm_trans(flag, MAP_DENYWRITE,  VM_DENYWRITE ) |
								_calc_vm_trans(flag, MAP_LOCKED,     VM_LOCKED    );

						vm_flags = vm_flags |
								mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
						addr = mmap_region_tlx(file, addr, len, vm_flags, pgoff);
						return addr;


}


unsigned long elf_map_tlx(struct file *filep, unsigned long addr,
		struct elf_phdr *eppnt, int prot, int type,
		unsigned long total_size)
{
	unsigned long map_addr;
	unsigned long size = eppnt->p_filesz + ELF_PAGEOFFSET(eppnt->p_vaddr);
	unsigned long off = eppnt->p_offset - ELF_PAGEOFFSET(eppnt->p_vaddr);
	addr = ELF_PAGESTART(addr);
	size = ELF_PAGEALIGN(size);
	if (total_size) {
		total_size = ELF_PAGEALIGN(total_size);
		map_addr = vm_mmap_tlx(filep, addr, total_size, prot, type, off);
	} else
		map_addr = vm_mmap_tlx(filep, addr, size, prot, type, off);

	return(map_addr);
}

int kernel_read_tlx(struct file *file, loff_t offset,
		char *addr, unsigned long count)
{
	mm_segment_t old_fs;
	loff_t pos = offset;
	int result;

	old_fs = get_fs();
	set_fs(get_ds());
	char __user *buf = (void __user *)addr;
		ssize_t ret;
		ret = count > MAX_RW_COUNT ? MAX_RW_COUNT : count;
			count = ret;
				struct file *filp = file;
				size_t len =  count;
				loff_t *ppos = &pos;
					struct iovec iov = { .iov_base = buf, .iov_len = len };
					struct kiocb kiocb;
					struct iov_iter iter;
					kiocb = (struct kiocb) {
							.ki_ctx = NULL,
							.ki_filp = filp,
							.ki_obj.tsk = current,
						};
					kiocb.ki_pos = *ppos;
					kiocb.ki_nbytes = len;
						struct iov_iter *i= &iter;
						int direction = READ;
         if (segment_eq(get_fs(), KERNEL_DS))
                 direction |= ITER_KVEC;
         i->type = direction;
         i->iov = &iov;
         i->nr_segs = 1;
         i->iov_offset = 0;
         i->count = len;
					ret = filp->f_op->read_iter(&kiocb, &iter);
	set_fs(old_fs);
	return result;
}

struct rb_augment_callbacks {
         void (*propagate)(struct rb_node *node, struct rb_node *stop);
         void (*copy)(struct rb_node *old, struct rb_node *new);
         void (*rotate)(struct rb_node *old, struct rb_node *new);
};

static long vma_compute_subtree_gap(struct vm_area_struct *vma)
{
	unsigned long max, subtree_gap;
	max = vma->vm_start;
	if (vma->vm_prev)
		max -= vma->vm_prev->vm_end;
	if (vma->vm_rb.rb_left) {
		subtree_gap = rb_entry(vma->vm_rb.rb_left,
				struct vm_area_struct, vm_rb)->rb_subtree_gap;
		if (subtree_gap > max)
			max = subtree_gap;
	}
	if (vma->vm_rb.rb_right) {
		subtree_gap = rb_entry(vma->vm_rb.rb_right,
				struct vm_area_struct, vm_rb)->rb_subtree_gap;
		if (subtree_gap > max)
			max = subtree_gap;
	}
	return max;
}

void
vma_gap_callbacks_propagate(struct rb_node *rb, struct rb_node *stop);

                                                         \
static inline void                                                      \
vma_gap_callbacks_copy(struct rb_node *rb_old, struct rb_node *rb_new)         \
{                                                                       \
				struct vm_area_struct *old = rb_entry(rb_old, struct vm_area_struct, vm_rb);            \
				struct vm_area_struct *new = rb_entry(rb_new, struct vm_area_struct, vm_rb);            \
				new->rb_subtree_gap = old->rb_subtree_gap;                            \
}                                                                       \
static void                                                             \
vma_gap_callbacks_rotate(struct rb_node *rb_old, struct rb_node *rb_new)       \
{                                                                       \
				struct vm_area_struct *old = rb_entry(rb_old, struct vm_area_struct, vm_rb);            \
				struct vm_area_struct *new = rb_entry(rb_new, struct vm_area_struct, vm_rb);            \
				new->rb_subtree_gap = old->rb_subtree_gap;                            \
				old->rb_subtree_gap = vma_compute_subtree_gap(old);                              \
}                                                                       \
const struct rb_augment_callbacks vma_gap_callbacks_tlx = {                   \
				vma_gap_callbacks_propagate, vma_gap_callbacks_copy, vma_gap_callbacks_rotate        \
};

static int is_mergeable_anon_vma(struct anon_vma *anon_vma1,
          struct anon_vma *anon_vma2,
          struct vm_area_struct *vma)
{
  if ((!anon_vma1 || !anon_vma2) && (!vma ||
    list_is_singular(&vma->anon_vma_chain)))
    return 1;
  return anon_vma1 == anon_vma2;
}

int vma_adjust_tlx(struct vm_area_struct *vma, unsigned long start,
	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert);







static inline int is_mergeable_vma(struct vm_area_struct *vma,
      struct file *file, unsigned long vm_flags)
{
  if ((vma->vm_flags ^ vm_flags) & ~VM_SOFTDIRTY)
    return 0;
  if (vma->vm_file != file)
    return 0;
  if (vma->vm_ops && vma->vm_ops->close)
    return 0;
  return 1;
}


int
can_vma_merge_before_tlx(struct vm_area_struct *vma, unsigned long vm_flags,
	struct anon_vma *anon_vma, struct file *file, pgoff_t vm_pgoff)
{
	if (is_mergeable_vma(vma, file, vm_flags) &&
			is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
		if (vma->vm_pgoff == vm_pgoff)
			return 1;
	}
	return 0;
}


int
can_vma_merge_after_tlx(struct vm_area_struct *vma, unsigned long vm_flags,
	struct anon_vma *anon_vma, struct file *file, pgoff_t vm_pgoff)
{
	if (is_mergeable_vma(vma, file, vm_flags) &&
			is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
		pgoff_t vm_pglen;
		vm_pglen = vma_pages_tlx(vma);
		if (vma->vm_pgoff + vm_pglen == vm_pgoff)
			return 1;
	}
	return 0;
}

struct vm_area_struct *vma_merge_tlx(struct mm_struct *mm,
			struct vm_area_struct *prev, unsigned long addr,
			unsigned long end, unsigned long vm_flags,
					struct anon_vma *anon_vma, struct file *file,
			pgoff_t pgoff, struct mempolicy *policy)
{
	pgoff_t pglen = (end - addr) >> PAGE_SHIFT;
	struct vm_area_struct *area, *next;
	int err;
	if (prev)
		next = prev->vm_next;
	else
		next = mm->mmap;
	area = next;
	if (next && next->vm_end == end)		/* cases 6, 7, 8 */
		next = next->vm_next;
	if (prev && prev->vm_end == addr &&
				mpol_equal(vma_policy(prev), policy) &&
			can_vma_merge_after_tlx(prev, vm_flags,
						anon_vma, file, pgoff)) {
		if (next && end == next->vm_start &&
				mpol_equal(policy, vma_policy(next)) &&
				can_vma_merge_before_tlx(next, vm_flags,
					anon_vma, file, pgoff+pglen) &&
				is_mergeable_anon_vma(prev->anon_vma,
									next->anon_vma, NULL)) {
							/* cases 1, 6 */
			err = vma_adjust_tlx(prev, prev->vm_start,
				next->vm_end, prev->vm_pgoff, NULL);
		} else					/* cases 2, 5, 7 */
			err = vma_adjust_tlx(prev, prev->vm_start,
				end, prev->vm_pgoff, NULL);
		return prev;
	}
	if (next && end == next->vm_start &&
			mpol_equal(policy, vma_policy(next)) &&
			can_vma_merge_before_tlx(next, vm_flags,
					anon_vma, file, pgoff+pglen)) {
		if (prev && addr < prev->vm_end)	/* case 4 */
			err = vma_adjust_tlx(prev, prev->vm_start,
				addr, prev->vm_pgoff, NULL);
		else					/* cases 3, 8 */
			err = vma_adjust_tlx(area, addr, next->vm_end,
				next->vm_pgoff - pglen, NULL);
	}
	return NULL;
}


static unsigned long do_brk_tlx(unsigned long addr, unsigned long len)
{
	struct mm_struct * mm = current->mm;
	struct vm_area_struct * vma, * prev;
	unsigned long flags;
	struct rb_node ** rb_link, * rb_parent;
	pgoff_t pgoff = addr >> PAGE_SHIFT;
	unsigned long end;
	struct vm_area_struct **pprev;

	len = PAGE_ALIGN(len);
	if (!len)
		return addr;

	flags = VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;
		unsigned long (*get_area)(struct file *, unsigned long,
						unsigned long, unsigned long, unsigned long);
		get_area = current->mm->get_unmapped_area;
		addr = get_area(NULL, addr, len, 0, MAP_FIXED);
		addr = addr;
munmap_back:
		end = addr + len;
		pprev = &prev;
	struct rb_node **__rb_link=  NULL, *__rb_parent = NULL, *rb_prev = NULL;
	__rb_link = &mm->mm_rb.rb_node;
	rb_prev = __rb_parent = NULL;
	while (*__rb_link) {
		struct vm_area_struct *vma_tmp;
		__rb_parent = *__rb_link;
		vma_tmp = rb_entry(__rb_parent, struct vm_area_struct, vm_rb);
		if (vma_tmp->vm_end > addr) {
			/* Fail if an existing vma overlaps the area */
			if (vma_tmp->vm_start < end)
				goto munmap_back;
			__rb_link = &__rb_parent->rb_left;
		} else {
			rb_prev = __rb_parent;
			__rb_link = &__rb_parent->rb_right;
		}
	}
	*pprev = NULL;
	if (rb_prev)
		*pprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);
	rb_link = __rb_link;
	rb_parent = __rb_parent;
				end = addr + len;
				unsigned long vm_flags = flags;
				struct anon_vma *anon_vma  = NULL;
				struct file *file  = NULL;
				struct mempolicy *policy = NULL;
		pgoff_t pglen = (end - addr) >> PAGE_SHIFT;
		struct vm_area_struct *area, *__next;
		int err;
		if (prev)
			__next = prev->vm_next;
		else
			__next = mm->mmap;
		area = __next;
		if (__next && __next->vm_end == end)		/* cases 6, 7, 8 */
			__next = __next->vm_next;

     int rez = 0;
     struct vm_area_struct *vma__ = prev;
      if (is_mergeable_vma(vma__, file, vm_flags) &&
          is_mergeable_anon_vma(anon_vma, vma__->anon_vma, vma__)) {
        pgoff_t vm_pglen;
        vm_pglen = vma_pages_tlx(vma__);
        if (vma__->vm_pgoff + vm_pglen == pgoff)
          rez = 1;
      }

		if (prev && prev->vm_end == addr &&
					mpol_equal(vma_policy(prev), policy) &&
				rez) {
          int rez = 0;
          struct vm_area_struct *__vma = __next;
            if (is_mergeable_vma(vma__, file, vm_flags) &&
                is_mergeable_anon_vma(anon_vma, vma__->anon_vma, vma__)) {
              if (vma__->vm_pgoff == pgoff+pglen)
                rez = 1;
            }


			if (__next && end == __next->vm_start &&
					mpol_equal(policy, vma_policy(__next)) &&
				  rez &&
					is_mergeable_anon_vma(prev->anon_vma,
										__next->anon_vma, NULL)) {
								/* cases 1, 6 */
				err = vma_adjust_tlx(prev, prev->vm_start,
					__next->vm_end, prev->vm_pgoff, NULL);
			} else					/* cases 2, 5, 7 */
				err = vma_adjust_tlx(prev, prev->vm_start,
					end, prev->vm_pgoff, NULL);
			vma = prev;
			goto have_vma;
		}
    rez = 0;
    struct vm_area_struct *__vma = __next;
      if (is_mergeable_vma(vma__, file, vm_flags) &&
          is_mergeable_anon_vma(anon_vma, vma__->anon_vma, vma__)) {
        if (vma__->vm_pgoff == pgoff+pglen)
          rez = 1;
      }

		if (__next && end == __next->vm_start &&
				mpol_equal(policy, vma_policy(__next)) &&
				rez) {
			if (prev && addr < prev->vm_end)	/* case 4 */
				err = vma_adjust_tlx(prev, prev->vm_start,
					addr, prev->vm_pgoff, NULL);
			else					/* cases 3, 8 */
				err = vma_adjust_tlx(area, addr, __next->vm_end,
					__next->vm_pgoff - pglen, NULL);
		}
have_vma:
	vma = kmem_cache_zalloc_tlx(vm_area_cachep_tlx, GFP_KERNEL);
	INIT_LIST_HEAD(&vma->anon_vma_chain);
	vma->vm_mm = mm;
	vma->vm_start = addr;
	vma->vm_end = addr + len;
	vma->vm_pgoff = pgoff;
	vma->vm_flags = flags;
	vma->vm_page_prot = __pgprot(pgprot_val(protection_map_tlx[flags &
																	(VM_READ|VM_WRITE|VM_EXEC|VM_SHARED)]) |
													pgprot_val(__pgprot(0)));

			struct vm_area_struct *next;
			vma->vm_prev = prev;
			if (prev) {
				next = prev->vm_next;
				prev->vm_next = vma;
			} else {
				mm->mmap = vma;
				if (rb_parent)
					next = rb_entry(rb_parent,
							struct vm_area_struct, vm_rb);
				else
					next = NULL;
			}
			vma->vm_next = next;
			if (next)
				next->vm_prev = vma;
			if (vma->vm_next)
			{
			} else
				mm->highest_vm_end = vma->vm_end;
			rb_link_node_tlx(&vma->vm_rb, rb_parent, rb_link);
			vma->rb_subtree_gap = 0;
//			rb_insert_augmented(&vma->vm_rb, &mm->mm_rb, &vma_gap_callbacks);
//			__rb_insert_augmented(&vma->vm_rb, &mm->mm_rb, vma_gap_callbacks.rotate);
      __rb_insert_tlx(&vma->vm_rb, &mm->mm_rb, vma_gap_callbacks_tlx.rotate);
		mm->map_count++;
out:
	mm->total_vm += len >> PAGE_SHIFT;
	if (flags & VM_LOCKED)
		mm->locked_vm += (len >> PAGE_SHIFT);
	vma->vm_flags |= VM_SOFTDIRTY;
	return addr;
}

unsigned long load_elf_interp_tlx(struct elfhdr *interp_elf_ex,
		struct file *interpreter, unsigned long *interp_map_addr,
		unsigned long no_base)
{
	struct elf_phdr *elf_phdata;
	struct elf_phdr *eppnt;
	unsigned long load_addr = 0;
	int load_addr_set = 0;
	unsigned long last_bss = 0, elf_bss = 0;
	unsigned long error = ~0UL;
	unsigned long total_size = 0;
	int retval, i, size;
	size = sizeof(struct elf_phdr) * interp_elf_ex->e_phnum;
	if (size > ELF_MIN_ALIGN)
		goto out;
	elf_phdata = kmalloc_tlx(size, GFP_KERNEL);
	retval = kernel_read_tlx(interpreter, interp_elf_ex->e_phoff,
					(char *)elf_phdata, size);
	error = -EIO;
		struct elf_phdr *cmds = elf_phdata;
		int nr = interp_elf_ex->e_phnum;
			int first_idx = -1, last_idx = -1;

			for (i = 0; i < nr; i++) {
				if (cmds[i].p_type == PT_LOAD) {
					last_idx = i;
					if (first_idx == -1)
						first_idx = i;
				}
			}
			if (first_idx == -1)
				goto have_ts;

			total_size = cmds[last_idx].p_vaddr + cmds[last_idx].p_memsz -
						ELF_PAGESTART(cmds[first_idx].p_vaddr);
have_ts:
	eppnt = elf_phdata;
	for (i = 0; i < interp_elf_ex->e_phnum; i++, eppnt++) {
		if (eppnt->p_type == PT_LOAD) {
			int elf_type = MAP_PRIVATE | MAP_DENYWRITE;
			int elf_prot = 0;
			unsigned long vaddr = 0;
			unsigned long k, map_addr;

			if (eppnt->p_flags & PF_R)
						elf_prot = PROT_READ;
			if (eppnt->p_flags & PF_W)
				elf_prot |= PROT_WRITE;
			if (eppnt->p_flags & PF_X)
				elf_prot |= PROT_EXEC;
			vaddr = eppnt->p_vaddr;
			if (interp_elf_ex->e_type == ET_EXEC || load_addr_set)
				elf_type |= MAP_FIXED;
			else if (no_base && interp_elf_ex->e_type == ET_DYN)
				load_addr = -vaddr;

			map_addr = elf_map_tlx(interpreter, load_addr + vaddr,
					eppnt, elf_prot, elf_type, total_size);
			total_size = 0;
			if (!*interp_map_addr)
				*interp_map_addr = map_addr;
			error = map_addr;
			if (!load_addr_set &&
					interp_elf_ex->e_type == ET_DYN) {
				load_addr = map_addr - ELF_PAGESTART(vaddr);
				load_addr_set = 1;
			}
			k = load_addr + eppnt->p_vaddr;
			k = load_addr + eppnt->p_vaddr + eppnt->p_filesz;
			if (k > elf_bss)
				elf_bss = k;
			k = load_addr + eppnt->p_memsz + eppnt->p_vaddr;
			if (k > last_bss)
				last_bss = k;
		}
	}

	if (last_bss > elf_bss) {
		unsigned long nbyte;
		nbyte = ELF_PAGEOFFSET(elf_bss);
		if (nbyte) {
			nbyte = ELF_MIN_ALIGN - nbyte;
				clear_user((void __user *) elf_bss, nbyte);
		}
		elf_bss = ELF_PAGESTART(elf_bss + ELF_MIN_ALIGN - 1);
		error = do_brk_tlx(elf_bss, last_bss - elf_bss);
	}

	error = load_addr;

out_close:
	kfree_tlx(elf_phdata);
out:
	return error;
}

int setup_arg_pages_tlx(struct linux_binprm *bprm,
				unsigned long stack_top,
				int executable_stack)
{
	unsigned long ret;
	unsigned long stack_shift;
	struct mm_struct *mm = current->mm;
	struct vm_area_struct *vma = bprm->vma;
	struct vm_area_struct *prev = NULL;
	unsigned long vm_flags;
	unsigned long stack_base;
	unsigned long stack_size;
	unsigned long stack_expand;
	unsigned long rlim_stack;
	stack_top = PAGE_ALIGN(stack_top);
	stack_shift = vma->vm_end - stack_top;
	bprm->p -= stack_shift;
	mm->arg_start = bprm->p;
	if (bprm->loader)
		bprm->loader -= stack_shift;
	bprm->exec -= stack_shift;
	down_write_tlx(&mm->mmap_sem);
	vm_flags = VM_STACK_FLAGS;
	if (unlikely(executable_stack == EXSTACK_ENABLE_X))
		vm_flags |= VM_EXEC;
	else if (executable_stack == EXSTACK_DISABLE_X)
		vm_flags &= ~VM_EXEC;
	vm_flags |= mm->def_flags;
	vm_flags |= VM_STACK_INCOMPLETE_SETUP;
	vma->vm_flags &= ~VM_STACK_INCOMPLETE_SETUP;
	stack_expand = 131072UL; /* randomly 32*4k (or 2*64k) pages */
	stack_size = vma->vm_end - vma->vm_start;
	rlim_stack = rlimit_tlx(RLIMIT_STACK) & PAGE_MASK;
	if (stack_size + stack_expand > rlim_stack)
		stack_base = vma->vm_end - rlim_stack;
	else
		stack_base = vma->vm_start - stack_expand;
	current->mm->start_stack = bprm->p;
	unsigned long address = stack_base;
	address &= PAGE_MASK;
	prev = vma->vm_prev;
	address &= PAGE_MASK;
	if (address < vma->vm_start) {
		unsigned long size, grow;
		size = vma->vm_end - address;
		grow = (vma->vm_start - address) >> PAGE_SHIFT;
		if (grow <= vma->vm_pgoff) {
				vma->vm_start = address;
				vma->vm_pgoff -= grow;
		}
	}
out_unlock:
	up_write_tlx(&mm->mmap_sem);
	return 0;
}

int load_elf_binary_tlx(struct linux_binprm *bprm)
{
	struct file *interpreter = NULL; /* to shut gcc up */
	unsigned long load_addr = 0, load_bias = 0;
	int load_addr_set = 0;
	char * elf_interpreter = NULL;
	unsigned long error;
	struct elf_phdr *elf_ppnt, *elf_phdata;
	unsigned long elf_bss, elf_brk;
	int retval, i;
	unsigned int size;
	unsigned long elf_entry;
	unsigned long interp_load_addr = 0;
	unsigned long start_code, end_code, start_data, end_data;
	unsigned long reloc_func_desc __maybe_unused = 0;
	int executable_stack = EXSTACK_DEFAULT;
	struct pt_regs *regs = ((struct pt_regs *)(THREAD_START_SP + (current)->stack) - 1);
	struct {
		struct elfhdr elf_ex;
		struct elfhdr interp_elf_ex;
	} *loc;

	loc = kmalloc_tlx(sizeof(*loc), GFP_KERNEL);
	loc->elf_ex = *((struct elfhdr *)bprm->buf);
	memcmp_tlx(loc->elf_ex.e_ident, ELFMAG, SELFMAG);
	size = loc->elf_ex.e_phnum * sizeof(struct elf_phdr);
	retval = -ENOMEM;
	elf_phdata = kmalloc_tlx(size, GFP_KERNEL);
	retval = kernel_read_tlx(bprm->file, loc->elf_ex.e_phoff,
					(char *)elf_phdata, size);
	elf_ppnt = elf_phdata;
	elf_bss = 0;
	elf_brk = 0;

	start_code = ~0UL;
	end_code = 0;
	start_data = 0;
	end_data = 0;

	for (i = 0; i < loc->elf_ex.e_phnum; i++) {
		if (elf_ppnt->p_type == PT_INTERP) {
			retval = -ENOEXEC;
			retval = -ENOMEM;
			elf_interpreter = kmalloc_tlx(elf_ppnt->p_filesz,
							GFP_KERNEL);
			retval = kernel_read_tlx(bprm->file, elf_ppnt->p_offset,
							elf_interpreter,
							elf_ppnt->p_filesz);
			retval = -ENOEXEC;
			#define LOOKUP_FOLLOW           0x0001
			#define LOOKUP_OPEN             0x0100
			#define LOOKUP_RCU              0x0040

			struct filename tmp = { .name = elf_interpreter };
			static const struct open_flags open_exec_flags = {
				.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,
				.acc_mode = MAY_EXEC | MAY_OPEN,
				.intent = LOOKUP_OPEN,
				.lookup_flags = LOOKUP_FOLLOW,
			};
			struct nameidata nd;
			interpreter = path_openat_tlx(AT_FDCWD, &tmp, &nd, &open_exec_flags, (&open_exec_flags)->lookup_flags | LOOKUP_RCU);

			retval = PTR_ERR_tlx(interpreter);
			retval = kernel_read_tlx(interpreter, 0, bprm->buf,
							BINPRM_BUF_SIZE);
			loc->interp_elf_ex = *((struct elfhdr *)bprm->buf);
			break;
		}
		elf_ppnt++;
	}

	elf_ppnt = elf_phdata;
	for (i = 0; i < loc->elf_ex.e_phnum; i++, elf_ppnt++)
		if (elf_ppnt->p_type == PT_GNU_STACK) {
			if (elf_ppnt->p_flags & PF_X)
				executable_stack = EXSTACK_ENABLE_X;
			else
				executable_stack = EXSTACK_DISABLE_X;
			break;
		}
	if (elf_interpreter) {
			memcmp_tlx(loc->interp_elf_ex.e_ident, ELFMAG, SELFMAG);
	}
	bprm->mm->exe_file = bprm->file;
	struct mm_struct *t_mm = current->mm;
	long diff = (long)(0 - bprm->vma_pages);
		if (t_mm && diff) {
			bprm->vma_pages = 0;
			atomic_long_add_tlx(diff, &t_mm->rss_stat.count[MM_ANONPAGES]);
	}
	struct mm_struct *mm = bprm->mm;
	struct task_struct *tsk;
	struct mm_struct *old_mm, *active_mm;
	tsk = current;
	old_mm = current->mm;
	mm_release_tlx(tsk, old_mm);

	if (old_mm) {
				sync_mm_rss_tlx(old_mm);
				down_read_tlx(&old_mm->mmap_sem);
	}
	spin_lock_tlx(&tsk->alloc_lock);
	active_mm = tsk->active_mm;
	tsk->mm = mm;
	tsk->active_mm = mm;
	if (!cpumask_test_and_set_cpu_tlx(smp_processor_id(), mm_cpumask_tlx(mm)) || active_mm != mm) {
				unsigned long ttbr = page_to_phys(empty_zero_page_tlx);
					asm(
						"       msr     ttbr0_el1, %0                   // set TTBR0\n"
						"       isb"
						:
						: "r" (ttbr));
				if (!((mm->context.id ^ cpu_last_asid_tlx) >> MAX_ASID_BITS))
					cpu_switch_mm(mm->pgd, mm);
				else if (irqs_disabled())
					set_ti_thread_flag_tlx(task_thread_info(tsk), TIF_SWITCH_MM);
				else {
					unsigned long flags;
					__new_context_tlx(mm);
					local_irq_save(flags);
					cpu_switch_mm(mm->pgd, mm); //!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
					local_irq_restore(flags);
				}
		}
	tsk->mm->vmacache_seqnum = 0;
	memset_tlx(tsk->vmacache, 0, sizeof(tsk->vmacache));
	spin_unlock_tlx(&tsk->alloc_lock);

	if (old_mm) {
				up_read_tlx(&old_mm->mmap_sem);
					unsigned long hiwater_rss = max(old_mm->hiwater_rss, atomic_long_read_tlx(&old_mm->rss_stat.count[MM_FILEPAGES]) +
									atomic_long_read_tlx(&old_mm->rss_stat.count[MM_ANONPAGES]));
				if (tsk->signal->maxrss < hiwater_rss)
								tsk->signal->maxrss = hiwater_rss;
				mm_update_next_owner_tlx(old_mm);
				mmput_tlx(old_mm);
				goto out;
	}
			mmdrop_tlx(active_mm);
out:
	bprm->mm = NULL;		/* We're using it now */
	set_fs(USER_DS);
	current->flags &= ~(PF_RANDOMIZE | PF_FORKNOEXEC | PF_KTHREAD |
						PF_NOFREEZE | PF_NO_SETAFFINITY);
		memset_tlx(&current->thread.fpsimd_state, 0, sizeof(struct fpsimd_state));
				set_thread_flag(TIF_FOREIGN_FPSTATE);
		asm ("msr tpidr_el0, xzr");
		if (is_compat_task_tlx()) {
								current->thread.tp_value = 0;
								barrier();
								asm ("msr tpidrro_el0, xzr");
		}
	current->personality &= ~bprm->per_clear;
	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space_tlx)
		current->flags |= PF_RANDOMIZE;
	current->mm->mmap_base = TASK_UNMAPPED_BASE;
	current->mm->get_unmapped_area = arch_get_unmapped_area_tlx;
	current->sas_ss_sp = current->sas_ss_size = 0;
	strlcpy_tlx(current->comm, kbasename(bprm->filename), sizeof(current->comm));
	current->mm->task_size = TASK_SIZE;
	current->self_exec_id++;
	retval = setup_arg_pages_tlx(bprm, PAGE_ALIGN(STACK_TOP),
				executable_stack);
	current->mm->start_stack = bprm->p;
	for(i = 0, elf_ppnt = elf_phdata;
			i < loc->elf_ex.e_phnum; i++, elf_ppnt++) {
		int elf_prot = 0, elf_flags;
		unsigned long k, vaddr;

		if (elf_ppnt->p_type != PT_LOAD)
			continue;
		if (elf_ppnt->p_flags & PF_R)
			elf_prot |= PROT_READ;
		if (elf_ppnt->p_flags & PF_W)
			elf_prot |= PROT_WRITE;
		if (elf_ppnt->p_flags & PF_X)
			elf_prot |= PROT_EXEC;

		elf_flags = MAP_PRIVATE | MAP_DENYWRITE | MAP_EXECUTABLE;

		vaddr = elf_ppnt->p_vaddr;
		if (loc->elf_ex.e_type == ET_EXEC || load_addr_set) {
			elf_flags |= MAP_FIXED;
		}
		error = elf_map_tlx(bprm->file, load_bias + vaddr, elf_ppnt,
				elf_prot, elf_flags, 0);
		if (!load_addr_set) {
			load_addr_set = 1;
			load_addr = (elf_ppnt->p_vaddr - elf_ppnt->p_offset);
			if (loc->elf_ex.e_type == ET_DYN) {
				load_bias += error -
										ELF_PAGESTART(load_bias + vaddr);
				load_addr += load_bias;
				reloc_func_desc = load_bias;
			}
		}
		k = elf_ppnt->p_vaddr;
		if (k < start_code)
			start_code = k;
		if (start_data < k)
			start_data = k;
		k = elf_ppnt->p_vaddr + elf_ppnt->p_filesz;

		if (k > elf_bss)
			elf_bss = k;
		if ((elf_ppnt->p_flags & PF_X) && end_code < k)
			end_code = k;
		if (end_data < k)
			end_data = k;
		k = elf_ppnt->p_vaddr + elf_ppnt->p_memsz;
		if (k > elf_brk)
			elf_brk = k;
	}
	loc->elf_ex.e_entry += load_bias;
	elf_bss += load_bias;
	elf_brk += load_bias;
	start_code += load_bias;
	end_code += load_bias;
	start_data += load_bias;
	end_data += load_bias;
	unsigned long start;
	unsigned long end;
	start = ELF_PAGEALIGN(elf_bss);
	end = ELF_PAGEALIGN(elf_brk);
	if (end > start) {
							unsigned long addr;
							addr = do_brk_tlx(start, end - start);
	}
	current->mm->start_brk = current->mm->brk = end;

	unsigned long nbyte;
	nbyte = ELF_PAGEOFFSET(elf_bss);
	nbyte = ELF_MIN_ALIGN - nbyte;
	clear_user((void __user *) elf_bss, nbyte);
	unsigned long interp_map_addr = 0;
	elf_entry = load_elf_interp_tlx(&loc->interp_elf_ex,
							interpreter,
							&interp_map_addr,
							load_bias);
	if (!IS_ERR_tlx((void *)elf_entry)) {
			interp_load_addr = elf_entry;
			elf_entry += loc->interp_elf_ex.e_entry;
	}
	fput(interpreter);
	kfree_tlx(elf_interpreter);
	kfree_tlx(elf_phdata);
	current->mm->binfmt = &elf_format_tlx;
	bprm->cred = NULL;
	mutex_unlock_tlx(&current->signal->cred_guard_mutex);
	retval = create_elf_tables_tlx(bprm, &loc->elf_ex,
				load_addr, interp_load_addr);
	current->mm->end_code = end_code;
	current->mm->start_code = start_code;
	current->mm->start_data = start_data;
	current->mm->end_data = end_data;
	current->mm->start_stack = bprm->p;
	memset_tlx(regs, 0, sizeof(*regs));
	regs->syscallno = ~0UL;
	regs->pc = elf_entry;
	regs->pstate = PSR_MODE_EL0t;
	regs->sp = bprm->p;
	retval = 0;
	kfree_tlx(loc);
	return retval;
}



struct vm_area_struct *
find_extend_vma_tlx(struct mm_struct * mm, unsigned long addr)
{
	struct vm_area_struct * vma;
	unsigned long start;

	addr &= PAGE_MASK;
	vma = find_vma_tlx(mm,addr);
	if (vma->vm_start <= addr)
		return vma;
	start = vma->vm_start;
		addr &= PAGE_MASK;
		if (addr < vma->vm_start) {
			unsigned long size, grow;
			size = vma->vm_end - addr;
			grow = (vma->vm_start - addr) >> PAGE_SHIFT;
			if (grow <= vma->vm_pgoff) {
					vma->vm_start = addr;
					vma->vm_pgoff -= grow;
			}
		}
	return vma;
}

static inline int pte_same(pte_t pte_a, pte_t pte_b)
{
       return pte_val(pte_a) == pte_val(pte_b);
}

static inline pte_t pte_mkspecial(pte_t pte)
{
       pte_val(pte) |= PTE_SPECIAL;
         return pte;
}


#define my_zero_pfn(addr)       page_to_pfn(ZERO_PAGE(addr))

#define set_pte_at_notify set_pte_at
#define mk_pte(page,prot)       pfn_pte(page_to_pfn(page),prot)
#define inc_mm_counter_fast(mm, member) inc_mm_counter(mm, member)
#define dec_mm_counter_fast(mm, member) dec_mm_counter(mm, member)
#define alloc_page_vma(gfp_mask, vma, addr)                     \
       alloc_pages_vma(gfp_mask, 0, vma, addr, numa_node_id_tlx())
#define alloc_pages_vma(gfp_mask, order, vma, addr, node)       \
				alloc_pages(gfp_mask, order)





int shmem_getpage_gfp_tlx(struct inode *inode, pgoff_t index,
	struct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type);


# define RWSEM_ACTIVE_MASK              0xffffffffL

enum rwsem_wake_type {
	RWSEM_WAKE_ANY,		/* Wake whatever's at head of wait list */
	RWSEM_WAKE_READERS,	/* Wake readers only */
	RWSEM_WAKE_READ_OWNED	/* Waker thread holds the read lock */
};





static inline int __atomic_add_unless(atomic_t *v, int a, int u)
 {
         int c, old;

         c = atomic_read(v);
         while (c != u && (old = atomic_cmpxchg_tlx((v), c, c + a)) != c)
                 c = old;
         return c;
}





/* Height component in node->path */




int shmem_getpage_gfp_tlx(struct inode *inode, pgoff_t index,
	struct page **pagep, enum sgp_type sgp, gfp_t gfp, int *fault_type)
{
	struct address_space *mapping = inode->i_mapping;
	struct shmem_inode_info *info;
	struct shmem_sb_info *sbinfo;
	struct page *page;
	swp_entry_t swap;
	int error = 0;
	int once = 0;
	int alloced = 0;
//repeat:
	swap.val = 0;
//	page = find_lock_entry(mapping, index);
// 	struct address_space *mapping, pgoff_t offset)
//     page = find_get_entry(mapping, index);
			void **pagep__;
		//	rcu_read_lock();
repeat:
			page = NULL;
			void **slot = NULL;
			pagep__ = NULL;
			if (__radix_tree_lookup_tlx_tlx(&mapping->page_tree, index, NULL, &slot))
				pagep__ = slot;
			if (pagep__) {
				page = rcu_dereference(*pagep__);
				if (unlikely(!page))
					goto out;
				if (unlikely((unsigned long)page &
                (RADIX_TREE_INDIRECT_PTR | RADIX_TREE_EXCEPTIONAL_ENTRY))) {
					if (unlikely((unsigned long)page & RADIX_TREE_INDIRECT_PTR))
						goto repeat;
					/*
					* A shadow entry of a recently evicted page,
					* or a swap entry from shmem/tmpfs.  Return
					* it without attempting to raise page count.
					*/
					goto out;
				}
				atomic_inc_tlx(&page->_count);
				if (!__atomic_add_unless((&page->_count), 1, 0) != 0)
					goto repeat;
//				if (! atomic_add_unless((&page->_count), 1, 0))
//					goto repeat;

			}
out:

//		if (page && !radix_tree_exception(page))
//								lock_page(page);


//	if (page && sgp == SGP_WRITE)
//		mark_page_accessed(page);

	/* fallocated page? */
	if (page && !test_bit_tlx(PG_uptodate, &(page)->flags)) {
		if (sgp != SGP_READ)
			goto clear;
//		unlock_page(page);
		put_page_tlx(page);
		page = NULL;
	}
	if (page || (sgp == SGP_READ && !swap.val)) {
		*pagep = page;
		return 0;
	}

	info = SHMEM_I(inode);
	sbinfo = SHMEM_SB(inode->i_sb);


//		if (shmem_acct_block(info->flags)) {
//		}
//		page = shmem_alloc_page(gfp, info, index);
			struct vm_area_struct pvma;
//       struct page *page;
			pvma.vm_start = 0;
			pvma.vm_pgoff = index + info->vfs_inode.i_ino;
			pvma.vm_ops = NULL;
//       pvma.vm_policy = mpol_shared_policy_lookup(&info->policy, index);

			page = alloc_page_vma(gfp, &pvma, 0);

//		__SetPageSwapBacked(page);
		__set_bit_tlx(PG_locked, &page->flags);
//		error = radix_tree_maybe_preload(gfp & GFP_RECLAIM_MASK);

		if (!error) {
	//		error = shmem_add_to_page_cache(page, mapping, index,
	//							gfp, NULL);
//         page_cache_get(page);
				atomic_inc_tlx(&page->_count);
				page->mapping = mapping;
				page->index = index;
				radix_tree_insert_tlx(&mapping->page_tree, index, page);
//			radix_tree_preload_end();
		}
//		lru_cache_add_anon(page);
//		spin_lock(&info->lock);
		info->alloced++;
		inode->i_blocks += BLOCKS_PER_PAGE;
//		shmem_recalc_inode(inode);
//		spin_unlock(&info->lock);
		alloced = true;
clear:
		alloced = true;
//	}

	*pagep = page;
	return 0;
}


int
shmem_write_end_tlx(struct file *file, struct address_space *mapping,
			loff_t pos, unsigned len, unsigned copied,
			struct page *page, void *fsdata)
{
	struct inode *inode = mapping->host;

	if (pos + copied > inode->i_size)
		inode->i_size = pos + copied;
//		i_size_write(inode, pos + copied);

	if (!test_bit_tlx(PG_uptodate, &(page)->flags)) {
		if (copied < PAGE_CACHE_SIZE) {
			unsigned from = pos & (PAGE_CACHE_SIZE - 1);
//					zero_user_segments(page, 0, from,
//					from + copied, PAGE_CACHE_SIZE);
		}
		set_bit_tlx(PG_uptodate, &(page)->flags);
	}
//	set_page_dirty(page);
//	unlock_page(page);
//	page_cache_release(page);
	__clear_bit_tlx(PG_locked, &page->flags);
	put_page_tlx(page);

	return copied;
}

int
shmem_write_begin_tlx(struct file *file, struct address_space *mapping,
			loff_t pos, unsigned len, unsigned flags,
			struct page **pagep, void **fsdata)
{
	struct inode *inode = mapping->host;
	pgoff_t index = pos >> PAGE_CACHE_SHIFT;
//	return shmem_getpage(inode, index, pagep, SGP_WRITE, NULL);
//	truct inode *inode, pgoff_t index,
	//	struct page **pagep,
//	enum sgp_type sgp, int *fault_type)
//	{
		return shmem_getpage_gfp_tlx(inode, index, pagep, SGP_WRITE,
				(__force gfp_t)(inode->i_mapping)->flags & __GFP_BITS_MASK, NULL);

}

static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
{
				if (likely(vma->vm_flags & VM_WRITE))
							pte = pte_mkwrite_tlx(pte);
			return pte;
}

unsigned int munlock_vma_page_tlx(struct page *page) {};

void page_remove_rmap_tlx(struct page *page)
{
		atomic_add_return_tlx(-1, &page->_mapcount);
	return;
}

#define VM_IO           0x00004000      /* Memory mapped I/O or similar */
#define VM_DONTEXPAND   0x00040000      /* Cannot expand with mremap() */
#define VM_PFNMAP       0x00000400      /* Page-ranges managed without "struct page", just pure PFN */
#define VM_MIXEDMAP     0x10000000      /* Can contain "struct page" and pure PFN pages */

#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)


void page_add_new_anon_rmap_tlx(struct page *page,
	struct vm_area_struct *vma, unsigned long address)
{
	atomic_set(&page->_mapcount, 0); /* increment count (starts at -1) */
	if (likely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) != VM_LOCKED)) {
		SetPageActive(page);
		return;
	}
}

#define copy_page(to,from)      memcpy_tlx((to), (from), PAGE_SIZE)





void cow_user_page_tlx(struct page *dst, struct page *src, unsigned long va, struct vm_area_struct *vma)
{
	if (unlikely(!src)) {
		void *kaddr = kmap_atomic_tlx(dst);
		void __user *uaddr = (void __user *)(va & PAGE_MASK);
			__copy_from_user_tlx(kaddr, uaddr, PAGE_SIZE);
			kunmap_atomic(kaddr);
	} else {
				char *vfrom, *vto;
				vfrom = kmap_atomic_tlx(src);
				vto = kmap_atomic_tlx(dst);
	//				__cpu_copy_user_page(vto, vfrom, va);
				copy_page(vto, vfrom);
				kunmap_atomic(vto);
				kunmap_atomic(vfrom);
	}
//		copy_user_highpage(dst, src, va, vma);
}

int anon_vma_prepare_tlx(struct vm_area_struct *vma)
{
	struct anon_vma *anon_vma = vma->anon_vma;
	struct anon_vma_chain *avc;

	might_sleep();
	if (unlikely(!anon_vma)) {
		struct mm_struct *mm = vma->vm_mm;
		struct anon_vma *allocated;

	avc =  slab_alloc_tlx(anon_vma_chain_cachep_tlx, GFP_KERNEL, _RET_IP_);
//		kmem_cache_alloc(anon_vma_chain_cachep_tlx, GFP_KERNEL);
//		anon_vma = find_mergeable_anon_vma(vma);
		allocated = NULL;
		if (!anon_vma) {
//			anon_vma = anon_vma_alloc();
				anon_vma = slab_alloc_tlx(anon_vma_cachep_tlx, GFP_KERNEL, _RET_IP_);
//				kmem_cache_alloc(anon_vma_cachep_tlx, GFP_KERNEL);
				if (anon_vma) {
								atomic_set(&anon_vma->refcount, 1);
								anon_vma->root = anon_vma;
				}
			allocated = anon_vma;
		}

//		anon_vma_lock_write(anon_vma);
//		spin_lock(&mm->page_table_lock);
		if (likely(!vma->anon_vma)) {
			vma->anon_vma = anon_vma;
//			anon_vma_chain_link(vma, avc, anon_vma);
			avc->vma = vma;
			avc->anon_vma = anon_vma;
			list_add(&avc->same_vma, &vma->anon_vma_chain);
//      __anon_vma_interval_tree_insert(avc, &anon_vma->rb_root);
			allocated = NULL;
			avc = NULL;
		}
//		spin_unlock(&mm->page_table_lock);
//		anon_vma_unlock_write(anon_vma);
	}
	return 0;

}


void do_set_pte_tlx(struct vm_area_struct *vma, unsigned long address,
		struct page *page, pte_t *pte, bool write, bool anon)
{
		pte_t entry;

		//	flush_icache_page(vma, page);
		entry = mk_pte(page, vma->vm_page_prot);
		if (write)
			entry = maybe_mkwrite(entry, vma);
		if (anon) {
		//		inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
			page_add_new_anon_rmap_tlx(page, vma, address);
		}
		set_pte_at_tlx(vma->vm_mm, address, pte, entry);

		/* no need to invalidate: a not-present page won't be cached */
		//	update_mmu_cache(vma, address, pte);
		dsb(ishst);
}



int __do_fault_tlx(struct vm_area_struct *vma, unsigned long address,
		pgoff_t pgoff, unsigned int flags, struct page **page)
{
	struct vm_fault vmf;
	int ret;

	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
	vmf.pgoff = pgoff;
	vmf.flags = flags;
	vmf.page = NULL;

	ret = vma->vm_ops->fault(vma, &vmf);
	*page = vmf.page;
	return ret;
}


#define GFP_HIGHUSER    (__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL | \
                        __GFP_HIGHMEM)

static inline struct page *
 __alloc_zeroed_user_highpage(gfp_t movableflags,
                         struct vm_area_struct *vma,
                         unsigned long vaddr)
{
       struct page *page = alloc_page_vma(GFP_HIGHUSER | movableflags,
                       vma, vaddr);
//     if (page)
//             clear_user_highpage(page, vaddr);
       return page;
 }

static inline struct page *
 alloc_zeroed_user_highpage_movable(struct vm_area_struct *vma,
                                         unsigned long vaddr)
{
       return __alloc_zeroed_user_highpage(__GFP_MOVABLE, vma, vaddr);
}


#define VM_FAULT_WRITE  0x0008  /* Special case for get_user_pages */



static inline void copy_user_highpage(struct page *to, struct page *from,
         unsigned long vaddr, struct vm_area_struct *vma)
{
         char *vfrom, *vto;

         vfrom = kmap_atomic_tlx(from);
         vto = kmap_atomic_tlx(to);
         copy_page(vto, vfrom);
         kunmap_atomic(vto);
         kunmap_atomic(vfrom);
 }

int do_cow_fault_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
		unsigned long address, pmd_t *pmd,
		pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
{
	struct page *fault_page, *new_page;
	spinlock_t *ptl;
	pte_t *pte;
	int ret;
	anon_vma_prepare_tlx(vma);
//	new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
	new_page = alloc_pages_node_tlx(-1, GFP_HIGHUSER_MOVABLE, 0);
	ret = __do_fault_tlx(vma, address, pgoff, flags, &fault_page);
	copy_user_highpage(new_page, fault_page, address, vma);

	pte = pte_offset_kernel(pmd, address);
	if (unlikely(!pte_same(*pte, orig_pte))) {
//		pte_unmap_unlock(pte, ptl);
		goto uncharge_out;
	}
	do_set_pte_tlx(vma, address, new_page, pte, true, true);
//	pte_unmap_unlock(pte, ptl);
	return ret;
uncharge_out:
	return ret;
}

int do_anonymous_page_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
		unsigned long address, pte_t *page_table, pmd_t *pmd,
		unsigned int flags)
{
	struct page *page;
	spinlock_t *ptl;
	pte_t entry;

	pte_unmap(page_table);
	if (!(flags & FAULT_FLAG_WRITE)) {
		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
						vma->vm_page_prot));
		page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
		if (!pte_none(*page_table))
			goto unlock;
		goto setpte;
	}
		anon_vma_prepare_tlx(vma);
	page = alloc_zeroed_user_highpage_movable(vma, address);
	entry = mk_pte(page, vma->vm_page_prot);
	if (vma->vm_flags & VM_WRITE)
		pte_val(entry) |= PTE_DIRTY;
		entry = pte_mkwrite_tlx(entry);
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (!pte_none(*page_table))
		goto release;
	page_add_new_anon_rmap_tlx(page, vma, address);
setpte:
	set_pte_at(mm, address, page_table, entry);
	dsb(ishst);
unlock:
	pte_unmap_unlock(page_table, ptl);
	return 0;
release:
//	page_cache_release(page);
	goto unlock;
}






#define VM_MIXEDMAP     0x10000000      /* Can contain "struct page" and pure PFN pages */
#define VM_PFNMAP       0x00000400      /* Page-ranges managed without "struct page", just pure PFN */

struct page *vm_normal_page_tlx(struct vm_area_struct *vma, unsigned long addr,
				pte_t pte)
{
	unsigned long pfn = pte_pfn(pte);
	if (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {
		if (vma->vm_flags & VM_MIXEDMAP) {
				goto out;
		} else {
			unsigned long off;
			off = (addr - vma->vm_start) >> PAGE_SHIFT;
			if (pfn == vma->vm_pgoff + off)
				return NULL;
			if (!((vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE))
				return NULL;
		}
	}


out:
	return pfn_to_page(pfn);
}

int do_wp_page_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
		unsigned long address, pte_t *page_table, pmd_t *pmd,
		spinlock_t *ptl, pte_t orig_pte)
{
struct page *old_page, *new_page = NULL;
pte_t entry;
int ret = 0;
int page_mkwrite = 0;
struct page *dirty_page = NULL;
unsigned long mmun_start = 0;	/* For mmu_notifiers */
unsigned long mmun_end = 0;	/* For mmu_notifiers */

old_page = vm_normal_page_tlx(vma, address, orig_pte);
	arch_spinlock_t *lock = &(&(ptl)->rlock)->raw_lock;
	unsigned int tmp;
	arch_spinlock_t lockval;
//		pte_unmap_unlock(page_table, ptl);
	asm volatile(
	"       stlrh   %w1, %0\n"
		: "=Q" (lock->owner)
		: "r" (lock->owner + 1)
		: "memory");
	anon_vma_prepare_tlx(vma);
	new_page = alloc_pages_node_tlx(0, GFP_HIGHUSER_MOVABLE, 0);
	cow_user_page_tlx(new_page, old_page, address, vma);
//	}
mmun_start  = address & PAGE_MASK;
mmun_end    = mmun_start + PAGE_SIZE;
//	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
page_table = pte_offset_map(pmd, address);
asm volatile(
"	prfm	pstl1strm, %2\n"
"1:	ldaxr	%w0, %2\n"
"	eor	%w1, %w0, %w0, ror #16\n"
"	cbnz	%w1, 2f\n"
"	add	%w0, %w0, %3\n"
"	stxr	%w1, %w0, %2\n"
"	cbnz	%w1, 1b\n"
"2:"
: "=&r" (lockval), "=&r" (tmp), "+Q" (*lock)
: "I" (1 << TICKET_SHIFT)
: "memory");

if (likely(pte_same(*page_table, orig_pte))) {
//		flush_cache_page(vma, address, pte_pfn(orig_pte));
	entry = mk_pte(new_page, vma->vm_page_prot);
	pte_val(entry) |= PTE_DIRTY;
	entry = maybe_mkwrite(entry, vma);
//		ptep_clear_flush(vma, address, page_table);
	page_add_new_anon_rmap_tlx(new_page, vma, address);
	set_pte_at_notify(mm, address, page_table, entry);
//	update_mmu_cache(vma, address, page_table);
	if (old_page) {
		page_remove_rmap_tlx(old_page);
	}
	new_page = old_page;
	ret |= VM_FAULT_WRITE;
}
//	if (new_page)
//		page_cache_release(new_page);
unlock:
//	pte_unmap_unlock(page_table, ptl);
	asm volatile(
	"       stlrh   %w1, %0\n"
		: "=Q" (lock->owner)
		: "r" (lock->owner + 1)
		: "memory");
return ret;
}



int handle_pte_fault_tlx(struct mm_struct *mm,
				struct vm_area_struct *vma, unsigned long address,
				pte_t *pte, pmd_t *pmd, unsigned int flags)
{
	pte_t entry;
	spinlock_t *ptl;

	entry = *pte;
	if (!pte_present(entry)) {
		if (pte_none(entry)) {
			if (vma->vm_ops) {
				if (likely(vma->vm_ops->fault)) {
						pte_t *page_table = pte;
						pgoff_t pgoff = (((address & PAGE_MASK)
								- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
						if (!(vma->vm_flags & VM_SHARED))
							return do_cow_fault_tlx(mm, vma, address, pmd, pgoff, flags,
									entry);
						return 0;
				}
//					return do_linear_fault(mm, vma, address,
//						pte, pmd, flags, entry);
			}
			return do_anonymous_page_tlx(mm, vma, address,
						pte, pmd, flags);
		}
		return 0;
	}
	struct page *page =  pmd_page(*pmd);
	ptl = &page->ptl;
//	pte_lockptr(mm, pmd);
//	spin_lock(ptl);

	arch_spinlock_t *lock = &(&(ptl)->rlock)->raw_lock;
	unsigned int tmp;
	arch_spinlock_t lockval;

	asm volatile(
	"	prfm	pstl1strm, %2\n"
	"1:	ldaxr	%w0, %2\n"
	"	eor	%w1, %w0, %w0, ror #16\n"
	"	cbnz	%w1, 2f\n"
	"	add	%w0, %w0, %3\n"
	"	stxr	%w1, %w0, %2\n"
	"	cbnz	%w1, 1b\n"
	"2:"
	: "=&r" (lockval), "=&r" (tmp), "+Q" (*lock)
	: "I" (1 << TICKET_SHIFT)
	: "memory");
	if (unlikely(!pte_same(*pte, entry)))
		goto unlock;
	if (flags & FAULT_FLAG_WRITE) {
		if (!pte_write(entry))
			return do_wp_page_tlx(mm, vma, address,
					pte, pmd, ptl, entry);
//		entry = pte_mkdirty(entry);
		pte_val(entry) |= PTE_DIRTY;
	}
//	entry = pte_mkyoung(entry);
	pte_val(entry) |= PTE_AF;
//	if (ptep_set_access_flags(vma, address, pte, entry, flags & FAULT_FLAG_WRITE)) {
//		update_mmu_cache(vma, address, pte);
		int changed = !pte_same(*pte, entry);
		if (changed) {
								set_pte_at(vma->vm_mm, address, pte, entry);
//                 flush_tlb_fix_spurious_fault(vma, address);
										dsb(ishst);
		}

//	}
unlock:
//	pte_unmap_unlock_tlx(pte, ptl);
//	spin_unlock(ptl);
//	spin_unlock(&lockref->lock);

	asm volatile(
	"       stlrh   %w1, %0\n"
		: "=Q" (lock->owner)
		: "r" (lock->owner + 1)
		: "memory");
	return 0;
}


int handle_mm_fault_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
				unsigned long address, unsigned int flags)
{
	int ret;
	__set_current_state(TASK_RUNNING);
	pgd_t *pgd;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *pte;
//	pgd = pgd_offset(mm, address);
	pgd = ((mm)->pgd+(pgd_t)(((address) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1)));
	pud = (pud_t *)pgd;
	int rez = 0;
	if (pud_none(*pud)) {
		pmd_t *new = (pmd_t *)  page_address(alloc_pages(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO, 0));
		smp_wmb(); /* See comment in __pte_alloc */
//		pud_populate(mm, pud, new);
//		set_pud(pud, __pud(__pa(new) | PMD_TYPE_TABLE));
		*pud = __pud(__pa(new) | PMD_TYPE_TABLE);
    dsb(ishst);
		rez = 0;
	}
	//rez = __pmd_alloc(mm, pud, address);
	pmd = (rez)?
								NULL: (pmd_t *)__va(pud_val(*pud) & PHYS_MASK & (s32)PAGE_MASK) + (((address) >> PMD_SHIFT) & (PTRS_PER_PMD - 1));
	if (unlikely(pmd_none(*pmd))) {
//			__pte_alloc(mm, vma, pmd, address);
//					pgtable_t new = pte_alloc_one(mm, address);
					pgtable_t new = alloc_pages(PGALLOC_GFP, 0);
	//				pte_alloc_one(new);
					smp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */
					if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
						atomic_long_inc_tlx(&mm->nr_ptes);
	//					pmd_populate(mm, pmd, new);
						set_pmd(pmd,  page_to_phys(new) | PMD_TYPE_TABLE);
						new = NULL;
					}

	}
//	    unlikely(__pte_alloc(mm, vma, pmd, address)))
//		return VM_FAULT_OOM;
	pte = (pte_t *) __va(pmd_val(*(pmd)) & PHYS_MASK & (s32)PAGE_MASK); // pmd_page_vaddr(*(pmd));
	pte = pte + (((address) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1));
	return handle_pte_fault_tlx(mm, vma, address, pte, pmd, flags);

}



#define FOLL_DUMP       0x08    /* give error on hole if it would be zero */

struct page *follow_page_pte_tlx(struct vm_area_struct *vma,
		unsigned long address, pmd_t *pmd, unsigned int flags)
{
	struct mm_struct *mm = vma->vm_mm;
	struct page *page;
	spinlock_t *ptl;
	pte_t *ptep, pte;

retry:
	ptep =  pte_offset_kernel(pmd, address);
	pte = *ptep;
	if (!pte_present(pte)) {
		if (!pte_val(pte) || pte_file(pte))
			goto no_page;
		goto retry;
	}
	page = vm_normal_page_tlx(vma, address, pte);
	return page;
no_page:
	if (pte_val(pte))
		return NULL;
	if ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))
						return ERR_PTR_tlx(-EFAULT);
	return NULL;
}




struct page *follow_page_mask_tlx(struct vm_area_struct *vma,
						unsigned long address, unsigned int flags,
						unsigned int *page_mask)
{
	pgd_t *pgd;
	pud_t *pud;
	pmd_t *pmd;
	spinlock_t *ptl;

	struct mm_struct *mm = vma->vm_mm;

	*page_mask = 0;
	pgd = pgd_offset_tlx(mm, address);
	pud = pud_offset_tlx(pgd, address);
	if (pud_none(*pud)) {
        if ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))
                  return ERR_PTR_tlx(-EFAULT);
        return NULL;
	}
	//	return no_page_table(vma, flags);
	pmd = pmd_offset_tlx(pud, address);
	if (pmd_none(*pmd)) {
        if ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))
                  return ERR_PTR_tlx(-EFAULT);
         return NULL;

	}
//		return no_page_table(vma, flags);
	return follow_page_pte_tlx(vma, address, pmd, flags);
}



long __get_user_pages_tlx(struct task_struct *tsk, struct mm_struct *mm,
		unsigned long start, unsigned long nr_pages,
		unsigned int gup_flags, struct page **pages,
		struct vm_area_struct **vmas, int *nonblocking)
{
	long i = 0;
	unsigned int page_mask;
	struct vm_area_struct *vma = NULL;
	if (!(gup_flags & FOLL_FORCE))
		gup_flags |= FOLL_NUMA;

	do {
		struct page *page;
		unsigned int foll_flags = gup_flags;
		unsigned int page_increm;

		/* first iteration or cross vma bound */
		if (!vma || start >= vma->vm_end) {
			vma = find_extend_vma_tlx(mm, start);
		}
retry:
		cond_resched();
		page = follow_page_mask_tlx(vma, start, foll_flags, &page_mask);
		if (!page) {
//			int ret;
	//		ret = faultin_page(tsk, vma, start, &foll_flags,
//					nonblocking);
			struct mm_struct *mm = vma->vm_mm;
			unsigned int fault_flags = 0;
			if (foll_flags & FOLL_WRITE)
				fault_flags |= FAULT_FLAG_WRITE;
			if (nonblocking)
				fault_flags |= FAULT_FLAG_ALLOW_RETRY;
			if (foll_flags & FOLL_NOWAIT)
				fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;

			handle_mm_fault_tlx(mm, vma, start, fault_flags);
				goto retry;
		}
		if (pages) {
			pages[i] = page;
			page_mask = 0;
		}
next_page:
		if (vmas) {
			vmas[i] = vma;
			page_mask = 0;
		}
		page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
		if (page_increm > nr_pages)
			page_increm = nr_pages;
		i += page_increm;
		start += page_increm * PAGE_SIZE;
		nr_pages -= page_increm;
	} while (nr_pages);
	return i;
}

long get_user_pages_tlx(struct task_struct *tsk, struct mm_struct *mm,
                 unsigned long start, unsigned long nr_pages, int write,
                 int force, struct page **pages, struct vm_area_struct **vmas)
{
         int flags = FOLL_TOUCH;

         if (pages)
                 flags |= FOLL_GET;
         if (write)
                 flags |= FOLL_WRITE;
         if (force)
                 flags |= FOLL_FORCE;

       return __get_user_pages_tlx(tsk, mm, start, nr_pages, flags, pages, vmas,
                                 NULL);
 }

int find_vma_links_tlx(struct mm_struct *mm, unsigned long addr,
		unsigned long end, struct vm_area_struct **pprev,
		struct rb_node ***rb_link, struct rb_node **rb_parent);



void __vma_link_rb_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
                 struct rb_node **rb_link, struct rb_node *rb_parent)
{
         /* Update tracking information for the gap following the new vma. */
         if (vma->vm_next)
								 vma_gap_callbacks_propagate(&(vma->vm_next)->vm_rb, NULL);
         else
                 mm->highest_vm_end = vma->vm_end;
     rb_link_node_tlx(&vma->vm_rb, rb_parent, rb_link);
       vma->rb_subtree_gap = 0;
  //     vma_gap_update(vma);
			 vma_gap_callbacks_propagate(&vma->vm_rb, NULL);
//         vma_rb_insert(vma, &mm->mm_rb);
	//		rb_insert_augmented(&vma->vm_rb, &mm->mm_rb, &vma_gap_callbacks);
			__rb_insert_tlx(&vma->vm_rb, &mm->mm_rb, vma_gap_callbacks_tlx.rotate);
}

void __vma_link_list_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
                 struct vm_area_struct *prev, struct rb_node *rb_parent)
 {
         struct vm_area_struct *next;

         vma->vm_prev = prev;
         if (prev) {
                 next = prev->vm_next;
                 prev->vm_next = vma;
         } else {
                 mm->mmap = vma;
                 if (rb_parent)
                         next = rb_entry(rb_parent,
                                         struct vm_area_struct, vm_rb);
                 else
                         next = NULL;
         }
         vma->vm_next = next;
         if (next)
                 next->vm_prev = vma;
 }

int do_execve_tlx(struct filename *filename,
	const char __user *const __user *__argv,
	const char __user *const __user *__envp)
{
	struct user_arg_ptr argv = { .ptr.native = __argv };
	struct user_arg_ptr envp = { .ptr.native = __envp };
	struct linux_binprm *bprm;
	struct file *file;
	struct files_struct *displaced;
	int retval;
	current->flags &= ~PF_NPROC_EXCEEDED;
	bprm = kzalloc_tlx(sizeof(*bprm), GFP_KERNEL);
	struct task_struct *task = current;
	const struct cred *old;
	struct cred *new;
	new = slab_alloc_tlx(cred_jar_tlx_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
	old = task->cred;
	memcpy_tlx(new, old, sizeof(struct cred));
	atomic_set(&new->usage, 1);
	bprm->cred = new;
	current->in_execve = 1;
	static const struct open_flags open_exec_flags = {
		.open_flag = O_LARGEFILE | O_RDONLY | __FMODE_EXEC,
		.acc_mode = MAY_EXEC | MAY_OPEN,
		.intent = LOOKUP_OPEN,
		.lookup_flags = LOOKUP_FOLLOW,
	};
	struct nameidata nd;
	int o_flags = (&open_exec_flags)->lookup_flags;
	file = path_openat_tlx(AT_FDCWD, filename, &nd, &open_exec_flags, o_flags| LOOKUP_RCU);
	bprm->file = file;
	bprm->filename = bprm->interp = filename->name;
	struct mm_struct *mm = NULL;
	mm = slab_alloc_tlx(mm_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
	memset_tlx(mm, 0, sizeof(*mm));
	atomic_set(&mm->mm_users, 1);
	atomic_set(&mm->mm_count, 1);
	init_rwsem(&mm->mmap_sem);
	INIT_LIST_HEAD(&mm->mmlist);
	mm->core_state = NULL;
	atomic_long_set_tlx(&mm->nr_ptes, 0);
	memset_tlx(&mm->rss_stat, 0, sizeof(mm->rss_stat));
	spin_lock_init(&mm->page_table_lock);
	clear_tlb_flush_pending_tlx(mm);
	if (current->mm) {
		mm->flags = current->mm->flags & MMF_INIT_MASK;
		mm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;
	} else {
		mm->flags = MMF_DUMP_FILTER_DEFAULT;
		mm->def_flags = 0;
	}
	mm->pgd = pgd_alloc_tlx(mm);
	bprm->mm = mm;
	struct vm_area_struct *vma = NULL;
	struct mm_struct *t_mm = bprm->mm;
	bprm->vma = vma = kmem_cache_zalloc_tlx(vm_area_cachep_tlx, GFP_KERNEL);
	vma->vm_mm = t_mm;
	vma->vm_end = STACK_TOP_MAX;
	vma->vm_start = vma->vm_end - PAGE_SIZE;
	vma->vm_flags = VM_SOFTDIRTY | VM_STACK_FLAGS | VM_STACK_INCOMPLETE_SETUP;
	vma->vm_page_prot = vm_get_page_prot_tlx(vma->vm_flags);
	INIT_LIST_HEAD(&vma->anon_vma_chain);
//	insert_vm_struct(t_mm, vma);
	struct vm_area_struct *prev;
  struct rb_node **rb_link, *rb_parent;
	find_vma_links_tlx(t_mm, vma->vm_start, vma->vm_end,
                            &prev, &rb_link, &rb_parent);
	__vma_link_list_tlx(t_mm, vma, prev, rb_parent);
	__vma_link_rb_tlx(t_mm, vma, rb_link, rb_parent);

	t_mm->stack_vm = mm->total_vm = 1;
	bprm->p = vma->vm_end - sizeof(void *);
	bprm->argc = 1;
	bprm->envc = 2;
//	retval = prepare_binprm(bprm);
//	struct linux_binprm *bprm)
//	{
		struct inode *inode = file_inode_tlx(bprm->file);
		umode_t mode = inode->i_mode;
//		int retval;
		bprm->cred->euid = current_euid();
		bprm->cred->egid = current_egid();
		if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
				!current->no_new_privs &&
				kuid_has_mapping_tlx(bprm->cred->user_ns, inode->i_uid) &&
				kgid_has_mapping_tlx(bprm->cred->user_ns, inode->i_gid)) {
			/* Set-uid? */
			if (mode & S_ISUID) {
//				bprm->per_clear |= PER_CLEAR_ON_SETID;
				bprm->cred->euid = inode->i_uid;
			}
			if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
//				bprm->per_clear |= PER_CLEAR_ON_SETID;
				bprm->cred->egid = inode->i_gid;
			}
		}
		bprm->cred_prepared = 1;
		memset_tlx(bprm->buf, 0, BINPRM_BUF_SIZE);
		kernel_read_tlx(bprm->file, 0, bprm->buf, BINPRM_BUF_SIZE);

	bprm->exec = bprm->p;
	int argc = 1;
	struct page *kmapped_page = NULL;
	char *kaddr = NULL;
	unsigned long kpos = 0;
	while (argc-- > 0) {
		const char __user *str;
		int len;
		unsigned long pos;
		get_user(str, argv.ptr.native + argc);
		len = strnlen_user(str, MAX_ARG_STRLEN);
		pos = bprm->p;
		str += len;
		bprm->p -= len;
		while (len > 0) {
			int offset, bytes_to_copy;
			cond_resched();
			offset = pos % PAGE_SIZE;
			if (offset == 0)
				offset = PAGE_SIZE;
			bytes_to_copy = offset;
			if (bytes_to_copy > len)
				bytes_to_copy = len;
			offset -= bytes_to_copy;
			pos -= bytes_to_copy;
			str -= bytes_to_copy;
			len -= bytes_to_copy;
			if (!kmapped_page || kpos != (pos & PAGE_MASK)) {
				struct page *page;
				get_user_pages_tlx(current, bprm->mm, pos,
							1, 1, 1, &page, NULL);
				if (kmapped_page) {
//					flush_kernel_dcache_page(kmapped_page);
//					kunmap(kmapped_page);
					put_page_tlx(kmapped_page);
				}
				kmapped_page = page;
				kaddr = kmap(kmapped_page);
				kpos = pos & PAGE_MASK;
			}
			if (copy_from_user(kaddr+offset, str, bytes_to_copy)) {
				goto out;
			}
		}
	}
out:
	if (kmapped_page) {
//		flush_kernel_dcache_page(kmapped_page);
//		kunmap(kmapped_page);
		put_page_tlx(kmapped_page);
	}
	retval = elf_format_tlx.load_binary(bprm);


}

static char *victim;
static unsigned count_;
static loff_t this_header;
static char *collected;
static char *vcollected;
static int wfd;
static char *collect;
static int remains;

static inline void  eat(unsigned n)
{
	victim += n;
	this_header += n;
	count_ -= n;
}

static unsigned long body_len, name_len;

int do_start_tlx(void)
{
	char *buf = header_buf_tlx;
	unsigned size = 110;
	enum state next = GotHeader;
	if (count_ >= size) {
		collected = victim;
		eat(size);
		state = next;
	} else {
		collect = collected = buf;
		remains = size;
		next_state = next;
		state = Collect;
	}

	return 0;
}


int do_collect_tlx(void)
{
	unsigned n = remains;
	if (count_ < n)
		n = count_;
	memcpy_tlx(collect, victim, n);
	eat(n);
	collect += n;
	if ((remains -= n) != 0)
		return 1;
	state = next_state;
	return 0;
}



static umode_t mode;
static unsigned rdev;
static loff_t next_header;
static time_t mtime;

#define KSTRTOX_OVERFLOW        (1U << 31)
#define isxdigit(c)     ((__ismask(c)&(_D|_X)) != 0)
static inline char _tolower(const char c)
{
         return c | 0x20;
}

unsigned int _parse_integer_tlx(const char *s, unsigned int base, unsigned long long *p)
{
	unsigned long long res;
	unsigned int rv;
	int overflow;

	res = 0;
	rv = 0;
	overflow = 0;
	while (*s) {
		unsigned int val;

		if ('0' <= *s && *s <= '9')
			val = *s - '0';
		else if ('a' <= _tolower(*s) && _tolower(*s) <= 'f')
			val = _tolower(*s) - 'a' + 10;
		else
			break;

		if (val >= base)
			break;
		/*
		* Check for overflow only if we are within range of
		* it in the max base we support (16)
		*/
		if (unlikely(res & (~0ull << 60))) {
			if (res > div_u64_tlx(ULLONG_MAX - val, base))
				overflow = 1;
		}
		res = res * base + val;
		rv++;
		s++;
	}
	*p = res;
	if (overflow)
		rv |= KSTRTOX_OVERFLOW;
	return rv;
}

const char *_parse_integer_fixup_radix_tlx(const char *s, unsigned int *base)
{
	if (*base == 0) {
		if (s[0] == '0') {
			if (_tolower(s[1]) == 'x' && isxdigit(s[2]))
				*base = 16;
			else
				*base = 8;
		} else
			*base = 10;
	}
	if (*base == 16 && s[0] == '0' && _tolower(s[1]) == 'x')
		s += 2;
	return s;
}

unsigned long long simple_strtoull_tlx(const char *cp, char **endp, unsigned int base)
 {
         unsigned long long result;
         unsigned int rv;

         cp = _parse_integer_fixup_radix_tlx(cp, &base);
         rv = _parse_integer_tlx(cp, base, &result);
         /* FIXME */
         cp += (rv & ~KSTRTOX_OVERFLOW);

         if (endp)
                 *endp = (char *)cp;

         return result;
 }

int  do_skip_tlx(void)
{
	if (this_header + count_ < next_header) {
		eat(count_);
		return 1;
	} else {
		eat(next_header - this_header);
		state = next_state;
		return 0;
	}
}

static inline u32 new_encode_dev(dev_t dev)
 {
       unsigned major = MAJOR(dev);
       unsigned minor = MINOR(dev);
       return (minor & 0xff) | (major << 8) | ((minor & ~0xff) << 12);
 }


 static inline dev_t new_decode_dev(u32 dev)
 {
         unsigned major = (dev & 0xfff00) >> 8;
         unsigned minor = (dev & 0xff) | ((dev >> 12) & 0xfff00);
         return MKDEV(major, minor);
 }

static unsigned long ino, major, minor, nlink;
static uid_t uid;
static gid_t gid;


#define S_ISLNK(m)      (((m) & S_IFMT) == S_IFLNK)

void __init parse_header_tlx(char *s)
{
	unsigned long parsed[12];
	char buf[9];
	int i;

	buf[8] = '\0';
	for (i = 0, s += 6; i < 12; i++, s += 8) {
		memcpy_tlx(buf, s, 8);
		parsed[i] = simple_strtoull_tlx(buf, NULL, 16);
	}
	ino = parsed[0];
	mode = parsed[1];
	uid = parsed[2];
	gid = parsed[3];
	nlink = parsed[4];
	mtime = parsed[5];
	body_len = parsed[6];
	major = parsed[7];
	minor = parsed[8];
	rdev = new_encode_dev(MKDEV(parsed[9], parsed[10]));
	name_len = parsed[11];
}

int do_header_tlx(void)
{
	parse_header_tlx(collected);
	next_header = this_header + N_ALIGN(name_len) + body_len;
	next_header = (next_header + 3) & ~3;
	state = SkipIt;
	if (name_len <= 0 || name_len > PATH_MAX)
		return 0;
	if (S_ISLNK(mode)) {
		if (body_len > PATH_MAX)
			return 0;
		collect = collected = symlink_buf_tlx;
		remains = N_ALIGN(name_len) + body_len;
		next_state = GotSymlink;
		state = Collect;
		return 0;
	}
	if (S_ISREG(mode) || !body_len) {
//		read_into(name_buf_tlx, N_ALIGN(name_len), GotName);
		char *buf = name_buf_tlx;
		unsigned size = N_ALIGN(name_len);
		enum state next =  GotName;
			if (count_ >= size) {
				collected = victim;
				eat(size);
				state = next;
			} else {
				collect = collected = buf;
				remains = size;
				next_state = next;
				state = Collect;
			}
	}
	return 0;
}

struct dentry *kern_path_create_tlx(int dfd, const char *pathname,
				struct path *path, unsigned int lookup_flags);


int do_name_tlx(void)
{
	state = SkipIt;
	next_state = Reset;
//	clean_path(collected, mode);
	if (S_ISREG(mode)) {
			int openflags = O_WRONLY|O_CREAT;
			wfd = sys_open(collected, openflags, mode);
			if (wfd >= 0) {
				state = CopyFile;
			}
	} else if (S_ISDIR(mode)) {
			int error = 0;
			struct dentry *dentry;
			struct path path;
			unsigned int lookup_flags = 0;
			struct filename *tmp = getname_tlx(collected);
			dentry = kern_path_create_tlx(AT_FDCWD, tmp->name, &path, lookup_flags);
			kmem_cache_free_tlx(names_cachep_tlx, (void *)(tmp));
			if (IS_ERR_tlx(dentry))
				return 0;
			if (!error) {
					error = path.dentry->d_inode->i_op->mkdir(path.dentry->d_inode, dentry, mode);
			}
	} else if (S_ISBLK(mode) || S_ISCHR(mode) ||
			S_ISFIFO(mode) || S_ISSOCK(mode)) {
				struct dentry *dentry;
				struct path path;
				unsigned int lookup_flags = 0;
				struct filename *tmp = getname_tlx(collected);
				dentry = kern_path_create_tlx(AT_FDCWD, tmp->name, &path, lookup_flags);
				kmem_cache_free_tlx(names_cachep_tlx, (void *)(tmp));
				path.dentry->d_inode->i_op->mknod(path.dentry->d_inode, dentry, mode, new_decode_dev(rdev));
	}
	return 0;
}



#define rcu_dereference_check_fdtable(files, fdtfd) \
         rcu_dereference_check((fdtfd), lockdep_is_held(&(files)->file_lock))

#define files_fdtable(files) \
         rcu_dereference_check_fdtable((files), (files)->fdt)

int __close_fd_tlx(struct files_struct *files, unsigned fd)
{
	struct file *file;
	struct fdtable *fdt;
	fdt = files_fdtable(files);
	file = fdt->fd[fd];
	rcu_assign_pointer(fdt->fd[fd], NULL);
		__clear_bit_tlx(fd, fdt->open_fds);
		if (fd < files->next_fd)
			files->next_fd = fd;
	return 0;
//	filp_close(file, files);
}
int do_copy_tlx(void);



static inline void * ERR_PTR(long error)
{
	return (void *) error;
}

#define LOOKUP_REVAL		0x0020

struct dentry *__lookup_hash_tlx(struct qstr *name,
		struct dentry *base, unsigned int flags)
{
	bool need_lookup;
	struct dentry *dentry;

//	dentry = lookup_dcache(name, base, flags, &need_lookup);

		need_lookup = false;
		dentry = __d_lookup_tlx_tlx(base, name);
		if (dentry) {
			if (dentry->d_flags & DCACHE_OP_REVALIDATE) {
						dput_tlx(dentry);
						dentry = NULL;
			}
		}
		if (!dentry) {
			dentry = d_alloc_tlx(base, name);
			need_lookup = true;
		}

	if (!need_lookup)
		return dentry;

		struct dentry *old;
		old = base->d_inode->i_op->lookup(base->d_inode, dentry, flags);
		return dentry;
//	lookup_real(base->d_inode, dentry, flags);
}

struct dentry *kern_path_create_tlx(int dfd, const char *pathname,
				struct path *path, unsigned int lookup_flags)
{
	struct dentry *dentry = ERR_PTR(-EEXIST);
	struct nameidata nd;
	int err2;
	int error;
	bool is_dir = (lookup_flags & LOOKUP_DIRECTORY);

	lookup_flags &= LOOKUP_REVAL;
	struct filename filename = { .name = pathname };
	error =  path_lookupat_tlx(dfd, (&filename)->name, LOOKUP_PARENT | lookup_flags | LOOKUP_RCU, &nd);
	if (error)
		return ERR_PTR(error);
	if (nd.last_type != LAST_NORM)
		goto out;
	nd.flags &= ~LOOKUP_PARENT;
	nd.flags |= LOOKUP_CREATE | LOOKUP_EXCL;
	dentry = __lookup_hash_tlx(&(&nd)->last, (&nd)->path.dentry, (&nd)->flags);
//	lookup_hash(&nd);
	if (IS_ERR_tlx(dentry))
		goto unlock;

	error = -EEXIST;
	if (unlikely(!is_dir && nd.last.name[nd.last.len])) {
		error = -ENOENT;
		goto fail;
	}
	if (unlikely(err2)) {
		error = err2;
		goto fail;
	}
	*path = nd.path;
	return dentry;
fail:
	dput_tlx(dentry);
	dentry = ERR_PTR(error);
unlock:
out:
	path_put_tlx(&nd.path);
	return dentry;
}

static inline void * __must_check ERR_CAST_tlx(__force const void *ptr)
{
         /* cast away the const */
         return (void *) ptr;
}

int symlinkat_tlx (char* oldname,
		int  newdfd, char* newname)
{
	int error;
	struct filename *from;
	struct dentry *dentry;
	struct path path;
	unsigned int lookup_flags = 0;

	from = getname_tlx(oldname);
	if (IS_ERR_tlx(from))
		return PTR_ERR_tlx(from);
//	dentry = user_path_create(newdfd, newname, &path, lookup_flags);
	int dfd = newdfd;
	char __user *pathname = newname;

	struct filename *tmp = getname_tlx(pathname);
	struct dentry *res;
	if (IS_ERR_tlx(tmp)) {
		dentry =  ERR_CAST_tlx(tmp);
		goto rez;
	}
	res = kern_path_create_tlx(dfd, tmp->name, &path, lookup_flags);
//	putname(tmp);
	kmem_cache_free_tlx(names_cachep_tlx, (void *)(tmp));
	dentry = res;
rez:
	error = PTR_ERR_tlx(dentry);
	if (IS_ERR_tlx(dentry))
		goto out_putname;
	error = path.dentry->d_inode->i_op->symlink(path.dentry->d_inode, dentry, from->name);
//	done_path_create(&path, dentry);
		dput_tlx(dentry);
	//	mnt_drop_write(path->mnt);
		path_put_tlx(&path);
out_putname:
	kmem_cache_free_tlx(names_cachep_tlx, (void *)(from));
//	putname(from);
	return error;
}


int do_symlink_tlx(void)
{
	collected[N_ALIGN(name_len) + body_len] = '\0';
//	clean_path(collected, 0);
//	sys_symlink(collected + N_ALIGN(name_len), collected);
	symlinkat_tlx(collected + N_ALIGN(name_len), AT_FDCWD, collected);
	state = SkipIt;
	next_state = Reset;
	return 0;
}

int do_reset_tlx(void)
{
	while(count_ && *victim == '\0')
		eat(1);
	return 1;
}


int (*actions_tlx[])(void) = {
	[Start]		= do_start_tlx,
	[Collect]	= do_collect_tlx,
	[GotHeader]	= do_header_tlx,
	[SkipIt]	= do_skip_tlx,
	[GotName]	= do_name_tlx,
	[CopyFile]	= do_copy_tlx,
	[GotSymlink]	= do_symlink_tlx,
	[Reset]		= do_reset_tlx,
};



int __init flush_buffer_tlx(void *bufv, unsigned len)
{
	char *buf = (char *) bufv;
	int written;
	int origLen = len;
	if (message_tlx)
		return -1;

	count_ = len;
	victim = buf;
	while (!actions_tlx[state]());
	written = len - count_;

	while (written < len && !message_tlx) {
				char c = buf[written];
		if (c == '0') {
			buf += written;
			len -= written;
			state = Start;
		} else if (c == 0) {
			buf += written;
			len -= written;
			state = Reset;
		}
		count_ = len;
		while (!actions_tlx[state]());
		written = len - count_;
	}
	return origLen;
}

struct srcu_struct fsnotify_mark_srcu_tlx;
typedef unsigned char  Byte;  /* 8 bits */
typedef unsigned int   uInt;  /* 16 bits or more */
typedef unsigned long  uLong; /* 32 bits or more */
typedef void     *voidp;

typedef enum {
    HEAD,       /* i: waiting for magic header */
    FLAGS,      /* i: waiting for method and flags (gzip) */
    TIME,       /* i: waiting for modification time (gzip) */
    OS,         /* i: waiting for extra flags and operating system (gzip) */
    EXLEN,      /* i: waiting for extra length (gzip) */
    EXTRA,      /* i: waiting for extra bytes (gzip) */
    NAME,       /* i: waiting for end of file name (gzip) */
    COMMENT,    /* i: waiting for end of comment (gzip) */
    HCRC,       /* i: waiting for header crc (gzip) */
    DICTID,     /* i: waiting for dictionary check value */
    DICT,       /* waiting for inflateSetDictionary() call */
        TYPE,       /* i: waiting for type bits, including last-flag bit */
        TYPEDO,     /* i: same, but skip check to exit inflate on new block */
        STORED,     /* i: waiting for stored size (length and complement) */
        COPY,       /* i/o: waiting for input or output to copy stored block */
        TABLE,      /* i: waiting for dynamic block table lengths */
        LENLENS,    /* i: waiting for code length code lengths */
        CODELENS,   /* i: waiting for length/lit and distance code lengths */
            LEN,        /* i: waiting for length/lit code */
            LENEXT,     /* i: waiting for length extra bits */
            DIST,       /* i: waiting for distance code */
            DISTEXT,    /* i: waiting for distance extra bits */
            MATCH,      /* o: waiting for output space to copy string */
            LIT,        /* o: waiting for output space to write literal */
    CHECK,      /* i: waiting for 32-bit check value */
    LENGTH,     /* i: waiting for 32-bit length (gzip) */
    DONE,       /* finished check, done -- remain here until reset */
    BAD,        /* got a data error -- remain here until reset */
    MEM,        /* got an inflate() memory error -- remain here until reset */
    SYNC        /* looking for synchronization bytes to restart inflate() */
} inflate_mode;

typedef struct z_stream_s {
     const Byte *next_in;   /* next input byte */
     uInt     avail_in;  /* number of bytes available at next_in */
     uLong    total_in;  /* total nb of input bytes read so far */

     Byte    *next_out;  /* next output byte should be put there */
     uInt     avail_out; /* remaining free space at next_out */
     uLong    total_out; /* total nb of bytes output so far */

     char     *msg;      /* last error message, NULL if no error */
     struct internal_state *state; /* not visible by applications */

     void     *workspace; /* memory allocated for this stream */

     int     data_type;  /* best guess about the data type: ascii or binary */
     uLong   adler;      /* adler32 value of the uncompressed data */
     uLong   reserved;   /* reserved for future use */
} z_stream;

#define ENOUGH 2048
typedef struct {
		unsigned char op;           /* operation, extra bits, table bits */
		unsigned char bits;         /* bits in this part of the code */
		unsigned short val;         /* offset in table or code value */
} code;

struct inflate_state {
     inflate_mode mode;          /* current inflate mode */
     int last;                   /* true if processing last block */
     int wrap;                   /* bit 0 true for zlib, bit 1 true for gzip */
     int havedict;               /* true if dictionary provided */
     int flags;                  /* gzip header method and flags (0 if zlib) */
     unsigned dmax;              /* zlib header max distance (INFLATE_STRICT) */
     unsigned long check;        /* protected copy of check value */
     unsigned long total;        /* protected copy of output count */
  /*   gz_headerp head; */           /* where to save gzip header information */
         /* sliding window */
     unsigned wbits;             /* log base 2 of requested window size */
     unsigned wsize;             /* window size or zero if not using window */
     unsigned whave;             /* valid bytes in the window */
     unsigned write;             /* window write index */
     unsigned char *window;  /* allocated sliding window, if needed */
         /* bit accumulator */
     unsigned long hold;         /* input bit accumulator */
     unsigned bits;              /* number of bits in "in" */
         /* for string and stored block copying */
     unsigned length;            /* literal or length of data to copy */
     unsigned offset;            /* distance back to copy string from */
         /* for table and code decoding */
     unsigned extra;             /* extra bits needed */
         /* fixed and dynamic code tables */
     code const *lencode;    /* starting table for length/literal codes */
     code const *distcode;   /* starting table for distance codes */
     unsigned lenbits;           /* index bits for lencode */
     unsigned distbits;          /* index bits for distcode */
         /* dynamic table building */
     unsigned ncode;             /* number of code length code lengths */
     unsigned nlen;              /* number of length code lengths */
     unsigned ndist;             /* number of distance code lengths */
     unsigned have;              /* number of code lengths in lens[] */
     code *next;             /* next available space in codes[] */
     unsigned short lens[320];   /* temporary storage for code lengths */
     unsigned short work[288];   /* work area for code table building */
     code codes[ENOUGH];         /* space for code tables */
};


typedef z_stream *z_streamp;


#define GZIP_IOBUF_SIZE (16*1024)
#define Z_STREAM_END    1
#define Z_OK            0

#define WS(z) ((struct inflate_workspace *)(z->workspace))



# define MAX_WBITS   15 /* 32K LZ77 window */
#define Z_NO_FLUSH      0
#define Z_PARTIAL_FLUSH 1 /* will be removed, use Z_SYNC_FLUSH instead */
#define Z_PACKET_FLUSH  2
#define Z_SYNC_FLUSH    3
#define Z_FULL_FLUSH    4
#define Z_FINISH        5
#define Z_BLOCK         6 /* Only for inflate at present */
/* Allowed flush values; see deflate() and inflate() below for details */

#define Z_OK            0
#define Z_STREAM_END    1
#define Z_NEED_DICT     2
#define Z_ERRNO        (-1)
#define Z_STREAM_ERROR (-2)
#define Z_DATA_ERROR   (-3)
#define Z_MEM_ERROR    (-4)
#define Z_BUF_ERROR    (-5)
#define Z_VERSION_ERROR (-6)

#define UPDATE(check, buf, len) zlib_adler32(check, buf, len)

/* Load registers with state in inflate() for speed */
#define LOAD() \
		do { \
				put = strm->next_out; \
				left = strm->avail_out; \
				next = strm->next_in; \
				have = strm->avail_in; \
				hold = state->hold; \
				bits = state->bits; \
		} while (0)

/* Restore state from registers in inflate() */
#define RESTORE() \
		do { \
				strm->next_out = put; \
				strm->avail_out = left; \
				strm->next_in = next; \
				strm->avail_in = have; \
				state->hold = hold; \
				state->bits = bits; \
		} while (0)

/* Clear the input bit accumulator */
#define INITBITS() \
		do { \
				hold = 0; \
				bits = 0; \
		} while (0)

/* Get a byte of input into the bit accumulator, or return from inflate()
	if there is no input available. */
#define PULLBYTE() \
		do { \
				if (have == 0) goto inf_leave; \
				have--; \
				hold += (unsigned long)(*next++) << bits; \
				bits += 8; \
		} while (0)

/* Assure that there are at least n bits in the bit accumulator.  If there is
	not enough available input to do that, then return from inflate(). */
#define NEEDBITS(n) \
		do { \
				while (bits < (unsigned)(n)) \
						PULLBYTE(); \
		} while (0)

/* Return the low n bits of the bit accumulator (n < 16) */
#define BITS(n) \
		((unsigned)hold & ((1U << (n)) - 1))

/* Remove n bits from the bit accumulator */
#define DROPBITS(n) \
		do { \
				hold >>= (n); \
				bits -= (unsigned)(n); \
		} while (0)

/* Remove zero to seven bits as needed to go to a byte boundary */
#define BYTEBITS() \
		do { \
				hold >>= bits & 7; \
				bits -= bits & 7; \
		} while (0)

/* Reverse the bytes in a 32-bit value */
#define REVERSE(q) \
		((((q) >> 24) & 0xff) + (((q) >> 8) & 0xff00) + \
		(((q) & 0xff00) << 8) + (((q) & 0xff) << 24))

#define Z_DEFLATED   8


typedef enum {
		CODES,
		LENS,
		DISTS
} codetype;

struct inflate_workspace {
				struct inflate_state inflate_state;
				unsigned char working_window[1 << MAX_WBITS];
};

int zlib_inflate_workspacesize_tlx(void)
{
		return sizeof(struct inflate_workspace);
}

int zlib_inflateReset_tlx(z_streamp strm)
{
		struct inflate_state *state;

		if (strm == NULL || strm->state == NULL) return Z_STREAM_ERROR;
		state = (struct inflate_state *)strm->state;
		strm->total_in = strm->total_out = state->total = 0;
		strm->msg = NULL;
		strm->adler = 1;        /* to support ill-conceived Java test suite */
		state->mode = HEAD;
		state->last = 0;
		state->havedict = 0;
		state->dmax = 32768U;
		state->hold = 0;
		state->bits = 0;
		state->lencode = state->distcode = state->next = state->codes;

		/* Initialise Window */
		state->wsize = 1U << state->wbits;
		state->write = 0;
		state->whave = 0;

		return Z_OK;
}

int zlib_inflateInit2_tlx(z_streamp strm, int windowBits)
{
		struct inflate_state *state;

		if (strm == NULL) return Z_STREAM_ERROR;
		strm->msg = NULL;                 /* in case we return an error */

		state = &WS(strm)->inflate_state;
		strm->state = (struct internal_state *)state;

		if (windowBits < 0) {
				state->wrap = 0;
				windowBits = -windowBits;
		}
		else {
				state->wrap = (windowBits >> 4) + 1;
		}
		if (windowBits < 8 || windowBits > 15) {
				return Z_STREAM_ERROR;
		}
		state->wbits = (unsigned)windowBits;
		state->window = &WS(strm)->working_window[0];

		return zlib_inflateReset_tlx(strm);
}


int zlib_inflateSyncPacket_tlx(z_streamp strm)
{
		struct inflate_state *state;

		if (strm == NULL || strm->state == NULL) return Z_STREAM_ERROR;
		state = (struct inflate_state *)strm->state;

		if (state->mode == STORED && state->bits == 0) {
	state->mode = TYPE;
				return Z_OK;
		}
		return Z_DATA_ERROR;
}

void zlib_updatewindow_tlx(z_streamp strm, unsigned out)
{
		struct inflate_state *state;
		unsigned copy, dist;

		state = (struct inflate_state *)strm->state;

		/* copy state->wsize or less output bytes into the circular window */
		copy = out - strm->avail_out;
		if (copy >= state->wsize) {
				memcpy_tlx(state->window, strm->next_out - state->wsize, state->wsize);
				state->write = 0;
				state->whave = state->wsize;
		}
		else {
				dist = state->wsize - state->write;
				if (dist > copy) dist = copy;
				memcpy_tlx(state->window + state->write, strm->next_out - copy, dist);
				copy -= dist;
				if (copy) {
						memcpy_tlx(state->window, strm->next_out - copy, copy);
						state->write = copy;
						state->whave = state->wsize;
				}
				else {
						state->write += dist;
						if (state->write == state->wsize) state->write = 0;
						if (state->whave < state->wsize) state->whave += dist;
				}
		}
}


union uu {
	unsigned short us;
	unsigned char b[2];
};

/* Endian independed version */
static inline unsigned short
get_unaligned16(const unsigned short *p)
{
	union uu  mm;
	unsigned char *b = (unsigned char *)p;

	mm.b[0] = b[0];
	mm.b[1] = b[1];
	return mm.us;
}

#ifdef POSTINC
#  define OFF 0
#  define PUP(a) *(a)++
#  define UP_UNALIGNED(a) get_unaligned16((a)++)
#else
#  define OFF 1
#  define PUP(a) *++(a)
#  define UP_UNALIGNED(a) get_unaligned16(++(a))
#endif

void zlib_fixedtables_tlx(struct inflate_state *state)
{
static const code lenfix[512] = {
		{96,7,0},{0,8,80},{0,8,16},{20,8,115},{18,7,31},{0,8,112},{0,8,48},
		{0,9,192},{16,7,10},{0,8,96},{0,8,32},{0,9,160},{0,8,0},{0,8,128},
		{0,8,64},{0,9,224},{16,7,6},{0,8,88},{0,8,24},{0,9,144},{19,7,59},
		{0,8,120},{0,8,56},{0,9,208},{17,7,17},{0,8,104},{0,8,40},{0,9,176},
		{0,8,8},{0,8,136},{0,8,72},{0,9,240},{16,7,4},{0,8,84},{0,8,20},
		{21,8,227},{19,7,43},{0,8,116},{0,8,52},{0,9,200},{17,7,13},{0,8,100},
		{0,8,36},{0,9,168},{0,8,4},{0,8,132},{0,8,68},{0,9,232},{16,7,8},
		{0,8,92},{0,8,28},{0,9,152},{20,7,83},{0,8,124},{0,8,60},{0,9,216},
		{18,7,23},{0,8,108},{0,8,44},{0,9,184},{0,8,12},{0,8,140},{0,8,76},
		{0,9,248},{16,7,3},{0,8,82},{0,8,18},{21,8,163},{19,7,35},{0,8,114},
		{0,8,50},{0,9,196},{17,7,11},{0,8,98},{0,8,34},{0,9,164},{0,8,2},
		{0,8,130},{0,8,66},{0,9,228},{16,7,7},{0,8,90},{0,8,26},{0,9,148},
		{20,7,67},{0,8,122},{0,8,58},{0,9,212},{18,7,19},{0,8,106},{0,8,42},
		{0,9,180},{0,8,10},{0,8,138},{0,8,74},{0,9,244},{16,7,5},{0,8,86},
		{0,8,22},{64,8,0},{19,7,51},{0,8,118},{0,8,54},{0,9,204},{17,7,15},
		{0,8,102},{0,8,38},{0,9,172},{0,8,6},{0,8,134},{0,8,70},{0,9,236},
		{16,7,9},{0,8,94},{0,8,30},{0,9,156},{20,7,99},{0,8,126},{0,8,62},
		{0,9,220},{18,7,27},{0,8,110},{0,8,46},{0,9,188},{0,8,14},{0,8,142},
		{0,8,78},{0,9,252},{96,7,0},{0,8,81},{0,8,17},{21,8,131},{18,7,31},
		{0,8,113},{0,8,49},{0,9,194},{16,7,10},{0,8,97},{0,8,33},{0,9,162},
		{0,8,1},{0,8,129},{0,8,65},{0,9,226},{16,7,6},{0,8,89},{0,8,25},
		{0,9,146},{19,7,59},{0,8,121},{0,8,57},{0,9,210},{17,7,17},{0,8,105},
		{0,8,41},{0,9,178},{0,8,9},{0,8,137},{0,8,73},{0,9,242},{16,7,4},
		{0,8,85},{0,8,21},{16,8,258},{19,7,43},{0,8,117},{0,8,53},{0,9,202},
		{17,7,13},{0,8,101},{0,8,37},{0,9,170},{0,8,5},{0,8,133},{0,8,69},
		{0,9,234},{16,7,8},{0,8,93},{0,8,29},{0,9,154},{20,7,83},{0,8,125},
		{0,8,61},{0,9,218},{18,7,23},{0,8,109},{0,8,45},{0,9,186},{0,8,13},
		{0,8,141},{0,8,77},{0,9,250},{16,7,3},{0,8,83},{0,8,19},{21,8,195},
		{19,7,35},{0,8,115},{0,8,51},{0,9,198},{17,7,11},{0,8,99},{0,8,35},
		{0,9,166},{0,8,3},{0,8,131},{0,8,67},{0,9,230},{16,7,7},{0,8,91},
		{0,8,27},{0,9,150},{20,7,67},{0,8,123},{0,8,59},{0,9,214},{18,7,19},
		{0,8,107},{0,8,43},{0,9,182},{0,8,11},{0,8,139},{0,8,75},{0,9,246},
		{16,7,5},{0,8,87},{0,8,23},{64,8,0},{19,7,51},{0,8,119},{0,8,55},
		{0,9,206},{17,7,15},{0,8,103},{0,8,39},{0,9,174},{0,8,7},{0,8,135},
		{0,8,71},{0,9,238},{16,7,9},{0,8,95},{0,8,31},{0,9,158},{20,7,99},
		{0,8,127},{0,8,63},{0,9,222},{18,7,27},{0,8,111},{0,8,47},{0,9,190},
		{0,8,15},{0,8,143},{0,8,79},{0,9,254},{96,7,0},{0,8,80},{0,8,16},
		{20,8,115},{18,7,31},{0,8,112},{0,8,48},{0,9,193},{16,7,10},{0,8,96},
		{0,8,32},{0,9,161},{0,8,0},{0,8,128},{0,8,64},{0,9,225},{16,7,6},
		{0,8,88},{0,8,24},{0,9,145},{19,7,59},{0,8,120},{0,8,56},{0,9,209},
		{17,7,17},{0,8,104},{0,8,40},{0,9,177},{0,8,8},{0,8,136},{0,8,72},
		{0,9,241},{16,7,4},{0,8,84},{0,8,20},{21,8,227},{19,7,43},{0,8,116},
		{0,8,52},{0,9,201},{17,7,13},{0,8,100},{0,8,36},{0,9,169},{0,8,4},
		{0,8,132},{0,8,68},{0,9,233},{16,7,8},{0,8,92},{0,8,28},{0,9,153},
		{20,7,83},{0,8,124},{0,8,60},{0,9,217},{18,7,23},{0,8,108},{0,8,44},
		{0,9,185},{0,8,12},{0,8,140},{0,8,76},{0,9,249},{16,7,3},{0,8,82},
		{0,8,18},{21,8,163},{19,7,35},{0,8,114},{0,8,50},{0,9,197},{17,7,11},
		{0,8,98},{0,8,34},{0,9,165},{0,8,2},{0,8,130},{0,8,66},{0,9,229},
		{16,7,7},{0,8,90},{0,8,26},{0,9,149},{20,7,67},{0,8,122},{0,8,58},
		{0,9,213},{18,7,19},{0,8,106},{0,8,42},{0,9,181},{0,8,10},{0,8,138},
		{0,8,74},{0,9,245},{16,7,5},{0,8,86},{0,8,22},{64,8,0},{19,7,51},
		{0,8,118},{0,8,54},{0,9,205},{17,7,15},{0,8,102},{0,8,38},{0,9,173},
		{0,8,6},{0,8,134},{0,8,70},{0,9,237},{16,7,9},{0,8,94},{0,8,30},
		{0,9,157},{20,7,99},{0,8,126},{0,8,62},{0,9,221},{18,7,27},{0,8,110},
		{0,8,46},{0,9,189},{0,8,14},{0,8,142},{0,8,78},{0,9,253},{96,7,0},
		{0,8,81},{0,8,17},{21,8,131},{18,7,31},{0,8,113},{0,8,49},{0,9,195},
		{16,7,10},{0,8,97},{0,8,33},{0,9,163},{0,8,1},{0,8,129},{0,8,65},
		{0,9,227},{16,7,6},{0,8,89},{0,8,25},{0,9,147},{19,7,59},{0,8,121},
		{0,8,57},{0,9,211},{17,7,17},{0,8,105},{0,8,41},{0,9,179},{0,8,9},
		{0,8,137},{0,8,73},{0,9,243},{16,7,4},{0,8,85},{0,8,21},{16,8,258},
		{19,7,43},{0,8,117},{0,8,53},{0,9,203},{17,7,13},{0,8,101},{0,8,37},
		{0,9,171},{0,8,5},{0,8,133},{0,8,69},{0,9,235},{16,7,8},{0,8,93},
		{0,8,29},{0,9,155},{20,7,83},{0,8,125},{0,8,61},{0,9,219},{18,7,23},
		{0,8,109},{0,8,45},{0,9,187},{0,8,13},{0,8,141},{0,8,77},{0,9,251},
		{16,7,3},{0,8,83},{0,8,19},{21,8,195},{19,7,35},{0,8,115},{0,8,51},
		{0,9,199},{17,7,11},{0,8,99},{0,8,35},{0,9,167},{0,8,3},{0,8,131},
		{0,8,67},{0,9,231},{16,7,7},{0,8,91},{0,8,27},{0,9,151},{20,7,67},
		{0,8,123},{0,8,59},{0,9,215},{18,7,19},{0,8,107},{0,8,43},{0,9,183},
		{0,8,11},{0,8,139},{0,8,75},{0,9,247},{16,7,5},{0,8,87},{0,8,23},
		{64,8,0},{19,7,51},{0,8,119},{0,8,55},{0,9,207},{17,7,15},{0,8,103},
		{0,8,39},{0,9,175},{0,8,7},{0,8,135},{0,8,71},{0,9,239},{16,7,9},
		{0,8,95},{0,8,31},{0,9,159},{20,7,99},{0,8,127},{0,8,63},{0,9,223},
		{18,7,27},{0,8,111},{0,8,47},{0,9,191},{0,8,15},{0,8,143},{0,8,79},
		{0,9,255}
};

static const code distfix[32] = {
		{16,5,1},{23,5,257},{19,5,17},{27,5,4097},{17,5,5},{25,5,1025},
		{21,5,65},{29,5,16385},{16,5,3},{24,5,513},{20,5,33},{28,5,8193},
		{18,5,9},{26,5,2049},{22,5,129},{64,5,0},{16,5,2},{23,5,385},
		{19,5,25},{27,5,6145},{17,5,7},{25,5,1537},{21,5,97},{29,5,24577},
		{16,5,4},{24,5,769},{20,5,49},{28,5,12289},{18,5,13},{26,5,3073},
		{22,5,193},{64,5,0}
};
		state->lencode = lenfix;
		state->lenbits = 9;
		state->distcode = distfix;
		state->distbits = 5;
}

void inflate_fast_tlx(z_streamp strm, unsigned start)
{
		struct inflate_state *state;
		const unsigned char *in;    /* local strm->next_in */
		const unsigned char *last;  /* while in < last, enough input available */
		unsigned char *out;         /* local strm->next_out */
		unsigned char *beg;         /* inflate()'s initial strm->next_out */
		unsigned char *end;         /* while out < end, enough space available */
#ifdef INFLATE_STRICT
		unsigned dmax;              /* maximum distance from zlib header */
#endif
		unsigned wsize;             /* window size or zero if not using window */
		unsigned whave;             /* valid bytes in the window */
		unsigned write;             /* window write index */
		unsigned char *window;      /* allocated sliding window, if wsize != 0 */
		unsigned long hold;         /* local strm->hold */
		unsigned bits;              /* local strm->bits */
		code const *lcode;          /* local strm->lencode */
		code const *dcode;          /* local strm->distcode */
		unsigned lmask;             /* mask for first level of length codes */
		unsigned dmask;             /* mask for first level of distance codes */
		code this;                  /* retrieved table entry */
		unsigned op;                /* code bits, operation, extra bits, or */
																/*  window position, window bytes to copy */
		unsigned len;               /* match length, unused bytes */
		unsigned dist;              /* match distance */
		unsigned char *from;        /* where to copy match from */

		/* copy state to local variables */
		state = (struct inflate_state *)strm->state;
		in = strm->next_in - OFF;
		last = in + (strm->avail_in - 5);
		out = strm->next_out - OFF;
		beg = out - (start - strm->avail_out);
		end = out + (strm->avail_out - 257);
#ifdef INFLATE_STRICT
		dmax = state->dmax;
#endif
		wsize = state->wsize;
		whave = state->whave;
		write = state->write;
		window = state->window;
		hold = state->hold;
		bits = state->bits;
		lcode = state->lencode;
		dcode = state->distcode;
		lmask = (1U << state->lenbits) - 1;
		dmask = (1U << state->distbits) - 1;

		/* decode literals and length/distances until end-of-block or not enough
			input data or output space */
		do {
				if (bits < 15) {
						hold += (unsigned long)(PUP(in)) << bits;
						bits += 8;
						hold += (unsigned long)(PUP(in)) << bits;
						bits += 8;
				}
				this = lcode[hold & lmask];
			dolen:
				op = (unsigned)(this.bits);
				hold >>= op;
				bits -= op;
				op = (unsigned)(this.op);
				if (op == 0) {                          /* literal */
						PUP(out) = (unsigned char)(this.val);
				}
				else if (op & 16) {                     /* length base */
						len = (unsigned)(this.val);
						op &= 15;                           /* number of extra bits */
						if (op) {
								if (bits < op) {
										hold += (unsigned long)(PUP(in)) << bits;
										bits += 8;
								}
								len += (unsigned)hold & ((1U << op) - 1);
								hold >>= op;
								bits -= op;
						}
						if (bits < 15) {
								hold += (unsigned long)(PUP(in)) << bits;
								bits += 8;
								hold += (unsigned long)(PUP(in)) << bits;
								bits += 8;
						}
						this = dcode[hold & dmask];
					dodist:
						op = (unsigned)(this.bits);
						hold >>= op;
						bits -= op;
						op = (unsigned)(this.op);
						if (op & 16) {                      /* distance base */
								dist = (unsigned)(this.val);
								op &= 15;                       /* number of extra bits */
								if (bits < op) {
										hold += (unsigned long)(PUP(in)) << bits;
										bits += 8;
										if (bits < op) {
												hold += (unsigned long)(PUP(in)) << bits;
												bits += 8;
										}
								}
								dist += (unsigned)hold & ((1U << op) - 1);
#ifdef INFLATE_STRICT
								if (dist > dmax) {
										strm->msg = (char *)"invalid distance too far back";
										state->mode = BAD;
										break;
								}
#endif
								hold >>= op;
								bits -= op;
								op = (unsigned)(out - beg);     /* max distance in output */
								if (dist > op) {                /* see if copy from window */
										op = dist - op;             /* distance back in window */
										if (op > whave) {
												strm->msg = (char *)"invalid distance too far back";
												state->mode = BAD;
												break;
										}
										from = window - OFF;
										if (write == 0) {           /* very common case */
												from += wsize - op;
												if (op < len) {         /* some from window */
														len -= op;
														do {
																PUP(out) = PUP(from);
														} while (--op);
														from = out - dist;  /* rest from output */
												}
										}
										else if (write < op) {      /* wrap around window */
												from += wsize + write - op;
												op -= write;
												if (op < len) {         /* some from end of window */
														len -= op;
														do {
																PUP(out) = PUP(from);
														} while (--op);
														from = window - OFF;
														if (write < len) {  /* some from start of window */
																op = write;
																len -= op;
																do {
																		PUP(out) = PUP(from);
																} while (--op);
																from = out - dist;      /* rest from output */
														}
												}
										}
										else {                      /* contiguous in window */
												from += write - op;
												if (op < len) {         /* some from window */
														len -= op;
														do {
																PUP(out) = PUP(from);
														} while (--op);
														from = out - dist;  /* rest from output */
												}
										}
										while (len > 2) {
												PUP(out) = PUP(from);
												PUP(out) = PUP(from);
												PUP(out) = PUP(from);
												len -= 3;
										}
										if (len) {
												PUP(out) = PUP(from);
												if (len > 1)
														PUP(out) = PUP(from);
										}
								}
								else {
				unsigned short *sout;
				unsigned long loops;

										from = out - dist;          /* copy direct from output */
				/* minimum length is three */
				/* Align out addr */
				if (!((long)(out - 1 + OFF) & 1)) {
			PUP(out) = PUP(from);
			len--;
				}
				sout = (unsigned short *)(out - OFF);
				if (dist > 2) {
			unsigned short *sfrom;

			sfrom = (unsigned short *)(from - OFF);
			loops = len >> 1;
			do
#ifdef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
					PUP(sout) = PUP(sfrom);
#else
					PUP(sout) = UP_UNALIGNED(sfrom);
#endif
			while (--loops);
			out = (unsigned char *)sout + OFF;
			from = (unsigned char *)sfrom + OFF;
				} else { /* dist == 1 or dist == 2 */
			unsigned short pat16;

			pat16 = *(sout-1+OFF);
			if (dist == 1) {
				union uu mm;
				/* copy one char pattern to both bytes */
				mm.us = pat16;
				mm.b[0] = mm.b[1];
				pat16 = mm.us;
			}
			loops = len >> 1;
			do
					PUP(sout) = pat16;
			while (--loops);
			out = (unsigned char *)sout + OFF;
				}
				if (len & 1)
			PUP(out) = PUP(from);
								}
						}
						else if ((op & 64) == 0) {          /* 2nd level distance code */
								this = dcode[this.val + (hold & ((1U << op) - 1))];
								goto dodist;
						}
						else {
								strm->msg = (char *)"invalid distance code";
								state->mode = BAD;
								break;
						}
				}
				else if ((op & 64) == 0) {              /* 2nd level length code */
						this = lcode[this.val + (hold & ((1U << op) - 1))];
						goto dolen;
				}
				else if (op & 32) {                     /* end-of-block */
						state->mode = TYPE;
						break;
				}
				else {
						strm->msg = (char *)"invalid literal/length code";
						state->mode = BAD;
						break;
				}
		} while (in < last && out < end);

		/* return unused bytes (on entry, bits < 8, so in won't go too far back) */
		len = bits >> 3;
		in -= len;
		bits -= len << 3;
		hold &= (1U << bits) - 1;

		/* update state and return */
		strm->next_in = in + OFF;
		strm->next_out = out + OFF;
		strm->avail_in = (unsigned)(in < last ? 5 + (last - in) : 5 - (in - last));
		strm->avail_out = (unsigned)(out < end ?
																257 + (end - out) : 257 - (out - end));
		state->hold = hold;
		state->bits = bits;
		return;
}

#define MAXBITS 15
#define MAXD 592

int zlib_inflate_table_tlx(codetype type, unsigned short *lens, unsigned codes,
			code **table, unsigned *bits, unsigned short *work)
{
		unsigned len;               /* a code's length in bits */
		unsigned sym;               /* index of code symbols */
		unsigned min, max;          /* minimum and maximum code lengths */
		unsigned root;              /* number of index bits for root table */
		unsigned curr;              /* number of index bits for current table */
		unsigned drop;              /* code bits to drop for sub-table */
		int left;                   /* number of prefix codes available */
		unsigned used;              /* code entries in table used */
		unsigned huff;              /* Huffman code */
		unsigned incr;              /* for incrementing code, index */
		unsigned fill;              /* index for replicating entries */
		unsigned low;               /* low bits for current root entry */
		unsigned mask;              /* mask for low root bits */
		code this;                  /* table entry for duplication */
		code *next;             /* next available space in table */
		const unsigned short *base;     /* base value table to use */
		const unsigned short *extra;    /* extra bits table to use */
		int end;                    /* use base and extra for symbol > end */
		unsigned short count[MAXBITS+1];    /* number of codes of each length */
		unsigned short offs[MAXBITS+1];     /* offsets in table for each length */
		static const unsigned short lbase[31] = { /* Length codes 257..285 base */
				3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
				35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
		static const unsigned short lext[31] = { /* Length codes 257..285 extra */
				16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 18, 18, 18, 18,
				19, 19, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 16, 201, 196};
		static const unsigned short dbase[32] = { /* Distance codes 0..29 base */
				1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193,
				257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145,
				8193, 12289, 16385, 24577, 0, 0};
		static const unsigned short dext[32] = { /* Distance codes 0..29 extra */
				16, 16, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22,
				23, 23, 24, 24, 25, 25, 26, 26, 27, 27,
				28, 28, 29, 29, 64, 64};

		/*
			Process a set of code lengths to create a canonical Huffman code.  The
			code lengths are lens[0..codes-1].  Each length corresponds to the
			symbols 0..codes-1.  The Huffman code is generated by first sorting the
			symbols by length from short to long, and retaining the symbol order
			for codes with equal lengths.  Then the code starts with all zero bits
			for the first code of the shortest length, and the codes are integer
			increments for the same length, and zeros are appended as the length
			increases.  For the deflate format, these bits are stored backwards
			from their more natural integer increment ordering, and so when the
			decoding tables are built in the large loop below, the integer codes
			are incremented backwards.

			This routine assumes, but does not check, that all of the entries in
			lens[] are in the range 0..MAXBITS.  The caller must assure this.
			1..MAXBITS is interpreted as that code length.  zero means that that
			symbol does not occur in this code.

			The codes are sorted by computing a count of codes for each length,
			creating from that a table of starting indices for each length in the
			sorted table, and then entering the symbols in order in the sorted
			table.  The sorted table is work[], with that space being provided by
			the caller.

			The length counts are used for other purposes as well, i.e. finding
			the minimum and maximum length codes, determining if there are any
			codes at all, checking for a valid set of lengths, and looking ahead
			at length counts to determine sub-table sizes when building the
			decoding tables.
		*/

		/* accumulate lengths for codes (assumes lens[] all in 0..MAXBITS) */
		for (len = 0; len <= MAXBITS; len++)
				count[len] = 0;
		for (sym = 0; sym < codes; sym++)
				count[lens[sym]]++;

		/* bound code lengths, force root to be within code lengths */
		root = *bits;
		for (max = MAXBITS; max >= 1; max--)
				if (count[max] != 0) break;
		if (root > max) root = max;
		if (max == 0) {                     /* no symbols to code at all */
				this.op = (unsigned char)64;    /* invalid code marker */
				this.bits = (unsigned char)1;
				this.val = (unsigned short)0;
				*(*table)++ = this;             /* make a table to force an error */
				*(*table)++ = this;
				*bits = 1;
				return 0;     /* no symbols, but wait for decoding to report error */
		}
		for (min = 1; min <= MAXBITS; min++)
				if (count[min] != 0) break;
		if (root < min) root = min;

		/* check for an over-subscribed or incomplete set of lengths */
		left = 1;
		for (len = 1; len <= MAXBITS; len++) {
				left <<= 1;
				left -= count[len];
				if (left < 0) return -1;        /* over-subscribed */
		}
		if (left > 0 && (type == CODES || max != 1))
				return -1;                      /* incomplete set */

		/* generate offsets into symbol table for each length for sorting */
		offs[1] = 0;
		for (len = 1; len < MAXBITS; len++)
				offs[len + 1] = offs[len] + count[len];

		/* sort symbols by length, by symbol order within each length */
		for (sym = 0; sym < codes; sym++)
				if (lens[sym] != 0) work[offs[lens[sym]]++] = (unsigned short)sym;

		/*
			Create and fill in decoding tables.  In this loop, the table being
			filled is at next and has curr index bits.  The code being used is huff
			with length len.  That code is converted to an index by dropping drop
			bits off of the bottom.  For codes where len is less than drop + curr,
			those top drop + curr - len bits are incremented through all values to
			fill the table with replicated entries.

			root is the number of index bits for the root table.  When len exceeds
			root, sub-tables are created pointed to by the root entry with an index
			of the low root bits of huff.  This is saved in low to check for when a
			new sub-table should be started.  drop is zero when the root table is
			being filled, and drop is root when sub-tables are being filled.

			When a new sub-table is needed, it is necessary to look ahead in the
			code lengths to determine what size sub-table is needed.  The length
			counts are used for this, and so count[] is decremented as codes are
			entered in the tables.

			used keeps track of how many table entries have been allocated from the
			provided *table space.  It is checked when a LENS table is being made
			against the space in *table, ENOUGH, minus the maximum space needed by
			the worst case distance code, MAXD.  This should never happen, but the
			sufficiency of ENOUGH has not been proven exhaustively, hence the check.
			This assumes that when type == LENS, bits == 9.

			sym increments through all symbols, and the loop terminates when
			all codes of length max, i.e. all codes, have been processed.  This
			routine permits incomplete codes, so another loop after this one fills
			in the rest of the decoding tables with invalid code markers.
		*/

		/* set up for code type */
		switch (type) {
		case CODES:
				base = extra = work;    /* dummy value--not used */
				end = 19;
				break;
		case LENS:
				base = lbase;
				base -= 257;
				extra = lext;
				extra -= 257;
				end = 256;
				break;
		default:            /* DISTS */
				base = dbase;
				extra = dext;
				end = -1;
		}

		/* initialize state for loop */
		huff = 0;                   /* starting code */
		sym = 0;                    /* starting code symbol */
		len = min;                  /* starting code length */
		next = *table;              /* current table to fill in */
		curr = root;                /* current table index bits */
		drop = 0;                   /* current bits to drop from code for index */
		low = (unsigned)(-1);       /* trigger new sub-table when len > root */
		used = 1U << root;          /* use root table entries */
		mask = used - 1;            /* mask for comparing low */

		/* check available table space */
		if (type == LENS && used >= ENOUGH - MAXD)
				return 1;

		/* process all codes and make table entries */
		for (;;) {
				/* create table entry */
				this.bits = (unsigned char)(len - drop);
				if ((int)(work[sym]) < end) {
						this.op = (unsigned char)0;
						this.val = work[sym];
				}
				else if ((int)(work[sym]) > end) {
						this.op = (unsigned char)(extra[work[sym]]);
						this.val = base[work[sym]];
				}
				else {
						this.op = (unsigned char)(32 + 64);         /* end of block */
						this.val = 0;
				}

				/* replicate for those indices with low len bits equal to huff */
				incr = 1U << (len - drop);
				fill = 1U << curr;
				min = fill;                 /* save offset to next table */
				do {
						fill -= incr;
						next[(huff >> drop) + fill] = this;
				} while (fill != 0);

				/* backwards increment the len-bit code huff */
				incr = 1U << (len - 1);
				while (huff & incr)
						incr >>= 1;
				if (incr != 0) {
						huff &= incr - 1;
						huff += incr;
				}
				else
						huff = 0;

				/* go to next symbol, update count, len */
				sym++;
				if (--(count[len]) == 0) {
						if (len == max) break;
						len = lens[work[sym]];
				}

				/* create new sub-table if needed */
				if (len > root && (huff & mask) != low) {
						/* if first time, transition to sub-tables */
						if (drop == 0)
								drop = root;

						/* increment past last table */
						next += min;            /* here min is 1 << curr */

						/* determine length of next table */
						curr = len - drop;
						left = (int)(1 << curr);
						while (curr + drop < max) {
								left -= count[curr + drop];
								if (left <= 0) break;
								curr++;
								left <<= 1;
						}

						/* check for enough space */
						used += 1U << curr;
						if (type == LENS && used >= ENOUGH - MAXD)
								return 1;

						/* point entry in root table to sub-table */
						low = huff & mask;
						(*table)[low].op = (unsigned char)curr;
						(*table)[low].bits = (unsigned char)root;
						(*table)[low].val = (unsigned short)(next - *table);
				}
		}

		/*
			Fill in rest of table for incomplete codes.  This loop is similar to the
			loop above in incrementing huff for table indices.  It is assumed that
			len is equal to curr + drop, so there is no loop needed to increment
			through high index bits.  When the current sub-table is filled, the loop
			drops back to the root table to fill in any remaining entries there.
		*/
		this.op = (unsigned char)64;                /* invalid code marker */
		this.bits = (unsigned char)(len - drop);
		this.val = (unsigned short)0;
		while (huff != 0) {
				/* when done with sub-table, drop back to root table */
				if (drop != 0 && (huff & mask) != low) {
						drop = 0;
						len = root;
						next = *table;
						this.bits = (unsigned char)len;
				}

				/* put invalid code marker in table */
				next[huff >> drop] = this;

				/* backwards increment the len-bit code huff */
				incr = 1U << (len - 1);
				while (huff & incr)
						incr >>= 1;
				if (incr != 0) {
						huff &= incr - 1;
						huff += incr;
				}
				else
						huff = 0;
		}

		/* set return parameters */
		*table += used;
		*bits = root;
		return 0;
}

#define BASE 65521L /* largest prime smaller than 65536 */
#define NMAX 5552
/* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */

#define DO1(buf,i)  {s1 += buf[i]; s2 += s1;}
#define DO2(buf,i)  DO1(buf,i); DO1(buf,i+1);
#define DO4(buf,i)  DO2(buf,i); DO2(buf,i+2);
#define DO8(buf,i)  DO4(buf,i); DO4(buf,i+4);
#define DO16(buf)   DO8(buf,0); DO8(buf,8);

static inline uLong zlib_adler32(uLong adler,
				const Byte *buf,
				uInt len)
{
		unsigned long s1 = adler & 0xffff;
		unsigned long s2 = (adler >> 16) & 0xffff;
		int k;

		if (buf == NULL) return 1L;

		while (len > 0) {
				k = len < NMAX ? len : NMAX;
				len -= k;
				while (k >= 16) {
						DO16(buf);
			buf += 16;
						k -= 16;
				}
				if (k != 0) do {
						s1 += *buf++;
			s2 += s1;
				} while (--k);
				s1 %= BASE;
				s2 %= BASE;
		}
		return (s2 << 16) | s1;
}

int zlib_inflate_tlx(z_streamp strm, int flush)
{
		struct inflate_state *state;
		const unsigned char *next;  /* next input */
		unsigned char *put;         /* next output */
		unsigned have, left;        /* available input and output */
		unsigned long hold;         /* bit buffer */
		unsigned bits;              /* bits in bit buffer */
		unsigned in, out;           /* save starting available input and output */
		unsigned copy;              /* number of stored or match bytes to copy */
		unsigned char *from;        /* where to copy match bytes from */
		code this;                  /* current decoding table entry */
		code last;                  /* parent table entry */
		unsigned len;               /* length to copy for repeats, bits to drop */
		int ret;                    /* return code */
		static const unsigned short order[19] = /* permutation of code lengths */
				{16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};

		/* Do not check for strm->next_out == NULL here as ppc zImage
			inflates to strm->next_out = 0 */

		if (strm == NULL || strm->state == NULL ||
				(strm->next_in == NULL && strm->avail_in != 0))
				return Z_STREAM_ERROR;

		state = (struct inflate_state *)strm->state;

		if (state->mode == TYPE) state->mode = TYPEDO;      /* skip check */
		LOAD();
		in = have;
		out = left;
		ret = Z_OK;
		for (;;)
				switch (state->mode) {
				case HEAD:
						if (state->wrap == 0) {
								state->mode = TYPEDO;
								break;
						}
						NEEDBITS(16);
						if (
								((BITS(8) << 8) + (hold >> 8)) % 31) {
								strm->msg = (char *)"incorrect header check";
								state->mode = BAD;
								break;
						}
						if (BITS(4) != Z_DEFLATED) {
								strm->msg = (char *)"unknown compression method";
								state->mode = BAD;
								break;
						}
						DROPBITS(4);
						len = BITS(4) + 8;
						if (len > state->wbits) {
								strm->msg = (char *)"invalid window size";
								state->mode = BAD;
								break;
						}
						state->dmax = 1U << len;
						strm->adler = state->check = zlib_adler32(0L, NULL, 0);
						state->mode = hold & 0x200 ? DICTID : TYPE;
						INITBITS();
						break;
				case DICTID:
						NEEDBITS(32);
						strm->adler = state->check = REVERSE(hold);
						INITBITS();
						state->mode = DICT;
				case DICT:
						if (state->havedict == 0) {
								RESTORE();
								return Z_NEED_DICT;
						}
						strm->adler = state->check = zlib_adler32(0L, NULL, 0);
						state->mode = TYPE;
				case TYPE:
						if (flush == Z_BLOCK) goto inf_leave;
				case TYPEDO:
						if (state->last) {
								BYTEBITS();
								state->mode = CHECK;
								break;
						}
						NEEDBITS(3);
						state->last = BITS(1);
						DROPBITS(1);
						switch (BITS(2)) {
						case 0:                             /* stored block */
								state->mode = STORED;
								break;
						case 1:                             /* fixed block */
								zlib_fixedtables_tlx(state);
								state->mode = LEN;              /* decode codes */
								break;
						case 2:                             /* dynamic block */
								state->mode = TABLE;
								break;
						case 3:
								strm->msg = (char *)"invalid block type";
								state->mode = BAD;
						}
						DROPBITS(2);
						break;
				case STORED:
						BYTEBITS();                         /* go to byte boundary */
						NEEDBITS(32);
						if ((hold & 0xffff) != ((hold >> 16) ^ 0xffff)) {
								strm->msg = (char *)"invalid stored block lengths";
								state->mode = BAD;
								break;
						}
						state->length = (unsigned)hold & 0xffff;
						INITBITS();
						state->mode = COPY;
				case COPY:
						copy = state->length;
						if (copy) {
								if (copy > have) copy = have;
								if (copy > left) copy = left;
								if (copy == 0) goto inf_leave;
								memcpy_tlx(put, next, copy);
								have -= copy;
								next += copy;
								left -= copy;
								put += copy;
								state->length -= copy;
								break;
						}
						state->mode = TYPE;
						break;
				case TABLE:
						NEEDBITS(14);
						state->nlen = BITS(5) + 257;
						DROPBITS(5);
						state->ndist = BITS(5) + 1;
						DROPBITS(5);
						state->ncode = BITS(4) + 4;
						DROPBITS(4);
#ifndef PKZIP_BUG_WORKAROUND
						if (state->nlen > 286 || state->ndist > 30) {
								strm->msg = (char *)"too many length or distance symbols";
								state->mode = BAD;
								break;
						}
#endif
						state->have = 0;
						state->mode = LENLENS;
				case LENLENS:
						while (state->have < state->ncode) {
								NEEDBITS(3);
								state->lens[order[state->have++]] = (unsigned short)BITS(3);
								DROPBITS(3);
						}
						while (state->have < 19)
								state->lens[order[state->have++]] = 0;
						state->next = state->codes;
						state->lencode = (code const *)(state->next);
						state->lenbits = 7;
						ret = zlib_inflate_table_tlx(CODES, state->lens, 19, &(state->next),
																&(state->lenbits), state->work);
						if (ret) {
								strm->msg = (char *)"invalid code lengths set";
								state->mode = BAD;
								break;
						}
						state->have = 0;
						state->mode = CODELENS;
				case CODELENS:
						while (state->have < state->nlen + state->ndist) {
								for (;;) {
										this = state->lencode[BITS(state->lenbits)];
										if ((unsigned)(this.bits) <= bits) break;
										PULLBYTE();
								}
								if (this.val < 16) {
										NEEDBITS(this.bits);
										DROPBITS(this.bits);
										state->lens[state->have++] = this.val;
								}
								else {
										if (this.val == 16) {
												NEEDBITS(this.bits + 2);
												DROPBITS(this.bits);
												if (state->have == 0) {
														strm->msg = (char *)"invalid bit length repeat";
														state->mode = BAD;
														break;
												}
												len = state->lens[state->have - 1];
												copy = 3 + BITS(2);
												DROPBITS(2);
										}
										else if (this.val == 17) {
												NEEDBITS(this.bits + 3);
												DROPBITS(this.bits);
												len = 0;
												copy = 3 + BITS(3);
												DROPBITS(3);
										}
										else {
												NEEDBITS(this.bits + 7);
												DROPBITS(this.bits);
												len = 0;
												copy = 11 + BITS(7);
												DROPBITS(7);
										}
										if (state->have + copy > state->nlen + state->ndist) {
												strm->msg = (char *)"invalid bit length repeat";
												state->mode = BAD;
												break;
										}
										while (copy--)
												state->lens[state->have++] = (unsigned short)len;
								}
						}

						/* handle error breaks in while */
						if (state->mode == BAD) break;

						/* build code tables */
						state->next = state->codes;
						state->lencode = (code const *)(state->next);
						state->lenbits = 9;
						ret = zlib_inflate_table_tlx(LENS, state->lens, state->nlen, &(state->next),
																&(state->lenbits), state->work);
						if (ret) {
								strm->msg = (char *)"invalid literal/lengths set";
								state->mode = BAD;
								break;
						}
						state->distcode = (code const *)(state->next);
						state->distbits = 6;
						ret = zlib_inflate_table_tlx(DISTS, state->lens + state->nlen, state->ndist,
														&(state->next), &(state->distbits), state->work);
						if (ret) {
								strm->msg = (char *)"invalid distances set";
								state->mode = BAD;
								break;
						}
						state->mode = LEN;
				case LEN:
						if (have >= 6 && left >= 258) {
								RESTORE();
								inflate_fast_tlx(strm, out);
								LOAD();
								break;
						}
						for (;;) {
								this = state->lencode[BITS(state->lenbits)];
								if ((unsigned)(this.bits) <= bits) break;
								PULLBYTE();
						}
						if (this.op && (this.op & 0xf0) == 0) {
								last = this;
								for (;;) {
										this = state->lencode[last.val +
														(BITS(last.bits + last.op) >> last.bits)];
										if ((unsigned)(last.bits + this.bits) <= bits) break;
										PULLBYTE();
								}
								DROPBITS(last.bits);
						}
						DROPBITS(this.bits);
						state->length = (unsigned)this.val;
						if ((int)(this.op) == 0) {
								state->mode = LIT;
								break;
						}
						if (this.op & 32) {
								state->mode = TYPE;
								break;
						}
						if (this.op & 64) {
								strm->msg = (char *)"invalid literal/length code";
								state->mode = BAD;
								break;
						}
						state->extra = (unsigned)(this.op) & 15;
						state->mode = LENEXT;
				case LENEXT:
						if (state->extra) {
								NEEDBITS(state->extra);
								state->length += BITS(state->extra);
								DROPBITS(state->extra);
						}
						state->mode = DIST;
				case DIST:
						for (;;) {
								this = state->distcode[BITS(state->distbits)];
								if ((unsigned)(this.bits) <= bits) break;
								PULLBYTE();
						}
						if ((this.op & 0xf0) == 0) {
								last = this;
								for (;;) {
										this = state->distcode[last.val +
														(BITS(last.bits + last.op) >> last.bits)];
										if ((unsigned)(last.bits + this.bits) <= bits) break;
										PULLBYTE();
								}
								DROPBITS(last.bits);
						}
						DROPBITS(this.bits);
						if (this.op & 64) {
								strm->msg = (char *)"invalid distance code";
								state->mode = BAD;
								break;
						}
						state->offset = (unsigned)this.val;
						state->extra = (unsigned)(this.op) & 15;
						state->mode = DISTEXT;
				case DISTEXT:
						if (state->extra) {
								NEEDBITS(state->extra);
								state->offset += BITS(state->extra);
								DROPBITS(state->extra);
						}
#ifdef INFLATE_STRICT
						if (state->offset > state->dmax) {
								strm->msg = (char *)"invalid distance too far back";
								state->mode = BAD;
								break;
						}
#endif
						if (state->offset > state->whave + out - left) {
								strm->msg = (char *)"invalid distance too far back";
								state->mode = BAD;
								break;
						}
						state->mode = MATCH;
				case MATCH:
						if (left == 0) goto inf_leave;
						copy = out - left;
						if (state->offset > copy) {         /* copy from window */
								copy = state->offset - copy;
								if (copy > state->write) {
										copy -= state->write;
										from = state->window + (state->wsize - copy);
								}
								else
										from = state->window + (state->write - copy);
								if (copy > state->length) copy = state->length;
						}
						else {                              /* copy from output */
								from = put - state->offset;
								copy = state->length;
						}
						if (copy > left) copy = left;
						left -= copy;
						state->length -= copy;
						do {
								*put++ = *from++;
						} while (--copy);
						if (state->length == 0) state->mode = LEN;
						break;
				case LIT:
						if (left == 0) goto inf_leave;
						*put++ = (unsigned char)(state->length);
						left--;
						state->mode = LEN;
						break;
				case CHECK:
						if (state->wrap) {
								NEEDBITS(32);
								out -= left;
								strm->total_out += out;
								state->total += out;
								if (out)
										strm->adler = state->check =
												UPDATE(state->check, put - out, out);
								out = left;
								if ((
										REVERSE(hold)) != state->check) {
										strm->msg = (char *)"incorrect data check";
										state->mode = BAD;
										break;
								}
								INITBITS();
						}
						state->mode = DONE;
				case DONE:
						ret = Z_STREAM_END;
						goto inf_leave;
				case BAD:
						ret = Z_DATA_ERROR;
						goto inf_leave;
				case MEM:
						return Z_MEM_ERROR;
				case SYNC:
				default:
						return Z_STREAM_ERROR;
				}

		/*
			Return from inflate(), updating the total counts and the check value.
			If there was no progress during the inflate() call, return a buffer
			error.  Call zlib_updatewindow_tlx() to create and/or update the window state.
		*/
	inf_leave:
		RESTORE();
		if (state->wsize || (state->mode < CHECK && out != strm->avail_out))
				zlib_updatewindow_tlx(strm, out);

		in -= strm->avail_in;
		out -= strm->avail_out;
		strm->total_in += in;
		strm->total_out += out;
		state->total += out;
		if (state->wrap && out)
				strm->adler = state->check =
						UPDATE(state->check, strm->next_out - out, out);

		strm->data_type = state->bits + (state->last ? 64 : 0) +
											(state->mode == TYPE ? 128 : 0);

		if (flush == Z_PACKET_FLUSH && ret == Z_OK &&
						strm->avail_out != 0 && strm->avail_in == 0)
		return zlib_inflateSyncPacket_tlx(strm);

		if (((in == 0 && out == 0) || flush == Z_FINISH) && ret == Z_OK)
				ret = Z_BUF_ERROR;

		return ret;
}



static int nofill(void *buffer, unsigned int len)
{
         return -1;
}

static int gunzip_tlx(unsigned char *buf, int len,
		       int(*fill)(void*, unsigned int),
		       int(*flush)(void*, unsigned int),
		       unsigned char *out_buf,
		       int *pos,
		       void(*error)(char *x)) {
	u8 *zbuf;
	struct z_stream_s *strm;
	int rc;
	size_t out_len;

	rc = -1;
	if (flush) {
		out_len = 0x8000; /* 32 K */
		out_buf =  __kmalloc_tlx(out_len, GFP_KERNEL);
	} else {
		out_len = ((size_t)~0) - (size_t)out_buf; /* no limit */
	}

	if (buf)
		zbuf = buf;
	else {
		zbuf =  __kmalloc_tlx(GZIP_IOBUF_SIZE, GFP_KERNEL);
		len = 0;
	}

	strm =  __kmalloc_tlx(sizeof(*strm), GFP_KERNEL);

	strm->workspace =  __kmalloc_tlx(flush ? zlib_inflate_workspacesize_tlx() :
				 sizeof(struct inflate_state), GFP_KERNEL);

	if (!fill)
		fill = nofill;

	if (len == 0)
		len = fill(zbuf, GZIP_IOBUF_SIZE);

	printk_tlx(KERN_ERR "decompress \n");

	/* skip over gzip header (1f,8b,08... 10 bytes total +
	 * possible asciz filename)
	 */
	strm->next_in = zbuf + 10;
	strm->avail_in = len - 10;
	/* skip over asciz filename */
	if (zbuf[3] & 0x8) {
		do {
			--strm->avail_in;
		} while (*strm->next_in++);
	}

	strm->next_out = out_buf;
	strm->avail_out = out_len;

	rc = zlib_inflateInit2_tlx(strm, -MAX_WBITS);

	if (!flush) {
		WS(strm)->inflate_state.wsize = 0;
		WS(strm)->inflate_state.window = NULL;
	}

	while (rc == Z_OK) {
		if (strm->avail_in == 0) {
			/* TODO: handle case where both pos and fill are set */
			len = fill(zbuf, GZIP_IOBUF_SIZE);
			if (len < 0) {
				rc = -1;
				error("read error");
				break;
			}
			strm->next_in = zbuf;
			strm->avail_in = len;
		}
		rc = zlib_inflate_tlx(strm, 0);

		/* Write any data generated */
		if (flush && strm->next_out > out_buf) {
			int l = strm->next_out - out_buf;
			if (l != flush(out_buf, l)) {
				rc = -1;
				break;
			}
			strm->next_out = out_buf;
			strm->avail_out = out_len;
		}

		/* after Z_FINISH, only Z_STREAM_END is "we unpacked it all" */
		if (rc == Z_STREAM_END) {
			rc = 0;
			break;
		} else if (rc != Z_OK) {
			rc = -1;
		}
	}

//	zlib_inflateEnd(strm);
	if (pos)
		/* add + 8 to skip over trailer */
		*pos = strm->next_in - zbuf+8;
	return rc; /* returns Z_OK (0) if successful */
}


struct compress_format compressed_formats_tlx_tlx[] = {
	{ {037, 0213}, "gzip", gunzip_tlx },
};

static int __ref kernel_init(void *unused)
{
	int std_nice[NR_STD_WORKER_POOLS] = { 0, HIGHPRI_NICE_LEVEL };
	pwq_cache_tlx = kmem_cache_create_tlx("pool_workqueue",
						256, 0, SLAB_PANIC, 0);
	struct worker_pool *pool;
	int cnt=0;
	for_each_cpu_worker_pool(pool, 0) {
			pool->id = -1;
			pool->cpu = -1;
			pool->node = NUMA_NO_NODE;
			pool->flags |= POOL_DISASSOCIATED;
			INIT_LIST_HEAD(&pool->worklist);
			INIT_LIST_HEAD(&pool->idle_list);
			struct tvec_base *t_base =
					RELOC_HIDE((typeof(*(&(tvec_bases_tlx))) __kernel __force *)(&(tvec_bases_tlx)), (my_cpu_offset));

			(&pool->idle_timer)->entry.next = NULL;
			(&pool->idle_timer)->base = (void *)((unsigned long)t_base | TIMER_DEFERRABLE);
			(&pool->idle_timer)->slack = -1;

			pool->idle_timer.function = idle_worker_timeout_tlx;
			pool->idle_timer.data = (unsigned long)pool;
			(&pool->mayday_timer)->entry.next = NULL;
			(&pool->mayday_timer)->base = (void *)((unsigned long)t_base | TIMER_DEFERRABLE);
			(&pool->mayday_timer)->slack = -1;
      (&pool->mayday_timer)->function = (pool_mayday_timeout_tlx);
      (&pool->mayday_timer)->data = ((unsigned long)pool);
			INIT_LIST_HEAD(&pool->workers);
			memset_tlx(&pool->worker_ida, 0, sizeof(struct ida));
			memset_tlx(&(&pool->worker_ida)->idr, 0, sizeof(struct idr));

			INIT_HLIST_NODE(&pool->hash_node);
			pool->refcnt = 1;
			pool->attrs = kzalloc_tlx(sizeof(*(pool->attrs)), GFP_KERNEL);
	}
	for_each_cpu_worker_pool(pool, 0) {
			struct worker *worker = NULL;
			int id = -1;
			char id_buf[16];
			ida_get_new_above_tlx(&pool->worker_ida, 0, &id);
			worker = kzalloc_tlx(sizeof(*worker), GFP_KERNEL);
			if (worker) {
				INIT_LIST_HEAD(&worker->entry);
				INIT_LIST_HEAD(&worker->scheduled);
				INIT_LIST_HEAD(&worker->node);
				worker->flags = WORKER_PREP;
			}
			worker->pool = pool;
			worker->id = id;
			worker->task = kthread_create_on_node_tlx(worker_thread_tlx, worker, pool->node,
										"kworker/%s", id_buf);
			set_user_nice_tlx(worker->task, pool->attrs->nice);
			worker->task->flags |= PF_NO_SETAFFINITY;
			list_add_tail(&worker->node, &pool->workers);
      worker->pool->nr_workers++;
			worker->flags |= WORKER_IDLE;
			pool->nr_idle++;
			worker->last_active = jiffies_tlx;
			list_add(&worker->entry, &pool->idle_list);
      wake_up_process_tlx(worker->task);
	}
		const char *fmt = "events";
		unsigned int flags = 0;
		int max_active = 0;
		size_t tbl_size = 0;
		va_list args;
		struct workqueue_struct *wq;
		struct pool_workqueue *pwq;
		wq = kzalloc_tlx(sizeof(*wq) + tbl_size, GFP_KERNEL);
		max_active = max_active ?: WQ_DFL_ACTIVE;
		wq->flags = flags;
		wq->saved_max_active = max_active;
		mutex_init(&wq->mutex);
		atomic_set(&wq->nr_pwqs_to_flush, 0);
		INIT_LIST_HEAD(&wq->pwqs);
		INIT_LIST_HEAD(&wq->flusher_queue);
		INIT_LIST_HEAD(&wq->flusher_overflow);
		INIT_LIST_HEAD(&wq->maydays);
		bool highpri = wq->flags & WQ_HIGHPRI;
		int cpu, ret;
		wq->cpu_pwqs = alloc_percpu(struct pool_workqueue);
		for_each_possible_cpu(cpu) {
				struct pool_workqueue *pwq =
					per_cpu_ptr(wq->cpu_pwqs, cpu);
				struct worker_pool *cpu_pools =
					per_cpu(cpu_worker_pools_tlx, cpu);
				memset_tlx(pwq, 0, sizeof(*pwq));
				pwq->pool = &cpu_pools[highpri];
				pwq->wq = wq;
				pwq->flush_color = -1;
				pwq->refcnt = 1;
				INIT_LIST_HEAD(&pwq->delayed_works);
				INIT_LIST_HEAD(&pwq->pwqs_node);
				INIT_LIST_HEAD(&pwq->mayday_node);
        (&pwq->unbound_release_work)->data = (atomic_long_t) ATOMIC_LONG_INIT(WORK_STRUCT_NO_POOL);
        INIT_LIST_HEAD(&(&pwq->unbound_release_work)->entry);
        (&pwq->unbound_release_work)->func = (pwq_unbound_release_workfn_tlx);
				mutex_lock_tlx(&wq->mutex);
				struct workqueue_struct *wq = pwq->wq;
				pwq->work_color = wq->work_color;
				bool freezable = wq->flags & WQ_FREEZABLE;
				if (!freezable || !workqueue_freezing_tlx) {
					pwq->max_active = wq->saved_max_active;
					while (!list_empty(&pwq->delayed_works) &&
								pwq->nr_active < pwq->max_active) {
									struct work_struct *work = list_first_entry(&pwq->delayed_works,
																struct work_struct, entry);

									struct list_head *head = &pwq->pool->worklist;
									struct work_struct *n;
									list_for_each_entry_safe_from(work, n, NULL, entry) {
										list_move_tail(&work->entry, head);
										if (!(*((unsigned long *)(&(work)->data)) & WORK_STRUCT_LINKED))
											break;
									}
								__clear_bit_tlx(WORK_STRUCT_DELAYED_BIT, ((unsigned long *)(&(work)->data)));
								pwq->nr_active++;
								}
					struct worker *worker = list_first_entry(&((struct worker_pool *)pwq)->idle_list, struct worker, entry);
					if (likely(worker))
							wake_up_process_tlx(worker->task);
				} else {
					pwq->max_active = 0;
				}
				list_add_rcu_tlx(&pwq->pwqs_node, &wq->pwqs);
				mutex_unlock_tlx(&wq->mutex);
			}
		system_wq_tlx = wq;

  bus_register_tlx(&platform_bus_type);
	struct srcu_struct *sp = &fsnotify_mark_srcu_tlx;
	sp->completed = 0;
	spin_lock_init(&sp->queue_lock);
	sp->running = false;
	(&sp->batch_queue)->head = NULL;
	(&sp->batch_check0)->head = NULL;
	(&sp->batch_check1)->head = NULL;
	(&sp->batch_done)->tail =	(&sp->batch_done)->head;
	(&sp->batch_queue)->tail = (&sp->batch_queue)->head;
	(&sp->batch_queue)->tail = (&sp->batch_check0)->head;
	(&sp->batch_check1)->tail = (&sp->batch_check1)->head;
	(&sp->batch_done)->tail = (&sp->batch_done)->head;
	sp->per_cpu_ref = alloc_percpu(struct srcu_struct_array_tlx);
	sock_inode_cachep_tlx = kmem_cache_create_tlx("sock_inode_cache",
	 	sizeof(struct socket_alloc), 0, (SLAB_HWCACHE_ALIGN | SLAB_RECLAIM_ACCOUNT |
							SLAB_MEM_SPREAD), init_once);
  sock_mnt_tlx = vfs_kern_mount_sk(&sock_fs_type, MS_KERNMOUNT, (&sock_fs_type)->name, NULL);
	bus_register_tlx(&amba_bustype_tlx);
	int i;
	vdso_pages_tlx = (&vdso_end_tlx - &vdso_start_tlx) >> PAGE_SHIFT;
	vdso_pagelist_tlx =  __kmalloc_tlx(vdso_pages_tlx + 1 * sizeof(struct page *),
				GFP_KERNEL | __GFP_ZERO);
	for (i = 0; i < vdso_pages_tlx; i++)
		vdso_pagelist_tlx[i] = virt_to_page(&vdso_start_tlx + i * PAGE_SIZE);
	vdso_pagelist_tlx[i] = virt_to_page(vdso_data_tlx);
	struct device_node *root = NULL;
	const struct of_device_id_tlx *matches = of_default_bus_match_table_tlx;
	const struct of_dev_auxdata *lookup = NULL;
	struct device *parent = NULL;
	struct device_node *child;
	int rc = 0;
	root = of_allnodes_tlx;
	for_each_child_of_node(root, child) {
		rc = of_platform_bus_create_tlx(child, matches, lookup, parent, true);
	}
#ifdef CONFIG_STACK_GROWSUP
	printk_tlx(KERN_INFO "Serial: AMBA PL011 UART driver UP \n");
#else
	printk_tlx(KERN_INFO "Serial: AMBA PL011 UART driver DOWN \n");
#endif
	(&pl011_driver_tlx)->drv.bus = &amba_bustype_tlx;
	(&pl011_driver_tlx)->drv.probe = amba_probe_tlx;
	driver_register_tlx(&((&pl011_driver_tlx)->drv));
	memset_tlx(&console_cdev_tlx, 0, sizeof (console_cdev_tlx));
	INIT_LIST_HEAD(&(console_cdev_tlx.list));
	kobject_init_tlx(&(console_cdev_tlx.kobj), &ktype_cdev_default_tlx);
	console_cdev_tlx.ops = &console_fops_tlx;
	(&console_cdev_tlx)->dev = MKDEV(TTYAUX_MAJOR, 1);
	(&console_cdev_tlx)->count = 1;
	struct kobj_map *domain = cdev_map_tlx;
	dev_t __dev = MKDEV(TTYAUX_MAJOR, 1);
	unsigned long range = 1;
	struct module *module = NULL;
	kobj_probe_t *probe = exact_match_tlx;
	int (*lock)(dev_t, void *) = exact_lock_tlx;
	void *data = (&console_cdev_tlx);
		unsigned n = MAJOR(__dev + range - 1) - MAJOR(__dev) + 1;
		unsigned index = MAJOR(__dev);
		struct probe *p;
		if (n > 255)
			n = 255;
		p = kmalloc_tlx(sizeof(struct probe) * n, GFP_KERNEL);
		for (i = 0; i < n; i++, p++) {
			p->owner = module;
			p->get = probe;
			p->lock = lock;
			p->dev = __dev;
			p->range = range;
			p->data = data;
		}
		mutex_lock_tlx(domain->lock);

		for (i = 0, p -= n; i < n; i++, p++, index++) {
			struct probe **s = &domain->probes[index % 255];
			while (*s && (*s)->range < range)
				s = &(*s)->next;
			p->next = *s;
			*s = p;
		}
		mutex_unlock_tlx(domain->lock);

	kobject_get_tlx((&console_cdev_tlx)->kobj.parent);
	register_chrdev_region_tlx(MKDEV(TTYAUX_MAJOR, 1), 1, "/dev/console");
		struct device *dev = NULL;
		int retval = -ENODEV;
		dev = kzalloc_tlx(sizeof(*dev), GFP_KERNEL);
		device_initialize_tlx(dev);
		dev->devt = MKDEV(TTYAUX_MAJOR, 1);
		dev->class = tty_class_tlx;
		dev->parent =  NULL;
		dev->groups = NULL;
		dev->release = device_create_release_tlx;
		dev->driver_data = NULL;
    (&dev->kobj)->name = "console";
		retval = device_add_tlx_tlx(dev);
		consdev_tlx = dev;

	char *buf = __initramfs_start;
	unsigned len =  __initramfs_size_tlx;
	int written, res;
	decompress_fn decompress;
	const char *compress_name;
	static __initdata char msg_buf[64];
	header_buf_tlx = kmalloc_tlx(110, GFP_KERNEL);
	symlink_buf_tlx = kmalloc_tlx(PATH_MAX + N_ALIGN(PATH_MAX) + 1, GFP_KERNEL);
	name_buf_tlx = kmalloc_tlx(N_ALIGN(PATH_MAX), GFP_KERNEL);
	state = Start;
	this_header_tlx = 0;
	message_tlx = NULL;
	while (!message_tlx && len) {
		loff_t saved_offset = this_header_tlx;
		if (*buf == '0' && !(this_header_tlx & 3)) {
			state = Start;
      count_ = len;
      victim = buf;
      while (!actions_tlx[state]());
			written = len - count_;
			buf += written;
			len -= written;
			continue;
		}
		if (!*buf) {
			buf++;
			len--;
			this_header_tlx++;
			continue;
		}
		this_header_tlx = 0;

		const struct compress_format *cf;
		for (cf = compressed_formats_tlx_tlx; cf->name; cf++) {
                 if (!memcmp_tlx(buf, cf->magic, 2))
                         break;
    }

		if (cf->magic[1]==0213) printk_tlx(KERN_ERR "=== 0213 \n");

		decompress = cf->decompressor;
		if (decompress) {
			res = decompress(buf, len, NULL, flush_buffer_tlx, NULL,
					&my_inpt_tlxr, error_tlx);
		}
		this_header_tlx = saved_offset + my_inpt_tlxr;
		buf += my_inpt_tlxr;
		len -= my_inpt_tlxr;
	}
	kfree_tlx(name_buf_tlx);
	kfree_tlx(symlink_buf_tlx);
	kfree_tlx(header_buf_tlx);
	sys_open((const char __user *) "/dev/console", O_RDWR, 0);
	int fd;
	struct files_struct *files = current->files;
	struct fdtable *fdt = (files)->fdt;
	struct file * file0 = fdt->fd[0];
	__set_bit_tlx(1, fdt->open_fds);
  __set_bit_tlx(2, fdt->open_fds);
  fdt->fd[1]  = 	file0;
	fdt->fd[2]  = 	file0;

	argv_init[0] = "ash";

	do_execve_tlx(getname_kernel_tlx("/bin/busybox"),
		(const char __user *const __user *)argv_init,
		(const char __user *const __user *)envp_init);

}




int child_wait_callback_tlx(wait_queue_t *wait, unsigned mode,
        int sync, void *key)
{
  struct wait_opts *wo = container_of(wait, struct wait_opts,
            child_wait);
  struct task_struct *p = key;
  if (! (wo->wo_type == PIDTYPE_MAX ||
    p->pids[wo->wo_type].pid == wo->wo_pid))
    return 0;
  if ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)
    return 0;
  return default_wake_function_tlx(wait, mode, sync, key);
}

int wait_noreap_copyout_tlx(struct wait_opts *wo, struct task_struct *p,
        pid_t pid, uid_t uid, int why, int status)
{
  struct siginfo __user *infop;
  int retval = wo->wo_rusage
    ? getrusage_tlx(p, RUSAGE_BOTH, wo->wo_rusage) : 0;

  put_task_struct_tlx(p);
  infop = wo->wo_info;
  if (infop) {
    if (!retval)
      retval = put_user(SIGCHLD, &infop->si_signo);
    if (!retval)
      retval = put_user(0, &infop->si_errno);
    if (!retval)
      retval = put_user((short)why, &infop->si_code);
    if (!retval)
      retval = put_user(pid, &infop->si_pid);
    if (!retval)
      retval = put_user(uid, &infop->si_uid);
    if (!retval)
      retval = put_user(status, &infop->si_status);
  }
  if (!retval)
    retval = pid;
  return retval;
}


u32 map_id_up_tlx(struct uid_gid_map *map, u32 id)
{
  unsigned idx, extents;
  u32 first, last;
  extents = map->nr_extents;
  smp_rmb();
  for (idx = 0; idx < extents; idx++) {
    first = map->extent[idx].lower_first;
    last = first + map->extent[idx].count - 1;
    if (id >= first && id <= last)
      break;
  }
  if (idx < extents)
    id = (id - first) + map->extent[idx].first;
  else
    id = (u32) -1;
  return id;
}


int wait_consider_task_tlx(struct wait_opts *wo, int ptrace,
        struct task_struct *p)
{
  int ret;
//  ret = eligible_child(wo, p);
  ret = 0;
  struct task_struct *task = p;
  if (wo->wo_type != PIDTYPE_PID)
                 task = task->group_leader;
  if (!wo->wo_type == PIDTYPE_MAX ||
                     task->pids[wo->wo_type].pid == wo->wo_pid);
    goto have_ret;
  if (((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))
             && !(wo->wo_flags & __WALL))
     goto have_ret;
  ret = 1;
have_ret:
  if (p->exit_state == EXIT_ZOMBIE) {
    if (!delay_group_leader(p)) {
      if (unlikely(ptrace) || likely(!p->ptrace)) {
            unsigned long state;
            int retval, status, traced;
            pid_t pid = task_pid_vnr_tlx(p);
            uid_t uid;
            struct user_namespace *targ = (&init_user_ns_tlx);
            kuid_t kuid = task_cred_xxx((p), uid);
             uid =  map_id_up_tlx(&targ->uid_map, kuid.val);
             if (uid == (uid_t) -1)
                 uid = overflowuid_tlx;
            struct siginfo __user *infop;
            if (!likely(wo->wo_flags & WEXITED))
              return 0;
            if (unlikely(wo->wo_flags & WNOWAIT)) {
              int exit_code = p->exit_code;
              int why;
              get_task_struct(p);
              read_unlock(&tasklist_lock_tlx);
              if ((exit_code & 0x7f) == 0) {
                why = CLD_EXITED;
                status = exit_code >> 8;
              } else {
                why = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;
                status = exit_code & 0x7f;
              }
              return wait_noreap_copyout_tlx(wo, p, pid, uid, why, status);
            }
            traced = 0;
            state = traced && thread_group_leader_tlx(p) ? EXIT_TRACE : EXIT_DEAD;
            if (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)
              return 0;
            if (likely(!traced) && thread_group_leader_tlx(p)) {
              struct signal_struct *psig;
              struct signal_struct *sig;
              unsigned long maxrss;
              cputime_t tgutime, tgstime;
  //            thread_group_cputime_adjust_tlxed(p, &tgutime, &tgstime);
              spin_lock_irq_tlx(&p->real_parent->sighand->siglock);
              psig = p->real_parent->signal;
              sig = p->signal;
              psig->cutime += tgutime + sig->cutime;
              psig->cstime += tgstime + sig->cstime;
              psig->cgtime += task_gtime_tlx(p) + sig->gtime + sig->cgtime;
              psig->cmin_flt +=
                p->min_flt + sig->min_flt + sig->cmin_flt;
              psig->cmaj_flt +=
                p->maj_flt + sig->maj_flt + sig->cmaj_flt;
              psig->cnvcsw +=
                p->nvcsw + sig->nvcsw + sig->cnvcsw;
              psig->cnivcsw +=
                p->nivcsw + sig->nivcsw + sig->cnivcsw;
              maxrss = max(sig->maxrss, sig->cmaxrss);
              if (psig->cmaxrss < maxrss)
                psig->cmaxrss = maxrss;
//              task_io_accounting_add(&psig->ioac, &p->ioac);
//              task_io_accounting_add(&psig->ioac, &sig->ioac);
              spin_unlock_irq_tlx(&p->real_parent->sighand->siglock);
            }
            read_unlock(&tasklist_lock_tlx);
            retval = wo->wo_rusage
              ? getrusage_tlx(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
            status = (p->signal->flags & SIGNAL_GROUP_EXIT)
              ? p->signal->group_exit_code : p->exit_code;
            if (!retval && wo->wo_stat)
              retval = put_user(status, wo->wo_stat);
            infop = wo->wo_info;
            if (!retval && infop)
              retval = put_user(SIGCHLD, &infop->si_signo);
            if (!retval && infop)
              retval = put_user(0, &infop->si_errno);
            if (!retval && infop) {
              int why;
              if ((status & 0x7f) == 0) {
                why = CLD_EXITED;
                status >>= 8;
              } else {
                why = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;
                status &= 0x7f;
              }
              retval = put_user((short)why, &infop->si_code);
              if (!retval)
                retval = put_user(status, &infop->si_status);
            }
            if (!retval && infop)
              retval = put_user(pid, &infop->si_pid);
            if (!retval && infop)
              retval = put_user(uid, &infop->si_uid);
            if (!retval)
              retval = pid;
            if (state == EXIT_TRACE) {
              write_lock_irq(&tasklist_lock_tlx);
              ptrace_unlink_tlx(p);
              state = EXIT_ZOMBIE;
//              if (do_notify_parent(p, p->exit_signal))
//                state = EXIT_DEAD;
              p->exit_state = state;
              write_unlock_irq(&tasklist_lock_tlx);
            }
            if (state == EXIT_DEAD)
//              release_task(p);
            return retval;
      }
    }
    if (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))
      wo->notask_error = 0;
  } else {
    wo->notask_error = 0;
  }
    struct siginfo __user *infop;
    int retval, exit_code, *p_code, why;
    uid_t uid = 0; /* unneeded, required by compiler */
    pid_t pid;
    retval = 0;
    if (!ptrace && !(wo->wo_flags & WUNTRACED))
      goto out_s;
//    if (!task_stopped_code(p, ptrace))
//      goto out_s;
//    struct task_struct *p, bool ptrace)
//    {
     bool rez = NULL;
      if (ptrace) {
        if (task_is_stopped_or_traced(p) &&
            !(p->jobctl & JOBCTL_LISTENING))
          rez = &p->exit_code;
      } else {
        if (p->signal->flags & SIGNAL_STOP_STOPPED)
          rez =  &p->signal->group_exit_code;
      }
    if (!rez)
      goto out_s;

    exit_code = 0;
    spin_lock_irq_tlx(&p->sighand->siglock);
    p_code = NULL;
    if (ptrace) {
      if (task_is_stopped_or_traced(p) &&
          !(p->jobctl & JOBCTL_LISTENING))
        p_code = &p->exit_code;
    } else {
      if (p->signal->flags & SIGNAL_STOP_STOPPED)
        p_code = &p->signal->group_exit_code;
    }
    if (unlikely(!p_code))
      goto unlock_sig;
    exit_code = *p_code;
    if (!exit_code)
      goto unlock_sig;
    if (!unlikely(wo->wo_flags & WNOWAIT))
      *p_code = 0;
//		uid = from_kuid_munged_tlx(current_user_ns(), task_uid(p));
    struct user_namespace *targ = (&init_user_ns_tlx);
    kuid_t kuid = task_cred_xxx((p), uid);
    uid =  map_id_up_tlx(&targ->uid_map, kuid.val);
    if (uid == (uid_t) -1)
        uid = overflowuid_tlx;

unlock_sig:
    spin_unlock_irq_tlx(&p->sighand->siglock);
    if (!exit_code)
      goto out_s;
    get_task_struct(p);
    pid = task_pid_vnr_tlx(p);
    why = ptrace ? CLD_TRAPPED : CLD_STOPPED;
    read_unlock(&tasklist_lock_tlx);
    if (unlikely(wo->wo_flags & WNOWAIT)) {
      retval = wait_noreap_copyout_tlx(wo, p, pid, uid, why, exit_code);
      goto out_s;
    }
    retval = wo->wo_rusage
      ? getrusage_tlx(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
    if (!retval && wo->wo_stat)
      retval = put_user((exit_code << 8) | 0x7f, wo->wo_stat);
    infop = wo->wo_info;
    if (!retval && infop)
      retval = put_user(SIGCHLD, &infop->si_signo);
    if (!retval && infop)
      retval = put_user(0, &infop->si_errno);
    if (!retval && infop)
      retval = put_user((short)why, &infop->si_code);
    if (!retval && infop)
      retval = put_user(exit_code, &infop->si_status);
    if (!retval && infop)
      retval = put_user(pid, &infop->si_pid);
    if (!retval && infop)
      retval = put_user(uid, &infop->si_uid);
    if (!retval)
      retval = pid;
    put_task_struct_tlx(p);
out_s:
    ret = retval;
  if (!ret) {
      int retval;
      pid_t pid;
      uid_t uid;
      if (!unlikely(wo->wo_flags & WCONTINUED))
        return 0;
      if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
        return 0;
      spin_lock_irq_tlx(&p->sighand->siglock);
      if (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {
        spin_unlock_irq_tlx(&p->sighand->siglock);
        return 0;
      }
      if (!unlikely(wo->wo_flags & WNOWAIT))
        p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
//			uid = from_kuid_munged_tlx(current_user_ns(), task_uid(p));
      struct user_namespace *targ = (&init_user_ns_tlx);
      kuid_t kuid = task_cred_xxx((p), uid);
      uid =  map_id_up_tlx(&targ->uid_map, kuid.val);
      if (uid == (uid_t) -1)
          uid = overflowuid_tlx;

      spin_unlock_irq_tlx(&p->sighand->siglock);
      pid = task_pid_vnr_tlx(p);
      get_task_struct(p);
      read_unlock(&tasklist_lock_tlx);
      if (!wo->wo_info) {
        retval = wo->wo_rusage
          ? getrusage_tlx(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
        put_task_struct_tlx(p);
        if (!retval && wo->wo_stat)
          retval = put_user(0xffff, wo->wo_stat);
        if (!retval)
          retval = pid;
      } else {
        retval = wait_noreap_copyout_tlx(wo, p, pid, uid,
                  CLD_CONTINUED, SIGCONT);
      }
      return retval;
  }
  return ret;
}


struct pid *find_pid_ns_tlx(int nr, struct pid_namespace *ns)
{
         struct upid *pnr;

         hlist_for_each_entry_rcu(pnr,
                       &pid_hash_tlx[pid_hash_tlxfn(nr, ns)], pid_chain)
               if (pnr->nr == nr && pnr->ns == ns)
                         return container_of(pnr, struct pid,
                                       numbers[ns->level]);

       return NULL;
}

SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
    int, options, struct rusage __user *, ru)
{
  struct wait_opts wo_;
  struct pid *pid = NULL;
  enum pid_type type;
  long ret;
  if (upid == -1)
    type = PIDTYPE_MAX;
  else if (upid < 0) {
    type = PIDTYPE_PGID;
		 pid = find_pid_ns_tlx(-upid,  current->pids[PIDTYPE_PID].pid->numbers[(current->pids[PIDTYPE_PID].pid)->level].ns);
		 if (pid)
                  atomic_inc_tlx(&pid->count);
//    pid = get_pid(find_vpid(-upid));
  } else if (upid == 0) {
    type = PIDTYPE_PGID;
    pid = get_task_pid_tlx(current, PIDTYPE_PGID);
  } else /* upid > 0 */ {
    type = PIDTYPE_PID;
//    pid = get_pid(find_vpid(upid));
		pid = find_pid_ns_tlx(upid,  current->pids[PIDTYPE_PID].pid->numbers[(current->pids[PIDTYPE_PID].pid)->level].ns);
		if (pid)
								atomic_inc_tlx(&pid->count);
  }

  wo_.wo_type	= type;
  wo_.wo_pid	= pid;
  wo_.wo_flags	= options | WEXITED;
  wo_.wo_info	= NULL;
  wo_.wo_stat	= stat_addr;
  wo_.wo_rusage	= ru;
  struct wait_opts *wo = &wo_;
    struct task_struct *tsk;
    int retval;
    init_waitqueue_func_entry_tlx(&wo->child_wait, child_wait_callback_tlx);
    wo->child_wait.private = current;
    add_wait_queue_tlx(&current->signal->wait_chldexit, &wo->child_wait);
repeat:
    wo->notask_error = -ECHILD;
    if ((wo->wo_type < PIDTYPE_MAX) &&
      (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))
      goto notask;

    set_current_state(TASK_INTERRUPTIBLE);
//    read_lock(&tasklist_lock_tlx);
		__raw_read_lock_tlx(&tasklist_lock_tlx);
    tsk = current;
    do {
      struct task_struct *p;
      list_for_each_entry(p, &tsk->children, sibling) {
        ret = wait_consider_task_tlx(wo, 0, p);
        if (ret)
              goto end;
      }
      retval = 0;
      if (retval)
        goto end;
      if (wo->wo_flags & __WNOTHREAD)
        break;
    } while_each_thread(current, tsk);
    read_unlock(&tasklist_lock_tlx);
notask:
    retval = wo->notask_error;
    if (!retval && !(wo->wo_flags & WNOHANG)) {
      retval = -ERESTARTSYS;
      if (!signal_pending_tlx(current)) {
        __schedule_tlx();
        goto repeat;
      }
    }
end:
    __set_current_state(TASK_RUNNING);
    remove_wait_queue_tlx(&current->signal->wait_chldexit, &wo->child_wait);
  put_pid_tlx(pid);
  return ret;
}

#define PROT_GROWSUP    0x02000000
#define PROT_GROWSDOWN  0x01000000





static pte_t *lock_pte_protection(struct vm_area_struct *vma, pmd_t *pmd,
       unsigned long addr, int prot_numa, spinlock_t **ptl)
 {
   pte_t *pte;
   spinlock_t *pmdl;

   /* !prot_numa is protected by mmap_sem held for write */
   if (!prot_numa)
     return pte_offset_map_lock(vma->vm_mm, pmd, addr, ptl);

   pmdl = pmd_lock_tlx(vma->vm_mm, pmd);
   pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, ptl);
   spin_unlock_tlx(pmdl);
   return pte;
 }

#define MAX_SWAPFILES_SHIFT     5
#define SWP_MIGRATION_NUM 2

#define SWP_HWPOISON_NUM 0
#define SWP_TYPE_SHIFT(e)       ((sizeof(e.val) * 8) - \
                         (MAX_SWAPFILES_SHIFT + RADIX_TREE_EXCEPTIONAL_SHIFT))

#define MAX_SWAPFILES \
         ((1 << MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)

#define SWP_MIGRATION_WRITE     (MAX_SWAPFILES + SWP_HWPOISON_NUM + 1)
#define SWP_MIGRATION_READ      (MAX_SWAPFILES + SWP_HWPOISON_NUM)

#define SWP_OFFSET_MASK(e)      ((1UL << SWP_TYPE_SHIFT(e)) - 1)


static inline pgoff_t swp_offset(swp_entry_t entry)
{
         return entry.val & SWP_OFFSET_MASK(entry);
}

static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)
{
         swp_entry_t ret;

         ret.val = (type << SWP_TYPE_SHIFT(ret)) |
                         (offset & SWP_OFFSET_MASK(ret));
          return ret;
 }

static inline unsigned swp_type(swp_entry_t entry)
{
         return (entry.val >> SWP_TYPE_SHIFT(entry));
}

static inline swp_entry_t pte_to_swp_entry(pte_t pte)
{
         swp_entry_t arch_entry;
         if (pte_swp_soft_dirty_tlx(pte))
                 pte = pte_swp_clear_soft_dirty_tlx(pte);
         arch_entry = __pte_to_swp_entry(pte);
         return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
}

static inline pte_t swp_entry_to_pte(swp_entry_t entry)
{
         swp_entry_t arch_entry;
         arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));
         return __swp_entry_to_pte(arch_entry);
 }

static inline void make_migration_entry_read(swp_entry_t *entry)
 {
         *entry = swp_entry(SWP_MIGRATION_READ, swp_offset(*entry));
 }

static inline int is_write_migration_entry(swp_entry_t entry)
 {
         return unlikely(swp_type(entry) == SWP_MIGRATION_WRITE);
 }

static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
    unsigned long addr, unsigned long end, pgprot_t newprot,
    int dirty_accountable, int prot_numa)
{
  struct mm_struct *mm = vma->vm_mm;
  pte_t *pte, oldpte;
  spinlock_t *ptl;
  unsigned long pages = 0;

  pte = lock_pte_protection(vma, pmd, addr, prot_numa, &ptl);
  if (!pte)
    return 0;
  do {
    oldpte = *pte;
    if (pte_present(oldpte)) {
      pte_t ptent;
      bool updated = false;
        ptent = ptep_modify_prot_start_tlx(mm, addr, pte);
        if (pte_numa_tlx(ptent))
          ptent = pte_mknonnuma_tlx(ptent);
        ptent = pte_modify_tlx(ptent, newprot);
        if (dirty_accountable && pte_dirty(ptent))
          ptent = pte_mkwrite_tlx(ptent);
        ptep_modify_prot_commit_tlx(mm, addr, pte, ptent);
        updated = true;
      if (updated)
        pages++;
    } else if (IS_ENABLED(CONFIG_MIGRATION) && !pte_file(oldpte)) {
      swp_entry_t entry = pte_to_swp_entry(oldpte);

      if (unlikely(swp_type(entry) == SWP_MIGRATION_WRITE)) {
        pte_t newpte;
        /*
         * A protection check is difficult so
         * just be safe and disable write
         */
        make_migration_entry_read(&entry);
        newpte = swp_entry_to_pte(entry);
        if (pte_swp_soft_dirty_tlx(oldpte))
          newpte = pte_swp_mksoft_dirty_tlx(newpte);
        set_pte_at_tlx(mm, addr, pte, newpte);

        pages++;
      }
    }
  } while (pte++, addr += PAGE_SIZE, addr != end);
  arch_leave_lazy_mmu_mode();
  pte_unmap_unlock(pte - 1, ptl);

  return pages;
}

static unsigned long change_pmd_range(struct vm_area_struct *vma,
    pud_t *pud, unsigned long addr, unsigned long end,
    pgprot_t newprot, int dirty_accountable, int prot_numa)
{
  pmd_t *pmd;
  struct mm_struct *mm = vma->vm_mm;
  unsigned long next;
  unsigned long pages = 0;
  unsigned long nr_huge_updates = 0;
  unsigned long mni_start = 0;

  pmd = pmd_offset_tlx(pud, addr);
  do {
    unsigned long this_pages;
    next = pmd_addr_end(addr, end);
      mni_start = addr;
    this_pages = change_pte_range(vma, pmd, addr, next, newprot,
         dirty_accountable, prot_numa);
    pages += this_pages;
  } while (pmd++, addr = next, addr != end);
  return pages;
}

void anon_vma_interval_tree_insert_tlx(struct anon_vma_chain *node, struct rb_root *root);

int __split_vma_tlx(struct mm_struct * mm, struct vm_area_struct * vma,
        unsigned long addr, int new_below)
{
  struct vm_area_struct *new;
  int err = -ENOMEM;

  new = slab_alloc_tlx(vm_area_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
//  *new = *vma; //memcpy
	memcpy_tlx(new, vma, sizeof(*new));
  INIT_LIST_HEAD(&new->anon_vma_chain);
  if (new_below)
    new->vm_end = addr;
  else {
    new->vm_start = addr;
    new->vm_pgoff += ((addr - vma->vm_start) >> PAGE_SHIFT);
  }
//  err = vma_dup_policy(vma, new);
//  anon_vma_clone(new, vma);
  struct vm_area_struct *dst = vma;
  struct vm_area_struct *src = new;
//  {
    struct anon_vma_chain *avc, *pavc;
    struct anon_vma *root = NULL;
    list_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {
      struct anon_vma *anon_vma;
      avc = slab_alloc_tlx(anon_vma_chain_cachep_tlx, GFP_NOWAIT | __GFP_NOWARN | __GFP_ZERO, _RET_IP_);
      if (unlikely(!avc)) {
        avc = slab_alloc_tlx(anon_vma_chain_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
      }
      anon_vma = pavc->anon_vma;
        avc->vma = dst;
        avc->anon_vma = anon_vma;
        list_add(&avc->same_vma, &dst->anon_vma_chain);
        anon_vma_interval_tree_insert_tlx(avc, &anon_vma->rb_root);
    }

  if (new->vm_file)
    get_file_tlx(new->vm_file);
  if (new->vm_ops && new->vm_ops->open)
    new->vm_ops->open(new);
  if (new_below)
    err = vma_adjust_tlx(vma, addr, vma->vm_end, vma->vm_pgoff +
      ((addr - new->vm_start) >> PAGE_SHIFT), new);
  else
    err = vma_adjust_tlx(vma, vma->vm_start, addr, vma->vm_pgoff, new);

  return 0;
}


//unsigned long change_pud_range_tlx(struct vm_area_struct *vma,
//    pgd_t *pgd, unsigned long addr, unsigned long end,
//    pgprot_t newprot, int dirty_accountable, int prot_numa);


unsigned long change_pud_range_tlx(struct vm_area_struct *vma,
    pgd_t *pgd, unsigned long addr, unsigned long end,
    pgprot_t newprot, int dirty_accountable, int prot_numa)
{
  pud_t *pud;
  unsigned long next;
  unsigned long pages = 0;

  pud = pud_offset_tlx(pgd, addr);
  do {
    next = pud_addr_end(addr, end);
    if (pud_none_or_clear_bad_tlx(pud))
      continue;
    pages += change_pmd_range(vma, pud, addr, next, newprot,
         dirty_accountable, prot_numa);
  } while (pud++, addr = next, addr != end);

  return pages;
}

int
mprotect_fixup_tlx(struct vm_area_struct *vma, struct vm_area_struct **pprev,
  unsigned long start, unsigned long end, unsigned long newflags)
{
  struct mm_struct *mm = vma->vm_mm;
  unsigned long oldflags = vma->vm_flags;
  long nrpages = (end - start) >> PAGE_SHIFT;
  unsigned long charged = 0;
  pgoff_t pgoff;
  int error;
  int dirty_accountable = 0;
  if (newflags == oldflags) {
    *pprev = vma;
    return 0;
  }
  if (newflags & VM_WRITE) {
    if (!(oldflags & (VM_ACCOUNT|VM_WRITE|VM_HUGETLB|
            VM_SHARED|VM_NORESERVE))) {
      charged = nrpages;
      newflags |= VM_ACCOUNT;
    }
  }
  pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
  *pprev = vma_merge_tlx(mm, *pprev, start, end, newflags,
      vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma));
  if (*pprev) {
    vma = *pprev;
    goto success;
  }
  *pprev = vma;
  if (start != vma->vm_start) {
    error = __split_vma_tlx(mm, vma, start, 1);
  }
  if (end != vma->vm_end) {
     error = __split_vma_tlx(mm, vma, end, 0);
//		error = split_vma(mm, vma, end, 0);
  }
success:
  vma->vm_flags = newflags;
  vma->vm_page_prot = vma->vm_page_prot;
//	,
//					  vm_get_page_prot_tlx(newflags));
      unsigned long addr = start;
      pgprot_t newprot = vma->vm_page_prot;
      int prot_numa = 0;
    mm = vma->vm_mm;
    pgd_t *pgd;
    unsigned long next;
    start = addr;
    unsigned long pages = 0;
    pgd = pgd_offset(mm, addr);
//    flush_cache_range(vma, addr, end);
     if (vma->vm_flags & VM_EXEC) {
//                 __flush_icache_all_tlx();
         asm("ic ialluis");
         dsb(ish);
    }
    set_tlb_flush_pending_tlx(mm);
    do {
      next = pgd_addr_end(addr, end);
      if (!pgd_none_tlx(*pgd))
        continue;
      pages += change_pud_range_tlx(vma, pgd, addr, next, newprot,
          dirty_accountable, prot_numa);
    } while (pgd++, addr = next, addr != end);
    if (pages) {
  //    flush_tlb_range(vma, start, end);
         unsigned long asid = (unsigned long)ASID(vma->vm_mm) << 48;
         unsigned long addr;
         start = asid | (start >> 12);
         end = asid | (end >> 12);
         dsb(ishst);
         for (addr = start; addr < end; addr += 1 << (PAGE_SHIFT - 12))
                 asm("tlbi vae1is, %0" : : "r"(addr));
         dsb(ish);
    }
    clear_tlb_flush_pending_tlx(mm);
//		return pages;
  return 0;
}


#define _calc_vm_trans(x, bit1, bit2) \
((bit1) <= (bit2) ? ((x) & (bit1)) * ((bit2) / (bit1)) \
: ((x) & (bit1)) / ((bit1) / (bit2)))


SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
    unsigned long, prot)
{
  unsigned long vm_flags, nstart, end, tmp, reqprot;
  struct vm_area_struct *vma, *prev;
  int error = -EINVAL;
  const int grows = prot & (PROT_GROWSDOWN|PROT_GROWSUP);
  prot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);
  if (!len)
    return 0;
  len = PAGE_ALIGN(len);
  end = start + len;
  if (end <= start)
    return -ENOMEM;
  reqprot = prot;
  if ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))
    prot |= PROT_EXEC;
//  vm_flags = calc_vm_prot_bits(prot);


    vm_flags = _calc_vm_trans(prot, PROT_READ,  VM_READ ) |
                _calc_vm_trans(prot, PROT_WRITE, VM_WRITE) |
                _calc_vm_trans(prot, PROT_EXEC,  VM_EXEC);

  down_write_tlx(&current->mm->mmap_sem);
  vma = find_vma_tlx(current->mm, start);
  error = -ENOMEM;
  if (!vma)
    goto out;
  prev = vma->vm_prev;
  if (unlikely(grows & PROT_GROWSDOWN)) {
    if (vma->vm_start >= end)
      goto out;
    start = vma->vm_start;
    error = -EINVAL;
    if (!(vma->vm_flags & VM_GROWSDOWN))
      goto out;
  } else {
    if (vma->vm_start > start)
      goto out;
    if (unlikely(grows & PROT_GROWSUP)) {
      end = vma->vm_end;
      error = -EINVAL;
      if (!(vma->vm_flags & VM_GROWSUP))
        goto out;
    }
  }
  if (start > vma->vm_start)
    prev = vma;

  for (nstart = start ; ; ) {
    unsigned long newflags;
    newflags = vm_flags;
    newflags |= (vma->vm_flags & ~(VM_READ | VM_WRITE | VM_EXEC));
    tmp = vma->vm_end;
    if (tmp > end)
      tmp = end;
    error = mprotect_fixup_tlx(vma, &prev, nstart, tmp, newflags);
    if (error)
      goto out;
    nstart = tmp;
    if (nstart < prev->vm_end)
      nstart = prev->vm_end;
    if (nstart >= end)
      goto out;
    vma = prev->vm_next;
    if (!vma || vma->vm_start != nstart) {
      error = -ENOMEM;
      goto out;
    }
  }
out:
  up_write_tlx(&current->mm->mmap_sem);
  return error;
}



#define rb_parent(r)   ((struct rb_node *)((r)->__rb_parent_color & ~3))



int find_vma_links_tlx(struct mm_struct *mm, unsigned long addr,
		unsigned long end, struct vm_area_struct **pprev,
		struct rb_node ***rb_link, struct rb_node **rb_parent)
{
	struct rb_node **__rb_link, *__rb_parent, *rb_prev;

	__rb_link = &mm->mm_rb.rb_node;
	rb_prev = __rb_parent = NULL;

	while (*__rb_link) {
		struct vm_area_struct *vma_tmp;

		__rb_parent = *__rb_link;
		vma_tmp = rb_entry(__rb_parent, struct vm_area_struct, vm_rb);

		if (vma_tmp->vm_end > addr) {
			/* Fail if an existing vma overlaps the area */
			if (vma_tmp->vm_start < end)
				return -ENOMEM;
			__rb_link = &__rb_parent->rb_left;
		} else {
			rb_prev = __rb_parent;
			__rb_link = &__rb_parent->rb_right;
		}
	}

	*pprev = NULL;
	if (rb_prev)
		*pprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);
	*rb_link = __rb_link;
	*rb_parent = __rb_parent;
	return 0;
}

#define USER_PGTABLES_CEILING   0UL
#define FIRST_USER_ADDRESS      0
#define MMU_GATHER_BUNDLE       8






static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
}

static inline void rb_set_parent_color(struct rb_node *rb,
							struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
}


static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
			struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
		if (parent->rb_left == old)
			parent->rb_left = new;
		else
			parent->rb_right = new;
	} else
		root->rb_node = new;
}

static __always_inline struct rb_node *
__rb_erase_augmented(struct rb_node *node, struct rb_root *root,
				const struct rb_augment_callbacks *augment)
{
	struct rb_node *child = node->rb_right, *tmp = node->rb_left;
	struct rb_node *parent, *rebalance;
	unsigned long pc;

	if (!tmp) {
		pc = node->__rb_parent_color;
		parent = __rb_parent(pc);
		__rb_change_child(node, child, parent, root);
		if (child) {
			child->__rb_parent_color = pc;
			rebalance = NULL;
		} else
			rebalance = __rb_is_black(pc) ? parent : NULL;
		tmp = parent;
	} else if (!child) {
		/* Still case 1, but this time the child is node->rb_left */
		tmp->__rb_parent_color = pc = node->__rb_parent_color;
		parent = __rb_parent(pc);
		__rb_change_child(node, tmp, parent, root);
		rebalance = NULL;
		tmp = parent;
	} else {
		struct rb_node *successor = child, *child2;
		tmp = child->rb_left;
		if (!tmp) {
			parent = successor;
			child2 = successor->rb_right;
			augment->copy(node, successor);
		} else {
			do {
				parent = successor;
				successor = tmp;
				tmp = tmp->rb_left;
			} while (tmp);
			parent->rb_left = child2 = successor->rb_right;
			successor->rb_right = child;
			rb_set_parent(child, successor);
			augment->copy(node, successor);
			augment->propagate(parent, successor);
		}

		successor->rb_left = tmp = node->rb_left;
		rb_set_parent(tmp, successor);

		pc = node->__rb_parent_color;
		tmp = __rb_parent(pc);
		__rb_change_child(node, successor, tmp, root);
		if (child2) {
			successor->__rb_parent_color = pc;
			rb_set_parent_color(child2, parent, RB_BLACK);
			rebalance = NULL;
		} else {
			unsigned long pc2 = successor->__rb_parent_color;
			successor->__rb_parent_color = pc;
			rebalance = __rb_is_black(pc2) ? parent : NULL;
		}
		tmp = successor;
	}

	augment->propagate(tmp, NULL);
	return rebalance;
}

static inline void dummy_propagate(struct rb_node *node, struct rb_node *stop) {}
static inline void dummy_copy(struct rb_node *old, struct rb_node *new) {}
static inline void dummy_rotate(struct rb_node *old, struct rb_node *new) {}

static const struct rb_augment_callbacks dummy_callbacks = {
	dummy_propagate, dummy_copy, dummy_rotate
};



void
__rb_rotate_set_parents(struct rb_node *old, struct rb_node *new,
			struct rb_root *root, int color)
{
	struct rb_node *parent = rb_parent(old);
	new->__rb_parent_color = old->__rb_parent_color;
	rb_set_parent_color(old, new, color);
	__rb_change_child(old, new, parent, root);
}

void rb_set_black(struct rb_node *rb)
{
	rb->__rb_parent_color |= RB_BLACK;
}

void
____rb_erase_color_tlx(struct rb_node *parent, struct rb_root *root,
	void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
	struct rb_node *node = NULL, *sibling, *tmp1, *tmp2;

	while (true) {
		/*
		* Loop invariants:
		* - node is black (or NULL on first iteration)
		* - node is not the root (parent is not NULL)
		* - All leaf paths going through parent and node have a
		*   black node count that is 1 lower than other leaf paths.
		*/
		sibling = parent->rb_right;
		if (node != sibling) {	/* node == parent->rb_left */
			if (rb_is_red(sibling)) {
				/*
				* Case 1 - left rotate at parent
				*
				*     P               S
				*    / \             / \
				*   N   s    -->    p   Sr
				*      / \         / \
				*     Sl  Sr      N   Sl
				*/
				parent->rb_right = tmp1 = sibling->rb_left;
				sibling->rb_left = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_right;
			if (!tmp1 || rb_is_black(tmp1)) {
				tmp2 = sibling->rb_left;
				if (!tmp2 || rb_is_black(tmp2)) {
					/*
					* Case 2 - sibling color flip
					* (p could be either color here)
					*
					*    (p)           (p)
					*    / \           / \
					*   N   S    -->  N   s
					*      / \           / \
					*     Sl  Sr        Sl  Sr
					*
					* This leaves us violating 5) which
					* can be fixed by flipping p to black
					* if it was red, or by recursing at p.
					* p is red when coming from Case 1.
					*/
					rb_set_parent_color(sibling, parent,
									RB_RED);
					if (rb_is_red(parent))
						rb_set_black(parent);
					else {
						node = parent;
						parent = rb_parent(node);
						if (parent)
							continue;
					}
					break;
				}
				/*
				* Case 3 - right rotate at sibling
				* (p could be either color here)
				*
				*   (p)           (p)
				*   / \           / \
				*  N   S    -->  N   Sl
				*     / \             \
				*    sl  Sr            s
				*                       \
				*                        Sr
				*/
				sibling->rb_left = tmp1 = tmp2->rb_right;
				tmp2->rb_right = sibling;
				parent->rb_right = tmp2;
				if (tmp1)
					rb_set_parent_color(tmp1, sibling,
									RB_BLACK);
				augment_rotate(sibling, tmp2);
				tmp1 = sibling;
				sibling = tmp2;
			}
			/*
			* Case 4 - left rotate at parent + color flips
			* (p and sl could be either color here.
			*  After rotation, p becomes black, s acquires
			*  p's color, and sl keeps its color)
			*
			*      (p)             (s)
			*      / \             / \
			*     N   S     -->   P   Sr
			*        / \         / \
			*      (sl) sr      N  (sl)
			*/
			parent->rb_right = tmp2 = sibling->rb_left;
			sibling->rb_left = parent;
			rb_set_parent_color(tmp1, sibling, RB_BLACK);
			if (tmp2)
				rb_set_parent(tmp2, parent);
			__rb_rotate_set_parents(parent, sibling, root,
						RB_BLACK);
			augment_rotate(parent, sibling);
			break;
		} else {
			sibling = parent->rb_left;
			if (rb_is_red(sibling)) {
				/* Case 1 - right rotate at parent */
				parent->rb_left = tmp1 = sibling->rb_right;
				sibling->rb_right = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_left;
			if (!tmp1 || rb_is_black(tmp1)) {
				tmp2 = sibling->rb_right;
				if (!tmp2 || rb_is_black(tmp2)) {
					/* Case 2 - sibling color flip */
					rb_set_parent_color(sibling, parent,
									RB_RED);
					if (rb_is_red(parent))
						rb_set_black(parent);
					else {
						node = parent;
						parent = rb_parent(node);
						if (parent)
							continue;
					}
					break;
				}
				/* Case 3 - right rotate at sibling */
				sibling->rb_right = tmp1 = tmp2->rb_left;
				tmp2->rb_left = sibling;
				parent->rb_left = tmp2;
				if (tmp1)
					rb_set_parent_color(tmp1, sibling,
									RB_BLACK);
				augment_rotate(sibling, tmp2);
				tmp1 = sibling;
				sibling = tmp2;
			}
			/* Case 4 - left rotate at parent + color flips */
			parent->rb_left = tmp2 = sibling->rb_right;
			sibling->rb_right = parent;
			rb_set_parent_color(tmp1, sibling, RB_BLACK);
			if (tmp2)
				rb_set_parent(tmp2, parent);
			__rb_rotate_set_parents(parent, sibling, root,
						RB_BLACK);
			augment_rotate(parent, sibling);
			break;
		}
	}
}

void rb_erase_tlx(struct rb_node *node, struct rb_root *root)
{
	struct rb_node *rebalance;
	rebalance = __rb_erase_augmented(node, root, &dummy_callbacks);
	if (rebalance)
		____rb_erase_color_tlx(rebalance, root, dummy_rotate);
}


static void
rb_erase_augmented(struct rb_node *node, struct rb_root *root,
										const struct rb_augment_callbacks *augment)
{
				struct rb_node *rebalance = __rb_erase_augmented(node, root, augment);
				if (rebalance)
							____rb_erase_color_tlx(rebalance, root, augment->rotate);
}



void                                                      \
vma_gap_callbacks_propagate(struct rb_node *rb, struct rb_node *stop)          \
 {                                                                       \
         while (rb != stop) {                                            \
                 struct vm_area_struct *node = rb_entry(rb, struct vm_area_struct, vm_rb);       \
                 unsigned long augmented = vma_compute_subtree_gap(node);                     \
                 if (node->rb_subtree_gap == augmented)                     \
                         break;                                          \
                 node->rb_subtree_gap = augmented;                          \
                 rb = rb_parent(&node->vm_rb);                         \
         }                                                               \
 }


void
	detach_vmas_to_be_unmapped_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
	struct vm_area_struct *prev, unsigned long end)
{
		struct vm_area_struct **insertion_point;
		struct vm_area_struct *tail_vma = NULL;

		insertion_point = (prev ? &prev->vm_next : &mm->mmap);
		vma->vm_prev = NULL;
		do {
//			vma_rb_erase_tlx(vma, &mm->mm_rb);
			struct rb_root *root  = &mm->mm_rb;
//			validate_mm_rb(root, vma);
      rb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks_tlx);
			mm->map_count--;
			tail_vma = vma;
			vma = vma->vm_next;
		} while (vma && vma->vm_start < end);
		*insertion_point = vma;
		if (vma) {
			vma->vm_prev = prev;
//			vma_gap_update(vma);
				vma_gap_callbacks_propagate(&vma->vm_rb, NULL);
		} else
			mm->highest_vm_end = prev ? prev->vm_end : 0;
		tail_vma->vm_next = NULL;

		/* Kill the cache */
//		vmacache_invalidate(mm);
		mm->vmacache_seqnum++;
     /* deal with overflows */
//    if (unlikely(mm->vmacache_seqnum == 0))
//                 vmacache_flush_all(mm);
}





struct mmu_gather_batch {
         struct mmu_gather_batch *next;
         unsigned int            nr;
         unsigned int            max;
         struct page             *pages[0];
 };


struct mmu_gather {
         struct mm_struct        *mm;
 #ifdef CONFIG_HAVE_RCU_TABLE_FREE
         struct mmu_table_batch  *batch;
 #endif
         unsigned long           start;
         unsigned long           end;
         unsigned int            need_flush : 1, /* Did free PTEs */
         /* we are in the middle of an operation to clear
          * a full mm and can make some optimizations */
                                 fullmm : 1,
         /* we have performed an operation which
          * requires a complete flush of the tlb */
                                 need_flush_all : 1;

         struct mmu_gather_batch *active;
         struct mmu_gather_batch local;
         struct page             *__pages[MMU_GATHER_BUNDLE];
         unsigned int            batch_count;
 };
static inline void flush_tlb_mm_tlx(struct mm_struct *mm)
 {
         unsigned long asid = (unsigned long)ASID(mm) << 48;

         dsb(ishst);
         asm("tlbi       aside1is, %0" : : "r" (asid));
         dsb(ish);
}

static inline void flush_tlb_range_tlx(struct vm_area_struct *vma,
                                         unsigned long start, unsigned long end)
 {
         unsigned long asid = (unsigned long)ASID(vma->vm_mm) << 48;
         unsigned long addr;
         start = asid | (start >> 12);
         end = asid | (end >> 12);

         dsb(ishst);
         for (addr = start; addr < end; addr += 1 << (PAGE_SHIFT - 12))
                 asm("tlbi vae1is, %0" : : "r"(addr));
         dsb(ish);
 }

static inline void tlb_flush_tlx(struct mmu_gather *tlb)
{
				if (tlb->fullmm) {
								flush_tlb_mm_tlx(tlb->mm);
				} else if (tlb->end > 0) {
								struct vm_area_struct vma = { .vm_mm = tlb->mm, };
								flush_tlb_range_tlx(&vma, tlb->start, tlb->end);
								tlb->start = TASK_SIZE;
								tlb->end = 0;
				}
	}






static inline void pgd_clear(pgd_t *pgd)        { }

static inline void tlb_add_flush(struct mmu_gather *tlb, unsigned long addr)
{
         if (!tlb->fullmm) {
                 tlb->start = min(tlb->start, addr);
                 tlb->end = max(tlb->end, addr + PAGE_SIZE);
         }
}


static inline void pgtable_page_dtor(struct page *page)
 {
//         pte_lock_deinit(page);
					 page->mapping = NULL;
//         	 kmem_cache_free_tlx(page_ptl_cachep_tlx, page->ptl);
//					 ptlock_free(page);
//         dec_zone_page_state(page, NR_PAGETABLE);
 }



void tlb_flush_mmu_tlx(struct mmu_gather *tlb)
 {
         if (!tlb->need_flush)
                 return;
				 tlb->need_flush = 0;
				 tlb_flush_tlx(tlb);
				 struct mmu_gather_batch *batch;

					for (batch = &tlb->local; batch; batch = batch->next) {
//									free_pages_and_swap_cache(batch->pages, batch->nr);
									batch->nr = 0;
					}
				tlb->active = &tlb->local;
 }

#define MAX_GATHER_BATCH        \
          ((PAGE_SIZE - sizeof(struct mmu_gather_batch)) / sizeof(void *))
#define MAX_GATHER_BATCH_COUNT  (10000UL/MAX_GATHER_BATCH)


static int tlb_next_batch_tlx(struct mmu_gather *tlb)
{
	struct mmu_gather_batch *batch;

	batch = tlb->active;
	if (batch->next) {
		tlb->active = batch->next;
		return 1;
	}

	if (tlb->batch_count == MAX_GATHER_BATCH_COUNT)
		return 0;

	batch = (void *)(unsigned long) page_address(alloc_pages(GFP_NOWAIT | __GFP_NOWARN, 0));
	if (!batch)
		return 0;

	tlb->batch_count++;
	batch->next = NULL;
	batch->nr   = 0;
	batch->max  = MAX_GATHER_BATCH;

	tlb->active->next = batch;
	tlb->active = batch;

	return 1;
}

int __tlb_remove_page_tlx(struct mmu_gather *tlb, struct page *page)
{
	struct mmu_gather_batch *batch;
	batch = tlb->active;
	batch->pages[batch->nr++] = page;
	if (batch->nr == batch->max) {
		if (!tlb_next_batch_tlx(tlb))
			return 0;
		batch = tlb->active;
	}
	return batch->max - batch->nr;
}


static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
{
				if (!__tlb_remove_page_tlx(tlb, page))
								tlb_flush_mmu_tlx(tlb);
}

static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte,
                                   unsigned long addr)
{
         pgtable_page_dtor(pte);
         tlb_add_flush(tlb, addr);
         tlb_remove_page(tlb, pte);
}

#define pud_free_tlb(tlb, pudp, address)                        \
				do {                                                    \
								tlb->need_flush = 1;                            \
				} while (0)
#define pmd_free_tlb(tlb, pudp, address)                        \
				do {                                                    \
								tlb->need_flush = 1;                            \
				} while (0)
#define pte_free_tlb(tlb, ptep, address)                        \
         do {                                                    \
                 tlb->need_flush = 1;                            \
                 __pte_free_tlb(tlb, ptep, address);             \
         } while (0)

static inline void pud_clear(pud_t *pudp)
 {
         set_pud_tlx(pudp, __pud(0));
 }

#define pmd_pgtable(pmd) pmd_page(pmd)



void free_pte_range_tlx(struct mmu_gather *tlb, pmd_t *pmd,
				unsigned long addr)
{
	pgtable_t token = pmd_pgtable(*pmd);
	pmd_clear(pmd);
	pte_free_tlb(tlb, token, addr);
	atomic_long_dec_tlx(&tlb->mm->nr_ptes);
}

void free_pmd_range_tlx(struct mmu_gather *tlb, pud_t *pud,
				unsigned long addr, unsigned long end,
				unsigned long floor, unsigned long ceiling)
{
	pmd_t *pmd;
	unsigned long next;
	unsigned long start;

	start = addr;
	pmd = pmd_offset_tlx(pud, addr);
	do {
		next = pmd_addr_end(addr, end);
		if (pmd_none_or_clear_bad_tlx(pmd))
			continue;
		free_pte_range_tlx(tlb, pmd, addr);
	} while (pmd++, addr = next, addr != end);

	start &= PUD_MASK;
	if (start < floor)
		return;
	if (ceiling) {
		ceiling &= PUD_MASK;
		if (!ceiling)
			return;
	}
	if (end - 1 > ceiling - 1)
		return;

	pmd = pmd_offset_tlx(pud, start);
	pud_clear(pud);
	pmd_free_tlb(tlb, pmd, start);
}


void free_pud_range_tlx(struct mmu_gather *tlb, pgd_t *pgd,
				unsigned long addr, unsigned long end,
				unsigned long floor, unsigned long ceiling)
{
	pud_t *pud;
	unsigned long next;
	unsigned long start;

	start = addr;
	pud = pud_offset(pgd, addr);
	do {
		next = pud_addr_end(addr, end);
		free_pmd_range_tlx(tlb, pud, addr, next, floor, ceiling);
	} while (pud++, addr = next, addr != end);

	start &= PGDIR_MASK;
	if (start < floor)
		return;
	if (ceiling) {
		ceiling &= PGDIR_MASK;
		if (!ceiling)
			return;
	}
	if (end - 1 > ceiling - 1)
		return;

	pud = pud_offset(pgd, start);
	pgd_clear(pgd);
	pud_free_tlb(tlb, pud, start);
}

void free_pgd_range_tlx(struct mmu_gather *tlb,
			unsigned long addr, unsigned long end,
			unsigned long floor, unsigned long ceiling)
{
	pgd_t *pgd;
	unsigned long next;
	addr &= PMD_MASK;
	if (addr < floor) {
		addr += PMD_SIZE;
		if (!addr)
			return;
	}
	if (ceiling) {
		ceiling &= PMD_MASK;
		if (!ceiling)
			return;
	}
	if (end - 1 > ceiling - 1)
		end -= PMD_SIZE;
	if (addr > end - 1)
		return;

	pgd = pgd_offset(tlb->mm, addr);
	do {
		next = pgd_addr_end(addr, end);
		free_pud_range_tlx(tlb, pgd, addr, next, floor, ceiling);
	} while (pgd++, addr = next, addr != end);
}



static inline unsigned long vma_last_pgoff(struct vm_area_struct *v)
 {
         return v->vm_pgoff + ((v->vm_end - v->vm_start) >> PAGE_SHIFT) - 1;
 }

static inline unsigned long vma_start_pgoff(struct vm_area_struct *v)
{
	return v->vm_pgoff;
}

static inline vma_interval_tree_compute_subtree_last(struct vm_area_struct *node)        \
 {                                                                             \
       unsigned long max = vma_last_pgoff(node), subtree_last;                              \
      if (node->shared.linear.rb.rb_left) {                                             \
                 subtree_last = rb_entry(node->shared.linear.rb.rb_left,                   \
                                         struct vm_area_struct, shared.linear.rb)->shared.linear.rb_subtree_last;           \
                 if (max < subtree_last)                                       \
                         max = subtree_last;                                   \
         }                                                                     \
         if (node->shared.linear.rb.rb_right) {                                            \
                 subtree_last = rb_entry(node->shared.linear.rb.rb_right,                  \
                                         struct vm_area_struct, shared.linear.rb)->shared.linear.rb_subtree_last;
                 if (max < subtree_last)                                       \
                         max = subtree_last;                                   \
         }                                                                     \
         return max;                                                           \
 }

static inline void                                                      \
vma_interval_tree_augment_propagate(struct rb_node *rb, struct rb_node *stop)          \
{                                                                       \
             while (rb != stop) {                                            \
                  struct vm_area_struct *node = rb_entry(rb, struct vm_area_struct, shared.linear.rb);       \
                  unsigned long augmented = vma_interval_tree_compute_subtree_last(node);                     \
                  if (node->shared.linear.rb_subtree_last == augmented)                     \
                          break;                                          \
                  node->shared.linear.rb_subtree_last = augmented;                          \
                  rb = rb_parent(&node->shared.linear.rb);                         \
          }
}                                                                       \
static inline void                                                      \
vma_interval_tree_augment_copy(struct rb_node *rb_old, struct rb_node *rb_new)         \
{                                                                       \
          struct vm_area_struct *old = rb_entry(rb_old, struct vm_area_struct, shared.linear.rb);            \
          struct vm_area_struct *new = rb_entry(rb_new, struct vm_area_struct, shared.linear.rb);            \
          new->shared.linear.rb_subtree_last = old->shared.linear.rb_subtree_last;
}                                                                       \
static void                                                             \
vma_interval_tree_augment_rotate(struct rb_node *rb_old, struct rb_node *rb_new)       \
{                                                                       \
         struct vm_area_struct *old = rb_entry(rb_old, struct vm_area_struct, shared.linear.rb);            \
         struct vm_area_struct *new = rb_entry(rb_new, struct vm_area_struct, shared.linear.rb);            \
         new->shared.linear.rb_subtree_last = old->shared.linear.rb_subtree_last;                            \
         old->shared.linear.rb_subtree_last = vma_interval_tree_compute_subtree_last(old);
}

const struct rb_augment_callbacks vma_interval_tree_augment_tlx = {
				vma_interval_tree_augment_propagate, vma_interval_tree_augment_copy, vma_interval_tree_augment_rotate
};


void vma_interval_tree_remove_tlx(struct vm_area_struct *node,
                               struct rb_root *root) {                                                                           \
       rb_erase_augmented(&node->shared.linear.rb, root, &vma_interval_tree_augment_tlx);         \
}

void vma_interval_tree_insert_tlx(struct vm_area_struct *node, struct rb_root *root)	      \
{									      \
	struct rb_node **link = &root->rb_node, *rb_parent = NULL;	      \
	unsigned long start = vma_start_pgoff(node), last = vma_last_pgoff(node);		      \
	struct vm_area_struct *parent;						      \
												\
	while (*link) {							      \
		rb_parent = *link;					      \
		parent = rb_entry(rb_parent, struct vm_area_struct, shared.linear.rb);		      \
		if (parent->shared.linear.rb_subtree_last < last)				      \
			parent->shared.linear.rb_subtree_last = last;			      \
		if (start < vma_start_pgoff(parent))				      \
			link = &parent->shared.linear.rb.rb_left;			      \
		else							      \
			link = &parent->shared.linear.rb.rb_right;			      \
	}								      \
												\
	node->shared.linear.rb_subtree_last = last;						      \
	rb_link_node_tlx(&node->shared.linear.rb, rb_parent, link);			      \
 	__rb_insert_tlx(&node->shared.linear.rb, root, vma_interval_tree_augment_tlx.rotate);

}


static inline int is_vm_hugetlb_page_tlx(struct vm_area_struct *vma)
{
         return !!(vma->vm_flags & VM_HUGETLB);
}


static inline unsigned long __anon_vma_interval_tree_augment_compute_subtree_last(struct anon_vma_chain *node)        \
{                                                                             \
				unsigned long max = ((struct vm_area_struct *)node)->vm_pgoff, subtree_last;                              \
				if (node->rb.rb_left) {                                             \
								subtree_last = rb_entry(node->rb.rb_left,                   \
																				struct anon_vma_chain, rb)->rb_subtree_last;           \
								if (max < subtree_last)                                       \
												max = subtree_last;                                   \
				}                                                                     \
				if (node->rb.rb_right) {                                            \
								subtree_last = rb_entry(node->rb.rb_right,                  \
																				struct anon_vma_chain, rb)->rb_subtree_last;           \
								if (max < subtree_last)                                       \
											max = subtree_last;                                   \
				}                                                                     \
				return max;
}


static inline void                                                      \
__anon_vma_interval_tree_augment_propagate(struct rb_node *rb, struct rb_node *stop)          \
{                                                                       \
				while (rb != stop) {                                            \
								struct anon_vma_chain *node = rb_entry(rb, struct anon_vma_chain, rb);       \
								unsigned long augmented = __anon_vma_interval_tree_augment_compute_subtree_last(node);                     \
							if (node->rb_subtree_last == augmented)                     \
												break;                                          \
								node->rb_subtree_last = augmented;                          \
								rb = rb_parent(&node->rb);                         \
				}                                                               \
}                                                                       \
static inline void                                                      \
__anon_vma_interval_tree_augment_copy(struct rb_node *rb_old, struct rb_node *rb_new)         \
{                                                                       \
				struct anon_vma_chain *old = rb_entry(rb_old, struct anon_vma_chain, rb);            \
			struct anon_vma_chain *new = rb_entry(rb_new, struct anon_vma_chain, rb);            \
				new->rb_subtree_last = old->rb_subtree_last;                            \
}                                                                       \
static void                                                             \
__anon_vma_interval_tree_augment_rotate(struct rb_node *rb_old, struct rb_node *rb_new)       \
{                                                                       \
				struct anon_vma_chain *old = rb_entry(rb_old, struct anon_vma_chain, rb);            \
			struct anon_vma_chain *new = rb_entry(rb_new, struct anon_vma_chain, rb);            \
				new->rb_subtree_last = old->rb_subtree_last;                            \
				old->rb_subtree_last = __anon_vma_interval_tree_augment_compute_subtree_last(old);                              \
}

const struct rb_augment_callbacks __anon_vma_interval_tree_augment_tlx = {
				__anon_vma_interval_tree_augment_propagate, __anon_vma_interval_tree_augment_copy, __anon_vma_interval_tree_augment_rotate
};

static inline unsigned long avc_start_pgoff(struct anon_vma_chain *avc)
{
       return vma_start_pgoff(avc->vma);
}

static inline unsigned long avc_last_pgoff(struct anon_vma_chain *avc)
{
         return vma_last_pgoff(avc->vma);
}

void anon_vma_interval_tree_insert_tlx(struct anon_vma_chain *node, struct rb_root *root)	      \
{									      \
	struct rb_node **link = &root->rb_node, *rb_parent = NULL;	      \
	unsigned long start = avc_start_pgoff(node), last = avc_last_pgoff(node);		      \
	struct anon_vma_chain *parent;						      \
												\
	while (*link) {							      \
		rb_parent = *link;					      \
		parent = rb_entry(rb_parent, struct anon_vma_chain, rb);		      \
		if (parent->rb_subtree_last < last)				      \
			parent->rb_subtree_last = last;			      \
		if (start < avc_start_pgoff(parent))				      \
			link = &parent->rb.rb_left;			      \
		else							      \
			link = &parent->rb.rb_right;			      \
	}								      \
												\
	node->rb_subtree_last = last;						      \
	rb_link_node_tlx(&node->rb, rb_parent, link);			      \
//	rb_insert_augmented(&node->rb, root, &__anon_vma_interval_tree_augment_tlx);	      \
	struct rb_node *node, struct rb_root *root,
   __rb_insert_tlx(&node->rb, root, __anon_vma_interval_tree_augment_tlx.rotate);
}



 void fput(struct file *file);


#define vma_policy(vma) NULL

static inline void mpol_put_tlx(struct mempolicy *p)
{
}

void unlink_anon_vmas_tlx(struct vm_area_struct *vma);

static inline void anon_vma_merge_tlx(struct vm_area_struct *vma,
																	struct vm_area_struct *next)
{
				unlink_anon_vmas_tlx(next);
}


int uprobe_mmap(struct vm_area_struct *vma) {};
void uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end) {};




void
__vma_unlink_tlx(struct mm_struct *mm, struct vm_area_struct *vma,
		struct vm_area_struct *prev)
{
	struct vm_area_struct *next;

//	vma_rb_erase_tlx(vma, &mm->mm_rb);
	struct rb_root *root  = &mm->mm_rb;
	rb_erase_augmented(&vma->vm_rb, root, &vma_gap_callbacks_tlx);
	prev->vm_next = next = vma->vm_next;
	if (next)
		next->vm_prev = prev;

	/* Kill the cache */
//	vmacache_invalidate(mm);
	mm->vmacache_seqnum++;
// if (unlikely(mm->vmacache_seqnum == 0))
//                 vmacache_flush_all(mm);

}

#define VM_NONLINEAR    0x00800000      /* Is non-linear (remap_file_pages) */

void __remove_shared_vm_struct_tlx(struct vm_area_struct *vma,
		struct file *file, struct address_space *mapping)
{
	if (vma->vm_flags & VM_DENYWRITE)
		atomic_inc_tlx(&file_inode_tlx(file)->i_writecount);
	if (vma->vm_flags & VM_SHARED)
		mapping->i_mmap_writable--;

//	flush_dcache_mmap_lock(mapping);
	if (unlikely(vma->vm_flags & VM_NONLINEAR))
		list_del_init(&vma->shared.nonlinear);
	else
		vma_interval_tree_remove_tlx(vma, &mapping->i_mmap);
//	flush_dcache_mmap_unlock(mapping);
}



void __insert_vm_struct_tlx(struct mm_struct *mm, struct vm_area_struct *vma)
{
	struct vm_area_struct *prev;
	struct rb_node **rb_link, *rb_parent;

//	if (
		find_vma_links_tlx(mm, vma->vm_start, vma->vm_end,
				&prev, &rb_link, &rb_parent);
//	__vma_link(mm, vma, prev, rb_link, rb_parent);
	__vma_link_list_tlx(mm, vma, prev, rb_parent);
	__vma_link_rb_tlx(mm, vma, rb_link, rb_parent);
	mm->map_count++;
}

void
anon_vma_interval_tree_post_update_vma_tlx(struct vm_area_struct *vma)
{
	struct anon_vma_chain *avc;

	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
		anon_vma_interval_tree_insert_tlx(avc, &avc->anon_vma->rb_root);
}

void
anon_vma_interval_tree_pre_update_vma_tlx(struct vm_area_struct *vma)
{
	struct anon_vma_chain *avc;

	list_for_each_entry(avc, &vma->anon_vma_chain, same_vma)
//		anon_vma_interval_tree_remove(avc, &avc->anon_vma->rb_root);
		rb_erase_augmented(&avc->rb, &avc->anon_vma->rb_root, &__anon_vma_interval_tree_augment_tlx);
}






int vma_adjust_tlx(struct vm_area_struct *vma, unsigned long start,
	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert)
{
	struct mm_struct *mm = vma->vm_mm;
	struct vm_area_struct *next = vma->vm_next;
	struct vm_area_struct *importer = NULL;
	struct address_space *mapping = NULL;
	struct rb_root *root = NULL;
	struct anon_vma *anon_vma = NULL;
	struct file *file = vma->vm_file;
	bool start_changed = false, end_changed = false;
	long adjust_next = 0;
	int remove_next = 0;

	if (next && !insert) {
		struct vm_area_struct *exporter = NULL;
		if (end >= next->vm_end) {
again:			remove_next = 1 + (end > next->vm_end);
			end = next->vm_end;
			exporter = next;
			importer = vma;
		} else if (end > next->vm_start) {
			adjust_next = (end - next->vm_start) >> PAGE_SHIFT;
			exporter = next;
			importer = vma;
		} else if (end < vma->vm_end) {
			adjust_next = - ((vma->vm_end - end) >> PAGE_SHIFT);
			exporter = vma;
			importer = next;
		}
	}

	if (file) {
		mapping = file->f_mapping;
		if (!(vma->vm_flags & VM_NONLINEAR)) {
			root = &mapping->i_mmap;
			uprobe_munmap(vma, vma->vm_start, vma->vm_end);

			if (adjust_next)
				uprobe_munmap(next, next->vm_start,
							next->vm_end);
		}

		mutex_lock_tlx(&mapping->i_mmap_mutex);
	}


	anon_vma = vma->anon_vma;
	if (!anon_vma && adjust_next)
		anon_vma = next->anon_vma;
	if (anon_vma) {
//		 down_write_tlx(&anon_vma->root->rwsem);
		anon_vma_interval_tree_pre_update_vma_tlx(vma);
		if (adjust_next)
			anon_vma_interval_tree_pre_update_vma_tlx(next);
	}

	if (root) {
//		flush_dcache_mmap_lock(mapping);
		vma_interval_tree_remove_tlx(vma, root);
		if (adjust_next)
			vma_interval_tree_remove_tlx(next, root);
	}

	if (start != vma->vm_start) {
		vma->vm_start = start;
		start_changed = true;
	}
	if (end != vma->vm_end) {
		vma->vm_end = end;
		end_changed = true;
	}
	vma->vm_pgoff = pgoff;
	if (adjust_next) {
		next->vm_start += adjust_next << PAGE_SHIFT;
		next->vm_pgoff += adjust_next;
	}

	if (root) {
		if (adjust_next)
			vma_interval_tree_insert_tlx(next, root);
		vma_interval_tree_insert_tlx(vma, root);
//		flush_dcache_mmap_unlock(mapping);
	}

	if (remove_next) {
		__vma_unlink_tlx(mm, next, vma);
		if (file)
			__remove_shared_vm_struct_tlx(next, file, mapping);
	} else if (insert) {
		__insert_vm_struct_tlx(mm, insert);
	} else {
		if (start_changed)
			vma_gap_callbacks_propagate(&vma->vm_rb, NULL);
		if (end_changed) {
			if (!next)
				mm->highest_vm_end = end;
			else if (!adjust_next)
				vma_gap_callbacks_propagate(&next->vm_rb, NULL);
		}
	}

	if (anon_vma) {
		anon_vma_interval_tree_post_update_vma_tlx(vma);
		if (adjust_next)
			anon_vma_interval_tree_post_update_vma_tlx(next);
//		anon_vma_unlock_write(anon_vma);
	}
	if (mapping)
		mutex_unlock_tlx(&mapping->i_mmap_mutex);

	if (root) {
		uprobe_mmap(vma);

		if (adjust_next)
			uprobe_mmap(next);
	}

	if (remove_next) {
		if (file) {
			uprobe_munmap(next, next->vm_start, next->vm_end);
			fput(file);
		}
		if (next->anon_vma)
			anon_vma_merge_tlx(vma, next);
		mm->map_count--;
		mpol_put_tlx(vma_policy(next));
		kmem_cache_free_tlx(vm_area_cachep_tlx, next);
		next = vma->vm_next;
		if (remove_next == 2)
			goto again;
		else if (next)
			vma_gap_callbacks_propagate(&next->vm_rb, NULL);
		else
			mm->highest_vm_end = end;
	}
	return 0;
}

#define PAGE_MAPPING_ANON       1
#define PAGE_MAPPING_KSM        2
#define PAGE_MAPPING_FLAGS      (PAGE_MAPPING_ANON | PAGE_MAPPING_KSM)

static inline void *page_rmapping_tlx(struct page *page)
{
         return (void *)((unsigned long)page->mapping & ~PAGE_MAPPING_FLAGS);
}

#define RB_EMPTY_ROOT(root)  ((root)->rb_node == NULL)

void unlink_anon_vmas_tlx(struct vm_area_struct *vma)
{
	struct anon_vma_chain *avc, *next;
	struct anon_vma *root = NULL;
	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
		struct anon_vma *anon_vma = avc->anon_vma;
		root = anon_vma->root;
//		anon_vma_interval_tree_remove(avc, &anon_vma->rb_root);
		rb_erase_augmented(&avc->rb, &anon_vma->rb_root, &__anon_vma_interval_tree_augment_tlx);
		if (RB_EMPTY_ROOT(&anon_vma->rb_root))
			continue;
		list_del(&avc->same_vma);
		kmem_cache_free_tlx(anon_vma_chain_cachep_tlx, avc);
	}
//	unlock_anon_vma_root(root);
	list_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {
		struct anon_vma *anon_vma = avc->anon_vma;
		if (atomic_dec_and_test(&anon_vma->refcount)) {
				struct anon_vma *root = anon_vma->root;
				might_sleep();
			kmem_cache_free_tlx(anon_vma_cachep_tlx, anon_vma);
				if (root != anon_vma && atomic_dec_and_test(&root->refcount)) {
						might_sleep();
						kmem_cache_free_tlx(anon_vma_cachep_tlx, root);
				}

		}
//	                 __put_anon_vma(anon_vma);
				page_rmapping_tlx(anon_vma);
		list_del(&avc->same_vma);
//		anon_vma_chain_free(avc);
		kmem_cache_free_tlx(anon_vma_chain_cachep_tlx, avc);
	}
}

void free_pgtables_tlx(struct mmu_gather *tlb, struct vm_area_struct *vma,
		unsigned long floor, unsigned long ceiling)
{
	while (vma) {
		struct vm_area_struct *next = vma->vm_next;
		unsigned long addr = vma->vm_start;
		unlink_anon_vmas_tlx(vma);
//		unlink_file_vma(vma);
		struct file *file = vma->vm_file;
		if (file) {
			struct address_space *mapping = file->f_mapping;
			mutex_lock_tlx(&mapping->i_mmap_mutex);
					if (vma->vm_flags & VM_DENYWRITE)
						atomic_inc_tlx(&(file->f_inode)->i_writecount);
					if (vma->vm_flags & VM_SHARED)
						mapping->i_mmap_writable--;
						vma_interval_tree_remove_tlx(vma, &mapping->i_mmap);

			mutex_unlock_tlx(&mapping->i_mmap_mutex);
		}
			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
						&& !is_vm_hugetlb_page_tlx(next)) {
				vma = next;
				next = vma->vm_next;
				unlink_anon_vmas_tlx(vma);
//				unlink_file_vma(vma);
						struct file *file = vma->vm_file;
						if (file) {
							struct address_space *mapping = file->f_mapping;
							mutex_lock_tlx(&mapping->i_mmap_mutex);
									if (vma->vm_flags & VM_DENYWRITE)
										atomic_inc_tlx(&(file->f_inode)->i_writecount);
									if (vma->vm_flags & VM_SHARED)
										mapping->i_mmap_writable--;
										vma_interval_tree_remove_tlx(vma, &mapping->i_mmap);

							mutex_unlock_tlx(&mapping->i_mmap_mutex);
						}
			}
			free_pgd_range_tlx(tlb, addr, vma->vm_end,
				floor, next? next->vm_start: ceiling);
		vma = next;
	}
}

int __split_vma_tlx(struct mm_struct * mm, struct vm_area_struct * vma,
				unsigned long addr, int new_below);

int do_munmap_tlx(struct mm_struct *mm, unsigned long start, size_t len)
{
	unsigned long end;
	struct vm_area_struct *vma, *prev, *last;
	vma = find_vma_tlx(mm, start);
	if (!vma)
		return 0;
	prev = vma->vm_prev;
	end = start + len;
	if (start > vma->vm_start) {
		__split_vma_tlx(mm, vma, start, 0);
		prev = vma;
	}
	last = find_vma_tlx(mm, end);
	if (last && end > last->vm_start) {
		__split_vma_tlx(mm, last, end, 1);
	}
	vma = prev? prev->vm_next: mm->mmap;
	detach_vmas_to_be_unmapped_tlx(mm, vma, prev, end);
//	unmap_region(mm, vma, prev, start, end);
//		struct mm_struct *mm,
//				struct vm_area_struct *vma, struct vm_area_struct *prev,
//				unsigned long start, unsigned long end)
//		{
			struct vm_area_struct *next = prev? prev->vm_next: mm->mmap;
			struct mmu_gather tlb;

//			lru_add_drain();
//			tlb_gather_mmu(&tlb, mm, start, end);
//			struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
//{
				 struct mmu_gather *tlb__ = &tlb;
         tlb__->mm = mm;

         /* Is it from 0 to ~0? */
         tlb__->fullmm     = !(start | (end+1));
         tlb__->need_flush_all = 0;
         tlb__->start      = start;
         tlb__->end        = end;
         tlb__->need_flush = 0;
         tlb__->local.next = NULL;
         tlb__->local.nr   = 0;
         tlb__->local.max  = ARRAY_SIZE(tlb__->__pages);
         tlb__->active     = &tlb__->local;
         tlb__->batch_count = 0;

 #ifdef CONFIG_HAVE_RCU_TABLE_FREE
         tlb->batch = NULL;
 #endif
			if (mm->hiwater_vm < mm->total_vm)
                 mm->hiwater_vm = mm->total_vm;
//			unmap_vmas(&tlb, vma, start, end);
			free_pgtables_tlx(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,
						next ? next->vm_start : USER_PGTABLES_CEILING);
//			tlb_finish_mmu(&tlb, start, end);
//		struct mmu_gather *tlb__ = &tlb;
//		, unsigned long start, unsigned long end
		struct mmu_gather_batch *batch, *next__;

//         tlb_flush_mmu(tlb__);
			if (tlb__->need_flush)
			{
//         tlb_flush_mmu_tlbonly(tlb__);
									tlb__->need_flush = 0;
				tlb_flush_tlx(tlb__);
					struct mmu_gather_batch *batch;
				for (batch = &tlb__->local; batch; batch = batch->next) {
//							free_pages_and_swap_cache(batch->pages, batch->nr);
							batch->nr = 0;
				}
				tlb__->active = &tlb__->local;
			}
//         check_pgt_cache();
				for (batch = tlb__->local.next; batch; batch = next__) {
							next__ = batch->next;
				free_pages_tlx((unsigned long)batch, 0);
	}
	tlb__->local.next = NULL;

		unsigned long nr_accounted = 0;
//		update_hiwater_vm(mm);
		if (mm->hiwater_vm < mm->total_vm)
							mm->hiwater_vm = mm->total_vm;
		do {
			long nrpages = vma_pages_tlx(vma);

			if (vma->vm_flags & VM_ACCOUNT)
				nr_accounted += nrpages;
				struct vm_area_struct *next = vma->vm_next;
				might_sleep();
				if (vma->vm_ops && vma->vm_ops->close)
					vma->vm_ops->close(vma);
				if (vma->vm_file)
					fput(vma->vm_file);
				kmem_cache_free_tlx(vm_area_cachep_tlx, vma);
				vma = next;

		} while (vma);

	return 0;
}

unsigned long mmap_region_tlx(struct file *file, unsigned long addr,
		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff)
{
	struct mm_struct *mm = current->mm;
	struct vm_area_struct *vma, *prev;
	int error;
	struct rb_node **rb_link, *rb_parent;
	unsigned long charged = 0;
//	count_vma_pages_range(mm, addr, addr + len);
munmap_back:
	if (find_vma_links_tlx(mm, addr, addr + len, &prev, &rb_link, &rb_parent)) {
		do_munmap_tlx(mm, addr, len);
		goto munmap_back;
	}

	vma = vma_merge_tlx(mm, prev, addr, addr + len, vm_flags, NULL, file, pgoff, NULL);
	vma = kmem_cache_zalloc_tlx(vm_area_cachep_tlx, GFP_KERNEL);
	vma->vm_mm = mm;
	vma->vm_start = addr;
	vma->vm_end = addr + len;
	vma->vm_flags = vm_flags;
	vma->vm_page_prot = vm_get_page_prot_tlx(vm_flags);
	vma->vm_pgoff = pgoff;
	INIT_LIST_HEAD(&vma->anon_vma_chain);

	if (file) {
//		vma->vm_file = get_file(file);
		atomic_long_inc_tlx(&file->f_count);
		vma->vm_file = file;
		error = file->f_op->mmap(file, vma);
		addr = vma->vm_start;
		vm_flags = vma->vm_flags;
	}
//	vma_link(mm, vma, prev, rb_link, rb_parent);
		struct address_space *mapping = NULL;
//		__vma_link(mm, vma, prev, rb_link, rb_parent);
//		__vma_link_list_tlx(mm, vma, prev, rb_parent);
			struct vm_area_struct *next;
			vma->vm_prev = prev;
			if (prev) {
				next = prev->vm_next;
				prev->vm_next = vma;
			} else {
				mm->mmap = vma;
				if (rb_parent)
					next = rb_entry(rb_parent,
							struct vm_area_struct, vm_rb);
				else
					next = NULL;
			}
			vma->vm_next = next;
			if (next)
				next->vm_prev = vma;
			if (vma->vm_next) {
				struct rb_node *rb = &(vma->vm_next)->vm_rb;
				struct rb_node *stop = NULL;
				while (rb != stop) {
						struct vm_area_struct *node = rb_entry(rb, struct vm_area_struct, vm_rb);
	//          unsigned long augmented = vma_compute_subtree_gap(node);
						struct vm_area_struct *vma = node;
	//					{
							unsigned long max, subtree_gap;
							max = vma->vm_start;
							if (vma->vm_prev)
								max -= vma->vm_prev->vm_end;
							if (vma->vm_rb.rb_left) {
								subtree_gap = rb_entry(vma->vm_rb.rb_left,
										struct vm_area_struct, vm_rb)->rb_subtree_gap;
								if (subtree_gap > max)
									max = subtree_gap;
							}
							if (vma->vm_rb.rb_right) {
								subtree_gap = rb_entry(vma->vm_rb.rb_right,
										struct vm_area_struct, vm_rb)->rb_subtree_gap;
								if (subtree_gap > max)
									max = subtree_gap;
							}
							unsigned long augmented = max;

						if (node->rb_subtree_gap == augmented)
												break;
						node->rb_subtree_gap = augmented;
						rb = rb_parent(&node->vm_rb);
				}
			}
//				vma_gap_callbacks_propagate(&(vma->vm_next)->vm_rb, NULL);
			else
				mm->highest_vm_end = vma->vm_end;
//			rb_link_node(&vma->vm_rb, rb_parent, rb_link);
//			struct rb_node * node, struct rb_node * parent,
// 80                                 struct rb_node ** rb_link)
// 81 {
      (&vma->vm_rb)->__rb_parent_color = (unsigned long)rb_parent;
      (&vma->vm_rb)->rb_left = (&vma->vm_rb)->rb_right = NULL;
      *rb_link = &vma->vm_rb;
			vma->rb_subtree_gap = 0;
			struct rb_node *rb = &vma->vm_rb;
			struct rb_node *stop = NULL;
			while (rb != stop) {
					struct vm_area_struct *node = rb_entry(rb, struct vm_area_struct, vm_rb);
//          unsigned long augmented = vma_compute_subtree_gap(node);
					struct vm_area_struct *vma = node;
//					{
						unsigned long max, subtree_gap;
						max = vma->vm_start;
						if (vma->vm_prev)
							max -= vma->vm_prev->vm_end;
						if (vma->vm_rb.rb_left) {
							subtree_gap = rb_entry(vma->vm_rb.rb_left,
									struct vm_area_struct, vm_rb)->rb_subtree_gap;
							if (subtree_gap > max)
								max = subtree_gap;
						}
						if (vma->vm_rb.rb_right) {
							subtree_gap = rb_entry(vma->vm_rb.rb_right,
									struct vm_area_struct, vm_rb)->rb_subtree_gap;
							if (subtree_gap > max)
								max = subtree_gap;
						}
						unsigned long augmented = max;

					if (node->rb_subtree_gap == augmented)
											break;
					node->rb_subtree_gap = augmented;
					rb = rb_parent(&node->vm_rb);
			}
//			vma_rb_insert(vma, &mm->mm_rb);
//			rb_insert_augmented(&vma->vm_rb, &mm->mm_rb, &vma_gap_callbacks);
			__rb_insert_tlx(&vma->vm_rb, &mm->mm_rb, vma_gap_callbacks_tlx.rotate);
		if (mapping)
			mutex_unlock_tlx(&mapping->i_mmap_mutex);
		mm->map_count++;

	/* Once vma denies write, undo our temporary denial count */
	if (vm_flags & VM_DENYWRITE)
			if (file)
								atomic_inc_tlx(&file_inode_tlx(file)->i_writecount);
//		allow_write_access(file);
	file = vma->vm_file;
out:
	vma->vm_flags |= VM_SOFTDIRTY;

	return addr;
}

SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,
    unsigned long, prot, unsigned long, flags,
    unsigned long, fd, unsigned long, pgoff)
{
  struct file *file = NULL;
  unsigned long retval = -EBADF;

  if (!(flags & MAP_ANONYMOUS)) {
      struct files_struct *files = current->files;
         struct fdtable *fdt = rcu_dereference_raw(files->fdt);
         if (fd < fdt->max_fds)
                 file = rcu_dereference_raw(fdt->fd[fd]);
         if (file) {
                 if ((file->f_mode & FMODE_PATH) ||
                     !atomic_long_inc_not_zero(&file->f_count))
                         file = NULL;
         }
  }
  flags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);
    unsigned long ret;
    struct mm_struct *mm = current->mm;
    unsigned long populate;
      down_write_tlx(&mm->mmap_sem);
        vm_flags_t vm_flags;
        len = PAGE_ALIGN(len);
          unsigned long (*get_area)(struct file *, unsigned long,
                  unsigned long, unsigned long, unsigned long);
          get_area = current->mm->get_unmapped_area;
          if (file && file->f_op->get_unmapped_area)
            get_area = file->f_op->get_unmapped_area;
          addr = get_area(file, addr, len, pgoff, flags);
          addr = addr;
          vm_flags = _calc_vm_trans(prot, PROT_READ,  VM_READ ) |
                      _calc_vm_trans(prot, PROT_WRITE, VM_WRITE) |
                      _calc_vm_trans(prot, PROT_EXEC,  VM_EXEC);

          vm_flags = vm_flags | _calc_vm_trans(flags, MAP_GROWSDOWN,  VM_GROWSDOWN ) |
                 _calc_vm_trans(flags, MAP_DENYWRITE,  VM_DENYWRITE ) |
                _calc_vm_trans(flags, MAP_LOCKED,     VM_LOCKED    ) |
                mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;


        addr = mmap_region_tlx(file, addr, len, vm_flags, pgoff);
      up_write_tlx(&mm->mmap_sem);
    retval = addr;
out_fput:
  if (file)
    fput(file);
out:
  return retval;
}


SYSCALL_DEFINE3(execve,
    const char __user *, filename,
    const char __user *const __user *, argv,
    const char __user *const __user *, envp)
{
  return do_execve_tlx(getname_tlx(filename), argv, envp);
}


SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
     int __user *, parent_tidptr,
     int, tls_val,
     int __user *, child_tidptr)
{

  return do_fork_tlx(clone_flags, newsp, 0, parent_tidptr, child_tidptr);
}

SYSCALL_DEFINE0(getuid)
{
  /* Only we change this so SMP safe */
//  return from_kuid_munged_tlx(current_user_ns(), current_uid());
    uid_t uid;
    struct user_namespace *targ = (&init_user_ns_tlx);
    kuid_t kuid = current_cred_xxx(uid);
    uid =  map_id_up_tlx(&targ->uid_map, kuid.val);
    if (uid == (uid_t) -1)
        uid = overflowuid_tlx;
}

SYSCALL_DEFINE0(getpid)
{
    struct pid *pid = current->group_leader->pids[PIDTYPE_PID].pid;
      struct pid_namespace *ns =
                      pid->numbers[current->pids[PIDTYPE_PID].pid->level].ns;
         struct upid *upid;
         pid_t nr = 0;

         if (pid && ns->level <= pid->level) {
                 upid = &pid->numbers[ns->level];
                 if (upid->ns == ns)
                         nr = upid->nr;
         }
         return nr;
}

SYSCALL_DEFINE1(umask, int, mask)
{
  mask = xchg(&current->fs->umask, mask & S_IRWXUGO);
  return mask;
}

#define __NEW_UTS_LEN 64

struct new_utsname {
         char sysname[__NEW_UTS_LEN + 1];
         char nodename[__NEW_UTS_LEN + 1];
         char release[__NEW_UTS_LEN + 1];
         char version[__NEW_UTS_LEN + 1];
         char machine[__NEW_UTS_LEN + 1];
         char domainname[__NEW_UTS_LEN + 1];
};

struct uts_namespace {
         struct kref kref;
         struct new_utsname name;
         struct user_namespace *user_ns;
         unsigned int proc_inum;
 };

#define UTS_SYSNAME "Linux"
#define UTS_NODENAME CONFIG_DEFAULT_HOSTNAME
#define UTS_RELEASE "3.16.0ajb"
#define UTS_MACHINE "arm64"
#define UTS_VERSION "#9452 SMP Wed Sep 9 10:11:27 EEST 2015"
#define UTS_DOMAINNAME "(none)" /* set by setdomainname() */

struct uts_namespace init_uts_ns_tlx = {
       .kref = {
               .refcount       = ATOMIC_INIT(2),
       },
			 .name = {
               .sysname        = UTS_SYSNAME,
               .nodename       = UTS_NODENAME,
               .release        = UTS_RELEASE,
               .version        = UTS_VERSION,
               .machine        = UTS_MACHINE,
               .domainname     = UTS_DOMAINNAME,
         },
       .user_ns = &init_user_ns_tlx,
       .proc_inum = PROC_UTS_INIT_INO,
};


SYSCALL_DEFINE1(newuname, struct new_utsname __user *, name)
{
    copy_to_user(name, &current->nsproxy->uts_ns->name, sizeof *name);
  return 0;
}

struct kmem_cache *sigqueue_cachep_tlx;

static int flush_sigqueue_mask(sigset_t *mask, struct sigpending *s)
{
  struct sigqueue *q, *n;
  sigset_t m;

  sigandsets_tlx(&m, mask, &s->signal);
  if (sigisemptyset_tlx(&m))
    return 0;

  sigandnsets_tlx(&s->signal, &s->signal, mask);
  list_for_each_entry_safe(q, n, &s->list, list) {
    if (sigismember_tlx(mask, q->info.si_signo)) {
      list_del_init(&q->list);
//      __sigqueue_free(q);
        if (q->flags & SIGQUEUE_PREALLOC)
          return 1;
        atomic_dec_tlx(&q->user->sigpending);
        free_uid_tlx(q->user);
        kmem_cache_free_tlx(sigqueue_cachep_tlx, q);
    }
  }
  return 1;
}

SYSCALL_DEFINE4(rt_sigaction, int, sig,
    const struct sigaction __user *, act,
    struct sigaction __user *, oact,
    size_t, sigsetsize)
{
  struct k_sigaction new_sa, old_sa;
  int ret = -EINVAL;
  if (sigsetsize != sizeof(sigset_t))
    goto out;
  if (act) {
    if (copy_from_user(&new_sa.sa, act, sizeof(new_sa.sa)))
      return -EFAULT;
  }
  struct k_sigaction *_act = act ? &new_sa : NULL;
  struct k_sigaction *_oact = oact ? &old_sa : NULL;
    struct task_struct *p = current, *t;
    struct k_sigaction *k;
    sigset_t mask;
    k = &p->sighand->action[sig-1];
    spin_lock_irq_tlx(&p->sighand->siglock);
    if (_oact)
      *_oact = *k;
    if (_act) {
      sigdelsetmask_tlx(&_act->sa.sa_mask,
              sigmask(SIGKILL) | sigmask(SIGSTOP));
      *k = *_act;
      void __user *handler = p->sighand->action[sig - 1].sa.sa_handler;//, sig)
      if (handler == SIG_IGN ||
        (handler == SIG_DFL && sig_kernel_ignore(sig))) {
        sigemptyset_tlx(&mask);
        sigaddset_tlx(&mask, sig);
        flush_sigqueue_mask(&mask, &p->signal->shared_pending);
        for_each_thread(p, t) {
          flush_sigqueue_mask(&mask, &t->pending);
        }
      }
    }
    spin_unlock_irq_tlx(&p->sighand->siglock);
  if (oact) {
    if (copy_to_user(oact, &old_sa.sa, sizeof(old_sa.sa)))
      return -EFAULT;
  }
out:
  return ret;
}

SYSCALL_DEFINE1(exit, int, error_code)
{
  do_exit_tlx((error_code&0xff)<<8);
}


SYSCALL_DEFINE1(fsync, unsigned int, fd)
{
  return 0;//do_fsync(fd, 0);
}

SYSCALL_DEFINE1(fdatasync, unsigned int, fd)
{
  return 0;//do_fsync(fd, 1);
}



enum wb_reason {
	WB_REASON_BACKGROUND,
	WB_REASON_TRY_TO_FREE_PAGES,
	WB_REASON_SYNC,
	WB_REASON_PERIODIC,
	WB_REASON_LAPTOP_TIMER,
	WB_REASON_FREE_MORE_MEM,
	WB_REASON_FS_FREE_SPACE,
	/*
	 * There is no bdi forker thread any more and works are done
	 * by emergency worker, however, this is TPs userland visible
	 * and we'll be exposing exactly the same information,
	 * so it has a mismatch name.
	 */
	WB_REASON_FORKER_THREAD,

	WB_REASON_MAX,
};


SYSCALL_DEFINE4(newfstatat, int, dfd, const char __user *, filename,
    struct stat __user *, statbuf, int, flag)
{
  struct kstat_tlx stat__;
  struct kstat_tlx *stat = &stat__;
  vfs_fstatat_tlx(dfd, filename, stat, flag);
    struct stat tmp;
    memset_tlx(&tmp, 0, sizeof(tmp));
    tmp.st_dev = new_encode_dev_tlx(stat->dev);
    tmp.st_ino = stat->ino;
    tmp.st_mode = stat->mode;
    tmp.st_nlink = stat->nlink;
    SET_UID(tmp.st_uid, from_kuid_munged_tlx(current_user_ns(), stat->uid));
    SET_GID(tmp.st_gid, from_kgid_munged_tlx(current_user_ns(), stat->gid));
    tmp.st_rdev = new_encode_dev_tlx(stat->rdev);
    tmp.st_size = stat->size;
    tmp.st_atime = stat->atime.tv_sec;
    tmp.st_mtime = stat->mtime.tv_sec;
    tmp.st_ctime = stat->ctime.tv_sec;
    tmp.st_blocks = stat->blocks;
    tmp.st_blksize = stat->blksize;
    return copy_to_user(statbuf,&tmp,sizeof(tmp)) ? -EFAULT : 0;
}

SYSCALL_DEFINE2(newfstat, unsigned int, fd, struct stat __user *, statbuf)
{
  return 0;
}


SYSCALL_DEFINE0(sync)
{
  return 0;
}
#define LOOKUP_REVAL            0x0020


SYSCALL_DEFINE4(readlinkat, int, dfd, const char __user *, pathname,
    char __user *, buf, int, bufsiz)
{
  return 0;
}


SYSCALL_DEFINE5(ppoll, struct pollfd __user *, ufds, unsigned int, nfds,
    struct timespec __user *, tsp, const sigset_t __user *, sigmask,
    size_t, sigsetsize)
{
  sigset_t ksigmask, sigsaved;
  struct timespec ts, end_time, *to = NULL;
  int ret = 0;
  if (tsp) {
      copy_from_user(&ts, tsp, sizeof(ts));
      to = &end_time;
      poll_select_set_timeout_tlx(to, ts.tv_sec, ts.tv_nsec);
  }
  if (sigmask) {
    copy_from_user(&ksigmask, sigmask, sizeof(ksigmask));
    sigdelsetmask_tlx(&ksigmask, sigmask(SIGKILL)|sigmask(SIGSTOP));
    sigprocmask_tlx(SIG_SETMASK, &ksigmask, &sigsaved);
  }
//  ret = do_sys_poll(ufds, nfds, to);
  sigprocmask_tlx(SIG_SETMASK, &sigsaved, NULL);
  return ret;
}



unsigned long __fdget_tlx_pos_tlx(unsigned int fd)
{
  unsigned long v = __fget_light_tlx(fd, FMODE_PATH);
  struct file *file = (struct file *)(v & ~3);

  if (file && (file->f_mode & FMODE_ATOMIC_POS)) {
    if (file_count(file) > 1) {
      v |= FDPUT_POS_UNLOCK;
      mutex_lock_tlx(&file->f_pos_lock);
    }
  }
  return v;
}

SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)
{
  struct fd f = __to_fd(__fdget_tlx_pos_tlx(fd));
  ssize_t ret = -EBADF;

  if (f.file) {
    loff_t pos = f.file->f_pos;
    ret = vfs_read_tlx(f.file, buf, count, &pos);
    if (ret >= 0)
      f.file->f_pos = pos;
    if (f.flags & FDPUT_POS_UNLOCK)
                 mutex_unlock_tlx(&f.file->f_pos_lock);
    fdput(f);
  }
  return ret;
}

SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf,
    size_t, count)
{
  struct fd f = __to_fd(__fdget_tlx_pos_tlx(fd));
  ssize_t ret = -EBADF;

  if (f.file) {
    loff_t pos = f.file->f_pos;
    ret = vfs_write_tlx(f.file, buf, count, &pos);
    if (ret >= 0)
      f.file->f_pos = pos;
//    fdput_pos(f);
    if (f.flags & FDPUT_POS_UNLOCK)
                 mutex_unlock_tlx(&f.file->f_pos_lock);
    fdput(f);
  }

  return ret;
}

int do_copy_tlx(void)
{
	if (count_ >= body_len) {
		sys_write(wfd, victim, body_len);
		__close_fd_tlx(current->files, wfd);
		kfree_tlx(vcollected);
		eat(body_len);
		state = SkipIt;
		return 0;
	} else {
		sys_write(wfd, victim, count_);
		body_len -= count_;
		eat(count_);
		return 1;
	}
}

struct linux_dirent64 {
       u64             d_ino;
       s64             d_off;
       unsigned short  d_reclen;
       unsigned char   d_type;
       char            d_name[0];
 };

struct getdents_callback64 {
         struct dir_context ctx;
         struct linux_dirent64 __user * current_dir;
         struct linux_dirent64 __user * previous;
         int count;
         int error;
 };


int filldir64_tlx(void * __buf, const char * name, int namlen, loff_t offset,
         u64 ino, unsigned int d_type)
{
  struct linux_dirent64 __user *dirent;
  struct getdents_callback64 * buf = (struct getdents_callback64 *) __buf;
  int reclen = ALIGN(offsetof(struct linux_dirent64, d_name) + namlen + 1,
    sizeof(u64));
  buf->error = -EINVAL;	/* only used if we fail.. */
  dirent = buf->previous;
  dirent = buf->current_dir;
    __put_user(ino, &dirent->d_ino);
    __put_user(0, &dirent->d_off);
    __put_user(reclen, &dirent->d_reclen);
    __put_user(d_type, &dirent->d_type);
    copy_to_user(dirent->d_name, name, namlen);
    __put_user(0, dirent->d_name + namlen);
  buf->previous = dirent;
  dirent = (void __user *)dirent + reclen;
  buf->current_dir = dirent;
  buf->count -= reclen;
  return 0;
}


SYSCALL_DEFINE3(getdents64, unsigned int, fd,
    struct linux_dirent64 __user *, dirent, unsigned int, count)
{
  struct fd f;
  struct linux_dirent64 __user * lastdirent;
  struct getdents_callback64 buf = {
    .ctx.actor = filldir64_tlx,
    .count = count,
    .current_dir = dirent
  };
  int error;
  f = __to_fd(__fdget_tlx_pos_tlx(fd));
  if (!f.file)
    return -EBADF;
  error = iterate_dir_tlx(f.file, &buf.ctx);
  if (error >= 0)
    error = buf.error;
  lastdirent = buf.previous;
  if (lastdirent) {
    typeof(lastdirent->d_off) d_off = buf.ctx.pos;
    if (__put_user(d_off, &lastdirent->d_off))
      error = -EFAULT;
    else
      error = count - buf.count;
  }
  fdput(f);
  return error;
}


SYSCALL_DEFINE4(openat, int, dfd, const char __user *, filename, int, flags,
    umode_t, mode)
{
  if (force_o_largefile())
    flags |= O_LARGEFILE;

  return do_sys_open_tlx(dfd, filename, flags, mode);
}

static inline int flock_translate_cmd(int cmd) {
  if (cmd & LOCK_MAND)
    return cmd & (LOCK_MAND | LOCK_RW);
  switch (cmd) {
  case LOCK_SH:
    return F_RDLCK;
  case LOCK_EX:
    return F_WRLCK;
  case LOCK_UN:
    return F_UNLCK;
  }
  return -EINVAL;
}

static int flock_make_lock(struct file *filp, struct file_lock **lock,
    unsigned int cmd)
{
  struct file_lock *fl;
  int type = flock_translate_cmd(cmd);
  if (type < 0)
    return type;

  fl = locks_alloc_lock_tlx();
  if (fl == NULL)
    return -ENOMEM;

  fl->fl_file = filp;
  fl->fl_owner = (fl_owner_t)filp;
  fl->fl_pid = current->tgid;
  fl->fl_flags = FL_FLOCK;
  fl->fl_type = type;
  fl->fl_end = OFFSET_MAX;

  *lock = fl;
  return 0;
}

SYSCALL_DEFINE2(flock, unsigned int, fd, unsigned int, cmd)
{
  struct fd f =__to_fd(__fdget_tlx_pos_tlx(fd));
  struct file_lock *lock;
  int can_sleep, unlock;
  int error = 0;
  can_sleep = !(cmd & LOCK_NB);
  cmd &= ~LOCK_NB;
  unlock = (cmd == LOCK_UN);
  if (!unlock && !(cmd & LOCK_MAND) &&
      !(f.file->f_mode & (FMODE_READ|FMODE_WRITE)))
    goto out_putf;
  error = flock_make_lock(f.file, &lock, cmd);
  if (error)
    goto out_putf;
  if (can_sleep)
    lock->fl_flags |= FL_SLEEP;
  if (f.file->f_op->flock)
    error = f.file->f_op->flock(f.file,
            (can_sleep) ? F_SETLKW : F_SETLK,
            lock);
  else
    error = flock_lock_file_wait_tlx(f.file, lock);

 out_free:
  locks_free_lock_tlx(lock);
 out_putf:
  fdput(f);
 out:
  return error;
}


SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)
{
  int error;
  struct fd f = __to_fd(__fdget_tlx_pos_tlx(fd));
  error = do_vfs_ioctl_tlx(f.file, fd, cmd, arg);
  fdput(f);
  return error;
}


int inotify_max_queued_events_tlx_tlx ;
int inotify_max_user_instances_tlx ;


struct fsnotify_group {
	/*
	 * How the refcnt is used is up to each group.  When the refcnt hits 0
	 * fsnotify will clean up all of the resources associated with this group.
	 * As an example, the dnotify group will always have a refcnt=1 and that
	 * will never change.  Inotify, on the other hand, has a group per
	 * inotify_init() and the refcnt will hit 0 only when that fd has been
	 * closed.
	 */
	atomic_t refcnt;		/* things with interest in this group */

	const struct fsnotify_ops *ops;	/* how this group handles things */

	/* needed to send notification to userspace */
	struct mutex notification_mutex;	/* protect the notification_list */
	struct list_head notification_list;	/* list of event_holder this group needs to send to userspace */
	wait_queue_head_t notification_waitq;	/* read() on the notification file blocks on this waitq */
	unsigned int q_len;			/* events on the queue */
	unsigned int max_events;		/* maximum events allowed on the list */
	/*
	 * Valid fsnotify group priorities.  Events are send in order from highest
	 * priority to lowest priority.  We default to the lowest priority.
	 */
	#define FS_PRIO_0	0 /* normal notifiers, no permissions */
	#define FS_PRIO_1	1 /* fanotify content based access control */
	#define FS_PRIO_2	2 /* fanotify pre-content access */
	unsigned int priority;

	/* stores all fastpath marks assoc with this group so they can be cleaned on unregister */
	struct mutex mark_mutex;	/* protect marks_list */
	atomic_t num_marks;		/* 1 for each mark and 1 for not being
					 * past the point of no return when freeing
					 * a group */
	struct list_head marks_list;	/* all inode marks for this group */

	struct fasync_struct *fsn_fa;    /* async notification */

	struct fsnotify_event *overflow_event;	/* Event we queue when the
						 * notification list is too
						 * full */

	/* groups can define private fields here or use the void *private */
	union {
		void *private;
#ifdef CONFIG_INOTIFY_USER
		struct inotify_group_private_data {
			spinlock_t	idr_lock;
			struct idr      idr;
			struct user_struct      *user;
		} inotify_data;
#endif
#ifdef CONFIG_FANOTIFY
		struct fanotify_group_private_data {
#ifdef CONFIG_FANOTIFY_ACCESS_PERMISSIONS
			/* allows a group to block waiting for a userspace response */
			spinlock_t access_lock;
			struct list_head access_list;
			wait_queue_head_t access_waitq;
			atomic_t bypass_perm;
#endif /* CONFIG_FANOTIFY_ACCESS_PERMISSIONS */
			int f_flags;
			unsigned int max_marks;
			struct user_struct *user;
		} fanotify_data;
#endif /* CONFIG_FANOTIFY */
	};
};

struct fsnotify_event {
         struct list_head list;
         /* inode may ONLY be dereferenced during handle_event(). */
         struct inode *inode;    /* either the inode the event happened to or its parent */
         u32 mask;               /* the type of access, bitwise OR for FS_* event types */
};

struct inotify_event_info {
       struct fsnotify_event fse;
       int wd;
       u32 sync_cookie;
       int name_len;
       char name[];
 };

#define FS_Q_OVERFLOW           0x00004000      /* Event queued overflowed */

SYSCALL_DEFINE1(inotify_init1, int, flags)
{
  struct fsnotify_group *group;
  int ret = 0;
  unsigned int max_events = inotify_max_queued_events_tlx_tlx;
    struct inotify_event_info *oevent;
      group = kzalloc_tlx(sizeof(struct fsnotify_group), GFP_KERNEL);
      atomic_set(&group->refcnt, 1);
      atomic_set(&group->num_marks, 0);
      mutex_init(&group->notification_mutex);
      INIT_LIST_HEAD(&group->notification_list);
      init_waitqueue_head(&group->notification_waitq);
      group->max_events = UINT_MAX;
      mutex_init(&group->mark_mutex);
      INIT_LIST_HEAD(&group->marks_list);
    oevent = kmalloc_tlx(sizeof(struct inotify_event_info), GFP_KERNEL);
    group->overflow_event = &oevent->fse;
      INIT_LIST_HEAD(&(group->overflow_event)->list);
      group->overflow_event->inode = NULL;
      group->overflow_event->mask = FS_Q_OVERFLOW;
    oevent->wd = -1;
    oevent->sync_cookie = 0;
    oevent->name_len = 0;
    group->max_events = max_events;
    spin_lock_init(&group->inotify_data.idr_lock);
//    idr_init(&group->inotify_data.idr);
      memset_tlx(&group->inotify_data.idr, 0, sizeof(struct idr));
     spin_lock_init(&(&group->inotify_data.idr)->lock);
    group->inotify_data.user = get_current_user();
    atomic_inc_return(&group->inotify_data.user->inotify_devs);// >
  return ret;
}


static int prepend_path(const struct path *path,
      const struct path *root,
      char **buffer, int *buflen)
{
  struct dentry *dentry;
  struct vfsmount *vfsmnt;
  struct mount *mnt;
  int error = 0;
  unsigned seq, m_seq = 0;
  char *bptr;
  int blen;
  rcu_read_lock_tlx();
restart_mnt:
  read_seqbegin_or_lock_tlx(&mount_lock_tlx, &m_seq);
  seq = 0;
  rcu_read_lock_tlx();
restart:
  bptr = *buffer;
  blen = *buflen;
  error = 0;
  dentry = path->dentry;
  vfsmnt = path->mnt;
  mnt = real_mount(vfsmnt);
  read_seqbegin_or_lock_tlx(&rename_lock_tlx, &seq);
  while (dentry != root->dentry || vfsmnt != root->mnt) {
    struct dentry * parent;
    if (dentry == vfsmnt->mnt_root || IS_ROOT(dentry)) {
      struct mount *parent = ACCESS_ONCE(mnt->mnt_parent);
      /* Global root? */
      if (mnt != parent) {
        dentry = ACCESS_ONCE(mnt->mnt_mountpoint);
        mnt = parent;
        vfsmnt = &mnt->mnt;
        continue;
      }
      if (!error)
        error = !IS_ERR_OR_NULL_tlx(real_mount(vfsmnt)->mnt_ns) ? 1 : 2;
      break;
    }
    parent = dentry->d_parent;
    prefetch_tlx(parent);
    char **buffer = &bptr;
    int *buflen =&blen;
    struct qstr *name = &dentry->d_name;
      const char *dname = ACCESS_ONCE(name->name);
      u32 dlen = ACCESS_ONCE(name->len);
      char *p;
      *buflen -= dlen + 1;
      p = *buffer -= dlen + 1;
      *p++ = '/';
      while (dlen--) {
        char c = *dname++;
        if (!c)
          break;
        *p++ = c;
      }
    dentry = parent;
  }
  if (!(seq & 1))
    rcu_read_unlock_tlx();
  if (need_seqretry_tlx(&rename_lock_tlx, seq)) {
    seq = 1;
    goto restart;
  }
  done_seqretry_tlx(&rename_lock_tlx, seq);
  if (!(m_seq & 1))
    rcu_read_unlock_tlx();
  if (need_seqretry_tlx(&mount_lock_tlx, m_seq)) {
    m_seq = 1;
    goto restart_mnt;
  }
  done_seqretry_tlx(&mount_lock_tlx, m_seq);
  if (error >= 0 && bptr == *buffer) {
    if (--blen < 0)
      error = -ENAMETOOLONG;
    else
      *--bptr = '/';
  }
  *buffer = bptr;
  *buflen = blen;
  return error;
}


SYSCALL_DEFINE2(getcwd, char __user *, buf, unsigned long, size)
{
  int error;
  struct path pwd, root;
  char *page = slab_alloc_tlx(names_cachep_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
  rcu_read_lock_tlx();
  struct fs_struct *fs = current->fs;
  unsigned seq;
  do {
                 seq = read_seqcount_begin_tlx(&fs->seq);
                 root = fs->root;
                 pwd = fs->pwd;
  } while (read_seqcount_retry_tlx(&fs->seq, seq));
  error = -ENOENT;
  if (!d_unlinked_tlx(pwd.dentry)) {
    unsigned long len;
    char *cwd = page + PATH_MAX;
    int buflen = PATH_MAX;
    buflen -= 1;
    cwd -= 1;
    memcpy_tlx(cwd, "\0", 1);
    error = prepend_path(&pwd, &root, &cwd, &buflen);
    rcu_read_unlock_tlx();
    if (error < 0)
      goto out;
    if (error > 0) {
        buflen -= 13;
        cwd -= 13;
        memcpy_tlx(cwd, "(unreachable)", 13);
    }
    error = -ERANGE;
    len = PATH_MAX + page - cwd;
    if (len <= size) {
      error = len;
      if (copy_to_user(buf, cwd, len))
        error = -EFAULT;
    }
  } else {
    rcu_read_unlock_tlx();
  }
out:
  __putname(page);
  return error;
}
union gic_base {
	void __iomem *common_base;
	void __percpu * __iomem *percpu_base;
};

struct gic_chip_data {
	union gic_base dist_base;
	union gic_base cpu_base;
#ifdef CONFIG_CPU_PM
	u32 saved_spi_enable[DIV_ROUND_UP(1020, 32)];
	u32 saved_spi_conf[DIV_ROUND_UP(1020, 16)];
	u32 saved_spi_target[DIV_ROUND_UP(1020, 4)];
	u32 __percpu *saved_ppi_enable;
	u32 __percpu *saved_ppi_conf;
#endif
	struct irq_domain *domain;
	unsigned int gic_irqs;
#ifdef CONFIG_GIC_NON_BANKED
	void __iomem *(*get_base)(union gic_base *);
#endif
};


#define NR_GIC_CPU_IF 8
u8 gic_cpu_map_tlx[NR_GIC_CPU_IF];
int gic_cnt_tlx;
#define GIC_CPU_CTRL                    0x00
#define GIC_DIST_ENABLE_CLEAR           0x180
#define GIC_DIST_ENABLE_SET             0x100
#define GIC_DIST_PRI                    0x400
#define GIC_CPU_PRIMASK                 0x04
#define GIC_DIST_TARGET                 0x800
#define GIC_DIST_CONFIG                 0xc00
#define GIC_DIST_CTRL                   0x000
#define GIC_DIST_CTR                    0x004
#define MAX_GIC_NR      1
#define GIC_CPU_EOI                     0x10

static inline void __raw_writel(u32 val, volatile void __iomem *addr)
{
       asm volatile("str %w0, [%1]" : : "r" (val), "r" (addr));
}

static inline u32 __raw_readl(const volatile void __iomem *addr)
{
         u32 val;
         asm volatile("ldr %w0, [%1]" : "=r" (val) : "r" (addr));
         return val;
}

 #ifdef __LITTLE_ENDIAN
 #define cpu_to_le32(x) ((__force __le32)(__u32)(x))
 #define le32_to_cpu(x) ((__force __u32)(__le32)(x))

 #endif
 /*
 #ifndef __LITTLE_ENDIAN
 #define cpu_to_le32(x) ((__force __le32)__swab32((x)))
 #endif
*/


#define writel_relaxed(v,c)     ((void)__raw_writel((__force u32)cpu_to_le32(v),(c)))
#define readl_relaxed(c)        ({ u32 __v = le32_to_cpu((__force __le32)__raw_readl(c)); __v; })
#define gic_data_dist_base(d)   ((d)->dist_base.common_base)
#define gic_data_cpu_base(d)    ((d)->cpu_base.common_base)

u8 gic_get_cpumask_tlx(struct gic_chip_data *gic)
{
	void __iomem *base = gic_data_dist_base(gic);
	u32 mask, i;

	for (i = mask = 0; i < 32; i += 4) {
		mask = readl_relaxed(base + GIC_DIST_TARGET + i);
		mask |= mask >> 16;
		mask |= mask >> 8;
		if (mask)
			break;
	}
	return mask;
}

void __init gic_dist_init_tlx(struct gic_chip_data *gic)
{
	unsigned int i;
	u32 cpumask;
	unsigned int gic_irqs = gic->gic_irqs;
	void __iomem *base = gic_data_dist_base(gic);

	writel_relaxed(0, base + GIC_DIST_CTRL);
	for (i = 32; i < gic_irqs; i += 16)
		writel_relaxed(0, base + GIC_DIST_CONFIG + i * 4 / 16);
	cpumask = gic_get_cpumask_tlx(gic);
	cpumask |= cpumask << 8;
	cpumask |= cpumask << 16;
	for (i = 32; i < gic_irqs; i += 4)
		writel_relaxed(cpumask, base + GIC_DIST_TARGET + i * 4 / 4);
	for (i = 32; i < gic_irqs; i += 4)
		writel_relaxed(0xa0a0a0a0, base + GIC_DIST_PRI + i * 4 / 4);
	for (i = 32; i < gic_irqs; i += 32)
		writel_relaxed(0xffffffff, base + GIC_DIST_ENABLE_CLEAR + i * 4 / 32);
	writel_relaxed(1, base + GIC_DIST_CTRL);
}


void gic_cpu_init_tlx(struct gic_chip_data *gic)
{
	void __iomem *dist_base = gic_data_dist_base(gic);
	void __iomem *base = gic_data_cpu_base(gic);
	unsigned int cpu_mask, cpu = smp_processor_id();
	int i;
	cpu_mask = gic_get_cpumask_tlx(gic);
	gic_cpu_map_tlx[cpu] = cpu_mask;
	for (i = 0; i < NR_GIC_CPU_IF; i++)
		if (i != cpu)
			gic_cpu_map_tlx[i] &= ~cpu_mask;
	writel_relaxed(0xffff0000, dist_base + GIC_DIST_ENABLE_CLEAR);
	writel_relaxed(0x0000ffff, dist_base + GIC_DIST_ENABLE_SET);

	for (i = 0; i < 32; i += 4)
		writel_relaxed(0xa0a0a0a0, dist_base + GIC_DIST_PRI + i * 4 / 4);

	writel_relaxed(0xf0, base + GIC_CPU_PRIMASK);
	writel_relaxed(1, base + GIC_CPU_CTRL);
}

void
__irq_set_handler_tlx(unsigned int irq, irq_flow_handler_t handle, int is_chained,
		  const char *name)
{
	unsigned long flags;
	struct irq_desc *desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, irq);

	desc->handle_irq = handle;
	desc->name = name;
}

static inline void *irq_data_get_irq_chip_data(struct irq_data *d)
 {
         return d->chip_data;
 }

static inline void __iomem *gic_cpu_base(struct irq_data *d)
 {
         struct gic_chip_data *gic_data = irq_data_get_irq_chip_data(d);
         return gic_data_cpu_base(gic_data);
 }

 static inline unsigned int gic_irq(struct irq_data *d)
 {
         return d->hwirq;
  }

 void gic_eoi_irq_tlx(struct irq_data *d)
{

	writel_relaxed(gic_irq(d), gic_cpu_base(d) + GIC_CPU_EOI);
}

static inline void __iomem *gic_dist_base(struct irq_data *d)
{
       struct gic_chip_data *gic_data = irq_data_get_irq_chip_data(d);
       return gic_data_dist_base(gic_data);
}

void gic_unmask_irq_tlx(struct irq_data *d)
{
	u32 mask = 1 << (gic_irq(d) % 32);
	writel_relaxed(mask, gic_dist_base(d) + GIC_DIST_ENABLE_SET + (gic_irq(d) / 32) * 4);
}

void gic_mask_irq_tlx(struct irq_data *d)
{
	u32 mask = 1 << (gic_irq(d) % 32);
	writel_relaxed(mask, gic_dist_base(d) + GIC_DIST_ENABLE_CLEAR + (gic_irq(d) / 32) * 4);
}

struct irq_chip gic_chip_tlx = {
	.name			= "GIC",
	.irq_mask		= gic_mask_irq_tlx,
	.irq_unmask		= gic_unmask_irq_tlx,
	.irq_eoi		= gic_eoi_irq_tlx,
};


struct gic_chip_data gic_data_tlx[MAX_GIC_NR];

static int gic_routable_irq_domain_map_tlx(struct irq_domain *d, unsigned int irq,
			      irq_hw_number_t hw)
{
	return 0;
}

static int gic_routable_irq_domain_xlate_tlx(struct irq_domain *d,
				struct device_node *controller,
				const u32 *intspec, unsigned int intsize,
				unsigned long *out_hwirq,
				unsigned int *out_type)
{
	*out_hwirq += 16;
	return 0;
}


struct irq_domain_ops gic_default_routable_irq_domain_ops_tlx = {
	.map = gic_routable_irq_domain_map_tlx,
	.xlate = gic_routable_irq_domain_xlate_tlx,
};

static inline void irqd_clear(struct irq_data *d, unsigned int mask)
{
         d->state_use_accessors &= ~mask;
}

static inline bool irqd_irq_masked(struct irq_data *d)
{
         return d->state_use_accessors & IRQD_IRQ_MASKED;
}

static inline bool irqd_irq_disabled(struct irq_data *d)
{
         return d->state_use_accessors & IRQD_IRQ_DISABLED;
}

static void irq_stat_tlxe_clr_masked(struct irq_desc *desc)
{
         irqd_clear(&desc->irq_data, IRQD_IRQ_MASKED);
}

void unmask_irq_tlx(struct irq_desc *desc)
{
         if (desc->irq_data.chip->irq_unmask) {
               desc->irq_data.chip->irq_unmask(&desc->irq_data);
                 irq_stat_tlxe_clr_masked(desc);
         }
}

static void irq_stat_tlxe_set_masked(struct irq_desc *desc)
{
         irqd_set(&desc->irq_data, IRQD_IRQ_MASKED);
}


void mask_irq_tlx(struct irq_desc *desc)
{
         if (desc->irq_data.chip->irq_mask) {
                 desc->irq_data.chip->irq_mask(&desc->irq_data);
                 irq_stat_tlxe_set_masked(desc);
         }
}



#define PF_EXITING      0x00000004      /* getting shut down */

void __irq_wake_thread_tlx(struct irq_desc *desc, struct irqaction *action)
{
	if (action->thread->flags & PF_EXITING)
		return;

	if (test_and_set_bit_tlx(IRQTF_RUNTHREAD, &action->thread_flags))
		return;
	desc->threads_oneshot |= action->thread_mask;
	atomic_inc_tlx(&desc->threads_active);
	wake_up_process_tlx(action->thread);
}


irqreturn_t
handle_irq_event_percpu_tlx(struct irq_desc *desc, struct irqaction *action)
{
	irqreturn_t retval = IRQ_NONE;
	unsigned int flags = 0, irq = desc->irq_data.irq;

	do {
		irqreturn_t res;
		res = action->handler(irq, action->dev_id);
		switch (res) {
		case IRQ_WAKE_THREAD:
			__irq_wake_thread_tlx(desc, action);
		case IRQ_HANDLED:
			flags |= action->flags;
			break;

		default:
			break;
		}
		retval |= res;
		action = action->next;
	} while (action);
	return retval;
}




irqreturn_t handle_irq_event_tlx(struct irq_desc *desc)
{
	struct irqaction *action = desc->action;
	irqreturn_t ret;

	irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
	raw_spin_unlock(&desc->lock);

	ret = handle_irq_event_percpu_tlx(desc, action);

	raw_spin_lock(&desc->lock);
	irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
	return ret;
}

void cond_unmask_eoi_irq_tlx(struct irq_desc *desc, struct irq_chip *chip)
{
	if (!irqd_irq_disabled(&desc->irq_data) &&
	    irqd_irq_masked(&desc->irq_data) && !desc->threads_oneshot) {
		chip->irq_eoi(&desc->irq_data);
		unmask_irq_tlx(desc);
	} else if (!(chip->flags & IRQCHIP_EOI_THREADED)) {
		chip->irq_eoi(&desc->irq_data);
	}
}


static inline bool irqd_irq_inprogress(struct irq_data *d)
{
         return d->state_use_accessors & IRQD_IRQ_INPROGRESS;
}

static bool irq_check_poll(struct irq_desc *desc)
{
}

void
handle_fasteoi_irq_tlx(unsigned int irq, struct irq_desc *desc)
{
	struct irq_chip *chip = desc->irq_data.chip;

	raw_spin_lock(&desc->lock);

	if (unlikely(irqd_irq_inprogress(&desc->irq_data)))
		if (!irq_check_poll(desc))
			goto out;

	if (unlikely(!desc->action || irqd_irq_disabled(&desc->irq_data))) {
		mask_irq_tlx(desc);
		goto out;
	}
//	preflow_handler(desc);
	handle_irq_event_tlx(desc);
	cond_unmask_eoi_irq_tlx(desc, chip);
	raw_spin_unlock(&desc->lock);
	return;
out:
	if (!(chip->flags & IRQCHIP_EOI_IF_HANDLED))
		chip->irq_eoi(&desc->irq_data);
	raw_spin_unlock(&desc->lock);
}

#define __this_cpu_ptr(ptr)     this_cpu_ptr(ptr)
static inline struct irq_chip *irq_desc_get_chip(struct irq_desc *desc)
 {
         return desc->irq_data.chip;
 }

void handle_percpu_devid_irq_tlx(unsigned int irq, struct irq_desc *desc)
{
	struct irq_chip *chip = irq_desc_get_chip(desc);
	struct irqaction *action = desc->action;
	void *dev_id = __this_cpu_ptr(action->percpu_dev_id);
	irqreturn_t res;

	if (chip->irq_ack)
		chip->irq_ack(&desc->irq_data);
	res = action->handler(irq, dev_id);

	if (chip->irq_eoi)
		chip->irq_eoi(&desc->irq_data);
}




int gic_irq_domain_map_tlx(struct irq_domain *d, unsigned int irq,
				irq_hw_number_t hw)
{
	struct irq_desc *desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, irq);
	if (hw < 32) {
//		irq_set_percpu_devid(irq);
//		irq_set_chip_and_handler(irq, &gic_chip,
//					 handle_percpu_devid_irq);
//		irq_set_chip_and_handler_name(irq, &gic_chip, handle_percpu_devid_irq, NULL);
//		irq_set_chip(irq, &gic_chip);

		 desc->irq_data.chip = &gic_chip_tlx;
		__irq_set_handler_tlx(irq, handle_percpu_devid_irq_tlx, 0, NULL);

	} else {
//		irq_set_chip_and_handler(irq, &gic_chip,
//					 handle_fasteoi_irq);
//		irq_set_chip_and_handler_name(irq, &gic_chip, handle_fasteoi_irq, NULL);
//		irq_set_chip(irq, &gic_chip);
		desc->irq_data.chip = &gic_chip_tlx;
		__irq_set_handler_tlx(irq, handle_fasteoi_irq_tlx, 0, NULL);

		gic_default_routable_irq_domain_ops_tlx.map(d, irq, hw);
	}
//	irq_set_chip_data(irq, d->host_data);

	desc->irq_data.chip_data = d->host_data;

	return 0;
}

int gic_irq_domain_xlate_tlx(struct irq_domain *d,
				struct device_node *controller,
				const u32 *intspec, unsigned int intsize,
				unsigned long *out_hwirq, unsigned int *out_type)
{
	unsigned long ret = 0;
	*out_hwirq = intspec[1] + 16;
	if (!intspec[0]) {
		ret = gic_default_routable_irq_domain_ops_tlx.xlate(d, controller,
							 intspec,
							 intsize,
							 out_hwirq,
							 out_type);

	}

	*out_type = intspec[2] & IRQ_TYPE_SENSE_MASK;
	return ret;
}


struct irq_domain_ops gic_irq_domain_ops_tlx = {
	.map = gic_irq_domain_map_tlx,
	.xlate = gic_irq_domain_xlate_tlx,
};

struct irq_chip gic_arch_extn_tlx = {
	.irq_eoi	= NULL,
	.irq_mask	= NULL,
	.irq_unmask	= NULL,
	.irq_retrigger	= NULL,
	.irq_set_type	= NULL,
	.irq_set_wake	= NULL,
};

enum {
         _IRQ_DEFAULT_INIT_FLAGS = IRQ_DEFAULT_INIT_FLAGS,
         _IRQ_PER_CPU            = IRQ_PER_CPU,
         _IRQ_LEVEL              = IRQ_LEVEL,
         _IRQ_NOPROBE            = IRQ_NOPROBE,
         _IRQ_NOREQUEST          = IRQ_NOREQUEST,
         _IRQ_NOTHREAD           = IRQ_NOTHREAD,
         _IRQ_NOAUTOEN           = IRQ_NOAUTOEN,
         _IRQ_MOVE_PCNTXT        = IRQ_MOVE_PCNTXT,
         _IRQ_NO_BALANCING       = IRQ_NO_BALANCING,
         _IRQ_NESTED_THREAD      = IRQ_NESTED_THREAD,
         _IRQ_PER_CPU_DEVID      = IRQ_PER_CPU_DEVID,
         _IRQ_IS_POLLED          = IRQ_IS_POLLED,
         _IRQF_MODIFY_MASK       = IRQF_MODIFY_MASK,
 };
#define GICC_IAR_INT_ID_MASK            0x3ff
#define GIC_CPU_INTACK                  0x0c

extern void (*handle_arch_irq_tlx)(struct pt_regs *);




#define __this_cpu_generic_to_op(pcp, val, op)                          \
 do {                                                                    \
         *__this_cpu_ptr(&(pcp)) op val;                                 \
 } while (0)


# ifndef __this_cpu_write_1
#  define __this_cpu_write_1(pcp, val)  __this_cpu_generic_to_op((pcp), (val), =)
# endif
 # ifndef __this_cpu_write_2
 #  define __this_cpu_write_2(pcp, val)  __this_cpu_generic_to_op((pcp), (val), =)
 # endif
 # ifndef __this_cpu_write_4
 #  define __this_cpu_write_4(pcp, val)  __this_cpu_generic_to_op((pcp), (val), =)
# endif
# ifndef __this_cpu_write_8
#  define __this_cpu_write_8(pcp, val)  __this_cpu_generic_to_op((pcp), (val), =)
# endif

struct pt_regs *__irq_regs_tlx;

struct pt_regs *set_irq_regs_tlx(struct pt_regs *new_regs)
 {
         struct pt_regs *old_regs;

         old_regs = __this_cpu_read(__irq_regs_tlx);
				 __pcpu_size_call(__this_cpu_write_, (__irq_regs_tlx), (new_regs));
//         __this_cpu_write(__irq_regs_tlx, new_regs);
         return old_regs;
 }

 void wakeup_softirqd_tlx(void)
 {
 	struct task_struct *tsk = __this_cpu_read(ksoftirqd_tlx);
 	if (tsk && tsk->state != TASK_RUNNING)
 					wake_up_process_tlx(tsk);
 }

 int generic_handle_irq_tlx(unsigned int irq)
 {
 	struct irq_desc *desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, irq);
 //	generic_handle_irq_desc(irq, desc);
 	 desc->handle_irq(irq, desc);
 	return 0;
 }

 #define local_softirq_pending() \
         __IRQ_STAT(smp_processor_id(), __softirq_pending)

 void handle_IRQ_tlx(unsigned int irq, struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs_tlx(regs);

 //	irq_enter();
 //  *preempt_count_ptr() += HARDIRQ_OFFSET;
  	generic_handle_irq_tlx(irq);
 	if (!in_interrupt() && local_softirq_pending())
 		wakeup_softirqd_tlx();

 	set_irq_regs_tlx(old_regs);
 }


int irq_find_mapping_tlx(struct irq_domain *domain,
			      irq_hw_number_t hwirq)
{
	struct irq_data *data;
	if (hwirq < domain->revmap_direct_max_irq) {
//		data = irq_get_irq_data(hwirq);
		struct irq_desc *desc = radix_tree_lookup_tlx(&irq_desc_tree_tlx, hwirq);
    data = desc ? &desc->irq_data : NULL;

		if (data && (data->domain == domain) && (data->hwirq == hwirq))
			return hwirq;
	}
	if (hwirq < domain->revmap_size)
		return domain->linear_revmap[hwirq];
	data = radix_tree_lookup_tlx(&domain->revmap_tree, hwirq);
	return data ? data->irq : 0;
}


void  gic_handle_irq_tlx(struct pt_regs *regs)
{
	u32 irqstat, irqnr;
	struct gic_chip_data *gic = &gic_data_tlx[0];
	void __iomem *cpu_base = gic_data_cpu_base(gic);

	do {
		irqstat = readl_relaxed(cpu_base + GIC_CPU_INTACK);
		irqnr = irqstat & GICC_IAR_INT_ID_MASK;

		if (likely(irqnr > 15 && irqnr < 1021)) {
			irqnr = irq_find_mapping_tlx(gic->domain, irqnr);
			handle_IRQ_tlx(irqnr, regs);

			continue;
		}
		if (irqnr < 16) {
			writel_relaxed(irqstat, cpu_base + GIC_CPU_EOI);
			continue;
		}
		break;
	} while (1);
}


int __ref
__irq_alloc_descs_tlx(int irq, unsigned int from, unsigned int cnt, int node,
		  struct module *owner)
{
	int start, ret;
	if (irq >= 0) {
		from = irq;
	}
	start = bitmap_find_next_zero_area_tlx(allocated_irqs_tlx, IRQ_BITMAP_BITS,
					   from, cnt, 0);
	if (start + cnt > nr_irqs_tlx) {
			nr_irqs_tlx = start + cnt;
	}
	bitmap_set_tlx(allocated_irqs_tlx, start, cnt);
	struct irq_desc *desc;
	int i;
	for (i = 0; i < cnt; i++) {
		  irq = start + i;
			gfp_t gfp = GFP_KERNEL;
			desc = kzalloc_tlx(sizeof(*desc), gfp);
//			desc->kstat_irqs = alloc_percpu(unsigned int);
//			zalloc_cpumask_var_node(&desc->irq_data.affinity, gfp, node);
			raw_spin_lock_init(&desc->lock);
	//		lockdep_set_class(&desc->lock, &irq_desc_lock_class);
				int cpu;
				desc->irq_data.irq = irq;
				desc->irq_data.chip = &no_irq_chip_tlx;
				desc->irq_data.chip_data = NULL;
				desc->irq_data.handler_data = NULL;
				desc->irq_data.msi_desc = NULL;
        desc->status_use_accessors &= ~(~0 & _IRQF_MODIFY_MASK);
        desc->status_use_accessors |= (IRQ_DEFAULT_INIT_FLAGS & _IRQF_MODIFY_MASK);
				(&desc->irq_data)->state_use_accessors |= IRQD_IRQ_DISABLED;
				desc->handle_irq = handle_bad_irq_tlx;
				desc->depth = 1;
				desc->irq_count = 0;
				desc->irqs_unhandled = 0;
				desc->name = NULL;
				desc->owner = owner;
//				for_each_possible_cpu(cpu)
//					*per_cpu_ptr(desc->kstat_irqs, cpu) = 0;
				 desc->irq_data.node = node;
//         cpumask_copy(desc->irq_data.affinity, irq_default_affinity_tlx);
 	  		 radix_tree_insert_tlx(&irq_desc_tree_tlx, start + i, desc);
	}
	return start;
}




void __iomem *of_iomap_tlx(struct device_node *np, int index)
{
         struct resource res;

         if (of_address_to_resource_tlx(np, index, &res))
                 return NULL;

         return ioremap(res.start, resource_size(&res));
}


#define gic_init_physaddr(node)  do { } while (0)

struct irq_domain *__irq_domain_add_tlx(struct device_node *of_node, int size,
				    irq_hw_number_t hwirq_max, int direct_max,
				    const struct irq_domain_ops *ops,
				    void *host_data)
{
	struct irq_domain *domain;
	domain = kzalloc_tlx(sizeof(*domain) + (sizeof(unsigned int) * size),
			      GFP_KERNEL);
	INIT_RADIX_TREE(&domain->revmap_tree, GFP_KERNEL);
	domain->ops = ops;
	domain->host_data = host_data;
	domain->of_node = of_node;
	domain->hwirq_max = hwirq_max;
	domain->revmap_size = size;
	domain->revmap_direct_max_irq = direct_max;
	list_add(&domain->link, &irq_domain_list_tlx);
	return domain;
}

void __init gic_init_bases_tlx(unsigned int gic_nr, int irq_start,
			   void __iomem *dist_base, void __iomem *cpu_base,
			   u32 percpu_offset, struct device_node *node)
{
	irq_hw_number_t hwirq_base;
	struct gic_chip_data *gic;
	int gic_irqs, irq_base, i;
	int nr_routable_irqs;
	gic = &gic_data_tlx[gic_nr];
	{			/* Normal, sane GIC... */
		gic->dist_base.common_base = dist_base;
		gic->cpu_base.common_base = cpu_base;
//		gic_set_base_accessor(gic, gic_get_common_base);
	}

	for (i = 0; i < NR_GIC_CPU_IF; i++)
		gic_cpu_map_tlx[i] = 0xff;
	if (gic_nr == 0 && (irq_start & 31) > 0) {
		hwirq_base = 16;
		if (irq_start != -1)
			irq_start = (irq_start & ~31) + 16;
	} else {
		hwirq_base = 32;
	}
	gic_irqs = readl_relaxed(gic_data_dist_base(gic) + GIC_DIST_CTR) & 0x1f;
	gic_irqs = (gic_irqs + 1) * 32;
	if (gic_irqs > 1020)
		gic_irqs = 1020;
	gic->gic_irqs = gic_irqs;
	gic_irqs -= hwirq_base; /* calculate # of irqs to allocate */
		irq_base = __irq_alloc_descs_tlx(irq_start, 16, gic_irqs,
					   0, THIS_MODULE);
	gic->domain = __irq_domain_add_tlx(node, hwirq_base + gic_irqs,
									  hwirq_base + gic_irqs, 0, &gic_irq_domain_ops_tlx, gic);
	if (gic_nr == 0) {
//		set_handle_irq(gic_handle_irq_tlx);
			handle_arch_irq_tlx = gic_handle_irq_tlx;
	}


	gic_dist_init_tlx(gic);
	gic_cpu_init_tlx(gic);
//	gic_pm_init(gic);
}

int
gic_of_init_tlx(struct device_node *node, struct device_node *parent)
{
	void __iomem *cpu_base;
	void __iomem *dist_base;
	u32 percpu_offset;
	int irq;
	dist_base = of_iomap_tlx(node, 0);
	cpu_base = of_iomap_tlx(node, 1);
	gic_init_bases_tlx(gic_cnt_tlx, -1, dist_base, cpu_base, percpu_offset, node);
	gic_cnt_tlx++;
	return 0;
}

#define _OF_DECLARE_TLX(table, name, compat, fn, fn_type)                   \
         static const struct of_device_id_tlx __of_table_##name              \
                 __used __section(__##table##_of_table)                  \
                  = { .compatible = compat,                              \
                      .data = (fn == (fn_type)NULL) ? fn : fn  }
 typedef int (*of_init_fn_2)(struct device_node *, struct device_node *);

_OF_DECLARE_TLX(irqchip, cortex_a15_gic, "arm,cortex-a15-gic", gic_of_init_tlx, of_init_fn_2);

struct clk_hw {
	struct clk *clk;
	const struct clk_init_data *init;
};

struct clk_init_data {
	const char		*name;
	const struct clk_ops	*ops;
	const char		**parent_names;
	u8			num_parents;
	unsigned long		flags;
};

struct clk_fixed_rate {
	struct		clk_hw hw;
	unsigned long	fixed_rate;
	unsigned long	fixed_accuracy;
	u8		flags;
};



#define CLK_IS_BASIC		BIT(5) /* Basic clk, can't do a to_clk_foo() */


struct clk_ops clk_fixed_rate_ops_tlx = {
};



struct clk *clk_register_tlx(struct device *dev, struct clk_hw *hw)
{
	int i, ret;
	struct clk *clk;

	clk = kzalloc_tlx(sizeof(*clk), GFP_KERNEL);
	clk->name = kstrdup_tlx(hw->init->name, GFP_KERNEL);
	clk->ops = hw->init->ops;
	if (dev && dev->driver)
		clk->owner = dev->driver->owner;
	clk->hw = hw;
	clk->flags = hw->init->flags;
	clk->num_parents = hw->init->num_parents;
	hw->clk = clk;
	clk->parent_names = kcalloc_tlx(clk->num_parents, sizeof(char *),
					GFP_KERNEL);
	for (i = 0; i < clk->num_parents; i++) {
		clk->parent_names[i] = kstrdup_tlx(hw->init->parent_names[i],
						GFP_KERNEL);
	}

		return clk;
}

struct clk *clk_register_fixed_rate_with_accuracy_tlx(struct device *dev,
		const char *name, const char *parent_name, unsigned long flags,
		unsigned long fixed_rate, unsigned long fixed_accuracy)
{
	struct clk_fixed_rate *fixed;
	struct clk *clk;
	struct clk_init_data init;

	/* allocate fixed-rate clock */
	fixed = kzalloc_tlx(sizeof(struct clk_fixed_rate), GFP_KERNEL);
	init.name = name;
	init.ops = &clk_fixed_rate_ops_tlx;
	init.flags = flags | CLK_IS_BASIC;
	init.parent_names = (parent_name ? &parent_name: NULL);
	init.num_parents = (parent_name ? 1 : 0);

	/* struct clk_fixed_rate assignments */
	fixed->fixed_rate = fixed_rate;
	fixed->fixed_accuracy = fixed_accuracy;
	fixed->hw.init = &init;

	/* register the clock */
	clk = clk_register_tlx(dev, &fixed->hw);
//	struct device *dev,
//		struct clk_hw *hw =&fixed->hw;


	return clk;
}

#define CLK_IS_ROOT		BIT(4) /* root clk, has no parent */



struct clk *of_clk_src_simple_get_tlx(struct of_phandle_args *clkspec,
						void *data)
{
	return data;
}

int of_clk_add_provider_tlx(struct device_node *np,
			struct clk *(*clk_src_get)(struct of_phandle_args *clkspec,
							void *data),
			void *data)
{
	struct of_clk_provider *cp;
	cp = kzalloc_tlx(sizeof(struct of_clk_provider), GFP_KERNEL);
	cp->node = np;
	cp->data = data;
	cp->get = clk_src_get;
//	mutex_lock(&of_clk_mutex);
	list_add(&cp->link, &of_clk_providers_tlx);
//	mutex_unlock(&of_clk_mutex);
	return 0;
}

void of_fixed_clk_setup_tlx(struct device_node *node)
{
	struct clk *clk;
	const char *clk_name = node->name;
	u32 rate = 0;
	u32 accuracy = 0;

//	if (of_property_read_u32(node, "clock-frequency", &rate))
	//	return;

//	of_property_read_u32(node, "clock-accuracy", &accuracy);

//	of_property_read_string(node, "clock-output-names", &clk_name);

	clk = clk_register_fixed_rate_with_accuracy_tlx(NULL, clk_name, NULL,
								CLK_IS_ROOT, rate,
								accuracy);
	of_clk_add_provider_tlx(node, of_clk_src_simple_get_tlx, clk);
}


#define CLK_OF_DECLARE(name, compat, fn) OF_DECLARE_1(clk, name, compat, fn)

#define OF_DECLARE_1(table, name, compat, fn) \
		_OF_DECLARE_TLX(table, name, compat, fn, of_init_fn_1)


CLK_OF_DECLARE(fixed_clk, "fixed-clock", of_fixed_clk_setup_tlx);




#define pr_fmt(fmt) fmt

void *__kmalloc_tlx_tlx(size_t size, gfp_t flags)
{
	struct kmem_cache *s;
	void *ret;

	if (unlikely(size > KMALLOC_MAX_CACHE_SIZE)) {
			unsigned int order = get_order(size);
				struct page *page;
				flags |= __GFP_COMP;
				page =  __alloc_pages_nodemask_tlx(flags, order, node_zonelist_tlx(numa_node_id_tlx(), flags), NULL);
				return page ? page_address(page) : NULL;
	}
		int index;
		if (size <= 192) {
			if (!size) {
				s = ZERO_SIZE_PTR;
				goto have_s;
			}
			index = size_index_tlx[ (size - 1) / 8];
		} else
			index = fls_tlx(size - 1);
		s = kmalloc_caches_tlx[index];
have_s:
	if (unlikely(ZERO_OR_NULL_PTR(s)))
		return s;
	ret = slab_alloc_tlx(s, flags, _RET_IP_);
	return ret;
}

struct file_system_type *get_fs_type_tlx(const char *name)
{
          struct file_system_type **p;
          for (p=&file_systems_tlx; *p; p=&(*p)->next)
                  if (strlen_tlx((*p)->name) == strlen_tlx(name) &&
                      strncmp_tlx((*p)->name, name, strlen_tlx(name)) == 0)
                          break;

	return *(p);
}

struct vfsmount *
vfs_kern_mount_tlx(struct file_system_type *type, int flags, const char *name, void *data)
{
	struct dentry *root;
	struct mount *mnt = slab_alloc_tlx(mnt_cache_tlx, GFP_KERNEL| __GFP_ZERO, _RET_IP_);
	if (mnt) {
		int err;
		int res;
		ida_pre_get_tlx(&mnt_id_ida_tlx, GFP_KERNEL);
		res = ida_get_new_above_tlx(&mnt_id_ida_tlx, mnt_id_start_tlx, &mnt->mnt_id);
		if (!res)
								mnt_id_start_tlx = mnt->mnt_id + 1;
		if (name) {
					size_t len;
					char *buf;
					len = strlen_tlx(name) + 1;
					buf =  __kmalloc_tlx_tlx(len, GFP_KERNEL);
					if (buf)
								memcpy_tlx(buf, name, len);
					mnt->mnt_devname = buf;
		}
		mnt->mnt_pcp = alloc_percpu(struct mnt_pcp);
		this_cpu_add(mnt->mnt_pcp->mnt_count, 1);
		INIT_HLIST_NODE(&mnt->mnt_hash);
		INIT_LIST_HEAD(&mnt->mnt_child);
		INIT_LIST_HEAD(&mnt->mnt_mounts);
		INIT_LIST_HEAD(&mnt->mnt_list);
		INIT_LIST_HEAD(&mnt->mnt_expire);
		INIT_LIST_HEAD(&mnt->mnt_share);
		INIT_LIST_HEAD(&mnt->mnt_slave_list);
		INIT_LIST_HEAD(&mnt->mnt_slave);
	}

	if (flags & MS_KERNMOUNT)
		mnt->mnt.mnt_flags = MNT_INTERNAL;
	struct super_block *sb;
	char *secdata = NULL;
	root = type->mount(type, flags, name, data);
	sb = root->d_sb;
	sb->s_flags |= MS_BORN;
	up_write_tlx(&sb->s_umount);
	free_page((unsigned long)secdata);
	mnt->mnt.mnt_root = root;
	mnt->mnt.mnt_sb = root->d_sb;
	mnt->mnt_mountpoint = mnt->mnt.mnt_root;
	mnt->mnt_parent = mnt;
	list_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);
	return &mnt->mnt;
}

struct file_system_type **find_filesystem_tlx(const char *name, unsigned len)
{
	struct file_system_type **p;
	for (p=&file_systems_tlx; *p; p=&(*p)->next)
		if (strlen_tlx((*p)->name) == len &&
				strncmp_tlx((*p)->name, name, len) == 0)
			break;
	return p;
}

void __init mnt_init_tlx(void)
{
	unsigned u;
	int err;
//	printk("================sizeof(struct mount) %d,", sizeof(struct mount));
	mnt_cache_tlx = kmem_cache_create_tlx("mnt_cache", sizeof(struct mount),
			0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);

	mount_hashtable_tlx = alloc_large_system_hash_tlx("Mount-cache",
				sizeof(struct hlist_head),
				mhash_entries_tlx, 19,
				0,
				&m_hash_shift_tlx, &m_hash_mask_tlx, 0, 0);
	mountpoint_hashtable_tlx = alloc_large_system_hash_tlx("Mountpoint-cache",
				sizeof(struct hlist_head),
				mphash_entries_tlx, 19,
				0,
				&mp_hash_shift_tlx, &mp_hash_mask_tlx, 0, 0);
	for (u = 0; u <= m_hash_mask_tlx; u++)
		INIT_HLIST_HEAD(&mount_hashtable_tlx[u]);
	for (u = 0; u <= mp_hash_mask_tlx; u++)
		INIT_HLIST_HEAD(&mountpoint_hashtable_tlx[u]);
	kernfs_node_cache_tlx = kmem_cache_create_tlx("kernfs_node_cache_tlx",
							120,
							0, SLAB_PANIC, NULL);
	struct kernfs_node *kn;
	sysfs_root_tlx = kzalloc_tlx(sizeof(*sysfs_root_tlx), GFP_KERNEL);
	INIT_LIST_HEAD(&sysfs_root_tlx->supers);
	char *dup_name = NULL;
	kn = slab_alloc_tlx(kernfs_node_cache_tlx, GFP_KERNEL | __GFP_ZERO, _RET_IP_);
//	kmem_cache_zalloc_tlx(kernfs_node_cache_tlx, GFP_KERNEL);
	atomic_set(&kn->count, 1);
	atomic_set(&kn->active, KN_DEACTIVATED_BIAS);
	RB_CLEAR_NODE(&kn->rb);
	kn->name = "";
	kn->mode = S_IFDIR | S_IRUGO | S_IXUGO;
	kn->flags = KERNFS_DIR;
	kn->priv = NULL;
	kn->dir.root = sysfs_root_tlx;
	sysfs_root_tlx->syscall_ops = NULL;
	sysfs_root_tlx->flags = KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK;
	sysfs_root_tlx->kn = kn;
	sysfs_root_kn_tlx = sysfs_root_tlx->kn;


	fs_kobj_tlx_tlx = kobject_create_and_add_tlx("fs", NULL);
//	err = register_filesystem(&rootfs_fs_type);
	struct file_system_type * fs = &rootfs_fs_type_tlx;
	find_filesystem_tlx(fs->name, strlen_tlx(fs->name));

	shmem_inode_cachep_tlx = kmem_cache_create_tlx("shmem_inode_cache",
			624,
			0, SLAB_PANIC, shmem_init_inode_tlx);

		is_tmpfs_tlx = true;
	struct vfsmount *mnt;
	struct mnt_namespace *ns;
	struct path root;
	struct file_system_type *type;

	type = get_fs_type_tlx("rootfs");
	mnt = vfs_kern_mount_tlx(type, 0, "rootfs", NULL);
	root.mnt = mnt;
	root.dentry = mnt->mnt_root;
	current->fs->pwd = root;
	current->fs->root = root;

}


void __init vfs_caches_init(unsigned long mempages)
{
	unsigned long reserve;
	reserve = min((mempages - nr_free_pages()) * 3/2, mempages - 1);
	mempages -= reserve;

		names_cachep_tlx = kmem_cache_create_tlx("names_cache", PATH_MAX, 0,
			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);

//		printk(KERN_ERR pr_fmt("=================d, %d") ,sizeof(struct dentry));
	//	pr_err("=================d, %d",sizeof(struct dentry));
		dentry_cache_tlx = kmem_cache_create_tlx("dentry", sizeof(struct dentry), 0,
			SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|SLAB_MEM_SPREAD, NULL);

		inode_cachep_tlx = kmem_cache_create_tlx("inode_cache",
				552, 0, (SLAB_RECLAIM_ACCOUNT|SLAB_PANIC| SLAB_MEM_SPREAD), init_once);


//		percpu_counter_init(&nr_files, 0);
		static struct lock_class_key __key;
		__percpu_counter_init_tlx(&nr_files_tlx, 0, &__key);

		mnt_init_tlx();
		struct kobj_map *p = __kmalloc_tlx_tlx(sizeof(struct kobj_map), GFP_KERNEL);
		struct probe *base = kzalloc_tlx(sizeof(*base), GFP_KERNEL);
		int i;
		base->dev = 1;
		base->range = ~0;
		base->get = base_probe_tlx;
		for (i = 0; i < 255; i++)
			p->probes[i] = base;
		p->lock = &chrdevs_lock_tlx;
		cdev_map_tlx = p;

}

void kobject_release_tlx(struct kref *kref) {};
struct kobject *fs_kobj_tlx;
struct kobject *fuse_kobj_tlx;
struct kobject *connections_kobj_tlx;


struct fuse_release_in {
         uint64_t        fh;
         uint32_t        flags;
         uint32_t        release_flags;
         uint64_t        lock_owner;
};

struct fuse_init_out {
         uint32_t        major;
         uint32_t        minor;
         uint32_t        max_readahead;
         uint32_t        flags;
         uint16_t        max_background;
         uint16_t        congestion_threshold;
         uint32_t        max_write;
         uint32_t        time_gran;
         uint32_t        unused[9];
};

struct cuse_init_in {
         uint32_t        major;
         uint32_t        minor;
         uint32_t        unused;
         uint32_t        flags;
 };


 struct fuse_read_in {
         uint64_t        fh;
         uint64_t        offset;
         uint32_t        size;
         uint32_t        read_flags;
         uint64_t        lock_owner;
         uint32_t        flags;
         uint32_t        padding;
};

struct fuse_notify_retrieve_in {
         uint64_t        dummy1;
         uint64_t        offset;
         uint32_t        size;
         uint32_t        dummy2;
         uint64_t        dummy3;
         uint64_t        dummy4;
};

struct fuse_lk_in {
         uint64_t        fh;
         uint64_t        owner;
         uint32_t        lk_flags;
         uint32_t        padding;
 };

struct fuse_write_in {
       uint64_t        fh;
       uint64_t        offset;
       uint32_t        size;
       uint32_t        write_flags;
       uint64_t        lock_owner;
       uint32_t        flags;
       uint32_t        padding;
 };

 struct fuse_write_out {
         uint32_t        size;
         uint32_t        padding;
  };

struct fuse_init_in {
         uint32_t        major;
         uint32_t        minor;
         uint32_t        max_readahead;
         uint32_t        flags;
};

struct fuse_in_header {
         uint32_t        len;
         uint32_t        opcode;
         uint64_t        unique;
         uint64_t        nodeid;
         uint32_t        uid;
         uint32_t        gid;
         uint32_t        pid;
         uint32_t        padding;
 };

 struct fuse_out_header {
         uint32_t        len;
         int32_t         error;
         uint64_t        unique;
};

/** Max number of pages that can be used in a single read request */
#define FUSE_MAX_PAGES_PER_REQ 32

/** Bias for fi->writectr, meaning new writepages must not be sent */
#define FUSE_NOWRITE INT_MIN

/** It could be as large as PATH_MAX, but would that have any uses? */
#define FUSE_NAME_MAX 1024

/** Number of dentries for each connection in the control filesystem */
#define FUSE_CTL_NUM_DENTRIES 5

/** If the FUSE_DEFAULT_PERMISSIONS flag is given, the filesystem
    module will check permissions based on the file mode.  Otherwise no
    permission checking is done in the kernel */
#define FUSE_DEFAULT_PERMISSIONS (1 << 0)

/** If the FUSE_ALLOW_OTHER flag is given, then not only the user
    doing the mount will be allowed to access the filesystem */
#define FUSE_ALLOW_OTHER         (1 << 1)

/** Number of page pointers embedded in fuse_req */
#define FUSE_REQ_INLINE_PAGES 1


/** Module parameters */
unsigned max_user_bgreq_tlx;
unsigned max_user_congthresh_tlx;

/* One forget request */

/** FUSE inode state bits */
enum {
	/** Advise readdirplus  */
	FUSE_I_ADVISE_RDPLUS,
	/** Initialized with readdirplus */
	FUSE_I_INIT_RDPLUS,
	/** An operation changing file size is in progress  */
	FUSE_I_SIZE_UNSTABLE,
};

struct fuse_conn;

/** FUSE specific file data */
struct fuse_file {
	/** Fuse connection for this file */
	struct fuse_conn *fc;

	/** Request reserved for flush and release */
	struct fuse_req *reserved_req;

	/** Kernel file handle guaranteed to be unique */
	u64 kh;

	/** File handle used by userspace */
	u64 fh;

	/** Node id of this file */
	u64 nodeid;

	/** Refcount */
	atomic_t count;

	/** FOPEN_* flags returned by open */
	u32 open_flags;

	/** Entry on inode's write_files list */
	struct list_head write_entry;

	/** RB node to be linked on fuse_conn->polled_files */
	struct rb_node polled_node;

	/** Wait queue head for poll */
	wait_queue_head_t poll_wait;

	/** Has flock been performed on this file? */
	bool flock:1;
};

/** One input argument of a request */
struct fuse_in_arg {
	unsigned size;
	const void *value;
};

/** The request input */
struct fuse_in {
	/** The request header */
	struct fuse_in_header h;

	/** True if the data for the last argument is in req->pages */
	unsigned argpages:1;

	/** Number of arguments */
	unsigned numargs;

	/** Array of arguments */
	struct fuse_in_arg args[3];
};

/** One output argument of a request */
struct fuse_arg {
	unsigned size;
	void *value;
};

/** The request output */
struct fuse_out {
	/** Header returned from userspace */
	struct fuse_out_header h;

	/*
	 * The following bitfields are not changed during the request
	 * processing
	 */

	/** Last argument is variable length (can be shorter than
	    arg->size) */
	unsigned argvar:1;

	/** Last argument is a list of pages to copy data to */
	unsigned argpages:1;

	/** Zero partially or not copied pages */
	unsigned page_zeroing:1;

	/** Pages may be replaced with new ones */
	unsigned page_replace:1;

	/** Number or arguments */
	unsigned numargs;

	/** Array of arguments */
	struct fuse_arg args[3];
};

/** FUSE page descriptor */
struct fuse_page_desc {
	unsigned int length;
	unsigned int offset;
};

/** The request state */
enum fuse_req_state {
	FUSE_REQ_INIT = 0,
	FUSE_REQ_PENDING,
	FUSE_REQ_READING,
	FUSE_REQ_SENT,
	FUSE_REQ_WRITING,
	FUSE_REQ_FINISHED
};

/** The request IO state (for asynchronous processing) */
struct fuse_io_priv {
	int async;
	spinlock_t lock;
	unsigned reqs;
	ssize_t bytes;
	size_t size;
	__u64 offset;
	bool write;
	int err;
	struct kiocb *iocb;
	struct file *file;
};

/**
 * A request to the client
 */
struct fuse_req {
	/** This can be on either pending processing or io lists in
	    fuse_conn */
	struct list_head list;

	/** Entry on the interrupts list  */
	struct list_head intr_entry;

	/** refcount */
	atomic_t count;

	/** Unique ID for the interrupt request */
	u64 intr_unique;

	/*
	 * The following bitfields are either set once before the
	 * request is queued or setting/clearing them is protected by
	 * fuse_conn->lock
	 */

	/** True if the request has reply */
	unsigned isreply:1;

	/** Force sending of the request even if interrupted */
	unsigned force:1;

	/** The request was aborted */
	unsigned aborted:1;

	/** Request is sent in the background */
	unsigned background:1;

	/** The request has been interrupted */
	unsigned interrupted:1;

	/** Data is being copied to/from the request */
	unsigned locked:1;

	/** Request is counted as "waiting" */
	unsigned waiting:1;

	/** State of the request */
	enum fuse_req_state state;

	/** The request input */
	struct fuse_in in;

	/** The request output */
	struct fuse_out out;

	/** Used to wake up the task waiting for completion of request*/
	wait_queue_head_t waitq;

	/** Data for asynchronous requests */
	union {
		struct {
			union {
				struct fuse_release_in in;
				struct work_struct work;
			};
			struct path path;
		} release;
		struct fuse_init_in init_in;
		struct fuse_init_out init_out;
		struct cuse_init_in cuse_init_in;
		struct {
			struct fuse_read_in in;
			u64 attr_ver;
		} read;
		struct {
			struct fuse_write_in in;
			struct fuse_write_out out;
			struct fuse_req *next;
		} write;
		struct fuse_notify_retrieve_in retrieve_in;
		struct fuse_lk_in lk_in;
	} misc;

	/** page vector */
	struct page **pages;

	/** page-descriptor vector */
	struct fuse_page_desc *page_descs;

	/** size of the 'pages' array */
	unsigned max_pages;

	/** inline page vector */
	struct page *inline_pages[FUSE_REQ_INLINE_PAGES];

	/** inline page-descriptor vector */
	struct fuse_page_desc inline_page_descs[FUSE_REQ_INLINE_PAGES];

	/** number of pages in vector */
	unsigned num_pages;

	/** File used in the request (or NULL) */
	struct fuse_file *ff;

	/** Inode used in the request or NULL */
	struct inode *inode;

	/** AIO control block */
	struct fuse_io_priv *io;

	/** Link on fi->writepages */
	struct list_head writepages_entry;

	/** Request completion callback */
	void (*end)(struct fuse_conn *, struct fuse_req *);

	/** Request is stolen from fuse_file->reserved_req */
	struct file *stolen_file;
};



struct fuse_inode {
	/** Inode data */
	struct inode inode;

	/** Unique ID, which identifies the inode between userspace
	 * and kernel */
	u64 nodeid;

	/** Number of lookups on this inode */
	u64 nlookup;

	/** The request used for sending the FORGET message */
	struct fuse_forget_link *forget;

	/** Time in jiffies until the file attributes are valid */
	u64 i_time;

	/** The sticky bit in inode->i_mode may have been removed, so
	    preserve the original mode */
	umode_t orig_i_mode;

	/** 64 bit inode number */
	u64 orig_ino;

	/** Version of last attribute change */
	u64 attr_version;

	/** Files usable in writepage.  Protected by fc->lock */
	struct list_head write_files;

	/** Writepages pending on truncate or fsync */
	struct list_head queued_writes;

	/** Number of sent writes, a negative bias (FUSE_NOWRITE)
	 * means more writes are blocked */
	int writectr;

	/** Waitq for writepage completion */
	wait_queue_head_t page_waitq;

	/** List of writepage requestst (pending or sent) */
	struct list_head writepages;

	/** Miscellaneous bits describing inode state */
	unsigned long state;
};


static int fuse_sysfs_init_tlx(void)
{
	int err;

	fuse_kobj_tlx = kobject_create_and_add_tlx("fuse", fs_kobj_tlx);
	if (!fuse_kobj_tlx) {
		err = -ENOMEM;
		goto out_err;
	}

	connections_kobj_tlx = kobject_create_and_add_tlx("connections", fuse_kobj_tlx);
	if (!connections_kobj_tlx) {
		err = -ENOMEM;
		goto out_fuse_unregister;
	}

	return 0;

 out_fuse_unregister:
 if (fuse_kobj_tlx) {
       kref_put_tlx(&fuse_kobj_tlx->kref, kobject_release_tlx);
 }

 out_err:
	return err;
}

void sanitize_global_limit_tlx(unsigned *limit)
{
	if (*limit == 0)
		*limit = ((totalram_pages_tlx << PAGE_SHIFT) >> 13) /
			 sizeof(struct fuse_req);

	if (*limit >= 1 << 16)
		*limit = (1 << 16) - 1;
}

struct kmem_cache *fuse_inode_cachep_tlx;

static void fuse_inode_init_once(void *foo)
{
}

#define LTO_REFERENCE_INITCALL(x)

static int __init fuse_init(void)
{
	int res;
//	INIT_LIST_HEAD(&fuse_conn_list);
	fuse_inode_cachep_tlx = kmem_cache_create_tlx("fuse_inode",
					      sizeof(struct fuse_inode),
					      0, SLAB_HWCACHE_ALIGN,
					      fuse_inode_init_once);

	res = fuse_sysfs_init_tlx();
	if (res)
		goto err_dev_cleanup;

	sanitize_global_limit_tlx(&max_user_bgreq_tlx);
	sanitize_global_limit_tlx(&max_user_congthresh_tlx);

	return 0;

 err_sysfs_cleanup:
//	fuse_sysfs_cleanup();
 err_dev_cleanup:
//	fuse_dev_cleanup();
 err_fs_cleanup:
//	fuse_fs_cleanup();
 err:
	return res;
}

#define __define_initcall_tlx(fn, id) \
	static initcall_t __initcall_##fn##id __used \
	__attribute__((__section__(".initcall" #id ".init"))) = fn; \
	LTO_REFERENCE_INITCALL(__initcall_##fn##id)


__define_initcall_tlx(fuse_init, 6s);

#define ENOSYS          38      /* Invalid system call number */
#define __NR_getdents64 61


long sys_mmap_tlx(unsigned long addr, unsigned long len,
			 unsigned long prot, unsigned long flags,
			 unsigned long fd, off_t off)
{
	return sys_mmap_pgoff(addr, len, prot, flags, fd, off >> PAGE_SHIFT);
}


#undef __SYSCALL
#define __SYSCALL(nr, sym)	[nr] = sym,

#undef __NR_syscalls
#define __NR_syscalls 277

static long sys_ni_syscall(void)
{
	return -ENOSYS;
}

SYSCALL_DEFINE1(close, unsigned int, fd)
{

}
/*
 * The sys_call_table array must be 4K aligned to be accessible from
 * kernel/entry.S.
 */



void *sys_call_table_tlx[__NR_syscalls] __aligned(4096) = {
	[0 ... __NR_syscalls - 1] = sys_ni_syscall,
//#include <asm/unistd.h>
#ifdef CONFIG_COMPAT
#define __ARCH_WANT_COMPAT_SYS_GETDENTS64
#define __ARCH_WANT_COMPAT_STAT64
#define __ARCH_WANT_SYS_GETHOSTNAME
#define __ARCH_WANT_SYS_PAUSE
#define __ARCH_WANT_SYS_GETPGRP
#define __ARCH_WANT_SYS_LLSEEK
#define __ARCH_WANT_SYS_NICE
#define __ARCH_WANT_SYS_SIGPENDING
#define __ARCH_WANT_SYS_SIGPROCMASK
#define __ARCH_WANT_COMPAT_SYS_SENDFILE
#define __ARCH_WANT_SYS_FORK
#define __ARCH_WANT_SYS_VFORK
#endif
#define __ARCH_WANT_SYS_CLONE

/*
* This file contains the system call numbers, based on the
* layout of the x86-64 architecture, which embeds the
* pointer to the syscall in the table.
*
* As a basic principle, no duplication of functionality
* should be added, e.g. we don't use lseek when llseek
* is present. New architectures should use this file
* and implement the less feature-full calls in user space.
*/

#ifndef __SYSCALL
#define __SYSCALL(x, y)
#endif

#if __BITS_PER_LONG == 32 || defined(__SYSCALL_COMPAT)
#define __SC_3264(_nr, _32, _64) __SYSCALL(_nr, _32)
#else
#define __SC_3264(_nr, _32, _64) __SYSCALL(_nr, _64)
#endif

#ifdef __SYSCALL_COMPAT
#define __SC_COMP(_nr, _sys, _comp) __SYSCALL(_nr, _comp)
#define __SC_COMP_3264(_nr, _32, _64, _comp) __SYSCALL(_nr, _comp)
#else
#define __SC_COMP(_nr, _sys, _comp) __SYSCALL(_nr, _sys)
#define __SC_COMP_3264(_nr, _32, _64, _comp) __SC_3264(_nr, _32, _64)
#endif

/* fs/dcache.c */
#define __NR_getcwd 17
__SYSCALL(__NR_getcwd, sys_getcwd)

/* fs/inotify_user.c */
#define __NR_inotify_init1 26
__SYSCALL(__NR_inotify_init1, sys_inotify_init1)
/* fs/ioctl.c */
#define __NR_ioctl 29
__SC_COMP(__NR_ioctl, sys_ioctl,  sys_ioctl)
/* fs/locks.c */
#define __NR_flock 32
__SYSCALL(__NR_flock, sys_flock)
#define __NR_openat 56
__SC_COMP(__NR_openat, sys_openat, sys_openat)
#define __NR_close 57
__SYSCALL(__NR_close, sys_close)
#define __ARCH_WANT_COMPAT_SYS_GETDENTS64
__SC_COMP(__NR_getdents64, sys_getdents64, sys_getdents64)
#define __NR_read 63
__SYSCALL(__NR_read, sys_read)
#define __NR_write 64
__SYSCALL(__NR_write, sys_write)
#define __NR_ppoll 73
__SC_COMP(__NR_ppoll, sys_ppoll, sys_ppoll)
#define __NR_readlinkat 78
__SYSCALL(__NR_readlinkat, sys_readlinkat)
#define __NR3264_fstatat 79
__SC_3264(__NR3264_fstatat, sys_newfstatat, sys_newfstatat)
#define __NR3264_fstat 80
__SC_3264(__NR3264_fstat, sys_newfstat, sys_newfstat)
/* fs/sync.c */
#define __NR_sync 81
__SYSCALL(__NR_sync, sys_sync)
#define __NR_fsync 82
__SYSCALL(__NR_fsync, sys_fsync)
#define __NR_fdatasync 83
__SYSCALL(__NR_fdatasync, sys_fdatasync)
#define __NR_exit 93
__SYSCALL(__NR_exit, sys_exit)
#define __NR_rt_sigaction 134
__SC_COMP(__NR_rt_sigaction, sys_rt_sigaction, sys_rt_sigaction)
#define __NR_uname 160
__SYSCALL(__NR_uname, sys_newuname)
#define __NR_umask 166
__SYSCALL(__NR_umask, sys_umask)
#define __NR_getpid 172
__SYSCALL(__NR_getpid, sys_getpid)
#define __NR_getuid 174
__SYSCALL(__NR_getuid, sys_getuid)
#define __NR_clone 220
__SYSCALL(__NR_clone, sys_clone)
#define __NR_execve 221
__SC_COMP(__NR_execve, sys_execve, sys_execve)
#define __NR3264_mmap 222
__SC_3264(__NR3264_mmap, sys_mmap_tlx, sys_mmap_tlx)
/* mm/fadvise.c */
#define __NR_mprotect 226
__SYSCALL(__NR_mprotect, sys_mprotect)
#define __NR_wait4 260
__SC_COMP(__NR_wait4, sys_wait4, sys_wait4)


#define NR_syscalls (__NR_syscalls)

};

void bad_mode_tlx(struct pt_regs *regs, int reason, unsigned int esr)
{
}

struct fault_info_tlx {
       int     (*fn)(unsigned long addr, unsigned int esr, struct pt_regs *regs);
       int     sig;
       int     code;
       const char *name;
};

int do_bad_tlx(unsigned long addr, unsigned int esr, struct pt_regs *regs)
{
         return 1;
}


#define ESR_LNX_EXEC            (1 << 24)
#define ESR_EL1_WRITE           (1 << 6)
#define ESR_EL1_CM              (1 << 8)

#define VM_FAULT_ERROR  (VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_HWPOISON | \
                          VM_FAULT_FALLBACK | VM_FAULT_HWPOISON_LARGE)

#define VM_FAULT_BADMAP         0x010000
#define VM_FAULT_BADACCESS      0x020000
#define VM_FAULT_OOM    0x0001
#define VM_FAULT_SIGBUS 0x0002
#define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */
#define VM_FAULT_FALLBACK 0x0800        /* huge page fault failed, fall back to small */
#define VM_FAULT_HWPOISON 0x0010        /* Hit poisoned small page */
#define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. */
#define PSR_MODE_MASK   0x0000000f
#define user_mode(regs) \
         (((regs)->pstate & PSR_MODE_MASK) == PSR_MODE_EL0t)
#define in_atomic()     ((preempt_count_tlx() & ~PREEMPT_ACTIVE) != 0)

int __do_page_fault_tlx(struct mm_struct *mm, unsigned long addr,
				unsigned int mm_flags, unsigned long vm_flags,
				struct task_struct *tsk)
{
	struct vm_area_struct *vma;
	int fault;

	vma = find_vma_tlx(mm, addr);
	return handle_mm_fault_tlx(mm, vma, addr & PAGE_MASK, mm_flags);
}

int __kprobes do_page_fault_tlx(unsigned long addr, unsigned int esr,
					struct pt_regs *regs)
{
	struct task_struct *tsk;
	struct mm_struct *mm;
	int fault, sig, code;
	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;

	tsk = current;
	mm  = tsk->mm;

	if (in_atomic() || !mm)
		goto no_context;

	if (user_mode(regs))
		mm_flags |= FAULT_FLAG_USER;

	if (esr & ESR_LNX_EXEC) {
		vm_flags = VM_EXEC;
	} else if ((esr & ESR_EL1_WRITE) && !(esr & ESR_EL1_CM)) {
		vm_flags = VM_WRITE;
		mm_flags |= FAULT_FLAG_WRITE;
	}


	fault = __do_page_fault_tlx(mm, addr, mm_flags, vm_flags, tsk);

	if (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP |
						VM_FAULT_BADACCESS))))
		return 0;


no_context:

	return 0;
}


int do_translation_fault_tlx(unsigned long addr,
						unsigned int esr,
						struct pt_regs *regs)
{
	if (addr < TASK_SIZE)
		return do_page_fault_tlx(addr, esr, regs);
	return 0;
}

#define __SI_FAULT      (3 << 16)
#define SEGV_MAPERR     (__SI_FAULT|1)  /* address not mapped to object */
#define SEGV_ACCERR     (__SI_FAULT|2)  /* invalid permissions for mapped object */
#define BUS_ADRALN      (__SI_FAULT|1)  /* invalid address alignment */
#define SIGSEGV         11
#define SIGBUS           7


struct fault_info_tlx fault_info_tlx[] = {
	{ do_bad_tlx,		SIGBUS,  0,		"ttbr address size fault"	},
	{ do_bad_tlx,		SIGBUS,  0,		"level 1 address size fault"	},
	{ do_bad_tlx,		SIGBUS,  0,		"level 2 address size fault"	},
	{ do_bad_tlx,		SIGBUS,  0,		"level 3 address size fault"	},
	{ do_translation_fault_tlx,	SIGSEGV, SEGV_MAPERR,	"input address range fault"	},
	{ do_translation_fault_tlx,	SIGSEGV, SEGV_MAPERR,	"level 1 translation fault"	},
	{ do_translation_fault_tlx,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
	{ do_page_fault_tlx,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
	{ do_bad_tlx,		SIGBUS,  0,		"reserved access flag fault"	},
	{ do_page_fault_tlx,	SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
	{ do_page_fault_tlx,	SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},
	{ do_page_fault_tlx,	SIGSEGV, SEGV_ACCERR,	"level 3 access flag fault"	},
	{ do_bad_tlx,		SIGBUS,  0,		"reserved permission fault"	},
	{ do_page_fault_tlx,	SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
	{ do_page_fault_tlx,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
	{ do_page_fault_tlx,	SIGSEGV, SEGV_ACCERR,	"level 3 permission fault"	},
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous external abort"	},
	{ do_bad_tlx,		SIGBUS,  0,		"asynchronous external abort"	},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 18"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 19"			},
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous parity error"	},
	{ do_bad_tlx,		SIGBUS,  0,		"asynchronous parity error"	},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 26"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 27"			},
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
	{ do_bad_tlx,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 32"			},
	{ do_bad_tlx,		SIGBUS,  BUS_ADRALN,	"alignment fault"		},
	{ do_bad_tlx,		SIGBUS,  0,		"debug event"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 35"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 36"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 37"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 38"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 39"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 40"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 41"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 42"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 43"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 44"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 45"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 46"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 47"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 48"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 49"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 50"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 51"			},
	{ do_bad_tlx,		SIGBUS,  0,		"implementation fault (lockdown abort)" },
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 53"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 54"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 55"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 56"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 57"			},
	{ do_bad_tlx,		SIGBUS,  0,		"implementation fault (coprocessor abort)" },
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 59"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 60"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 61"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 62"			},
	{ do_bad_tlx,		SIGBUS,  0,		"unknown 63"			},
};


void do_mem_abort_tlx(unsigned long addr, unsigned int esr,
					 struct pt_regs *regs)
{
	const struct fault_info_tlx *inf = fault_info_tlx + (esr & 63);
	struct siginfo info;

	if (!inf->fn(addr, esr, regs))
		return;
}

void do_sp_pc_abort_tlx(unsigned long addr,
					   unsigned int esr,
					   struct pt_regs *regs)
{
}

void do_undefinstr_tlx(struct pt_regs *regs)
{
}

int do_debug_exception_tlx(unsigned long addr,
                                             unsigned int esr,
                                             struct pt_regs *regs)
{
}

void do_fpsimd_acc_tlx(unsigned int esr, struct pt_regs *regs)
{
}

void finish_task_switch_tlx(struct rq *rq, struct task_struct *prev)
{
	struct mm_struct *mm = rq->prev_mm;
	long prev_state;

	rq->prev_mm = NULL;
	prev_state = prev->state;
	#define for_each_task_context_nr(ctxn)					\
		for ((ctxn) = 0; (ctxn) < perf_nr_task_contexts; (ctxn)++)

					struct perf_event_context *ctx;
					int ctxn;
					for_each_task_context_nr(ctxn) {
						ctx = current->perf_event_ctxp[ctxn];
						if (likely(!ctx))
							continue;
								struct perf_cpu_context *cpuctx;
								cpuctx =  this_cpu_ptr(ctx->pmu->pmu_cpu_context);
								if (cpuctx->task_ctx == ctx)
									continue;
								if (ctx->nr_events)
									cpuctx->task_ctx = ctx;
								struct perf_cpu_context *cpuctx_ = this_cpu_ptr(ctx->pmu->pmu_cpu_context);
								struct list_head *head = &__get_cpu_var(rotation_list_tlx);
								if (list_empty(&cpuctx_->rotation_list_tlx))
									list_add(&cpuctx_->rotation_list_tlx, head);

									if (atomic_read(&__get_cpu_var(perf_branch_stack_events_tlx))) {
												struct task_struct *task = current;
												struct perf_cpu_context *cpuctx;
												struct pmu *pmu;
												unsigned long flags;
												if (prev == task)
													return;
												local_irq_save(flags);
												list_for_each_entry_rcu(pmu, &pmus_tlx, entry) {
													cpuctx = this_cpu_ptr(pmu->pmu_cpu_context);
													if (cpuctx->ctx.nr_branch_stack > 0
															&& pmu->flush_branch_stack) {
															pmu->flush_branch_stack();
														}
												}
												local_irq_restore(flags);
									}
						}

		smp_wmb();
		prev->on_cpu = 0;
			spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
		raw_spin_unlock_irq(&rq->lock);

		if (mm)
			mmdrop_tlx(mm);
		if (unlikely(prev_state == TASK_DEAD)) {
			if (prev->sched_class->task_dead)
				prev->sched_class->task_dead(prev);
			if (atomic_dec_and_test(&prev->usage)) {
					if (atomic_dec_and_test(&(prev->signal)->sigcnt)) {
							kref_put_tlx(&(prev->signal->autogroup)->kref, autogroup_destroy);
							kmem_cache_free_tlx(signal_cachep_tlx, prev->signal);
						}
			}
		}
}




void schedule_tail_tlx(struct task_struct *prev)
{
		struct rq *rq = &__get_cpu_var(runqueues_tlx);

		finish_task_switch_tlx(rq, prev);
		//	post_schedule(rq);
		if (current->set_child_tid)
			put_user(task_pid_vnr_tlx(current), current->set_child_tid);
}

unsigned int processor_id_tlx;
struct cpu_info {
         unsigned int    cpu_id_val;
         unsigned int    cpu_id_mask;
         const char      *cpu_name;
         unsigned long   (*cpu_setup)(void);
};

extern unsigned long __cpu_setup_tlx(void);

struct cpu_info cpu_table_tlx[] = {
         {
                 .cpu_id_val     = 0x000f0000,
                 .cpu_id_mask    = 0x000f0000,
                 .cpu_name       = "AArch64 Processor",
                 .cpu_setup      = __cpu_setup_tlx,
         },
         { /* Empty */ },
 };

u32 main_extable_sort_needed_tlx = 1;


int crypto_shash_update(struct shash_desc *desc, const u8 *data,
                         unsigned int len) {};
void crypto_destroy_tfm(void *mem, struct crypto_tfm *tfm) {};

struct crypto_shash *crypto_alloc_shash(const char *alg_name, u32 type,
                                         u32 mask);

__u16 crc_t10dif_generic(__u16 crc, const unsigned char *buffer, size_t len) {};
struct crypto_tfm *crypto_alloc_base(const char *alg_name, u32 type, u32 mask) {};
 int crypto_has_alg(const char *name, u32 type, u32 mask) {};

 int cap_settime(const struct timespec *ts, const struct timezone *tz) {};
 int cap_capable(const struct cred *cred, struct user_namespace *ns,
                        int cap, int audit) {};
int cap_ptrace_access_check(struct task_struct *child, unsigned int mode) {};
int cap_ptrace_traceme(struct task_struct *parent) {};
int cap_task_setnice(struct task_struct *p, int nice) {};
unsigned long mmap_min_addr;
int cap_task_fix_setuid(struct cred *new, const struct cred *old, int flags) {};
int cap_capget(struct task_struct *target, kernel_cap_t *effective, kernel_cap_t *inheritable, kernel_cap_t *permitted) {};
int cap_task_prctl(int option, unsigned long arg2, unsigned long arg3,
				unsigned long arg4, unsigned long arg5) {};
int cap_task_setscheduler(struct task_struct *p) {};
int cap_task_setioprio(struct task_struct *p, int ioprio) {};
unsigned long dac_mmap_min_addr;
int mmap_min_addr_handler(struct ctl_table *table, int write,
				void __user *buffer, size_t *lenp, loff_t *ppos) {};
int cap_vm_enough_memory(struct mm_struct *mm, long pages) {};
int cap_inode_killpriv(struct dentry *dentry) {};
int cap_inode_need_killpriv(struct dentry *dentry)  {};
int cap_inode_setxattr(struct dentry *dentry, const char *name,
						const void *value, size_t size, int flags) {};
int cap_inode_removexattr(struct dentry *dentry, const char *name) {};
int cap_netlink_send(struct sock *sk, struct sk_buff *skb) {};
int cap_capset(struct cred *new, const struct cred *old,
                       const kernel_cap_t *effective,
                       const kernel_cap_t *inheritable,
                       const kernel_cap_t *permitted) {};
int cap_bprm_secureexec(struct linux_binprm *bprm) {};
void exit_sem(struct task_struct *tsk) {};
void put_ipc_ns(struct ipc_namespace *ns) {};
struct proc_ns_operations {
         const char *name;
         int type;
         void *(*get)(struct task_struct *task);
         void (*put)(void *ns);
         int (*install)(struct nsproxy *nsproxy, void *ns);
         unsigned int (*inum)(void *ns);
 };

struct ipc_ids {
         int in_use;
         unsigned short seq;
         struct rw_semaphore rwsem;
         struct idr ipcs_idr;
         int next_id;
 };

typedef int (*notifier_fn_t)(struct notifier_block *nb,
                         unsigned long action, void *data);

struct notifier_block {
         notifier_fn_t notifier_call;
         struct notifier_block __rcu *next;
         int priority;
 };

struct ipc_namespace {
	atomic_t	count;
	struct ipc_ids	ids[3];

	int		sem_ctls[4];
	int		used_sems;

	unsigned int	msg_ctlmax;
	unsigned int	msg_ctlmnb;
	unsigned int	msg_ctlmni;
	atomic_t	msg_bytes;
	atomic_t	msg_hdrs;
	int		auto_msgmni;

	size_t		shm_ctlmax;
	size_t		shm_ctlall;
	unsigned long	shm_tot;
	int		shm_ctlmni;
	/*
	* Defines whether IPC_RMID is forced for _all_ shm segments regardless
	* of shmctl()
	*/
	int		shm_rmid_forced;

	struct notifier_block ipcns_nb;

	/* The kern_mount of the mqueuefs sb.  We take a ref on it */
	struct vfsmount	*mq_mnt;

	/* # queues in this ns, protected by mq_lock */
	unsigned int    mq_queues_count;

	/* next fields are set through sysctl */
	unsigned int    mq_queues_max;   /* initialized to DFLT_QUEUESMAX */
	unsigned int    mq_msg_max;      /* initialized to DFLT_MSGMAX */
	unsigned int    mq_msgsize_max;  /* initialized to DFLT_MSGSIZEMAX */
	unsigned int    mq_msg_default;
	unsigned int    mq_msgsize_default;

	/* user_ns which owns the ipc ns */
	struct user_namespace *user_ns;

	unsigned int	proc_inum;
};

struct ipc_namespace init_ipc_ns;
const struct proc_ns_operations ipcns_operations;
const char linux_banner[];
struct uts_namespace init_uts_ns;
unsigned int real_root_dev;
struct file_system_type rootfs_fs_type;
bool is_tmpfs;
typedef s32             compat_pid_t;
typedef u32             __compat_uid32_t;
typedef s32             compat_timer_t;
typedef s32             compat_clock_t;
typedef s32             compat_long_t;
typedef s32             compat_int_t;
typedef u32             compat_ulong_t;

typedef union compat_sigval {
         compat_int_t    sival_int;
         compat_uptr_t   sival_ptr;
 } compat_sigval_t;

typedef struct compat_siginfo {
	int si_signo;
	int si_errno;
	int si_code;

	union {
		/* The padding is the same size as AArch64. */
		int _pad[128/sizeof(int) - 3];

		/* kill() */
		struct {
			compat_pid_t _pid;	/* sender's pid */
			__compat_uid32_t _uid;	/* sender's uid */
		} _kill;

		/* POSIX.1b timers */
		struct {
			compat_timer_t _tid;	/* timer id */
			int _overrun;		/* overrun count */
			compat_sigval_t _sigval;	/* same as below */
			int _sys_private;       /* not to be passed to user */
		} _timer;

		/* POSIX.1b signals */
		struct {
			compat_pid_t _pid;	/* sender's pid */
			__compat_uid32_t _uid;	/* sender's uid */
			compat_sigval_t _sigval;
		} _rt;

		/* SIGCHLD */
		struct {
			compat_pid_t _pid;	/* which child */
			__compat_uid32_t _uid;	/* sender's uid */
			int _status;		/* exit code */
			compat_clock_t _utime;
			compat_clock_t _stime;
		} _sigchld;

		/* SIGILL, SIGFPE, SIGSEGV, SIGBUS */
		struct {
			compat_uptr_t _addr; /* faulting insn/memory ref. */
			short _addr_lsb; /* LSB of the reported address */
		} _sigfault;

		/* SIGPOLL */
		struct {
			compat_long_t _band;	/* POLL_IN, POLL_OUT, POLL_MSG */
			int _fd;
		} _sigpoll;
	} _sifields;
} compat_siginfo_t;

void arm64_notify_die(const char *str, struct pt_regs *regs,
                       struct siginfo *info, int err) {};
int __cpu_up(unsigned int cpu, struct task_struct *idle) {};
void release_thread(struct task_struct *dead_task) {};
void update_vsyscall_tz(void) {};
struct user_regset_view *task_user_regset_view(struct task_struct *task) {};
int copy_siginfo_to_user32(compat_siginfo_t __user *to, const siginfo_t *from) {};
int copy_siginfo_from_user32(siginfo_t *to, compat_siginfo_t __user *from) {};

void user_disable_single_step(struct task_struct *t) {};
void user_enable_single_step(struct task_struct *task) {};
unsigned int compat_elf_hwcap;
void (*pm_power_off)(void);
enum reboot_mode {
         REBOOT_COLD = 0,
         REBOOT_WARM,
         REBOOT_HARD,
         REBOOT_SOFT,
         REBOOT_GPIO,
 };

void (*arm_pm_restart)(enum reboot_mode reboot_mode, const char *cmd);
struct cpu_topology {
     int thread_id;
     int core_id;
     int cluster_id;
     cpumask_t thread_sibling;
     cpumask_t core_sibling;
 };
struct cpu_topology cpu_topology[NR_CPUS];
unsigned long elf_hwcap;
void show_regs(struct pt_regs * regs) {};
void show_stack(struct task_struct *tsk, unsigned long *sp) {};
void handle_IRQ(unsigned int i, struct pt_regs * p) {};
unsigned long get_wchan(struct task_struct *p) {};
struct vm_area_struct *get_gate_vma(struct mm_struct *mm) {};
void flush_thread(void) {};
void __pgd_error(const char *file, int line, unsigned long val) {};
long arch_ptrace(struct task_struct *child, long request,
                  unsigned long addr, unsigned long data) {};
void ptrace_disable(struct task_struct *child) {};
long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
                         compat_ulong_t caddr, compat_ulong_t cdata) {};
void machine_restart(char *cmd) {};
void machine_halt(void) {};
void machine_power_off(void) {};
void smp_send_reschedule(int cpu) {};
void *return_address(unsigned int level) {};
unsigned long irq_err_count;
int setup_profiling_timer(unsigned int multiplier) {};
unsigned long profile_pc(struct pt_regs *regs) {};
void update_vsyscall(struct timekeeper *tk){};
//int tick_receive_broadcast(void) {};
void arch_send_call_function_single_ipi(int cpu) {};
void arch_send_call_function_ipi_mask(const struct cpumask *mask) {};
//int tick_receive_broadcast(void);
void tick_broadcast(const struct cpumask *mask) {};
int in_gate_area_no_mm(unsigned long addr) {};
void save_stack_trace(struct stack_trace *trace) {};
u64 perf_reg_abi(struct task_struct *task) {};
typedef struct elf64_shdr {
	Elf64_Word sh_name;           /* Section name, index in string tbl */
	Elf64_Word sh_type;           /* Type of section */
	Elf64_Xword sh_flags;         /* Miscellaneous section attributes */
	Elf64_Addr sh_addr;           /* Section virtual addr at execution */
	Elf64_Off sh_offset;          /* Section file offset */
	Elf64_Xword sh_size;          /* Size of section in bytes */
	Elf64_Word sh_link;           /* Index of another section */
	Elf64_Word sh_info;           /* Additional section information */
	Elf64_Xword sh_addralign;     /* Section alignment */
	Elf64_Xword sh_entsize;       /* Entry size if section holds table */
} Elf64_Shdr;
int apply_relocate_add(Elf64_Shdr *sechdrs,
											 const char *strtab,
											 unsigned int symindex,
											 unsigned int relsec,
											 struct module *me) {};
u64 perf_reg_value(struct pt_regs *regs, int idx) {};
unsigned long perf_misc_flags(struct pt_regs *regs) {};
unsigned long perf_instruction_pointer(struct pt_regs *regs) {};
int perf_reg_validate(u64 mask)  {};
void arch_uninstall_hw_breakpoint(struct perf_event *bp) {};
int arch_install_hw_breakpoint(struct perf_event *bp) {};
int arch_validate_hwbkpt_settings(struct perf_event *bp) {};
int arch_check_bp_in_kernelspace(struct perf_event *bp) {};
void smp_cpus_done(unsigned int max_cpus) {};
int hw_breakpoint_slots(int type)  {};
void smp_send_stop(void)  {};
int __cpu_disable(void)  {};
void __cpu_die(unsigned int cpu)  {};
int show_unhandled_signals;
void hw_breakpoint_pmu_read(struct perf_event *bp)  {};
int hw_breakpoint_exceptions_notify(struct notifier_block *unused,
																		unsigned long val, void *data)  {};
int in_gate_area(struct mm_struct *mm, unsigned long addr)  {};
void __pmd_error(const char *file, int line, unsigned long val)  {};
u32 ethtool_op_get_link(struct net_device *dev) {};
int eth_validate_addr(struct net_device *dev) {};
int eth_mac_addr(struct net_device *dev, void *p) {};
void eth_header_cache_update(struct hh_cache *hh,
                              const struct net_device *dev,
                              const unsigned char *haddr) {};
int eth_header_cache(const struct neighbour *neigh, struct hh_cache *hh, __be16 type) {};
int eth_rebuild_header(struct sk_buff *skb) {};
int eth_header_parse(const struct sk_buff *skb, unsigned char *haddr) {};
	int ndo_dflt_fdb_dump(struct sk_buff *skb,
                       struct netlink_callback *cb,
                       struct net_device *dev,
                       int idx) {};
int rtnl_link_register(struct rtnl_link_ops *ops) {};
unsigned long netdev_boot_base(const char *prefix, int unit) {};
int unregister_netdevice_notifier(struct notifier_block *nb) {};
int register_netdevice_notifier(struct notifier_block *nb) {};
void rtnl_unlock(void) {};
void free_netdev(struct net_device *dev) {};

	int register_netdevice(struct net_device *dev) {};
	struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,
                 void (*setup)(struct net_device *),
                 unsigned int txqs, unsigned int rxqs) {};
void __rtnl_link_unregister(struct rtnl_link_ops *ops){};
int __rtnl_link_register(struct rtnl_link_ops *ops){};
void rtnl_lock(void){};
 int netlink_add_tap(struct netlink_tap *nt){};
int netlink_remove_tap(struct netlink_tap *nt){};
void consume_skb(struct sk_buff *skb){};
void netif_carrier_on(struct net_device *dev){};
void netif_carrier_off(struct net_device *dev){};
void unregister_netdev(struct net_device *dev){};
int register_netdev(struct net_device *dev){};
int netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq){};
int netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq){};
struct net_device *alloc_etherdev_mqs(int sizeof_priv, unsigned int txqs,
                                              unsigned int rxqs) {};
// void rtnl_lock(void) {};
  void netif_device_attach(struct net_device *dev){};
	void __napi_schedule(struct napi_struct *n){};
//	void consume_skb(struct sk_buff *skb){};
  struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
                             int flags, int node){};
int net_ratelimit(void){};
bool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off){};
void skb_trim(struct sk_buff *skb, unsigned int len){};
int netif_receive_skb(struct sk_buff *skb){};
void skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,
                           unsigned int truesize){};
void skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,
                      int size, unsigned int truesize){};
__be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev){};
void napi_complete(struct napi_struct *n){};
bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t prio){};
int skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len){};
unsigned char *skb_put(struct sk_buff *skb, unsigned int len){};
struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
                                    unsigned int length, gfp_t gfp_mask){};
void netif_napi_del(struct napi_struct *napi){};
void netif_device_detach(struct net_device *dev){};
void netif_napi_add(struct net_device *dev, struct napi_struct *napi,
                     int (*poll)(struct napi_struct *, int), int weight){};
 enum skb_free_reason {
          SKB_REASON_CONSUMED,
          SKB_REASON_DROPPED,
  };
void __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason){};
//int skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg, int offset, int len){};
int netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,
                         u16 index) {};
void eth_commit_mac_addr_change(struct net_device *dev, void *p) {};
int eth_prepare_mac_addr_change(struct net_device *dev, void *p) {};
void netdev_notify_peers(struct net_device *dev) {};
void __netif_schedule(struct Qdisc *q) {};
struct net {
				 atomic_t                passive;
};

struct net init_net;
int netif_rx(struct sk_buff *skb) {};
struct header_ops {
         int     (*create) (struct sk_buff *skb, struct net_device *dev,
                            unsigned short type, const void *daddr,
                            const void *saddr, unsigned int len);
};
struct header_ops eth_header_ops;
void kfree_skb(struct sk_buff *skb) {};
int dev_queue_xmit(struct sk_buff *skb) {};
int dev_forward_skb(struct net_device *dev, struct sk_buff *skb){};
int dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv){};
int __dev_forward_skb(struct net_device *dev, struct sk_buff *skb){};
int netif_rx_ni(struct sk_buff *skb){};
struct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask){};
void netif_stacked_transfer_operstate(const struct net_device *rootdev,
                                         struct net_device *dev){};
void unregister_netdevice_many(struct list_head *head){};
int dev_set_mtu(struct net_device *dev, int new_mtu){};
void netdev_update_features(struct net_device *dev){};
int call_netdevice_notifiers(unsigned long val, struct net_device *dev){};
int dev_set_mac_address(struct net_device *dev, struct sockaddr *sa){};
int dev_uc_del(struct net_device *dev, const unsigned char *addr){};
int dev_uc_add(struct net_device *dev, const unsigned char *addr){};
//struct sk_buff *skb_clone(struct sk_buff *skb, gfp_t gfp_mask){};
struct sk_buff *ip_check_defrag(struct sk_buff *skb, u32 user){};
int dev_set_allmulti(struct net_device *dev, int inc){};
int dev_set_promiscuity(struct net_device *dev, int inc){};
void netdev_upper_dev_unlink(struct net_device *dev,
                              struct net_device *upper_dev){};
void unregister_netdevice_queue(struct net_device *dev, struct list_head *head){};

enum rx_handler_result {
         RX_HANDLER_CONSUMED,
         RX_HANDLER_ANOTHER,
         RX_HANDLER_EXACT,
         RX_HANDLER_PASS,
};

typedef enum rx_handler_result rx_handler_result_t;
typedef rx_handler_result_t rx_handler_func_t(struct sk_buff **pskb);

int netdev_rx_handler_register(struct net_device *dev,
                                rx_handler_func_t *rx_handler,
                                void *rx_handler_data){};
//void unregister_netdevice_queue(struct net_device *dev, struct list_head *head){};
int netdev_upper_dev_link(struct net_device *dev,
                           struct net_device *upper_dev){};
int dev_get_nest_level(struct net_device *dev,
                        bool (*type_check)(struct net_device *dev)){};
struct net_device *__dev_get_by_index(struct net *net, int ifindex){};
typedef u64 netdev_features_t;

void dev_mc_unsync(struct net_device *to, struct net_device *from){};
netdev_features_t netdev_increment_features(netdev_features_t all,
         netdev_features_t one, netdev_features_t mask){};
int dev_uc_add_excl(struct net_device *dev, const unsigned char *addr){};

void netdev_rx_handler_unregister(struct net_device *dev){};
void dev_uc_unsync(struct net_device *to, struct net_device *from){};
int dev_mc_sync(struct net_device *to, struct net_device *from){};
int dev_uc_sync(struct net_device *to, struct net_device *from){};
int dev_mc_add_excl(struct net_device *dev, const unsigned char *addr){};
int dev_mc_del(struct net_device *dev, const unsigned char *addr){};
int __ethtool_get_settings(struct net_device *dev, struct ethtool_cmd *cmd){};
void ether_setup(struct net_device *dev){};
int netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid,
         u32 group, gfp_t allocation,
         int (*filter)(struct sock *dsk, struct sk_buff *skb, void *data),
         void *filter_data){};
int netlink_has_listeners(struct sock *sk, unsigned int group){};
struct proc_ns_operations netns_operations;
int register_pernet_subsys(struct pernet_operations *ops){};
enum p9_debug_flags {
          P9_DEBUG_ERROR =        (1<<0),
 };
void _p9_debug(enum p9_debug_flags level, const char *func,
                 const char *fmt, ...){};
int p9_client_clunk(struct p9_fid *fid){};
int
 p9_client_write(struct p9_fid *fid, char *data, const char __user *udata,
                                                         u64 offset, u32 count){};
int p9_client_xattrcreate(struct p9_fid *fid, const char *name,
                         u64 attr_size, int flags){};
struct p9_fid *p9_client_walk(struct p9_fid *oldfid, uint16_t nwname,
                 char **wnames, int clone){};
int
 p9_client_read(struct p9_fid *fid, char *data, char __user *udata, u64 offset,
                                                                 u32 count) {};
struct p9_fid *p9_client_xattrwalk(struct p9_fid *p, const char *c, u64 *u) {};
struct p9_fid *p9_client_attach(struct p9_client *clnt, struct p9_fid *afid,
         char *uname, kuid_t n_uname, char *aname) {};
void p9_client_begin_disconnect(struct p9_client *clnt) {};
void p9_client_disconnect(struct p9_client *clnt) {};
void p9_client_destroy(struct p9_client *clnt) {};
int p9_client_open(struct p9_fid *fid, int mode){};
int p9_is_proto_dotu(struct p9_client *clnt){};
unsigned int p9_debug_level;
int p9_is_proto_dotl(struct p9_client *clnt){};
int p9_client_readdir(struct p9_fid *fid, char *data, u32 count, u64 offset){};
int p9dirent_read(struct p9_client *clnt, char *buf, int len,
                   struct p9_dirent *dirent){};
void p9stat_free(struct p9_wstat *stbuf){};
int p9stat_read(struct p9_client *clnt, char *buf, int len, struct p9_wstat *st){};
int p9_client_getlock_dotl(struct p9_fid *fid, struct p9_getlock *glock){};
int p9_client_lock_dotl(struct p9_fid *fid, struct p9_flock *flock, u8 *status){};
int p9_client_wstat(struct p9_fid *fid, struct p9_wstat *wst){};
int p9_client_fsync(struct p9_fid *fid, int datasync){};
//int p9_client_open(struct p9_fid *fid, int mode){};
int p9_client_link(struct p9_fid *dfid, struct p9_fid *oldfid, char *newname){};
struct p9_stat_dotl *p9_client_getattr_dotl(struct p9_fid *fid,
                                                         u64 request_mask){};
int p9_client_create_dotl(struct p9_fid *ofid, char *name, u32 flags, u32 mode,
                 kgid_t gid, struct p9_qid *qid){};
int p9_client_symlink(struct p9_fid *dfid, char *name, char *symtgt, kgid_t gid,
                 struct p9_qid *qid){};
int p9_client_mkdir_dotl(struct p9_fid *fid, char *name, int mode,
                                 kgid_t gid, struct p9_qid *qid){};
int p9_client_mknod_dotl(struct p9_fid *fid, char *name, int mode,
                         dev_t rdev, kgid_t gid, struct p9_qid *qid){};
int p9_client_readlink(struct p9_fid *fid, char **target){};
int p9_client_setattr(struct p9_fid *fid, struct p9_iattr_dotl *p9attr){};
struct p9_wstat *p9_client_stat(struct p9_fid *fid){};
int p9_client_fcreate(struct p9_fid *fid, char *name, u32 perm, int mode,
                      char *extension){};
struct p9_client *p9_client_create(const char *dev_name, char *options){};
int p9_client_rename(struct p9_fid *fid,
                      struct p9_fid *newdirfid, const char *name){};
int p9_client_renameat(struct p9_fid *olddirfid, const char *old_name,
                        struct p9_fid *newdirfid, const char *new_name){};
int p9_client_unlinkat(struct p9_fid *dfid, const char *name, int flags){};
int p9_client_remove(struct p9_fid *fid){};
int p9_client_statfs(struct p9_fid *fid, struct p9_rstatfs *sb){};
void __put_net(struct net *net){};
struct file_operations bad_sock_fops;
unsigned int sysctl_net_busy_poll;
int __genl_register_family(struct genl_family *family){};
int netlink_unicast(struct sock *ssk, struct sk_buff *skb,
                     u32 portid, int nonblock){};
void *genlmsg_put(struct sk_buff *skb, u32 portid, u32 seq,
                                 struct genl_family *family, int flags, u8 cmd){};
//int netlink_unicast(struct sock *ssk, struct sk_buff *skb,
//                     u32 portid, int nonblock){};
struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q, const int cpu){};
void kill_litter_super(struct super_block *sb){};
loff_t no_llseek(struct file *file, loff_t offset, int whence){};
int single_release(struct inode *inode, struct file *file){};
ssize_t seq_read(struct file *file, char __user *buf, size_t size, loff_t *ppos){};
loff_t seq_lseek(struct file *file, loff_t offset, int whence){};
int nonseekable_open(struct inode *inode, struct file *filp){};
loff_t noop_llseek(struct file *file, loff_t offset, int whence){};
int seq_release(struct inode *inode, struct file *file){};
int simple_open(struct inode *inode, struct file *file){};
loff_t default_llseek(struct file *file, loff_t offset, int whence){};
void __bio_clone_fast(struct bio *bio, struct bio *bio_src){};
struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs){};
void blk_cleanup_queue(struct request_queue *q){};
void put_disk(struct gendisk *disk){};
struct dentry *debugfs_create_dir(const char *name, struct dentry *parent){};
struct dentry *debugfs_create_file(const char *name, umode_t mode,
                                    struct dentry *parent, void *data,
                                    const struct file_operations *fops){};
int register_blkdev(unsigned int major, const char *name){};
int alloc_chrdev_region(dev_t *dev, unsigned baseminor, unsigned count,
                         const char *name){};
void unregister_chrdev_region(dev_t from, unsigned count){};
struct proc_dir_entry *proc_create_data(const char *name, umode_t mode,
                                         struct proc_dir_entry *parent,
                                         const struct file_operations *proc_fops,
                                         void *data){};
struct proc_dir_entry *proc_mkdir(const char *name,
                 struct proc_dir_entry *parent){};
int register_chrdev_region(dev_t from, unsigned count, const char *name){};
void unregister_blkdev(unsigned int major, const char *name){};
void blk_register_region(dev_t devt, unsigned long range, struct module *module,
                          struct kobject *(*probe)(dev_t, int *, void *),
                          int (*lock)(dev_t, void *), void *data){};
void remove_proc_entry(const char *name, struct proc_dir_entry *parent){};
struct ctl_table_header *register_sysctl_table(struct ctl_table *table){};
void add_disk(struct gendisk *disk){};
int unregister_filesystem(struct file_system_type * fs){};
int register_filesystem(struct file_system_type * fs){};
void debugfs_remove_recursive(struct dentry *dentry){};
int cdev_add(struct cdev *p, dev_t dev, unsigned count){};
void cdev_init(struct cdev *cdev, const struct file_operations *fops){};
int __register_chrdev(unsigned int major, unsigned int baseminor,
                       unsigned int count, const char *name,
                       const struct file_operations *fops){};
int seq_printf(struct seq_file *m, const char *f, ...){};
struct dentry *debugfs_rename(struct dentry *old_dir, struct dentry *old_dentry,
                 struct dentry *new_dir, const char *new_name){};
int seq_puts(struct seq_file *m, const char *s){};
struct dentry *debugfs_create_u32(const char *name, umode_t mode,
                                  struct dentry *parent, u32 *value){};
int single_open(struct file *file, int (*show)(struct seq_file *, void *),
                 void *data){};
void sysfs_remove_bin_file(struct kobject *kobj,
                            const struct bin_attribute *attr){};
ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
                                 const void *from, size_t available){};
void kernfs_put(struct kernfs_node *kn){};
struct kernfs_node *kernfs_find_and_get_ns(struct kernfs_node *parent,
                                            const char *name, const void *ns){};
void blk_finish_plug(struct blk_plug *plug){};
void blk_start_plug(struct blk_plug *plug){};
int bio_get_nr_vecs(struct block_device *bdev){};
int bio_add_page(struct bio *bio, struct page *page, unsigned int len,
                  unsigned int offset){};
struct dentry *debugfs_create_x32(const char *name, umode_t mode,
                                  struct dentry *parent, u32 *value){};
int sysfs_create_bin_file(struct kobject *kobj,
                           const struct bin_attribute *attr){};
void submit_bio(int rw, struct bio *bio){};
void zero_fill_bio(struct bio *bio){};
void bio_put(struct bio *bio){};
void bioset_free(struct bio_set *bs){};
struct bio_set *bioset_create(unsigned int pool_size, unsigned int front_pad){};
void set_disk_ro(struct gendisk *disk, int flag){};
void bio_endio(struct bio *bio, int error){};
void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt){};
void blk_limits_io_min(struct queue_limits *limits, unsigned int min){};
int __blkdev_driver_ioctl(struct block_device *bdev, fmode_t mode,
                         unsigned cmd, unsigned long arg){};
int scsi_verify_blk_ioctl(struct block_device *bd, unsigned int cmd){};
bool blk_integrity_is_initialized(struct gendisk *disk){};
int blk_integrity_register(struct gendisk *disk, struct blk_integrity *template){};
void blk_queue_flush(struct request_queue *q, unsigned int flush){};
char *bdevname(struct block_device *bdev, char *buf){};
int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
                      sector_t start){};
void blk_set_stacking_limits(struct queue_limits *lim){};
int blk_integrity_compare(struct gendisk *gd1, struct gendisk *gd2){};
void blk_run_queue_async(struct request_queue *q){};
void bdput(struct block_device *bdev){};
struct block_device *lookup_bdev(const char *pathname){};
void blkdev_put(struct block_device *bdev, fmode_t mode){};
void bd_unlink_disk_holder(struct block_device *bdev, struct gendisk *disk){};
int bd_link_disk_holder(struct block_device *bdev, struct gendisk *disk){};
struct block_device *blkdev_get_by_dev(dev_t dev, fmode_t mode, void *holder){};
void blk_limits_max_hw_sectors(struct queue_limits *limits, unsigned int max_hw_sectors){};
int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,
                       sector_t start){};
int bioset_integrity_create(struct bio_set *bs, int pool_size){};
struct super_block *freeze_bdev(struct block_device *bdev){};
void blk_delay_queue(struct request_queue *q, unsigned long msecs){};
void __tracepoint_block_rq_remap (struct request_queue *q, struct request *rq, dev_t dev,
		 sector_t from){};
struct request *blk_peek_request(struct request_queue *q){};
void blk_start_request(struct request *req){};
int elv_register_queue(struct request_queue *q){};
typedef int (lld_busy_fn) (struct request_queue *q);
typedef int (prep_rq_fn) (struct request_queue *, struct request *);
typedef void (softirq_done_fn)(struct request *);
typedef void (request_fn_proc) (struct request_queue *q);

void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn){};
void blk_queue_prep_rq(struct request_queue *q, prep_rq_fn *pfn){};
void blk_queue_softirq_done(struct request_queue *q, softirq_done_fn *fn){};
struct request_queue *
blk_init_allocated_queue(struct request_queue *q, request_fn_proc *rfn,
                          spinlock_t *lock){};
void del_gendisk(struct gendisk *disk){};
void bio_init(struct bio *bio){};
struct block_device *bdget_disk(struct gendisk *disk, int partno){};
struct gendisk *alloc_disk(int minors){};
struct request_queue *blk_alloc_queue(gfp_t gfp_mask){};
void generic_make_request(struct bio *bio){};
void blk_queue_bio(struct request_queue *q, struct bio *bio){};
void bio_integrity_trim(struct bio *bio, unsigned int offset,
                         unsigned int sectors){};
void bio_advance(struct bio *bio, unsigned bytes){};
int bio_integrity_clone(struct bio *bio, struct bio *bio_src,
                         gfp_t gfp_mask){};
void part_round_stats(int cpu, struct hd_struct *part){};
int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
                       struct bio_set *bs, gfp_t gfp_mask,
                       int (*bio_ctr)(struct bio *, struct bio *, void *),
                       void *data){};
//void del_gendisk(struct gendisk *disk){};
void blk_integrity_unregister(struct gendisk *disk){};
int thaw_bdev(struct block_device *bdev, struct super_block *sb){};
void blk_end_request_all(struct request *rq, int error){};
void blk_start_queue(struct request_queue *q){};
void blk_stop_queue(struct request_queue *q){};
bool blk_update_request(struct request *req, int error, unsigned int nr_bytes){};
void blk_complete_request(struct request *req){};
void __blk_put_request(struct request_queue *q, struct request *req){};
typedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,
                              struct bio_vec *);
typedef void (make_request_fn) (struct request_queue *q, struct bio *bio);
void blk_queue_merge_bvec(struct request_queue *q, merge_bvec_fn *mbfn){};
void blk_queue_bounce_limit(struct request_queue *q, u64 max_addr){};
void blk_queue_make_request(struct request_queue *q, make_request_fn *mfn){};
void  __tracepoint_block_bio_remap (struct request_queue *q, struct bio *bio, dev_t dev,
		 sector_t from){};
void __tracepoint_block_bio_complete (struct request_queue *q, struct bio *bio, int error){};
int blk_lld_busy(struct request_queue *q){};
int blk_insert_cloned_request(struct request_queue *q, struct request *rq){};
void blk_requeue_request(struct request_queue *q, struct request *rq){};
void blk_rq_unprep_clone(struct request *rq){};
void sysfs_remove_link(struct kobject *kobj, const char *name){};
int sysfs_create_link(struct kobject *kobj, struct kobject *target,
                       const char *name) 	{};
void *PDE_DATA(const struct inode *i){};
void cdev_del(struct cdev *i){};
int fasync_helper(int r, struct file *i, int p, struct fasync_struct **o){};
void kill_fasync(struct fasync_struct **i, int p, int o){};
//int fasync_helper(int r, struct file *i, int p, struct fasync_struct **o){};
void blk_abort_request(struct request *i){};
unsigned char scsi_command_size_tbl[];
typedef int (dma_drain_needed_fn)(struct request *);
int blk_queue_dma_drain(struct request_queue *q,
                                dma_drain_needed_fn *dma_drain_needed,
                                void *buf, unsigned int size){};
void blk_queue_update_dma_pad(struct request_queue *q, unsigned int mask){};
void blk_queue_max_hw_sectors(struct request_queue *i, unsigned int o){};
void blk_queue_flush_queueable(struct request_queue *q, bool queueable){};
void blk_queue_update_dma_alignment(struct request_queue *i, int o){};
int sg_scsi_ioctl(struct request_queue *i, struct gendisk *o, fmode_t p,
                          struct scsi_ioctl_command __user * l){};
int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm){};
unsigned long iov_shorten(struct iovec *iov, unsigned long nr_segs, size_t to){};
int blk_rq_map_user(struct request_queue *q, struct request *rq,
                     struct rq_map_data *map_data, void __user *ubuf,
                     unsigned long len, gfp_t gfp_mask){};
typedef void (rq_end_io_fn)(struct request *, int);
void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
                            struct request *rq, int at_head,
                            rq_end_io_fn *done){};
int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
                         struct rq_map_data *map_data, const struct sg_iovec *iov,
                         int iov_count, unsigned int len, gfp_t gfp_mask){};
void blk_rq_set_block_pc(struct request *rq){};
struct request *blk_get_request(struct request_queue *q, int rw, gfp_t gfp_mask){};
void blk_put_request(struct request *req){};
int seq_open(struct file *file, const struct seq_operations *op){};
void blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout){};
void blk_queue_logical_block_size(struct request_queue *q, unsigned short size){};
void blk_queue_io_opt(struct request_queue *q, unsigned int opt){};
void blk_queue_io_min(struct request_queue *q, unsigned int min){};
void blk_queue_physical_block_size(struct request_queue *q, unsigned int size){};
void blk_queue_alignment_offset(struct request_queue *q, unsigned int offset){};
int check_disk_change(struct block_device *bdev){};
int revalidate_disk(struct gendisk *disk){};
int scsi_cmd_blk_ioctl(struct block_device *bd, fmode_t mode,
                        unsigned int cmd, void __user *arg){};
void blk_add_request_payload(struct request *rq, struct page *page,
                 unsigned int len){};
void blk_queue_max_write_same_sectors(struct request_queue *q,
                                     unsigned int max_write_same_sectors){};
int blk_rq_unmap_user(struct bio *bio){};
struct cdev *cdev_alloc(void){};
int single_open_size(struct file *file, int (*show)(struct seq_file *, void *),
                void *data, size_t size){};
void unregister_sysctl_table(struct ctl_table_header * header){};
void bsg_unregister_queue(struct request_queue *q){};
int bsg_register_queue(struct request_queue *q, struct device *parent,
                        const char *name, void (*release)(struct device *)){};
void blk_put_queue(struct request_queue *q){};
bool blk_get_queue(struct request_queue *q){};
typedef enum blk_eh_timer_return (rq_timed_out_fn)(struct request *);
typedef void (unprep_rq_fn) (struct request_queue *, struct request *);

void blk_queue_rq_timed_out(struct request_queue *q, rq_timed_out_fn *fn){};
void blk_queue_unprep_rq(struct request_queue *q, unprep_rq_fn *ufn){};
bool blk_end_request(struct request *rq, int error, unsigned int nr_bytes){};
void blk_unprep_request(struct request *req){};
bool blk_end_request_err(struct request *rq, int error){};
int kblockd_schedule_work(struct work_struct *work){};
void blk_dump_rq_flags(struct request *rq, char *msg){};
int blk_queue_start_tag(struct request_queue *q, struct request *rq){};
int blk_rq_map_integrity_sg(struct request_queue *q, struct bio *bio,
                              struct scatterlist *sglist){};
int blk_rq_count_integrity_sg(struct request_queue *q, struct bio *bio){};
int blk_rq_map_sg(struct request_queue *q, struct request *w, struct scatterlist *e){};
void blk_queue_segment_boundary(struct request_queue *q, unsigned long mask){};
void blk_queue_dma_alignment(struct request_queue *q, int mask){};
void blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size){};
void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments){};
struct request_queue *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock){};
void blk_run_queue(struct request_queue *q){};
int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,
                     unsigned int len, gfp_t gfp_mask){};
int blk_execute_rq(struct request_queue *q, struct gendisk *bd_disk,
                    struct request *rq, int at_head){};
void blk_rq_init(struct request_queue *q, struct request *rq){};
typedef struct {struct page *v;} Sector;
unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p){};
void blk_free_tags(struct blk_queue_tag *bqt){};
int blk_queue_resize_tags(struct request_queue *q, int new_depth){};
void blk_mq_free_tag_set(struct blk_mq_tag_set *set){};
struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set){};
int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set){};
struct request *blk_make_request(struct request_queue *q, struct bio *bio,
                                  gfp_t gfp_mask){};
struct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,
                          gfp_t gfp_mask){};
void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async){};
void blk_mq_stop_hw_queues(struct request_queue *q){};
void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx){};
//void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async){};
void blk_mq_complete_request(struct request *rq){};
void blk_mq_end_io(struct request *rq, int error){};
void bd_set_size(struct block_device *bdev, loff_t size){};
void invalidate_bdev(struct block_device *bdev){};
void sysfs_remove_group(struct kobject *kobj,
                         const struct attribute_group *grp){};
int ioctl_by_bdev(struct block_device *bdev, unsigned cmd, unsigned long arg){};
//void fput(struct file *file){};
struct block_device *bdgrab(struct block_device *bdev){};
//int ioctl_by_bdev(struct block_device *bdev, unsigned cmd, unsigned long arg){};
int set_blocksize(struct block_device *bdev, int size){};
int sysfs_create_group(struct kobject *kobj,
                        const struct attribute_group *grp){};
void set_device_ro(struct block_device *bdev, int flag){};
struct file *fget(unsigned int fd){};
struct bio_set *fs_bio_set;
typedef int (splice_actor)(struct pipe_inode_info *, struct pipe_buffer *,
														struct splice_desc *);
typedef int (splice_direct_actor)(struct pipe_inode_info *,
																struct splice_desc *);
struct kobject *get_disk(struct gendisk *disk){};
char *d_path(const struct path *path, char *buf, int buflen){};
ssize_t __splice_from_pipe(struct pipe_inode_info *pipe, struct splice_desc *sd,
                            splice_actor *actor){};
int vfs_fsync(struct file *file, int datasync){};


ssize_t splice_direct_to_actor(struct file *in, struct splice_desc *sd,
                                splice_direct_actor *actor){};
void __sb_end_write(struct super_block *sb, int level){};
int __sb_start_write(struct super_block *sb, int level, bool wait){};
int vfs_getattr(struct path *path, struct kstat *stat){};
void kill_bdev(struct block_device *bdev){};
struct dentry *debugfs_create_bool(const char *name, umode_t mode,
                                    struct dentry *parent, u32 *value){};
ssize_t simple_read_from_buffer(void __user *to, size_t count, loff_t *ppos,
                                 const void *from, size_t available){};
struct kmem_cache *names_cachep;
struct file *filp_open(const char *filename, int flags, umode_t mode){};
int kernel_read(struct file *file, loff_t offset,
                 char *addr, unsigned long count){};
void sysfs_unmerge_group(struct kobject *kobj,
                        const struct attribute_group *grp){};
int sysfs_merge_group(struct kobject *kobj,
                        const struct attribute_group *grp){};
long sys_mount(char __user *dev_name, char __user *dir_name,
                                 char __user *type, unsigned long flags,
                                 void __user *data){};
struct class block_class;
long sys_chroot(const char __user *filename){};
long sys_chdir(const char __user *filename){};
struct dentry *kern_path_create(int q, const char *w, struct path *e, unsigned int t){};
void done_path_create(struct path *path, struct dentry *dentry){};
int vfs_mkdir(struct inode *q, struct dentry *w, umode_t t){};
int notify_change(struct dentry *q, struct iattr *w, struct inode **t){};
//void done_path_create(struct path *q, struct dentry *w){};
int vfs_mknod(struct inode *q, struct dentry *w, umode_t e, dev_t r){};
int vfs_rmdir(struct inode *q, struct dentry *w){};
void path_put(const struct path *q){};
void dput(struct dentry *dentry){};
struct dentry *kern_path_locked(const char *w, struct path *q){};
int vfs_unlink(struct inode *q, struct dentry *w, struct inode **e){};
struct dentry *mount_single(struct file_system_type *fs_type,
         int flags, void *data,
         int (*fill_super)(struct super_block *, void *, int)){};
void sysfs_remove_file_ns(struct kobject *kobj, const struct attribute *attr,
                           const void *ns){};
int sysfs_create_file_ns(struct kobject *kobj, const struct attribute *attr,
                          const void *ns){};
void sysfs_remove_groups(struct kobject *kobj,
                          const struct attribute_group **groups){};
int sysfs_create_groups(struct kobject *kobj,
                         const struct attribute_group **groups){};
void sysfs_delete_link(struct kobject *kobj, struct kobject *targ,
                         const char *name){};
struct device_type {
         const char *name;
};

struct device_type part_type;
int sysfs_rename_link_ns(struct kobject *kobj, struct kobject *targ,
                          const char *old, const char *new, const void *new_ns){};
bool sysfs_remove_file_self(struct kobject *kobj, const struct attribute *attr){};
void __unregister_chrdev(unsigned int major, unsigned int baseminor,
                         unsigned int count, const char *name){};
void debugfs_remove(struct dentry *dentry){};
void pipe_unlock(struct pipe_inode_info *pipe){};
void pipe_lock(struct pipe_inode_info *pipe){};
void sysfs_notify(struct kobject *kobj, const char *dir, const char *attr){};
loff_t fixed_size_llseek(struct file *file, loff_t offset, int whence, loff_t size){};
void emergency_thaw_all(void){};
void emergency_sync(void){};
void emergency_remount(void){};
void devpts_pty_kill(struct inode *inode){};
void devpts_kill_index(struct inode *ptmx_inode, int idx){};
struct inode *devpts_pty_new(struct inode *ptmx_inode, dev_t device, int index,
                void *priv){};
int devpts_new_index(struct inode *ptmx_inode){};
void *devpts_get_priv(struct inode *pts_inode){};
int iterate_fd(struct files_struct *files, unsigned n,
                 int (*f)(const void *, struct file *, unsigned),
                 const void *p){};
ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_t *pos){};
void proc_tty_unregister_driver(struct tty_driver *driver){};
struct kobj_map *cdev_map;
int sysfs_move_dir_ns(struct kobject *kobj, struct kobject *new_parent_kobj,
                      const void *new_ns){};
int sysfs_create_dir_ns(struct kobject *kobj, const void *ns){};
int sysfs_rename_dir_ns(struct kobject *kobj, const char *new_name,
                         const void *new_ns){};
void sysfs_remove_dir(struct kobject *kobj){};
int seq_release_private(struct inode *inode, struct file *file){};
int simple_attr_release(struct inode *inode, struct file *file){};
ssize_t simple_attr_write(struct file *file, const char __user *buf,
                           size_t len, loff_t *ppos){};
ssize_t simple_attr_read(struct file *file, char __user *buf,
                         size_t len, loff_t *ppos){};
loff_t generic_file_llseek(struct file *file, loff_t offset, int whence){};
int try_to_free_buffers(struct page *page){};
void unlock_buffer(struct buffer_head *bh){};
void set_bh_page(struct buffer_head *bh,
                 struct page *page, unsigned long offset){};
void __lock_buffer(struct buffer_head *bh){};
//void unlock_buffer(struct buffer_head *bh){};
struct block_device *I_BDEV(struct inode *inode){};
int filp_close(struct file *filp, fl_owner_t id){};
void final_putname(struct filename *name){};
struct file *file_open_name(struct filename *name, int flags, umode_t mode){};
struct filename *
getname(const char __user * filename){};
int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
                  sector_t nr_sects, gfp_t gfp_mask, unsigned long flags){};
int blkdev_get(struct block_device *bdev, fmode_t mode, void *holder){};
int sysfs_add_file_mode_ns(struct kernfs_node *parent,
                            const struct attribute *attr, bool is_bin,
                            umode_t mode, const void *ns){};
int seq_path(struct seq_file *m, const struct path *path, const char *esc){};
int bdev_read_page(struct block_device *bdev, sector_t sector,
                         struct page *page){};
sector_t bmap(struct inode *inode, sector_t block){};
int seq_putc(struct seq_file *m, char c){};
int vfs_fsync_range(struct file *file, loff_t start, loff_t end, int datasync){};
int simple_attr_open(struct inode *inode, struct file *file,
                      int (*get)(void *, u64 *), int (*set)(void *, u64),
                      const char *fmt){};
int do_fallocate(struct file *file, int mode, loff_t offset, loff_t len){};
struct list_head *seq_list_start(struct list_head *head, loff_t pos){};
struct list_head *seq_list_next(void *v, struct list_head *head, loff_t *ppos){};
void __tracepoint_writeback_wait_iff_congested (unsigned int usec_timeout, unsigned int usec_delayed){};
int bdev_write_page(struct block_device *bdev, sector_t sector,
                         struct page *page, struct writeback_control *wbc){};
void __tracepoint_writeback_congestion_wait (unsigned int usec_timeout, unsigned int usec_delayed){};
void bdi_writeback_workfn(struct work_struct *work){};
void  __tracepoint_writeback_bdi_unregister (struct backing_dev_info *bdi){};
void  __tracepoint_writeback_bdi_register (struct backing_dev_info *bdi){};
spinlock_t sb_lock;
struct list_head super_blocks;
void wakeup_flusher_threads(long nr_pages, enum wb_reason reason){};
int buffer_heads_over_limit;
void block_invalidatepage(struct page *page, unsigned int offset,
                           unsigned int length){};
unsigned long __fdget(unsigned int fd){};
void bdi_start_writeback(struct backing_dev_info *bdi, long nr_pages,
                         enum wb_reason reason){};
void __tracepoint_balance_dirty_pages (struct backing_dev_info *bdi,
		 unsigned long thresh,
		 unsigned long bg_thresh,
		 unsigned long dirty,
		 unsigned long bdi_thresh,
		 unsigned long bdi_dirty,
		 unsigned long dirty_ratelimit,
		 unsigned long task_ratelimit,
		 unsigned long dirtied,
		 unsigned long period,
		 long pause,
		 unsigned long start_time){};
void __tracepoint_bdi_dirty_ratelimit (struct backing_dev_info *bdi,
		 unsigned long dirty_rate,
		 unsigned long task_ratelimit){};
void __tracepoint_global_dirty_state (unsigned long background_thresh,
		 unsigned long dirty_thresh
	){};
void  __tracepoint_wbc_writepage (struct writeback_control *wbc, struct backing_dev_info *bdi){};
//void
void bdi_start_background_writeback(struct backing_dev_info *bdi){};
int writeback_in_progress(struct backing_dev_info *bdi){};
void __mark_inode_dirty(struct inode *inode, int flags){};
int __set_page_dirty_buffers(struct page *page) {};
void  __tracepoint_writeback_dirty_page (struct page *page, struct address_space *mapping){};
void touch_atime(const struct path *path){};
int file_update_time(struct file *file){};
int bdev_read_only(struct block_device *bdev){};
int compat_log;
char core_pattern[];
int core_uses_pid;
int sysctl_vfs_cache_pressure;
int drop_caches_sysctl_handler(struct ctl_table *table, int write,
          void __user *buffer, size_t *length, loff_t *ppos){};
int sysctl_drop_caches;
int sysctl_hugetlb_shm_group;
unsigned int pipe_max_size, pipe_min_size;
int pipe_proc_fn(struct ctl_table *q, int w, void __user * e, size_t *r, loff_t *t){};
int suid_dumpable;
int sysctl_protected_hardlinks;
int sysctl_protected_symlinks;
int sysctl_protected_hardlinks;
struct ctl_table
{
         const char *procname;           /* Text ID for /proc/sys, or zero */
};
struct files_stat_struct {
         unsigned long nr_files;         /* read only */
};
struct dentry_stat_t {
          long nr_dentry;
};

struct inodes_stat_t {
					long nr_dentry;
};

struct ctl_table inotify_table[];
unsigned long aio_nr;
unsigned long aio_max_nr;
int lease_break_time;
int dir_notify_enable;
int leases_enable;
int proc_nr_dentry(struct ctl_table *table, int write,
                   void __user *buffer, size_t *lenp, loff_t *ppos){};
struct dentry_stat_t dentry_stat;
int sysctl_nr_open_max;
int sysctl_nr_open_min;
int sysctl_nr_open;
struct files_stat_struct files_stat;
int proc_nr_files(struct ctl_table *table, int write,
                   void __user *buffer, size_t *lenp, loff_t *ppos){};
int proc_nr_inodes(struct ctl_table *table, int write,
                    void __user *buffer, size_t *lenp, loff_t *ppos){};
void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)	{};
int seq_write(struct seq_file *seq, const void *data, size_t len){};
unsigned int core_pipe_limit;
struct ctl_table epoll_table[];
struct inodes_stat_t inodes_stat;
void proc_set_size(struct proc_dir_entry * q, loff_t w){};
struct kobject *fs_kobj;
void fd_install(unsigned int fd, struct file *file){};
struct file *anon_inode_getfile(const char *name,
                                  const struct file_operations *fops,
                                  void *priv, int flags){};
void put_unused_fd(unsigned int fd){};
int get_unused_fd_flags(unsigned flags){};
char *mangle_path(char *s, const char *p, const char *esc){};
bool debugfs_initialized(void){};
void proc_sys_poll_notify(struct ctl_table_poll *poll){};
struct kernfs_node *kernfs_node_from_dentry(struct dentry *dentry){};
char *kernfs_path(struct kernfs_node *kn, char *buf,
                                 size_t buflen){};
void kernfs_activate(struct kernfs_node *kn){};
void kernfs_get(struct kernfs_node *kn){};
struct kernfs_node *kernfs_create_dir_ns(struct kernfs_node *parent,
                                          const char *name, umode_t mode,
                                          void *priv, const void *ns){};
void kernfs_remove(struct kernfs_node *kn){};
void kernfs_kill_sb(struct super_block *sb){};
struct kernfs_root *kernfs_root_from_sb(struct super_block *sb){};
//void kernfs_activate(struct kernfs_node *kn){};
void kernfs_destroy_root(struct kernfs_root *root){};
void deactivate_super(struct super_block *sb){};
struct super_block *kernfs_pin_sb(struct kernfs_root *root, const void *ns){};
struct dentry *
 kernfs_mount_ns(struct file_system_type *fs_type, int flags,
                 struct kernfs_root *root, unsigned long magic,
                 bool *new_sb_created, const void *ns){};
struct kernfs_root *kernfs_create_root(struct kernfs_syscall_ops *scops,
                                       unsigned int flags, void *priv){};
void kernfs_unbreak_active_protection(struct kernfs_node *kn){};
int kernfs_rename_ns(struct kernfs_node *kn, struct kernfs_node *new_parent,
                      const char *new_name, const void *new_ns){};
void kernfs_break_active_protection(struct kernfs_node *kn){};
int kernfs_remove_by_name_ns(struct kernfs_node *parent, const char *name,
                              const void *ns){};
struct kernfs_node *__kernfs_create_file(struct kernfs_node *parent,
                                          const char *name,
                                          umode_t mode, loff_t size,
                                          const struct kernfs_ops *ops,
                                          void *priv, const void *ns,
                                          bool name_is_static,
                                         struct lock_class_key *key){};
int kernfs_setattr(struct kernfs_node *kn, const struct iattr *iattr){};
void kernfs_notify(struct kernfs_node *kn){};
long sys_fchown(unsigned int fd, uid_t user, gid_t group){};
long sys_lchown(const char __user *filename,
                                old_uid_t user, old_gid_t group){};
long sys_chown(const char __user *filename,
                                uid_t user, gid_t group){};
void iput(struct inode *inode){};
void ihold(struct inode *inode){};
int seq_open_private(struct file *w, const struct seq_operations *e, int r){};
int seq_bitmap(struct seq_file *m, const unsigned long *bits,
                                   unsigned int nr_bits){};
void proc_remove(struct proc_dir_entry *e){};
int seq_bitmap_list(struct seq_file *m, const unsigned long *bits,
                unsigned int nr_bits){};
//int seq_bitmap(struct seq_file *m, const unsigned long *bits,
//                                    unsigned int nr_bits){};
void set_dumpable(struct mm_struct *mm, int value){};
struct proc_ns *get_proc_ns(struct inode *q){};
struct file *proc_ns_fget(int fd){};
void put_mnt_ns(struct mnt_namespace *ns){};
void timerfd_clock_was_set(void){};
void __set_task_comm(struct task_struct *tsk, const char *buf, bool exec){};
int do_execve(struct filename *q,
                      const char __user * const __user * w,
                      const char __user * const __user * e){};
struct filename *getname_kernel(const char *q){};
char *get_task_comm(char *buf, struct task_struct *tsk){};
//void set_dumpable(struct mm_struct *mm, int value){};
int inode_permission(struct inode *q, int w){};
void do_coredump(const siginfo_t *siginfo){};
void proc_flush_task(struct task_struct *w){};
void free_fs_struct(struct fs_struct *s){};
void put_files_struct(struct files_struct *fs){};
struct fs_struct *copy_fs_struct(struct fs_struct *old){};
void signalfd_cleanup(struct sighand_struct *sighand){};
void exit_aio(struct mm_struct *mm){};
struct files_struct *dup_fd(struct files_struct *s, int *a){};
struct clk *clk_get(struct device *dev, const char *id){};
void clk_put(struct clk *clk){};
int clk_prepare(struct clk *clk){};
void clk_disable(struct clk *clk){};
int clk_enable(struct clk *clk){};
void clk_unprepare(struct clk *clk){};
struct clk *of_clk_get_by_name(struct device_node *np, const char *name){};
//--------------------------
int of_property_read_u32_array(const struct device_node *np,
                                const char *propname, u32 *out_values,
                                size_t sz){};
void  *of_iomap(struct device_node *device, int index){};
int of_irq_get(struct device_node *dev, int index){};
struct device_node *of_get_next_available_child(const struct device_node *node,
        struct device_node *prev){};
struct device_node *of_find_matching_node_and_match(struct device_node *from,
                                         const struct of_device_id *matches,
                                         const struct of_device_id **match){};
int of_device_is_available(const struct device_node *device){};
int of_scan_flat_dt(int (*it)(unsigned long node, const char *uname,
                                       int depth, void *data),
                            void *data){};
void *of_get_flat_dt_prop(unsigned long node, const char *name,
                                         int *size){};
struct of_device_id *of_match_device(
         const struct of_device_id *matches, const struct device *dev){};
struct device_node *of_parse_phandle(const struct device_node *np,
                                            const char *phandle_name,
                                             int index){};
int of_alias_get_id(struct device_node *np, const char *stem){};
struct device_node *of_find_compatible_node(
                                                 struct device_node *from,
                                                 const char *type,
                                                const char *compat){};
//struct device_node *of_find_compatible_node(struct device_node *from,
//         const char *type, const char *compat){};
struct device_node *of_get_next_child(const struct device_node *node,
                                              struct device_node *prev){};
int of_device_is_compatible(const struct device_node *device,
                                    const char *q){};
struct property *of_find_property(const struct device_node *np,
                                          const char *name,
                                          int *lenp){};
struct device_node *of_get_cpu_node(int cpu, unsigned int *thread){};
 int of_irq_get_byname(struct device_node *dev, const char *name){};
//int of_property_read_u32_array(const struct device_node *np,
//                                       const char *propname,
//                                       u32 *out_values,
//                                       size_t sz){};
__be32 *of_prop_next_u32(struct property *prop, const __be32 *cur,
                               u32 *pu){};
struct device_node *of_find_node_by_path(const char *path){};
int of_parse_phandle_with_args(struct device_node *np,
                                              const char *list_name,
                                              const char *cells_name,
                                              int index,
                                              struct of_phandle_args *out_args){};
struct device_node *of_get_next_parent(struct device_node *node){};
unsigned int irq_of_parse_and_map(struct device_node *node, int index){};
struct class *rtc_class;
void rtc_timer_init(struct rtc_timer *timer, void (*f)(void* p), void* data){};
int rtc_timer_start(struct rtc_device *rtc, struct rtc_timer* timer,
                         ktime_t expires, ktime_t period){};
struct rtc_time {
          int tm_sec;
};
ktime_t rtc_tm_to_ktime(struct rtc_time tm){};
int rtc_read_time(struct rtc_device *rtc, struct rtc_time *tm){};
int rtc_timer_cancel(struct rtc_device *rtc, struct rtc_timer* timer){};
int rtc_set_ntp_time(struct timespec now){};
//-------------------------------------
int  input_register_handler(struct input_handler *q){};

void input_inject_event(struct input_handle *handle, unsigned int type, unsigned int code, int value){};
int input_handler_for_each_handle(struct input_handler *handler, void *data,
                                   int (*fn)(struct input_handle *, void *)){};
int input_set_keycode(struct input_dev *dev,
                       const struct input_keymap_entry *ke){};
void input_unregister_handle(struct input_handle *q){};
int input_open_device(struct input_handle *w){};
void input_close_device(struct input_handle *q){};
int input_register_handle(struct input_handle *q){};
void input_unregister_handler(struct input_handler *q){};
int input_get_keycode(struct input_dev *dev, struct input_keymap_entry *ke){};
int sg_big_buff;
//---------------------------------------------------
//
/*










//int vt_kmsg_redirect(int new){};





*/
void get_random_bytes(void *buf, int nbytes){};
unsigned int get_random_int(void){};
void add_device_randomness(const void *q, unsigned int w){};
struct ctl_table random_table[];
int misc_register(struct miscdevice *misc){};

/*
void tty_kref_put(struct tty_struct *tty){};
int sysrq_toggle_support(int enable_mask){};
void proc_clear_tty(struct task_struct *p){};
int vt_move_to_console(unsigned int vt, int alloc){};
int vt_kmsg_redirect(int new){};
int vc_resize(struct vc_data *vc, unsigned int cols, unsigned int lines){};
void unblank_screen(void){};
*/

struct scatterlist {
					unsigned long   page_link;
					unsigned long   offset;
					unsigned long   length;
};

int misc_deregister(struct miscdevice *misc) {};
int register_virtio_driver(struct virtio_driver *drv){};
void virtio_check_driver_offered_feature(const struct virtio_device *vdev,
																					unsigned int fbit){};
bool virtqueue_notify(struct virtqueue *vq){};
bool virtqueue_kick_prepare(struct virtqueue *vq){};
int virtqueue_add_sgs(struct virtqueue *vq,
												struct scatterlist *sgs[],
												unsigned int out_sgs,
												unsigned int in_sgs,
												void *data,
												gfp_t gfp){};
bool virtqueue_kick(struct virtqueue *vq){};
int virtqueue_add_inbuf(struct virtqueue *vq,
												struct scatterlist sg[], unsigned int num,
													void *data,
													gfp_t gfp){};
bool virtqueue_enable_cb(struct virtqueue *vq){};
bool virtqueue_is_broken(struct virtqueue *vq){};
void *virtqueue_get_buf(struct virtqueue *vq, unsigned int *len){};
void virtqueue_disable_cb(struct virtqueue *vq){};
void amba_shutdown(struct device *dev){};
int amba_remove(struct device *dev){};
int amba_probe(struct device *dev){};
struct bus_type amba_bustype;
/*
void class_destroy(struct class *cls){};
struct class * __class_create(struct module *owner,
                                                   const char *name,
                                                   struct lock_class_key *key){};
void class_unregister(struct class *class){};
int __class_register(struct class *class,
                                          struct lock_class_key *key);
int transport_class_register(struct transport_class *q){};
int dev_printk(const char *level, const struct device *dev,
                const char *fmt, ...){};
struct device *device_create(struct class *cls, struct device *parent,
                              dev_t devt, void *drvdata,
                              const char *fmt, ...){};
struct device *get_device(struct device *dev){};
int __must_check device_add(struct device *dev){};
int dev_set_name(struct device *dev, const char *name, ...){};
void device_del(struct device *dev){};
void device_initialize(struct device *dev){};
void put_device(struct device *dev){};
int  __class_register(struct class *class,
                                         struct lock_class_key *key){};
void device_destroy(struct class *cls, dev_t devt){};
void device_del(struct device *dev){};
int  attribute_container_unregister(struct attribute_container *cont){};
struct attribute_container *attribute_container_classdev_to_container(struct device *q){};
int attribute_container_register(struct attribute_container *cont){};
struct device *attribute_container_find_class_device(struct attribute_container *q, struct device *w){};
void device_unregister(struct device *dev){};
struct device *bus_find_device(struct bus_type *bus, struct device *start,
                               void *data,
                              int (*match)(struct device *dev, void *data)){};
void transport_setup_device(struct device *q){};
void transport_configure_device(struct device *w){};
void transport_add_device(struct device *w){};
//void transport_setup_device(struct device *w){};
int device_create_file(struct device *device,
                              const struct device_attribute *entry){};
bool device_remove_file_self(struct device *dev,
                                     const struct device_attribute *attr){};
void transport_remove_device(struct device *q){};
void transport_destroy_device(struct device *w){};
int dev_err(const struct device *dev, const char *fmt, ...){};
int bus_register(struct bus_type *bus){};
void bus_unregister(struct bus_type *bus){};
int  class_interface_register(struct class_interface *w){};
int  driver_register(struct device_driver *drv){};
int device_for_each_child(struct device *dev, void *data,
                      int (*fn)(struct device *dev, void *data)){};
struct device *class_find_device(struct class *class,
                                         struct device *start, const void *data,
                                         int (*match)(struct device *, const void *)){};
void platform_device_put(struct platform_device *pdev){};
void platform_device_del(struct platform_device *pdev){};
int __platform_driver_register(struct platform_driver *w,
                                         struct module *q){};
int platform_device_add(struct platform_device *pdev){};
struct platform_device *platform_device_alloc(const char *name, int id){};
struct device *device_find_child(struct device *dev, void *data,
                                 int (*match)(struct device *dev, void *data)){};
void device_remove_file(struct device *dev,
                                const struct device_attribute *attr){};
typedef int (*dr_match_t)(struct device *dev, void *res, void *match_data);
int kobj_map(struct kobj_map *q, dev_t w, unsigned long e, struct module *r,
              kobj_probe_t *t, int (* y)(dev_t, void *), void *u){};

void *devm_kmalloc(struct device *dev, size_t size, gfp_t gfp){};
struct resource *platform_get_resource_byname(struct platform_device *q,
                                                       unsigned int w,
                                                       const char * e){};
int devres_release(struct device *dev, dr_release_t release,
                          dr_match_t match, void *match_data){};
void devres_free(void *res){};
void devres_add(struct device *dev, void *res){};
void *devres_alloc(dr_release_t release, size_t size, gfp_t gfp){};
int dev_warn(const struct device *dev, const char *fmt, ...){};
int devres_destroy(struct device *dev, dr_release_t release,
                          dr_match_t match, void *match_data){};
//int devres_release(struct device *dev, dr_release_t release,
  //                         dr_match_t match, void *match_data){};
int dma_release_from_coherent(struct device *dev, int order, void *vaddr){};
int dma_alloc_from_coherent(struct device *dev, ssize_t size,
                                        dma_addr_t *dma_handle, void **ret){};
struct device *device_create_vargs(struct class *cls,
                                           struct device *parent,
                                           dev_t devt,
                                           void *drvdata,
                                           const char *fmt,
                                         va_list vargs){};
void register_syscore_ops(struct syscore_ops *ops){};
int  device_register(struct device *dev){};
int subsys_system_register(struct bus_type *subsys,
                            const struct attribute_group **groups){};
void class_interface_unregister(struct class_interface *q){};
struct wakeup_source *wakeup_source_register(const char *name){};
void platform_driver_unregister(struct platform_driver *w){};
struct platform_device *platform_device_register_full(
                  const struct platform_device_info *pdevinfo){};
const char *dev_driver_string(const struct device *dev){};
struct suspend_stats {
          int     success;
};
struct suspend_stats suspend_stats;
int dpm_suspend_end(pm_message_t state){};
void syscore_resume(void){};
bool events_check_enabled;
bool pm_wakeup_pending(void){};
void dpm_resume_end(pm_message_t state){};
int syscore_suspend(void){};
void syscore_shutdown(void){};
void dpm_resume_start(pm_message_t state){};
//int dpm_suspend_end(pm_message_t state){};
int dpm_suspend_start(pm_message_t state){};
void device_shutdown(void){};
int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
                            void *vaddr, size_t size, int *ret){};
void __pm_wakeup_event(struct wakeup_source *ws, unsigned int msec){};
*/

void scsi_host_put(struct Scsi_Host *t){};
int __must_check scsi_add_host_with_dma(struct Scsi_Host *q,
                                               struct device *w,
                                               struct device *e){};
struct Scsi_Host *scsi_host_alloc(struct scsi_host_template *r, int t){};
void scsi_remove_host(struct Scsi_Host *q){};
void scsi_scan_host(struct Scsi_Host *w){};
void scsi_rescan_device(struct device *e){};
int scsi_add_device(struct Scsi_Host *host, uint channel,
                           uint target, uint lun){};
 void scsi_device_put(struct scsi_device *q){};
void scsi_remove_device(struct scsi_device *w){};
struct scsi_device *scsi_device_lookup(struct Scsi_Host *w,
                                               uint q, uint t, uint y) {};
struct vring_used_elem {
         /* Index of start of used descriptor chain. */
         __u32 id;
         /* Total length of the descriptor chain which was used (written to) */
         __u32 len;
 };

struct vring_avail {
         __u16 flags;
         __u16 idx;
         __u16 ring[];
 };

struct vring_used {
         __u16 flags;
         __u16 idx;
         struct vring_used_elem ring[];
 };

struct virtqueue {
				struct list_head list;
				void (*callback)(struct virtqueue *vq);
				const char *name;
				struct virtio_device *vdev;
				unsigned int index;
				unsigned int num_free;
				void *priv;
};

struct vring {
				unsigned int num;

				struct vring_desc *desc;

				struct vring_avail *avail;

				struct vring_used *used;
};

struct vring_virtqueue
{
	struct virtqueue vq;

	/* Actual memory layout for this queue */
	struct vring vring;

	/* Can we use weak barriers? */
	bool weak_barriers;

	/* Other side has made a mess, don't try any more. */
	bool broken;

	/* Host supports indirect buffers */
	bool indirect;

	/* Host publishes avail event idx */
	bool event;

	/* Head of free buffer list. */
	unsigned int free_head;
	/* Number we've added since last sync. */
	unsigned int num_added;

	/* Last used index we've seen. */
	u16 last_used_idx;

	/* How to notify other side. FIXME: commonalize hcalls! */
	bool (*notify)(struct virtqueue *vq);

#ifdef DEBUG
	/* They're supposed to lock for us. */
	unsigned int in_use;

	/* Figure out if their kicks are too delayed. */
	bool last_add_time_valid;
	ktime_t last_add_time;
#endif

	/* Tokens for callbacks. */
	void *data[];
};

#define VRING_USED_F_NO_NOTIFY  1

int vring_need_event_tlx(__u16 event_idx, __u16 new_idx, __u16 old)
 {
         return (__u16)(new_idx - event_idx - 1) < (__u16)(new_idx - old);
 }

void virtio_mb_tlx(bool weak_barriers)
 {
                 smp_mb();
 }

#define vring_avail_event(vr) (*(__u16 *)&(vr)->used->ring[(vr)->num])
#define to_vvq(_vq) container_of(_vq, struct vring_virtqueue, vq)
bool virtqueue_kick_prepare_tlx(struct virtqueue *_vq)
{
	struct vring_virtqueue *vq = to_vvq(_vq);
	u16 new, old;
	bool needs_kick;

//	START_USE(vq);
	/* We need to expose available array entries before checking avail
	* event. */
	virtio_mb_tlx(vq->weak_barriers);

	old = vq->vring.avail->idx - vq->num_added;
	new = vq->vring.avail->idx;
	vq->num_added = 0;

	if (vq->event) {
		needs_kick = vring_need_event_tlx(vring_avail_event(&vq->vring),
								new, old);
	} else {
		needs_kick = !(vq->vring.used->flags & VRING_USED_F_NO_NOTIFY);
	}
//	END_USE(vq);
	return needs_kick;
}

struct sg_table {
         struct scatterlist *sgl;        /* the list */
         unsigned int nents;             /* number of mapped entries */
         unsigned int orig_nents;        /* original size of list */
 };

enum dma_data_direction {
         DMA_BIDIRECTIONAL = 0,
         DMA_TO_DEVICE = 1,
         DMA_FROM_DEVICE = 2,
         DMA_NONE = 3,
 };
struct scsi_data_buffer {
         struct sg_table table;
         unsigned length;
         int resid;
};
struct scsi_pointer {
         char *ptr;              /* data pointer */
         int this_residual;      /* left in this buffer */
         struct scatterlist *buffer;     /* which buffer */
         int buffers_residual;   /* how many buffers left */

         dma_addr_t dma_handle;

         volatile int Status;
         volatile int Message;
         volatile int have_data_in;
         volatile int sent_command;
         volatile int phase;
 };

struct request {
	void *special;		/* opaque pointer available for LLD use */
	struct request *next_rq;
};

struct scsi_cmnd {
	struct scsi_device *device;
	struct list_head list;  /* scsi_cmnd participates in queue lists */
	struct list_head eh_entry; /* entry for the host eh_cmd_q */
	struct delayed_work abort_work;
	int eh_eflags;		/* Used by error handlr */

	/*
	* A SCSI Command is assigned a nonzero serial_number before passed
	* to the driver's queue command function.  The serial_number is
	* cleared when scsi_done is entered indicating that the command
	* has been completed.  It is a bug for LLDDs to use this number
	* for purposes other than printk (and even that is only useful
	* for debugging).
	*/
	unsigned long serial_number;

	/*
	* This is set to jiffies as it was when the command was first
	* allocated.  It is used to time how long the command has
	* been outstanding
	*/
	unsigned long jiffies_at_alloc;

	int retries;
	int allowed;

	unsigned char prot_op;
	unsigned char prot_type;

	unsigned short cmd_len;
	enum dma_data_direction sc_data_direction;

	/* These elements define the operation we are about to perform */
	unsigned char *cmnd;


	/* These elements define the operation we ultimately want to perform */
	struct scsi_data_buffer sdb;
	struct scsi_data_buffer *prot_sdb;

	unsigned underflow;	/* Return error if less than
					this amount is transferred */

	unsigned transfersize;	/* How much we are guaranteed to
					transfer with each SCSI transfer
					(ie, between disconnect /
					reconnects.   Probably == sector
					size */

	struct request *request;	/* The command we are
								working on */

#define SCSI_SENSE_BUFFERSIZE 	96
	unsigned char *sense_buffer;
				/* obtained by REQUEST SENSE when
				* CHECK CONDITION is received on original
				* command (auto-sense) */

	/* Low-level done function - can be used by low-level driver to point
	*        to completion function.  Not used by mid/upper level code. */
	void (*scsi_done) (struct scsi_cmnd *);

	/*
	* The following fields can be written to by the host specific code.
	* Everything else should be left alone.
	*/
	struct scsi_pointer SCp;	/* Scratchpad used by some host adapters */

	unsigned char *host_scribble;	/* The host adapter is allowed to
					* call scsi_malloc and get some memory
					* and hang it here.  The host adapter
					* is also expected to call scsi_free
					* to release this memory.  (The memory
					* obtained by scsi_malloc is guaranteed
					* to be at an address < 16Mb). */

	int result;		/* Status code from lower level driver */

	unsigned char tag;	/* SCSI-II queued command tag */
};

struct scatterlist *scsi_prot_sglist_tlx(struct scsi_cmnd *cmd)
{
     return cmd->prot_sdb ? cmd->prot_sdb->table.sgl : NULL;
}



struct vring_desc {
         /* Address (guest-physical). */
         __u64 addr;
         /* Length. */
         __u32 len;
         /* The flags as indicated above. */
         __u16 flags;
         /* We chain unused descriptors via this, too */
         __u16 next;
 };
#define VRING_DESC_F_NEXT       1
#define VRING_DESC_F_WRITE      2

struct page *sg_page(struct scatterlist *sg)
{
         return (struct page *)((sg)->page_link & ~0x3);
}

dma_addr_t sg_phys(struct scatterlist *sg)
{
       return page_to_phys(sg_page(sg)) + sg->offset;
}

int virtqueue_add_tlx(struct virtqueue *_vq,
				struct scatterlist *sgs[],
				struct scatterlist *(*next)
					(struct scatterlist *, unsigned int *),
				unsigned int total_out,
				unsigned int total_in,
				unsigned int out_sgs,
				unsigned int in_sgs,
				void *data,
				gfp_t gfp)
{
	struct vring_virtqueue *vq = to_vvq(_vq);
	struct scatterlist *sg;
	unsigned int i, n, avail, prev, total_sg;
	int head;

	total_sg = total_in + total_out;
	vq->vq.num_free -= total_sg;
	head = i = vq->free_head;
	for (n = 0; n < out_sgs; n++) {
		for (sg = sgs[n]; sg; sg = next(sg, &total_out)) {
			vq->vring.desc[i].flags = VRING_DESC_F_NEXT;
			vq->vring.desc[i].addr = sg_phys(sg);
			vq->vring.desc[i].len = sg->length;
			prev = i;
			i = vq->vring.desc[i].next;
		}
	}
	for (; n < (out_sgs + in_sgs); n++) {
		for (sg = sgs[n]; sg; sg = next(sg, &total_in)) {
			vq->vring.desc[i].flags = VRING_DESC_F_NEXT|VRING_DESC_F_WRITE;
			vq->vring.desc[i].addr = sg_phys(sg);
			vq->vring.desc[i].len = sg->length;
			prev = i;
			i = vq->vring.desc[i].next;
		}
	}
	vq->vring.desc[prev].flags &= ~VRING_DESC_F_NEXT;
	vq->free_head = i;

add_head:
	vq->data[head] = data;
	avail = (vq->vring.avail->idx & (vq->vring.num-1));
	vq->vring.avail->ring[avail] = head;
	virtio_mb_tlx(vq->weak_barriers);
	vq->vring.avail->idx++;
	vq->num_added++;
	return 0;
}

#define sg_chain_ptr(sg)        \
				((struct scatterlist *) ((sg)->page_link & ~0x03))
#define sg_is_chain(sg)         ((sg)->page_link & 0x01)
#define sg_is_last(sg)          ((sg)->page_link & 0x02)

struct scatterlist *sg_next_tlx(struct scatterlist *sg)
{
				if (sg_is_last(sg))
								return NULL;

				sg++;
				if (unlikely(sg_is_chain(sg)))
								sg = sg_chain_ptr(sg);

				return sg;
}

static inline struct scatterlist *sg_next_chained(struct scatterlist *sg,
																									unsigned int *count)
{
				return sg_next_tlx(sg);
}

int virtqueue_add_sgs_tlx(struct virtqueue *_vq,
					struct scatterlist *sgs[],
					unsigned int out_sgs,
					unsigned int in_sgs,
					void *data,
					gfp_t gfp)
{
	unsigned int i, total_out, total_in;


	for (i = total_out = total_in = 0; i < out_sgs; i++) {
		struct scatterlist *sg;
		for (sg = sgs[i]; sg; sg = sg_next_tlx(sg))
			total_out++;
	}
	for (; i < out_sgs + in_sgs; i++) {
		struct scatterlist *sg;
		for (sg = sgs[i]; sg; sg = sg_next_tlx(sg))
			total_in++;
	}
	return virtqueue_add_tlx(_vq, sgs, sg_next_chained,
					total_out, total_in, out_sgs, in_sgs, data, gfp);
}

unsigned scsi_prot_sg_count_tlx(struct scsi_cmnd *cmd)
{
         return cmd->prot_sdb ? cmd->prot_sdb->table.nents : 0;
}

#define blk_bidi_rq(rq)         ((rq)->next_rq != NULL)

int scsi_bidi_cmnd(struct scsi_cmnd *cmd)
{
         return blk_bidi_rq(cmd->request) &&
                 (cmd->request->next_rq->special != NULL);
}

struct scsi_data_buffer *scsi_out_tlx(struct scsi_cmnd *cmd)
{
         return &cmd->sdb;
}

struct scsi_data_buffer *scsi_in_tlx(struct scsi_cmnd *cmd)
{
         return scsi_bidi_cmnd(cmd) ?
                 cmd->request->next_rq->special : &cmd->sdb;
}

#define VIRTIO_SCSI_CDB_SIZE   32

struct virtio_scsi_cmd_req {
          u8 lun[8];              /* Logical Unit Number */
          u64 tag;                /* Command identifier */
          u8 task_attr;           /* Task attribute */
          u8 prio;                /* SAM command priority field */
          u8 crn;
 };

struct virtio_scsi_cmd_req_pi {
         u8 lun[8];              /* Logical Unit Number */
         u64 tag;                /* Command identifier */
         u8 task_attr;           /* Task attribute */
         u8 prio;                /* SAM command priority field */
         u8 crn;
         u32 pi_bytesout;        /* DataOUT PI Number of bytes */
         u32 pi_bytesin;         /* DataIN PI Number of bytes */
				u8 cdb[VIRTIO_SCSI_CDB_SIZE];
};

struct virtio_scsi_cmd_resp {
				u32 sense_len;          /* Sense data length */
				u32 resid;              /* Residual bytes in data buffer */
				u16 status_qualifier;   /* Status qualifier */
				u8 status;              /* Command completion status */
				u8 response;            /* Response values */
};

struct virtio_scsi_cmd {
          struct scsi_cmnd *sc;
					union {
						struct virtio_scsi_cmd_req       cmd;
						struct virtio_scsi_cmd_req_pi    cmd_pi;

					} req;
					union {
						struct virtio_scsi_cmd_resp      cmd;
					} resp;
};





int virtscsi_add_cmd_tlx(struct virtqueue *vq,
					struct virtio_scsi_cmd *cmd,
					size_t req_size, size_t resp_size)
{
	struct scsi_cmnd *sc = cmd->sc;
	struct scatterlist *sgs[6], req, resp;
	struct sg_table *out, *in;
	unsigned out_num = 0, in_num = 0;

	out = in = NULL;

	if (sc && sc->sc_data_direction != DMA_NONE) {
		if (sc->sc_data_direction != DMA_FROM_DEVICE)
			out = &scsi_out_tlx(sc)->table;
		if (sc->sc_data_direction != DMA_TO_DEVICE)
			in = &scsi_in_tlx(sc)->table;
	}

	/* Request header.  */
//	sg_init_one(&req, &cmd->req, req_size);
	sgs[out_num++] = &req;

	/* Data-out buffer.  */
	if (out) {
		/* Place WRITE protection SGLs before Data OUT payload */
		if (scsi_prot_sg_count_tlx(sc))
			sgs[out_num++] = scsi_prot_sglist_tlx(sc);
		sgs[out_num++] = out->sgl;
	}

	/* Response header.  */
//	sg_init_one(&resp, &cmd->resp, resp_size);
	sgs[out_num + in_num++] = &resp;

	/* Data-in buffer */
	if (in) {
		/* Place READ protection SGLs before Data IN payload */
		if (scsi_prot_sg_count_tlx(sc))
			sgs[out_num + in_num++] = scsi_prot_sglist_tlx(sc);
		sgs[out_num + in_num++] = in->sgl;
	}

	return virtqueue_add_sgs_tlx(vq, sgs, out_num, in_num, cmd, GFP_ATOMIC);
}

typedef void vq_callback_t(struct virtqueue *);

struct virtio_config_ops {
	void (*get)(struct virtio_device *vdev, unsigned offset,
				void *buf, unsigned len);
	void (*set)(struct virtio_device *vdev, unsigned offset,
				const void *buf, unsigned len);
	u8 (*get_status)(struct virtio_device *vdev);
	void (*set_status)(struct virtio_device *vdev, u8 status);
	void (*reset)(struct virtio_device *vdev);
	int (*find_vqs)(struct virtio_device *, unsigned nvqs,
			struct virtqueue *vqs[],
			vq_callback_t *callbacks[],
			const char *names[]);
	void (*del_vqs)(struct virtio_device *);
	u32 (*get_features)(struct virtio_device *vdev);
	void (*finalize_features)(struct virtio_device *vdev);
	const char *(*bus_name)(struct virtio_device *vdev);
	int (*set_vq_affinity)(struct virtqueue *vq, int cpu);
};

struct virtio_device {
				void *priv;
				const struct virtio_config_ops *config;
};

struct virtio_scsi_vq {
				/* Protects vq */
				spinlock_t vq_lock;
				struct virtqueue *vq;
};

struct virtio_scsi {
	struct virtio_device *vdev;
	struct virtio_scsi_vq req_vqs[];
};

struct Scsi_Host *virtio_scsi_host(struct virtio_device *vdev)
{
         return vdev->priv;
}


void *scsi_cmd_priv(struct scsi_cmnd *cmd)
{
         return cmd + 1;
}




int virtscsi_queuecommand_tlx(struct virtio_scsi *vscsi,
				struct virtio_scsi_vq *req_vq,
				struct scsi_cmnd *sc)
{
	struct Scsi_Host *shost = virtio_scsi_host(vscsi->vdev);
	struct virtio_scsi_cmd *cmd = scsi_cmd_priv(sc);
	int req_size;

	memset_tlx(cmd, 0, sizeof(*cmd));
	cmd->sc = sc;
		memcpy_tlx(cmd->req.cmd_pi.cdb, sc->cmnd, sc->cmd_len);
		req_size = sizeof(cmd->req.cmd_pi);
	struct virtio_scsi_vq *vq = req_vq;
	size_t resp_size = sizeof(cmd->resp.cmd) != 0;
	unsigned long flags;
	int err;
	bool needs_kick = false;

	spin_lock_irqsave(&vq->vq_lock, flags);
	err = virtscsi_add_cmd_tlx(vq->vq, cmd, req_size, resp_size);
	if (!err)
		needs_kick = virtqueue_kick_prepare_tlx(vq->vq);

	spin_unlock_irqrestore_tlx(&vq->vq_lock, flags);

	if (needs_kick) {
				struct virtqueue *_vq = vq->vq;
				struct vring_virtqueue *vq2 = to_vvq(_vq);

				if (!vq2->notify(_vq)) {
								vq2->broken = true;
				}
	};
	return 0;
}
struct scsi_target {
		struct device           dev;
		void *hostdata;         /* available to low-level driver */
};

#define to_scsi_target(d)       container_of(d, struct scsi_target, dev)
struct scsi_device {
	struct device           sdev_gendev;
};

struct scsi_target *scsi_target_tlx(struct scsi_device *sdev)
{
         return to_scsi_target(sdev->sdev_gendev.parent);
}

struct Scsi_Host {
		unsigned long hostdata[0];
};

void *shost_priv_tlx(struct Scsi_Host *shost)
 {
       return (void *)shost->hostdata;
 }
struct virtio_scsi_target_state {
	atomic_t reqs;
};

int virtscsi_queuecommand_single_tlx(struct Scsi_Host *sh,
					struct scsi_cmnd *sc)
{
	struct virtio_scsi *vscsi = shost_priv_tlx(sh);
	struct virtio_scsi_target_state *tgt =
				scsi_target_tlx(sc->device)->hostdata;

	atomic_inc_tlx(&tgt->reqs);
	return virtscsi_queuecommand_tlx(vscsi, &vscsi->req_vqs[0], sc);
}


static inline u8 virtio_cread8(struct virtio_device *vdev, unsigned int offset)
{
	u8 ret;
	vdev->config->get(vdev, offset, &ret, sizeof(ret));
	return ret;
}

static inline u16 virtio_cread16(struct virtio_device *vdev,
				unsigned int offset)
{
	u16 ret;
	vdev->config->get(vdev, offset, &ret, sizeof(ret));
	return ret;
}

static inline u32 virtio_cread32(struct virtio_device *vdev,
				unsigned int offset)
{
	u32 ret;
	vdev->config->get(vdev, offset, &ret, sizeof(ret));
	return ret;
}

static inline u64 virtio_cread64(struct virtio_device *vdev,
				unsigned int offset)
{
	u64 ret;
	vdev->config->get(vdev, offset, &ret, sizeof(ret));
	return ret;
}

#define virtio_cread(vdev, structname, member, ptr)			\
	do {								\
		/* Must match the member's type, and be integer */	\
		if (!typecheck(typeof((((structname*)0)->member)), *(ptr))) \
			(*ptr) = 1;					\
									\
		switch (sizeof(*ptr)) {					\
		case 1:							\
			*(ptr) = virtio_cread8(vdev,			\
								offsetof(structname, member)); \
			break;						\
		case 2:							\
			*(ptr) = virtio_cread16(vdev,			\
						offsetof(structname, member)); \
			break;						\
		case 4:							\
			*(ptr) = virtio_cread32(vdev,			\
						offsetof(structname, member)); \
			break;						\
		case 8:							\
			*(ptr) = virtio_cread64(vdev,			\
						offsetof(structname, member)); \
			break;						\
		}							\
	} while(0)

struct virtio_scsi_config {
          u32 num_queues;
          u32 seg_max;
          u32 max_sectors;
          u32 cmd_per_lun;
         u32 event_info_size;
         u32 sense_size;
         u32 cdb_size;
         u16 max_channel;
         u16 max_target;
         u32 max_lun;
};

#define virtscsi_config_get(vdev, fld) \
	({ \
		typeof(((struct virtio_scsi_config *)0)->fld) __val; \
		virtio_cread(vdev, struct virtio_scsi_config, fld, &__val); \
		__val; \
	})

struct scsi_host_template {
	int (* queuecommand)(struct Scsi_Host *, struct scsi_cmnd *);
};

struct scsi_host_template virtscsi_host_template_multi_tlx = {
	.queuecommand = virtscsi_queuecommand_single_tlx,
};


struct scsi_host_template virtscsi_host_template_single_tlx = {};

int virtscsi_probe_tlx(struct virtio_device *vdev)
{
	struct Scsi_Host *shost;
	struct virtio_scsi *vscsi;
	int err, host_prot;
	u32 sg_elems, num_targets;
	u32 cmd_per_lun;
	u32 num_queues;
	struct scsi_host_template *hostt;

	/* We need to know how many queues before we allocate. */
	num_queues = virtscsi_config_get(vdev, num_queues) ? : 1;

	num_targets = virtscsi_config_get(vdev, max_target) + 1;

	if (num_queues == 1)
		hostt = &virtscsi_host_template_single_tlx;
	else
		hostt = &virtscsi_host_template_multi_tlx;

	shost = scsi_host_alloc(hostt,
		sizeof(*vscsi) + sizeof(vscsi->req_vqs[0]) * num_queues);


	return 0;

}

struct virtio_driver {
         int (*probe)(struct virtio_device *dev);
};


struct virtio_driver virtio_scsi_driver_tlx = {
	.probe = virtscsi_probe_tlx,
};

#define __exit          __section(.exit.text)


static void __exit fini(void)
{
//	unregister_virtio_driver(&virtio_scsi_driver_tlx);
}

static const struct of_device_id_tlx
irqchip_of_match_end __used __section(__irqchip_of_table_end);

struct gpio_chip {
	const char		*label;
	struct device		*dev;
	struct module		*owner;
	struct list_head        list;

	int			(*request)(struct gpio_chip *chip,
						unsigned offset);
	void			(*free)(struct gpio_chip *chip,
						unsigned offset);
	int			(*get_direction)(struct gpio_chip *chip,
						unsigned offset);
	int			(*direction_input)(struct gpio_chip *chip,
						unsigned offset);
	int			(*direction_output)(struct gpio_chip *chip,
						unsigned offset, int value);
	int			(*get)(struct gpio_chip *chip,
						unsigned offset);
	void			(*set)(struct gpio_chip *chip,
						unsigned offset, int value);
	int			(*set_debounce)(struct gpio_chip *chip,
						unsigned offset,
						unsigned debounce);

	int			(*to_irq)(struct gpio_chip *chip,
						unsigned offset);

	void			(*dbg_show)(struct seq_file *s,
						struct gpio_chip *chip);
	int			base;
	u16			ngpio;
	struct gpio_desc	*desc;
	const char		*const *names;
	bool			can_sleep;
	bool			exported;

#ifdef CONFIG_GPIOLIB_IRQCHIP
	/*
	 * With CONFIG_GPIO_IRQCHIP we get an irqchip inside the gpiolib
	 * to handle IRQs for most practical cases.
	 */
	struct irq_chip		*irqchip;
	struct irq_domain	*irqdomain;
	unsigned int		irq_base;
	irq_flow_handler_t	irq_handler;
	unsigned int		irq_default_type;
#endif

#if defined(CONFIG_OF_GPIO)
	/*
	 * If CONFIG_OF is enabled, then all GPIO controllers described in the
	 * device tree automatically may have an OF translation
	 */
	struct device_node *of_node;
	int of_gpio_n_cells;
	int (*of_xlate)(struct gpio_chip *gc,
			const struct of_phandle_args *gpiospec, u32 *flags);
#endif
#ifdef CONFIG_PINCTRL
	/*
	 * If CONFIG_PINCTRL is enabled, then gpio controllers can optionally
	 * describe the actual pin range which they serve in an SoC. This
	 * information would be used by pinctrl subsystem to configure
	 * corresponding pins for gpio usage.
	 */
	struct list_head pin_ranges;
#endif
};

struct bgpio_chip {
         struct gpio_chip gc;
         unsigned long (*read_reg)(void __iomem *reg);
         void (*write_reg)(void __iomem *reg, unsigned long data);
         void __iomem *reg_dat;
         void __iomem *reg_set;
         void __iomem *reg_clr;
         void __iomem *reg_dir;
         int bits;
         unsigned long (*pin2mask)(struct bgpio_chip *bgc, unsigned int pin);
         spinlock_t lock;
         unsigned long data;
         unsigned long dir;
  };

	static inline struct bgpio_chip *to_bgpio_chip(struct gpio_chip *gc)
	 {
	         return container_of(gc, struct bgpio_chip, gc);
	  }

int bgpio_dir_out_inv_tlx(struct gpio_chip *gc, unsigned int gpio, int val)
{
		struct bgpio_chip *bgc = to_bgpio_chip(gc);
		unsigned long flags;

		gc->set(gc, gpio, val);

		spin_lock_irqsave(&bgc->lock, flags);

		bgc->dir &= ~bgc->pin2mask(bgc, gpio);
		bgc->write_reg(bgc->reg_dir, bgc->dir);

		spin_unlock_irqrestore_tlx(&bgc->lock, flags);

		return 0;
}


int bgpio_dir_in_inv_tlx(struct gpio_chip *gc, unsigned int gpio)
{
	struct bgpio_chip *bgc = to_bgpio_chip(gc);
	unsigned long flags;

	spin_lock_irqsave(&bgc->lock, flags);

	bgc->dir |= bgc->pin2mask(bgc, gpio);
	bgc->write_reg(bgc->reg_dir, bgc->dir);

	spin_unlock_irqrestore_tlx(&bgc->lock, flags);

	return 0;
}
static inline void __raw_writeb(u8 val, volatile void __iomem *addr)
 {
         asm volatile("strb %w0, [%1]" : : "r" (val), "r" (addr));
  }

static inline u8 __raw_readb(const volatile void __iomem *addr)
 {
	         u8 val;
	         asm volatile("ldrb %w0, [%1]" : "=r" (val) : "r" (addr));
         return val;
}

static inline void __raw_writeq(u64 val, volatile void __iomem *addr)
{
         asm volatile("str %0, [%1]" : : "r" (val), "r" (addr));
}

static inline u64 __raw_readq(const volatile void __iomem *addr)
{
         u64 val;
         asm volatile("ldr %0, [%1]" : "=r" (val) : "r" (addr));
         return val;
}
#define cpu_to_le64 __cpu_to_le64
#define le64_to_cpu __le64_to_cpu
#define ___constant_swab16(x) ((__u16)(                         \
         (((__u16)(x) & (__u16)0x00ffU) << 8) |                  \
         (((__u16)(x) & (__u16)0xff00U) >> 8)))
static inline __attribute_const__ __u16 __fswab16(__u16 val)
{
         return ___constant_swab16(val);
}
#define __cpu_to_le64(x) ((__force __le64)(__u64)(x))
#define __le64_to_cpu(x) ((__force __u64)(__le64)(x))

#define writeb_relaxed(v,c)     ((void)__raw_writeb((v),(c)))
#define readb_relaxed(c)        ({ u8  __v = __raw_readb(c); __v; })

int dev_err_tlx(const struct device *dev, const char *fmt, ...)
{ return 0; };

#define __be16_to_cpu(x) __swab16((__force __u16)(__be16)(x))
#define __be32_to_cpu(x) __swab32((__force __u32)(__be32)(x))

#define writeb(v,c)             ({ __iowmb(); writeb_relaxed((v),(c)); })
#define readb(c)                ({ u8  __v = readb_relaxed(c); __iormb(); __v; })
#define writel(v,c)             ({ __iowmb(); writel_relaxed((v),(c)); })
#define iowrite16be(v, addr)    __raw_writew(__cpu_to_be16(v), addr)
#define iowrite32be(v, addr)    __raw_writel(__cpu_to_be32(v), addr)
#define ioread16be(addr)        __be16_to_cpu(__raw_readw(addr))
#define ioread32be(addr)        __be32_to_cpu(__raw_readl(addr))

#define readq_relaxed(c)        ({ u64 __v = le64_to_cpu((__force __le64)__raw_readq(c)); __v; })
#define writeq_relaxed(v,c)     ((void)__raw_writeq((__force u64)cpu_to_le64(v),(c)))


#define writeq(v,c)             ({ __iowmb(); writeq_relaxed((v),(c)); })
#define readq(c)                ({ u64 __v = readq_relaxed(c); __iormb(); __v; })
#define __cpu_to_be16(x) ((__force __be16)__swab16((x)))
#define __swab16(x)                             \
         (__builtin_constant_p((__u16)(x)) ?     \
         ___constant_swab16(x) :                 \
         __fswab16(x))

static unsigned long bgpio_pin2mask(struct bgpio_chip *bgc, unsigned int pin)
{
	return 1 << pin;
}

static unsigned long bgpio_pin2mask_be(struct bgpio_chip *bgc,
				       unsigned int pin)
{
	return 1 << (bgc->bits - 1 - pin);
}


static void bgpio_write16be(void __iomem *reg, unsigned long data)
{
	iowrite16be(data, reg);
}

static unsigned long bgpio_read16be(void __iomem *reg)
{
	return ioread16be(reg);
}

static void bgpio_write32be(void __iomem *reg, unsigned long data)
{
	iowrite32be(data, reg);
}

static unsigned long bgpio_read32be(void __iomem *reg)
{
	return ioread32be(reg);
}

static void bgpio_write64(void __iomem *reg, unsigned long data)
{
	writeq(data, reg);
}

static unsigned long bgpio_read64(void __iomem *reg)
{
	return readq(reg);
}



static void bgpio_write8(void __iomem *reg, unsigned long data)
{
	writeb(data, reg);
}

static unsigned long bgpio_read8(void __iomem *reg)
{
	return readb(reg);
}

static void bgpio_write16(void __iomem *reg, unsigned long data)
{
	writew(data, reg);
}

static unsigned long bgpio_read16(void __iomem *reg)
{
	return readw(reg);
}

static void bgpio_write32(void __iomem *reg, unsigned long data)
{
	writel(data, reg);
}

static unsigned long bgpio_read32(void __iomem *reg)
{
	return readl(reg);
}

int bgpio_setup_accessors_tlx(struct device *dev,
				 struct bgpio_chip *bgc,
				 bool bit_be,
				 bool byte_be)
{

	switch (bgc->bits) {
	case 8:
		bgc->read_reg	= bgpio_read8;
		bgc->write_reg	= bgpio_write8;
		break;
	case 16:
		if (byte_be) {
			bgc->read_reg	= bgpio_read16be;
			bgc->write_reg	= bgpio_write16be;
		} else {
			bgc->read_reg	= bgpio_read16;
			bgc->write_reg	= bgpio_write16;
		}
		break;
	case 32:
		if (byte_be) {
			bgc->read_reg	= bgpio_read32be;
			bgc->write_reg	= bgpio_write32be;
		} else {
			bgc->read_reg	= bgpio_read32;
			bgc->write_reg	= bgpio_write32;
		}
		break;
#if BITS_PER_LONG >= 64
	case 64:
		if (byte_be) {
			dev_err_tlx(dev,
				"64 bit big endian byte order unsupported\n");
			return -EINVAL;
		} else {
			bgc->read_reg	= bgpio_read64;
			bgc->write_reg	= bgpio_write64;
		}
		break;
#endif /* BITS_PER_LONG >= 64 */
	default:
		dev_err_tlx(dev, "unsupported data width %u bits\n", bgc->bits);
		return -EINVAL;
	}

	bgc->pin2mask = bit_be ? bgpio_pin2mask_be : bgpio_pin2mask;

	return 0;
};

static int bgpio_get(struct gpio_chip *gc, unsigned int gpio)
{
         struct bgpio_chip *bgc = to_bgpio_chip(gc);
         return !!(bgc->read_reg(bgc->reg_dat) & bgc->pin2mask(bgc, gpio));
}

static void bgpio_set(struct gpio_chip *gc, unsigned int gpio, int val)
{
	struct bgpio_chip *bgc = to_bgpio_chip(gc);
	unsigned long mask = bgc->pin2mask(bgc, gpio);
	unsigned long flags;

	spin_lock_irqsave(&bgc->lock, flags);

	if (val)
		bgc->data |= mask;
	else
		bgc->data &= ~mask;

	bgc->write_reg(bgc->reg_dat, bgc->data);

	spin_unlock_irqrestore_tlx(&bgc->lock, flags);
}

static void bgpio_set_with_clear(struct gpio_chip *gc, unsigned int gpio,
				 int val)
{
	struct bgpio_chip *bgc = to_bgpio_chip(gc);
	unsigned long mask = bgc->pin2mask(bgc, gpio);

	if (val)
		bgc->write_reg(bgc->reg_set, mask);
	else
		bgc->write_reg(bgc->reg_clr, mask);
}

static void bgpio_set_set(struct gpio_chip *gc, unsigned int gpio, int val)
{
	struct bgpio_chip *bgc = to_bgpio_chip(gc);
	unsigned long mask = bgc->pin2mask(bgc, gpio);
	unsigned long flags;

	spin_lock_irqsave(&bgc->lock, flags);

	if (val)
		bgc->data |= mask;
	else
		bgc->data &= ~mask;

	bgc->write_reg(bgc->reg_set, bgc->data);

	spin_unlock_irqrestore_tlx(&bgc->lock, flags);
}

int bgpio_setup_io_tlx(struct bgpio_chip *bgc,
			  void __iomem *dat,
			  void __iomem *set,
			  void __iomem *clr)
{

	bgc->reg_dat = dat;
	if (!bgc->reg_dat)
		return -EINVAL;

	if (set && clr) {
		bgc->reg_set = set;
		bgc->reg_clr = clr;
		bgc->gc.set = bgpio_set_with_clear;
	} else if (set && !clr) {
		bgc->reg_set = set;
		bgc->gc.set = bgpio_set_set;
	} else {
		bgc->gc.set = bgpio_set;
	}

	bgc->gc.get = bgpio_get;

	return 0;
}


int bgpio_request_tlx(struct gpio_chip *chip, unsigned gpio_pin)
{
	if (gpio_pin < chip->ngpio)
		return 0;

	return -EINVAL;
}
#define BGPIOF_BIG_ENDIAN               BIT(0)
#define BGPIOF_UNREADABLE_REG_DIR       BIT(2) /* reg_dir is unreadable */
#define BGPIOF_UNREADABLE_REG_SET       BIT(1) /* reg_set is unreadable */
#define BGPIOF_BIG_ENDIAN_BYTE_ORDER    BIT(3)

bool is_power_of_2(unsigned long n)
 {
          return (n != 0 && ((n & (n - 1)) == 0));
 }

int bgpio_init(struct bgpio_chip *bgc, struct device *dev,
	       unsigned long sz, void __iomem *dat, void __iomem *set,
	       void __iomem *clr, void __iomem *dirout, void __iomem *dirin,
	       unsigned long flags)
{
	int ret;

	if (!is_power_of_2(sz))
		return -EINVAL;

	bgc->bits = sz * 8;
	if (bgc->bits > BITS_PER_LONG)
		return -EINVAL;

	spin_lock_init(&bgc->lock);
	bgc->gc.dev = dev;
	bgc->gc.label = dev_name(dev);
	bgc->gc.base = -1;
	bgc->gc.ngpio = bgc->bits;
	bgc->gc.request = bgpio_request_tlx;

	ret = bgpio_setup_io_tlx(bgc, dat, set, clr);
	if (ret)
		return ret;

	ret = bgpio_setup_accessors_tlx(dev, bgc, flags & BGPIOF_BIG_ENDIAN,
				    flags & BGPIOF_BIG_ENDIAN_BYTE_ORDER);
	if (ret)
		return ret;
	if (dirout && dirin) {
		return ret;
	} else {
		if (dirin) {
			bgc->reg_dir = dirin;
			bgc->gc.direction_output = bgpio_dir_out_inv_tlx;
			bgc->gc.direction_input = bgpio_dir_in_inv_tlx;
		}
	}
	bgc->data = bgc->read_reg(bgc->reg_dat);
	if (bgc->gc.set == bgpio_set_set &&
			!(flags & BGPIOF_UNREADABLE_REG_SET))
		bgc->data = bgc->read_reg(bgc->reg_set);
	if (bgc->reg_dir && !(flags & BGPIOF_UNREADABLE_REG_DIR))
		bgc->dir = bgc->read_reg(bgc->reg_dir);

	return ret;
}

/*
void of_gpiochip_add(struct gpio_chip *gc) {};
void of_gpiochip_remove(struct gpio_chip *gc) {};
struct gpio_desc *of_get_named_gpiod_flags(struct device_node *np,
                       const char *propname, int index, enum of_gpio_flags *flags) {};
*/

int sysrq_toggle_support(int enable_mask){};
void handle_sysrq(int key){};
void ldsem_up_write(struct ld_semaphore *sem){};
int __sched ldsem_down_write(struct ld_semaphore *sem, long timeout){};
int ldsem_down_read_trylock(struct ld_semaphore *sem){};
void __init_ldsem(struct ld_semaphore *sem, const char *name,
                   struct lock_class_key *key){};
int __ldsem_down_read_nested(struct ld_semaphore *sem,
                                            int subclass, long timeout){};
void ldsem_up_read(struct ld_semaphore *sem){};
int ldsem_down_read(struct ld_semaphore *sem, long timeout){};
void __lockfunc tty_lock(struct tty_struct *tty){};
void __lockfunc tty_unlock(struct tty_struct *tty){};
void __lockfunc tty_lock_pair(struct tty_struct *tty,
                                 struct tty_struct *tty2){};
void __lockfunc tty_unlock_pair(struct tty_struct *tty,
                                 struct tty_struct *tty2){};
int is_ignored(int sig){};
struct tty_ldisc_ops tty_ldisc_N_TTY;
/*

struct tty_ldisc_ops *tty_ldiscs[];
void tty_ldisc_flush(struct tty_struct *tty){};
struct tty_ldisc *tty_ldisc_ref_wait(struct tty_struct *tty){};
 void tty_ldisc_hangup(struct tty_struct *tty){};
 void tty_ldisc_init(struct tty_struct *tty){};
 void tty_ldisc_deinit(struct tty_struct *tty){};
 void tty_ldisc_begin(void){};
*/

/*
speed_t tty_termios_input_baud_rate(struct ktermios *termios){};
speed_t tty_termios_baud_rate(struct ktermios *termios){};
void tty_driver_flush_buffer(struct tty_struct *tty){};
int tty_chars_in_buffer(struct tty_struct *tty){};
void tty_termios_encode_baud_rate(struct ktermios *termios,
                                   speed_t ibaud, speed_t obaud){};
void tty_wait_until_sent(struct tty_struct *tty, long timeout){};
*/

int vt_move_to_console(unsigned int vt, int alloc){};
int vt_kmsg_redirect(int new){};

void unblank_screen(void){};
void tty_buffer_flush(struct tty_struct *tty){};



//---

void tty_kref_put(struct tty_struct *tty){};
void proc_clear_tty(struct task_struct *p){};
int tty_write_lock(struct tty_struct *tty, int ndelay){};
void tty_write_unlock(struct tty_struct *tty){};
void start_tty(struct tty_struct *tty){};
void stop_tty(struct tty_struct *tty){};
int tty_check_change(struct tty_struct *tty){};
char *tty_name(struct tty_struct *tty, char *buf){};
speed_t tty_termios_input_baud_rate(struct ktermios *termios){};
speed_t tty_termios_baud_rate(struct ktermios *termios){};
void tty_driver_flush_buffer(struct tty_struct *tty){};

/*
typedef int (*dr_match_t)(struct device *dev, void *res, void *match_data);
int devres_release(struct device *dev, dr_release_t release,
                          dr_match_t match, void *match_data){};
void devres_free(void *res){};
void devres_add(struct device *dev, void *res){};
void *devres_alloc(dr_release_t release, size_t size, gfp_t gfp){};
int dev_warn(const struct device *dev, const char *fmt, ...){};
int devres_destroy(struct device *dev, dr_release_t release,
                          dr_match_t match, void *match_data){};
int dev_err(const struct device *dev, const char *fmt, ...){};
void device_remove_file(struct device *dev,
                                const struct device_attribute *attr){};
int dma_release_from_coherent(struct device *dev, int order, void *vaddr){};
int dma_alloc_from_coherent(struct device *dev, ssize_t size,
                                        dma_addr_t *dma_handle, void **ret){};
int device_create_file(struct device *device,
                              const struct device_attribute *entry){};
void device_unregister(struct device *dev){};
struct device *device_create_vargs(struct class *cls,
                                           struct device *parent,
                                           dev_t devt,
                                           void *drvdata,
                                           const char *fmt,
                                         va_list vargs){};
int bus_register(struct bus_type *bus){};
void register_syscore_ops(struct syscore_ops *ops){};
int  device_register(struct device *dev){};
int subsys_system_register(struct bus_type *subsys,
                            const struct attribute_group **groups){};
void class_interface_unregister(struct class_interface *q){};
void platform_driver_unregister(struct platform_driver *w){};
struct platform_device *platform_device_register_full(
                  const struct platform_device_info *pdevinfo){};
int __platform_driver_register(struct platform_driver *w,
                                         struct module *q){};
int  class_interface_register(struct class_interface *w){};
void put_device(struct device *dev){};
int dev_set_name(struct device *dev, const char *name, ...){};
void device_del(struct device *dev){};
void device_initialize(struct device *dev){};
const char *dev_driver_string(const struct device *dev){};
struct suspend_stats {
          int     success;
};
struct suspend_stats suspend_stats;
void __pm_wakeup_event(struct wakeup_source *ws, unsigned int msec){};
struct device *get_device(struct device *dev){};
struct wakeup_source *wakeup_source_register(const char *name){};
int __must_check device_add(struct device *dev){};
int dpm_suspend_end(pm_message_t state){};
void syscore_resume(void){};
bool events_check_enabled;
bool pm_wakeup_pending(void){};
void dpm_resume_end(pm_message_t state){};
int syscore_suspend(void){};
void syscore_shutdown(void){};
void dpm_resume_start(pm_message_t state){};
//int dpm_suspend_end(pm_message_t state){};
int dpm_suspend_start(pm_message_t state){};
void device_shutdown(void){};
int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
                            void *vaddr, size_t size, int *ret){};
*/
void module_remove_driver(struct device_driver *drv){};
int dma_release_from_coherent(struct device *dev, int order, void *vaddr){};
int dma_alloc_from_coherent(struct device *dev, ssize_t size,
                                        dma_addr_t *dma_handle, void **ret){};
int dma_declare_coherent_memory(struct device *dev, phys_addr_t phys_addr,
                                   dma_addr_t device_addr, size_t size, int flags){};
void dma_release_declared_memory(struct device *dev){};
int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,
                            void *vaddr, size_t size, int *ret){};
void device_pm_unlock(void){};
void device_pm_lock(void){};
void device_pm_move_last(struct device *dev){};
void device_pm_move_before(struct device *deva, struct device *devb){};
void device_pm_remove(struct device *dev){};
void dpm_sysfs_remove(struct device *dev){};
struct wakeup_source *wakeup_source_register(const char *name){};
void __pm_wakeup_event(struct wakeup_source *ws, unsigned int msec){};
struct suspend_stats {
          int     success;
};
typedef int (*dr_match_t)(struct device *dev, void *res, void *match_data);
struct suspend_stats suspend_stats;
int dpm_suspend_end(pm_message_t state){};
//void syscore_resume(void){};
bool events_check_enabled;
bool pm_wakeup_pending(void){};
void dpm_resume_end(pm_message_t state){};
int dpm_suspend_start(pm_message_t state){};
void device_pm_move_after(struct device *deva, struct device *devb){};
void dpm_resume_start(pm_message_t state){};
//int devtmpfs_delete_node(struct device *dev){};
void devres_free(void *res){};
void devres_add(struct device *dev, void *res){};
void *devres_alloc(dr_release_t release, size_t size, gfp_t gfp){};
//int dev_warn(const struct device *dev, const char *fmt, ...){};
int devres_destroy(struct device *dev, dr_release_t release,
                          dr_match_t match, void *match_data){};
int devres_release_group(struct device *dev, void *id){};
void * devres_open_group(struct device *dev, void *id, gfp_t gfp){};
void devres_remove_group(struct device *dev, void *id){};
int devres_release_all(struct device *dev){};
int devres_release(struct device *dev, dr_release_t release,
                   dr_match_t match, void *match_data){};
 void devres_close_group(struct device *dev, void *id){};
 int __platform_driver_register(struct platform_driver *w,
                                          struct module *q){};
struct platform_device *platform_device_register_full(
                const struct platform_device_info *pdevinfo){};
void platform_driver_unregister(struct platform_driver *drv){};
int class_interface_register(struct class_interface *class_intf){};
void class_interface_unregister(struct class_interface *q){};
struct device *class_find_device(struct class *class, struct device *start,
                                  const void *data,
                                  int (*match)(struct device *, const void *)){};
void driver_remove_groups(struct device_driver *drv,
                           const struct attribute_group **groups){};
void driver_remove_file(struct device_driver *drv,
                        const struct driver_attribute *attr){};
int syscore_suspend(void){};
void syscore_shutdown(void){};
void syscore_resume(void){};
void register_syscore_ops(struct syscore_ops *ops){};
void driver_deferred_probe_del(struct device *dev){};
int device_attach(struct device *dev){};
void device_release_driver(struct device *dev){};
int driver_probe_device(struct device_driver *drv, struct device *dev){};
void driver_detach(struct device_driver *drv)	{};
int driver_attach(struct device_driver *drv) {};
int subsys_system_register(struct bus_type *subsys,
                            const struct attribute_group **groups){};
int bus_register(struct bus_type *bus){};
void bus_remove_device(struct device *dev){};
char *device_get_devnode(struct device *dev,
                                umode_t *mode, kuid_t *uid, kgid_t *gid,
                                const char **tmp){};
int dev_err(const struct device *dev, const char *fmt, ...){};
int _dev_info(const struct device *dev, const char *fmt, ...){};
void device_remove_file(struct device *dev,
                         const struct device_attribute *attr){};
int device_create_file(struct device *device,
                              const struct device_attribute *entry){};
void device_unregister(struct device *dev){};
int  device_register(struct device *dev){};
struct device *device_create_vargs(struct class *cls,
                                           struct device *parent,
                                           dev_t devt,
                                           void *drvdata,
                                           const char *fmt,
                                         va_list vargs){};
void put_device(struct device *dev){};
int dev_set_name(struct device *dev, const char *name, ...){};
void device_del(struct device *dev){};
void device_initialize(struct device *dev){};
const char *dev_driver_string(const struct device *dev){};
void device_shutdown(void){};
struct device *get_device(struct device *dev){};
int __must_check device_add(struct device *dev){};
int dev_warn(const struct device *dev, const char *fmt, ...){};

#ifdef CONFIG_BROKEN_RODATA
#define __constsection(x)
#else
#define __constsection(x) __section(x)
#endif

#define __initconst	__constsection(.init.rodata)

#define __setup(str, fn)                                        \
         __setup_param(str, fn, fn, 0)

#define __setup_param(str, unique_id, fn, early)                        \
         static const char __setup_str_##unique_id[] __initconst \
                 __aligned(1) = str; \
         static struct obs_kernel_param __setup_##unique_id      \
                 __used __section(.init.setup)                   \
                 __attribute__((aligned((sizeof(long)))))        \
                 = { __setup_str_##unique_id, fn, early }

int mount_dev;

static int __init mount_param(char *str)
{
	mount_dev = simple_strtoull_tlx(str, NULL, 0);
	return 1;
}
__setup("devtmpfs.mount=", mount_param);
